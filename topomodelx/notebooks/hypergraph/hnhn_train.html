
<!DOCTYPE html>


<html lang="en" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Train a Hypergraph Networks with Hyperedge Neurons (HNHN) &#8212; TopoModelX latest documentation</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../_static/styles/theme.css?digest=ac02cc09edc035673794" rel="stylesheet" />
<link href="../../_static/styles/bootstrap.css?digest=ac02cc09edc035673794" rel="stylesheet" />
<link href="../../_static/styles/pydata-sphinx-theme.css?digest=ac02cc09edc035673794" rel="stylesheet" />

  
  <link href="../../_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=ac02cc09edc035673794" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=a746c00c" />
    <link rel="stylesheet" type="text/css" href="../../_static/sg_gallery.css?v=61a4c737" />
    <link rel="stylesheet" type="text/css" href="../../_static/nbsphinx-code-cells.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../_static/scripts/bootstrap.js?digest=ac02cc09edc035673794" />
<link rel="preload" as="script" href="../../_static/scripts/pydata-sphinx-theme.js?digest=ac02cc09edc035673794" />
  <script src="../../_static/vendor/fontawesome/6.1.2/js/all.min.js?digest=ac02cc09edc035673794"></script>

    <script src="../../_static/documentation_options.js?v=f4332903"></script>
    <script src="../../_static/doctools.js?v=888ff710"></script>
    <script src="../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script>window.MathJax = {"tex": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true}, "options": {"ignoreHtmlClass": "tex2jax_ignore|mathjax_ignore|document", "processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'notebooks/hypergraph/hnhn_train';</script>
    <link rel="canonical" href="pyt-team.github.io/notebooks/hypergraph/hnhn_train.html" />
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Train a Hypergraph Network with Hyperedge Neurons (HNHN)" href="hnhn_train_bis.html" />
    <link rel="prev" title="Train a Hypergraph Message Passing Neural Network (HMPNN)" href="hmpnn_train.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
    <meta name="docbuild:last-update" content="Sep 20, 2023, 1:13:36â€¯PM"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <div id="pst-scroll-pixel-helper"></div>

  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>
    Back to top
  </button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search the docs ..."
         aria-label="Search the docs ..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
<div class="bd-header__inner bd-page-width">
  <label class="sidebar-toggle primary-toggle" for="__primary">
    <span class="fa-solid fa-bars"></span>
  </label>
  
  
  <div class="col-lg-3 navbar-header-items__start">
    
      <div class="navbar-item">
  

<a class="navbar-brand logo" href="../../index.html">
  
  
  
  
  
    <p class="title logo__title">TopoModelX latest documentation</p>
  
</a></div>
    
  </div>
  
  <div class="col-lg-9 navbar-header-items">
    
    <div class="me-auto navbar-header-items__center">
      
        <div class="navbar-item"><nav class="navbar-nav">
  <p class="sidebar-header-items__title"
     role="heading"
     aria-level="1"
     aria-label="Site Navigation">
    Site Navigation
  </p>
  <ul class="bd-navbar-elements navbar-nav">
    
                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../api/index.html">
                        API Reference
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../contributing/index.html">
                        Contributing
                      </a>
                    </li>
                

                    <li class="nav-item current active">
                      <a class="nav-link nav-internal" href="../../tutorials/index.html">
                        Tutorials
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../challenge/index.html">
                        ICML 2023 Topological Deep Learning Challenge
                      </a>
                    </li>
                
  </ul>
</nav></div>
      
    </div>
    
    
    <div class="navbar-header-items__end">
      
        <div class="navbar-item navbar-persistent--container">
          
 <script>
 document.write(`
   <button class="btn navbar-btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script>
        </div>
      
      
        <div class="navbar-item">
<script>
document.write(`
  <button class="btn btn-sm navbar-btn theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch nav-link" data-mode="light"><i class="fa-solid fa-sun fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="dark"><i class="fa-solid fa-moon fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="auto"><i class="fa-solid fa-circle-half-stroke fa-lg"></i></span>
  </button>
`);
</script></div>
      
    </div>
    
  </div>
  
  
    <div class="navbar-persistent--mobile">
 <script>
 document.write(`
   <button class="btn navbar-btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script>
    </div>
  

  
    <label class="sidebar-toggle secondary-toggle" for="__secondary" tabindex="0">
      <span class="fa-solid fa-outdent"></span>
    </label>
  
</div>

    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
      <div class="sidebar-header-items__center">
        
          <div class="navbar-item"><nav class="navbar-nav">
  <p class="sidebar-header-items__title"
     role="heading"
     aria-level="1"
     aria-label="Site Navigation">
    Site Navigation
  </p>
  <ul class="bd-navbar-elements navbar-nav">
    
                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../api/index.html">
                        API Reference
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../contributing/index.html">
                        Contributing
                      </a>
                    </li>
                

                    <li class="nav-item current active">
                      <a class="nav-link nav-internal" href="../../tutorials/index.html">
                        Tutorials
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../challenge/index.html">
                        ICML 2023 Topological Deep Learning Challenge
                      </a>
                    </li>
                
  </ul>
</nav></div>
        
      </div>
    
    
    
      <div class="sidebar-header-items__end">
        
          <div class="navbar-item">
<script>
document.write(`
  <button class="btn btn-sm navbar-btn theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch nav-link" data-mode="light"><i class="fa-solid fa-sun fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="dark"><i class="fa-solid fa-moon fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="auto"><i class="fa-solid fa-circle-half-stroke fa-lg"></i></span>
  </button>
`);
</script></div>
        
      </div>
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item"><nav class="bd-docs-nav bd-links"
     aria-label="Section Navigation">
  <p class="bd-links__title" role="heading" aria-level="1">Section Navigation</p>
  <div class="bd-toc-item navbar-nav"><ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../cell/can_train.html">Train a Cell Attention Network (CAN)</a></li>




<li class="toctree-l1"><a class="reference internal" href="../cell/ccxn_train.html">Train a Convolutional Cell Complex Network (CCXN)</a></li>





<li class="toctree-l1"><a class="reference internal" href="../cell/cwn_train.html">Train a CW Network (CWN)</a></li>




</ul>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="allset_train.html">Train an All-Set TNN</a></li>



<li class="toctree-l1"><a class="reference internal" href="allset_transformer_train.html">Train an All-Set-Transformer TNN</a></li>



<li class="toctree-l1"><a class="reference internal" href="dhgcn_train.html">Train a DHGCN TNN</a></li>



<li class="toctree-l1"><a class="reference internal" href="hmpnn_train.html">Train a Hypergraph Message Passing Neural Network (HMPNN)</a></li>



<li class="toctree-l1 current active"><a class="current reference internal" href="#">Train a Hypergraph Networks with Hyperedge Neurons (HNHN)</a></li>



<li class="toctree-l1"><a class="reference internal" href="hnhn_train_bis.html">Train a Hypergraph Network with Hyperedge Neurons (HNHN)</a></li>



<li class="toctree-l1"><a class="reference internal" href="hypergat_train.html">Train a Hypergraph Neural Network</a></li>



<li class="toctree-l1"><a class="reference internal" href="hypersage_train.html">Train a Hypersage TNN</a></li>



<li class="toctree-l1"><a class="reference internal" href="unigcn_train.html">Train a UNIGCN TNN</a></li>



<li class="toctree-l1"><a class="reference internal" href="unigcnii_train.html">Train a hypergraph neural network using UniGCNII layers</a></li>
<li class="toctree-l1"><a class="reference internal" href="unigin_train.html">Train a UNIGIN TNN</a></li>



<li class="toctree-l1"><a class="reference internal" href="unisage_train.html">Train a Uni-sage TNN</a></li>



</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../simplicial/dist2cycle_train.html">Train a Simplicial Neural Network for Homology Localization (Dist2Cycle)</a></li>




<li class="toctree-l1"><a class="reference internal" href="../simplicial/hsn_train.html">Train a Simplicial High-Skip Network (HSN)</a></li>



<li class="toctree-l1"><a class="reference internal" href="../simplicial/san_train.html">Train a Simplicial Attention Network (SAN)</a></li>



<li class="toctree-l1"><a class="reference internal" href="../simplicial/sca_cmps_train.html">Train a Simplicial Complex Autoencoder (SCA) with Coadjacency Message Passing Scheme (CMPS)</a></li>




<li class="toctree-l1"><a class="reference internal" href="../simplicial/sccn_train.html">Train a Simplicial Complex Convolutional Network (SCCN)</a></li>



<li class="toctree-l1"><a class="reference internal" href="../simplicial/sccnn_train.html">Train a SCCNN</a></li>





<li class="toctree-l1"><a class="reference internal" href="../simplicial/scconv_train.html">Train a Simplicial 2-complex convolutional neural network (SCConv)</a></li>



<li class="toctree-l1"><a class="reference internal" href="../simplicial/scn2_train.html">Train a Simplex Convolutional Network (SCN) of Rank 2</a></li>


<li class="toctree-l1"><a class="reference internal" href="../simplicial/scnn_train.html">Train a Simplicial Convolutional Neural Network (SCNN)</a></li>






<li class="toctree-l1"><a class="reference internal" href="../simplicial/scone_train.html">Train a Simplicial Complex Net (SCoNe)</a></li>






</ul>
</div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        
          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item">



<nav aria-label="Breadcrumb">
  <ul class="bd-breadcrumbs">
    
    <li class="breadcrumb-item breadcrumb-home">
      <a href="../../index.html" class="nav-link" aria-label="Home">
        <i class="fa-solid fa-home"></i>
      </a>
    </li>
    
    <li class="breadcrumb-item"><a href="../../tutorials/index.html" class="nav-link">Tutorials</a></li>
    
    <li class="breadcrumb-item active" aria-current="page">Train a...</li>
  </ul>
</nav>
</div>
      
    </div>
  
  
</div>
</div>
              
              
              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <section id="Train-a-Hypergraph-Networks-with-Hyperedge-Neurons-(HNHN)">
<h1>Train a Hypergraph Networks with Hyperedge Neurons (HNHN)<a class="headerlink" href="#Train-a-Hypergraph-Networks-with-Hyperedge-Neurons-(HNHN)" title="Link to this heading">#</a></h1>
<p>In this notebook, we will create and train a Hypergraph Networks with Hyperedge Neurons in the hypergraph domain, as proposed in the paper by <a class="reference external" href="https://grlplus.github.io/papers/40.pdf">Dong et al. : HNHN: Hypergraph networks with hyperedge neurons (2020)</a>.</p>
<p>We train the model to perform binary node classification using the KarateClub benchmark dataset.</p>
<p>The equations of one layer of this neural network are given by:</p>
<p>ðŸŸ¥ <span class="math notranslate nohighlight">\(\quad m_{y \rightarrow x}^{(0 \rightarrow 1)} = \sigma((B_1^T \cdot W^{(0)})_{xy} \cdot h_y^{t,(0)} \cdot \Theta^{t,(0)} + b^{t,(0)})\)</span></p>
<p>ðŸŸ¥ <span class="math notranslate nohighlight">\(\quad m_{y \rightarrow x}^{(1 \rightarrow 0)} = \sigma((B_1 \cdot W^{(1)})_{xy} \cdot h_y^{t,(1)} \cdot \Theta^{t,(1)} + b^{t,(1)})\)</span></p>
<p>ðŸŸ§ <span class="math notranslate nohighlight">\(\quad m_x^{(0 \rightarrow 1)} = \sum_{y \in \mathcal{B}(x)} m_{y \rightarrow x}^{(0 \rightarrow 1)}\)</span></p>
<p>ðŸŸ§ <span class="math notranslate nohighlight">\(\quad m_x^{(1 \rightarrow 0)} = \sum_{y \in \mathcal{C}(x)} m_{y \rightarrow x}^{(1 \rightarrow 0)}\)</span></p>
<p>ðŸŸ© <span class="math notranslate nohighlight">\(\quad m_x^{(0)} = m_x^{(1 \rightarrow 0)}\)</span></p>
<p>ðŸŸ© <span class="math notranslate nohighlight">\(\quad m_x^{(1)} = m_x^{(0 \rightarrow 1)}\)</span></p>
<p>ðŸŸ¦ <span class="math notranslate nohighlight">\(\quad h_x^{t+1,(0)} = m_x^{(0)}\)</span></p>
<p>ðŸŸ¦ <span class="math notranslate nohighlight">\(\quad h_x^{t+1,(1)} = m_x^{(1)}\)</span></p>
<p>Where the notations are defined in <a class="reference external" href="https://arxiv.org/abs/2304.10031">Papillon et al : Architectures of Topological Deep Learning: A Survey of Topological Neural Networks (2023)</a>.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span> In [13]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">import</span> <span class="nn">toponetx.datasets.graph</span> <span class="k">as</span> <span class="nn">graph</span>
<span class="kn">from</span> <span class="nn">topomodelx.nn.hypergraph.hnhn_layer</span> <span class="kn">import</span> <span class="n">HNHNLayer</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
</pre></div>
</div>
</div>
<p>If there is an available GPU, we will use it. Otherwise, this will run on CPU.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span> In [14]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cuda&quot;</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s2">&quot;cpu&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
cpu
</pre></div></div>
</div>
</section>
<section id="Pre-processing">
<h1>Pre-processing<a class="headerlink" href="#Pre-processing" title="Link to this heading">#</a></h1>
<section id="Import-dataset">
<h2>Import dataset<a class="headerlink" href="#Import-dataset" title="Link to this heading">#</a></h2>
<p>The first step is to import the Karate Club (<a class="reference external" href="https://www.jstor.org/stable/3629752">https://www.jstor.org/stable/3629752</a>) dataset. This is a singular graph with 34 nodes that belong to two different social groups. We will use these groups for the task of node-level binary classification.</p>
<p>We must first lift our graph dataset into the hypergraph domain.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span> In [15]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">dataset_sim</span> <span class="o">=</span> <span class="n">graph</span><span class="o">.</span><span class="n">karate_club</span><span class="p">(</span><span class="n">complex_type</span><span class="o">=</span><span class="s2">&quot;simplicial&quot;</span><span class="p">)</span>
<span class="n">dataset_hyp</span> <span class="o">=</span> <span class="n">dataset_sim</span><span class="o">.</span><span class="n">to_hypergraph</span><span class="p">()</span>
</pre></div>
</div>
</div>
</section>
<section id="Define-neighborhood-structures.">
<h2>Define neighborhood structures.<a class="headerlink" href="#Define-neighborhood-structures." title="Link to this heading">#</a></h2>
<p>Now we retrieve the neighborhood structures (i.e., their representative matrices) that we will use to send messages on the domain. In this case, we need the boundary matrix (or incidence matrix) <span class="math notranslate nohighlight">\(B_1\)</span>. For a santiy check, we show that the shape of the <span class="math notranslate nohighlight">\(B_1 = n_\text{nodes} \times n_\text{edges}\)</span>. We also convert the neighborhood structures to sparse torch tensors.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span> In [16]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">incidence_1</span> <span class="o">=</span> <span class="n">dataset_sim</span><span class="o">.</span><span class="n">incidence_matrix</span><span class="p">(</span><span class="n">rank</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">signed</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">incidence_1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">incidence_1</span><span class="o">.</span><span class="n">todense</span><span class="p">())</span><span class="o">.</span><span class="n">to_sparse</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;The incidence matrix B1 has shape: </span><span class="si">{</span><span class="n">incidence_1</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
The incidence matrix B1 has shape: torch.Size([34, 78]).
</pre></div></div>
</div>
</section>
<section id="Import-signal">
<h2>Import signal<a class="headerlink" href="#Import-signal" title="Link to this heading">#</a></h2>
<p>Our task will be node classification. To classify the nodes, we will use: - node features <span class="math notranslate nohighlight">\(X_0\)</span>; shape: <span class="math notranslate nohighlight">\(n_{nodes} \times channels_{node}\)</span> - edge features <span class="math notranslate nohighlight">\(X_1\)</span>; shape: <span class="math notranslate nohighlight">\(n_{edges} \times channels_{edge}\)</span></p>
<p>For the this specific dataset, we have <span class="math notranslate nohighlight">\(channels_{node}\)</span> = <span class="math notranslate nohighlight">\(channels_{edge}\)</span> = 2. Next, we will retrieve node features, edge features, and node labels <span class="math notranslate nohighlight">\(y\)</span>. The node labels will be one-hot encoded.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span> In [17]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Node features</span>
<span class="n">x_0</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">_</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">dataset_sim</span><span class="o">.</span><span class="n">get_simplex_attributes</span><span class="p">(</span><span class="s2">&quot;node_feat&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
    <span class="n">x_0</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">v</span><span class="p">)</span>
<span class="n">x_0</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">x_0</span><span class="p">))</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="n">n_nodes</span><span class="p">,</span> <span class="n">channels_node</span> <span class="o">=</span> <span class="n">x_0</span><span class="o">.</span><span class="n">shape</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;There are </span><span class="si">{</span><span class="n">n_nodes</span><span class="si">}</span><span class="s2"> nodes with features of dimension </span><span class="si">{</span><span class="n">channels_node</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
There are 34 nodes with features of dimension 2.
</pre></div></div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span> In [18]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Edge features</span>
<span class="n">x_1</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">dataset_sim</span><span class="o">.</span><span class="n">get_simplex_attributes</span><span class="p">(</span><span class="s2">&quot;edge_feat&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
    <span class="n">x_1</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">v</span><span class="p">)</span>
<span class="n">x_1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">x_1</span><span class="p">))</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="n">n_edges</span><span class="p">,</span> <span class="n">channels_edge</span> <span class="o">=</span> <span class="n">x_1</span><span class="o">.</span><span class="n">shape</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;There are </span><span class="si">{</span><span class="n">n_edges</span><span class="si">}</span><span class="s2"> edges with features of dimension </span><span class="si">{</span><span class="n">channels_edge</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
There are 78 edges with features of dimension 2.
</pre></div></div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span> In [19]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Node labels</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span>
    <span class="p">[</span>
        <span class="mi">1</span><span class="p">,</span>
        <span class="mi">1</span><span class="p">,</span>
        <span class="mi">1</span><span class="p">,</span>
        <span class="mi">1</span><span class="p">,</span>
        <span class="mi">1</span><span class="p">,</span>
        <span class="mi">1</span><span class="p">,</span>
        <span class="mi">1</span><span class="p">,</span>
        <span class="mi">1</span><span class="p">,</span>
        <span class="mi">1</span><span class="p">,</span>
        <span class="mi">0</span><span class="p">,</span>
        <span class="mi">1</span><span class="p">,</span>
        <span class="mi">1</span><span class="p">,</span>
        <span class="mi">1</span><span class="p">,</span>
        <span class="mi">1</span><span class="p">,</span>
        <span class="mi">0</span><span class="p">,</span>
        <span class="mi">0</span><span class="p">,</span>
        <span class="mi">1</span><span class="p">,</span>
        <span class="mi">1</span><span class="p">,</span>
        <span class="mi">0</span><span class="p">,</span>
        <span class="mi">1</span><span class="p">,</span>
        <span class="mi">0</span><span class="p">,</span>
        <span class="mi">1</span><span class="p">,</span>
        <span class="mi">0</span><span class="p">,</span>
        <span class="mi">0</span><span class="p">,</span>
        <span class="mi">0</span><span class="p">,</span>
        <span class="mi">0</span><span class="p">,</span>
        <span class="mi">0</span><span class="p">,</span>
        <span class="mi">0</span><span class="p">,</span>
        <span class="mi">0</span><span class="p">,</span>
        <span class="mi">0</span><span class="p">,</span>
        <span class="mi">0</span><span class="p">,</span>
        <span class="mi">0</span><span class="p">,</span>
        <span class="mi">0</span><span class="p">,</span>
        <span class="mi">0</span><span class="p">,</span>
    <span class="p">]</span>
<span class="p">)</span>
<span class="n">n_classes</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">y</span><span class="p">))</span>
<span class="n">y_1h</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="n">n_classes</span><span class="p">)[</span><span class="n">y</span><span class="p">]</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>  <span class="c1"># 1-hot representation</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;There are </span><span class="si">{</span><span class="n">y_1h</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s2"> labels, one for each node.&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
There are 34 labels, one for each node.
</pre></div></div>
</div>
<p>We will now split the dataset into stratified training (85%) and test sets (15%)</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span> In [20]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span><span class="p">,</span> <span class="n">ind_train</span><span class="p">,</span> <span class="n">ind_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span>
    <span class="n">y_1h</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">n_nodes</span><span class="p">),</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.15</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">stratify</span><span class="o">=</span><span class="n">y</span>
<span class="p">)</span>
<span class="n">y_train</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">y_train</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="n">y_test</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span>
    <span class="sa">f</span><span class="s2">&quot;Fraction of class-1 samples in the training set: </span><span class="si">{</span><span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">y_train</span><span class="p">[:,</span><span class="mi">0</span><span class="p">])</span><span class="o">/</span><span class="n">y_train</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span>
<span class="p">)</span>
<span class="nb">print</span><span class="p">(</span>
    <span class="sa">f</span><span class="s2">&quot;Fraction of class-1 samples in the test set: </span><span class="si">{</span><span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">y_test</span><span class="p">[:,</span><span class="mi">0</span><span class="p">])</span><span class="o">/</span><span class="n">y_test</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Fraction of class-1 samples in the training set: 0.5
Fraction of class-1 samples in the test set: 0.5
</pre></div></div>
</div>
</section>
</section>
<section id="Create-the-Neural-Network">
<h1>Create the Neural Network<a class="headerlink" href="#Create-the-Neural-Network" title="Link to this heading">#</a></h1>
<p>Using the HNHNLayer class, we define a neural network for node classification.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span> In [21]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">HNHNNetwork</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Hypergraph Networks with Hyperedge Neurons. Implementation for multiclass node classification.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ---------</span>
<span class="sd">    channels_node : int</span>
<span class="sd">        Dimension of node features.</span>
<span class="sd">    channels_edge : int</span>
<span class="sd">        Dimension of edge features.</span>
<span class="sd">    incidence_1 : torch.sparse</span>
<span class="sd">        Incidence matrix mapping edges to nodes (B_1).</span>
<span class="sd">        shape=[n_nodes, n_edges]</span>
<span class="sd">    n_classes: int</span>
<span class="sd">        Number of classes</span>
<span class="sd">    n_layers : int</span>
<span class="sd">        Number of HNHN message passing layers.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">channels_node</span><span class="p">,</span> <span class="n">channels_edge</span><span class="p">,</span> <span class="n">incidence_1</span><span class="p">,</span> <span class="n">n_classes</span><span class="p">,</span> <span class="n">n_layers</span><span class="o">=</span><span class="mi">2</span>
    <span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layers</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">(</span>
            <span class="p">[</span>
                <span class="n">HNHNLayer</span><span class="p">(</span>
                    <span class="n">channels_node</span><span class="o">=</span><span class="n">channels_node</span><span class="p">,</span>
                    <span class="n">channels_edge</span><span class="o">=</span><span class="n">channels_edge</span><span class="p">,</span>
                    <span class="n">incidence_1</span><span class="o">=</span><span class="n">incidence_1</span><span class="p">,</span>
                <span class="p">)</span>
                <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_layers</span><span class="p">)</span>
            <span class="p">]</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">linear</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">channels_node</span><span class="p">,</span> <span class="n">n_classes</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_0</span><span class="p">,</span> <span class="n">x_1</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Forward computation.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ---------</span>
<span class="sd">        x_0 : torch.Tensor</span>
<span class="sd">            shape = [n_nodes, channels_node]</span>
<span class="sd">            Hypernode features.</span>

<span class="sd">        x_1 : torch.Tensor</span>
<span class="sd">            shape = [n_nodes, channels_edge]</span>
<span class="sd">            Hyperedge features.</span>

<span class="sd">        incidence_1 : tensor</span>
<span class="sd">            shape = [n_nodes, n_edges]</span>
<span class="sd">            Boundary matrix of rank 1.</span>

<span class="sd">        Returns</span>
<span class="sd">        --------</span>
<span class="sd">        logits : torch.Tensor</span>
<span class="sd">            The predicted node logits</span>
<span class="sd">            shape = [n_nodes, n_classes]</span>
<span class="sd">        classes : torch.Tensor</span>
<span class="sd">            The predicted node class</span>
<span class="sd">            shape = [n_nodes]</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">:</span>
            <span class="n">x_0</span><span class="p">,</span> <span class="n">x_1</span> <span class="o">=</span> <span class="n">layer</span><span class="p">(</span><span class="n">x_0</span><span class="p">,</span> <span class="n">x_1</span><span class="p">)</span>
        <span class="n">logits</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear</span><span class="p">(</span><span class="n">x_0</span><span class="p">)</span>
        <span class="n">classes</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">logits</span><span class="p">,</span> <span class="n">classes</span>
</pre></div>
</div>
</div>
</section>
<section id="Train-the-Neural-Network">
<h1>Train the Neural Network<a class="headerlink" href="#Train-the-Neural-Network" title="Link to this heading">#</a></h1>
<p>We initialize the HNHNNetwork model with our pre-made neighborhood structures and specify an optimizer and a suitable loss.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span> In [22]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">HNHNNetwork</span><span class="p">(</span>
    <span class="n">channels_node</span><span class="o">=</span><span class="n">channels_node</span><span class="p">,</span>
    <span class="n">channels_edge</span><span class="o">=</span><span class="n">channels_edge</span><span class="p">,</span>
    <span class="n">incidence_1</span><span class="o">=</span><span class="n">incidence_1</span><span class="p">,</span>
    <span class="n">n_classes</span><span class="o">=</span><span class="n">n_classes</span><span class="p">,</span>
    <span class="n">n_layers</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
<span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.5</span> <span class="o">*</span> <span class="mf">1e-2</span><span class="p">)</span>
<span class="n">criterion</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">()</span>
</pre></div>
</div>
</div>
<p>Next, we train the model for 2000 epochs.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span> In [23]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">test_interval</span> <span class="o">=</span> <span class="mi">20</span>
<span class="n">num_epochs</span> <span class="o">=</span> <span class="mi">2000</span>
<span class="n">get_accuracy</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">yhat</span><span class="p">,</span> <span class="n">ytrue</span><span class="p">:</span> <span class="p">(</span><span class="n">yhat</span> <span class="o">==</span> <span class="n">ytrue</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
<span class="n">losses</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">full</span><span class="p">((</span><span class="n">num_epochs</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">nan</span><span class="p">)</span>
<span class="n">train_accs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">full</span><span class="p">((</span><span class="n">num_epochs</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">nan</span><span class="p">)</span>
<span class="n">test_accs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">full</span><span class="p">((</span><span class="n">num_epochs</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">nan</span><span class="p">)</span>
<span class="k">for</span> <span class="n">epoch_i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_epochs</span><span class="p">):</span>
    <span class="c1"># Train model</span>
    <span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
    <span class="n">y_hat</span><span class="p">,</span> <span class="n">y_hat_cls</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">x_0</span><span class="p">,</span> <span class="n">x_1</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">y_hat</span><span class="p">[</span><span class="n">ind_train</span><span class="p">],</span> <span class="n">y_train</span><span class="p">)</span>
    <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
    <span class="n">train_acc</span> <span class="o">=</span> <span class="n">get_accuracy</span><span class="p">(</span><span class="n">y_hat_cls</span><span class="p">[</span><span class="n">ind_train</span><span class="p">],</span> <span class="n">y</span><span class="p">[</span><span class="n">ind_train</span><span class="p">])</span>
    <span class="nb">print</span><span class="p">(</span>
        <span class="sa">f</span><span class="s2">&quot;Epoch: </span><span class="si">{</span><span class="n">epoch_i</span><span class="si">}</span><span class="s2">-------- </span><span class="se">\n</span><span class="s2">Loss: </span><span class="si">{</span><span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2"> </span><span class="se">\n</span><span class="s2">Train_acc: </span><span class="si">{</span><span class="n">train_acc</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span>
        <span class="n">flush</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="c1"># Evaluate model</span>
    <span class="k">if</span> <span class="n">epoch_i</span> <span class="o">%</span> <span class="n">test_interval</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">or</span> <span class="n">epoch_i</span> <span class="o">==</span> <span class="n">num_epochs</span> <span class="o">-</span> <span class="mi">1</span><span class="p">:</span>
        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
            <span class="n">y_hat</span><span class="p">,</span> <span class="n">y_hat_cls</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">x_0</span><span class="p">,</span> <span class="n">x_1</span><span class="p">)</span>
            <span class="n">test_acc</span> <span class="o">=</span> <span class="n">get_accuracy</span><span class="p">(</span><span class="n">y_hat_cls</span><span class="p">[</span><span class="n">ind_test</span><span class="p">],</span> <span class="n">y</span><span class="p">[</span><span class="n">ind_test</span><span class="p">])</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Test_acc: </span><span class="si">{</span><span class="n">test_acc</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">flush</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
            <span class="n">test_accs</span><span class="p">[</span><span class="n">epoch_i</span><span class="p">]</span> <span class="o">=</span> <span class="n">test_acc</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>

    <span class="c1"># Store metrics</span>
    <span class="n">losses</span><span class="p">[</span><span class="n">epoch_i</span><span class="p">]</span> <span class="o">=</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
    <span class="n">train_accs</span><span class="p">[</span><span class="n">epoch_i</span><span class="p">]</span> <span class="o">=</span> <span class="n">train_acc</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Epoch: 0--------
Loss: 0.7157
Train_acc: 0.5000
Test_acc: 0.5000
Epoch: 1--------
Loss: 0.7135
Train_acc: 0.5000
Epoch: 2--------
Loss: 0.7113
Train_acc: 0.5000
Epoch: 3--------
Loss: 0.7093
Train_acc: 0.5000
Epoch: 4--------
Loss: 0.7074
Train_acc: 0.5000
Epoch: 5--------
Loss: 0.7056
Train_acc: 0.5000
Epoch: 6--------
Loss: 0.7039
Train_acc: 0.5000
Epoch: 7--------
Loss: 0.7024
Train_acc: 0.5000
Epoch: 8--------
Loss: 0.7010
Train_acc: 0.5000
Epoch: 9--------
Loss: 0.6997
Train_acc: 0.5000
Epoch: 10--------
Loss: 0.6985
Train_acc: 0.5000
Epoch: 11--------
Loss: 0.6975
Train_acc: 0.5000
Epoch: 12--------
Loss: 0.6966
Train_acc: 0.5000
Epoch: 13--------
Loss: 0.6958
Train_acc: 0.5000
Epoch: 14--------
Loss: 0.6951
Train_acc: 0.5000
Epoch: 15--------
Loss: 0.6945
Train_acc: 0.5000
Epoch: 16--------
Loss: 0.6940
Train_acc: 0.5000
Epoch: 17--------
Loss: 0.6936
Train_acc: 0.5000
Epoch: 18--------
Loss: 0.6933
Train_acc: 0.5000
Epoch: 19--------
Loss: 0.6931
Train_acc: 0.5000
Epoch: 20--------
Loss: 0.6930
Train_acc: 0.5000
Test_acc: 0.5000
Epoch: 21--------
Loss: 0.6929
Train_acc: 0.5000
Epoch: 22--------
Loss: 0.6928
Train_acc: 0.5000
Epoch: 23--------
Loss: 0.6928
Train_acc: 0.5000
Epoch: 24--------
Loss: 0.6929
Train_acc: 0.5000
Epoch: 25--------
Loss: 0.6929
Train_acc: 0.5000
Epoch: 26--------
Loss: 0.6930
Train_acc: 0.5000
Epoch: 27--------
Loss: 0.6930
Train_acc: 0.5000
Epoch: 28--------
Loss: 0.6931
Train_acc: 0.5000
Epoch: 29--------
Loss: 0.6931
Train_acc: 0.5000
Epoch: 30--------
Loss: 0.6932
Train_acc: 0.5000
Epoch: 31--------
Loss: 0.6932
Train_acc: 0.5000
Epoch: 32--------
Loss: 0.6932
Train_acc: 0.5000
Epoch: 33--------
Loss: 0.6932
Train_acc: 0.5000
Epoch: 34--------
Loss: 0.6932
Train_acc: 0.5000
Epoch: 35--------
Loss: 0.6931
Train_acc: 0.5000
Epoch: 36--------
Loss: 0.6931
Train_acc: 0.5000
Epoch: 37--------
Loss: 0.6931
Train_acc: 0.5000
Epoch: 38--------
Loss: 0.6930
Train_acc: 0.5000
Epoch: 39--------
Loss: 0.6929
Train_acc: 0.5000
Epoch: 40--------
Loss: 0.6929
Train_acc: 0.5000
Test_acc: 0.5000
Epoch: 41--------
Loss: 0.6928
Train_acc: 0.5000
Epoch: 42--------
Loss: 0.6928
Train_acc: 0.5000
Epoch: 43--------
Loss: 0.6927
Train_acc: 0.5000
Epoch: 44--------
Loss: 0.6927
Train_acc: 0.5000
Epoch: 45--------
Loss: 0.6926
Train_acc: 0.5000
Epoch: 46--------
Loss: 0.6926
Train_acc: 0.5000
Epoch: 47--------
Loss: 0.6925
Train_acc: 0.5000
Epoch: 48--------
Loss: 0.6925
Train_acc: 0.5000
Epoch: 49--------
Loss: 0.6925
Train_acc: 0.5000
Epoch: 50--------
Loss: 0.6924
Train_acc: 0.5000
Epoch: 51--------
Loss: 0.6924
Train_acc: 0.5000
Epoch: 52--------
Loss: 0.6924
Train_acc: 0.6429
Epoch: 53--------
Loss: 0.6924
Train_acc: 1.0000
Epoch: 54--------
Loss: 0.6924
Train_acc: 0.5000
Epoch: 55--------
Loss: 0.6923
Train_acc: 0.5000
Epoch: 56--------
Loss: 0.6923
Train_acc: 0.5000
Epoch: 57--------
Loss: 0.6923
Train_acc: 0.5000
Epoch: 58--------
Loss: 0.6923
Train_acc: 0.5000
Epoch: 59--------
Loss: 0.6923
Train_acc: 0.5000
Epoch: 60--------
Loss: 0.6923
Train_acc: 0.5000
Test_acc: 0.5000
Epoch: 61--------
Loss: 0.6922
Train_acc: 0.5000
Epoch: 62--------
Loss: 0.6922
Train_acc: 0.5000
Epoch: 63--------
Loss: 0.6922
Train_acc: 0.5000
Epoch: 64--------
Loss: 0.6922
Train_acc: 0.5000
Epoch: 65--------
Loss: 0.6921
Train_acc: 0.5000
Epoch: 66--------
Loss: 0.6921
Train_acc: 0.5000
Epoch: 67--------
Loss: 0.6921
Train_acc: 0.5000
Epoch: 68--------
Loss: 0.6920
Train_acc: 0.5000
Epoch: 69--------
Loss: 0.6920
Train_acc: 0.5000
Epoch: 70--------
Loss: 0.6920
Train_acc: 0.5000
Epoch: 71--------
Loss: 0.6919
Train_acc: 0.8571
Epoch: 72--------
Loss: 0.6919
Train_acc: 0.9643
Epoch: 73--------
Loss: 0.6919
Train_acc: 0.8571
Epoch: 74--------
Loss: 0.6918
Train_acc: 0.7500
Epoch: 75--------
Loss: 0.6918
Train_acc: 0.6429
Epoch: 76--------
Loss: 0.6918
Train_acc: 0.6429
Epoch: 77--------
Loss: 0.6917
Train_acc: 0.6429
Epoch: 78--------
Loss: 0.6917
Train_acc: 0.5714
Epoch: 79--------
Loss: 0.6917
Train_acc: 0.5357
Epoch: 80--------
Loss: 0.6916
Train_acc: 0.5357
Test_acc: 0.5000
Epoch: 81--------
Loss: 0.6916
Train_acc: 0.5000
Epoch: 82--------
Loss: 0.6916
Train_acc: 0.5000
Epoch: 83--------
Loss: 0.6915
Train_acc: 0.5000
Epoch: 84--------
Loss: 0.6915
Train_acc: 0.5000
Epoch: 85--------
Loss: 0.6915
Train_acc: 0.5000
Epoch: 86--------
Loss: 0.6914
Train_acc: 0.5000
Epoch: 87--------
Loss: 0.6914
Train_acc: 0.5000
Epoch: 88--------
Loss: 0.6913
Train_acc: 0.5000
Epoch: 89--------
Loss: 0.6913
Train_acc: 0.5000
Epoch: 90--------
Loss: 0.6913
Train_acc: 0.5000
Epoch: 91--------
Loss: 0.6912
Train_acc: 0.5357
Epoch: 92--------
Loss: 0.6912
Train_acc: 0.5357
Epoch: 93--------
Loss: 0.6911
Train_acc: 0.5357
Epoch: 94--------
Loss: 0.6911
Train_acc: 0.5714
Epoch: 95--------
Loss: 0.6910
Train_acc: 0.5714
Epoch: 96--------
Loss: 0.6910
Train_acc: 0.6429
Epoch: 97--------
Loss: 0.6909
Train_acc: 0.6429
Epoch: 98--------
Loss: 0.6909
Train_acc: 0.6429
Epoch: 99--------
Loss: 0.6908
Train_acc: 0.6429
Epoch: 100--------
Loss: 0.6908
Train_acc: 0.6429
Test_acc: 0.6667
Epoch: 101--------
Loss: 0.6907
Train_acc: 0.6429
Epoch: 102--------
Loss: 0.6907
Train_acc: 0.6429
Epoch: 103--------
Loss: 0.6906
Train_acc: 0.6429
Epoch: 104--------
Loss: 0.6906
Train_acc: 0.6429
Epoch: 105--------
Loss: 0.6905
Train_acc: 0.6429
Epoch: 106--------
Loss: 0.6905
Train_acc: 0.6429
Epoch: 107--------
Loss: 0.6904
Train_acc: 0.6429
Epoch: 108--------
Loss: 0.6904
Train_acc: 0.6429
Epoch: 109--------
Loss: 0.6903
Train_acc: 0.6429
Epoch: 110--------
Loss: 0.6902
Train_acc: 0.6429
Epoch: 111--------
Loss: 0.6902
Train_acc: 0.6429
Epoch: 112--------
Loss: 0.6901
Train_acc: 0.6429
Epoch: 113--------
Loss: 0.6901
Train_acc: 0.6429
Epoch: 114--------
Loss: 0.6900
Train_acc: 0.6429
Epoch: 115--------
Loss: 0.6899
Train_acc: 0.6429
Epoch: 116--------
Loss: 0.6899
Train_acc: 0.6429
Epoch: 117--------
Loss: 0.6898
Train_acc: 0.6429
Epoch: 118--------
Loss: 0.6897
Train_acc: 0.6429
Epoch: 119--------
Loss: 0.6897
Train_acc: 0.6429
Epoch: 120--------
Loss: 0.6896
Train_acc: 0.6429
Test_acc: 0.6667
Epoch: 121--------
Loss: 0.6895
Train_acc: 0.6429
Epoch: 122--------
Loss: 0.6894
Train_acc: 0.6429
Epoch: 123--------
Loss: 0.6894
Train_acc: 0.6429
Epoch: 124--------
Loss: 0.6893
Train_acc: 0.6429
Epoch: 125--------
Loss: 0.6892
Train_acc: 0.6429
Epoch: 126--------
Loss: 0.6891
Train_acc: 0.6429
Epoch: 127--------
Loss: 0.6891
Train_acc: 0.6429
Epoch: 128--------
Loss: 0.6890
Train_acc: 0.6429
Epoch: 129--------
Loss: 0.6889
Train_acc: 0.6429
Epoch: 130--------
Loss: 0.6888
Train_acc: 0.6429
Epoch: 131--------
Loss: 0.6887
Train_acc: 0.6429
Epoch: 132--------
Loss: 0.6886
Train_acc: 0.6429
Epoch: 133--------
Loss: 0.6886
Train_acc: 0.6429
Epoch: 134--------
Loss: 0.6885
Train_acc: 0.6429
Epoch: 135--------
Loss: 0.6884
Train_acc: 0.6429
Epoch: 136--------
Loss: 0.6883
Train_acc: 0.6429
Epoch: 137--------
Loss: 0.6882
Train_acc: 0.6429
Epoch: 138--------
Loss: 0.6881
Train_acc: 0.6429
Epoch: 139--------
Loss: 0.6880
Train_acc: 0.6429
Epoch: 140--------
Loss: 0.6879
Train_acc: 0.6429
Test_acc: 0.6667
Epoch: 141--------
Loss: 0.6878
Train_acc: 0.6429
Epoch: 142--------
Loss: 0.6877
Train_acc: 0.6429
Epoch: 143--------
Loss: 0.6876
Train_acc: 0.6429
Epoch: 144--------
Loss: 0.6875
Train_acc: 0.6429
Epoch: 145--------
Loss: 0.6874
Train_acc: 0.6429
Epoch: 146--------
Loss: 0.6873
Train_acc: 0.6429
Epoch: 147--------
Loss: 0.6871
Train_acc: 0.6429
Epoch: 148--------
Loss: 0.6870
Train_acc: 0.6429
Epoch: 149--------
Loss: 0.6869
Train_acc: 0.6429
Epoch: 150--------
Loss: 0.6868
Train_acc: 0.6429
Epoch: 151--------
Loss: 0.6867
Train_acc: 0.6429
Epoch: 152--------
Loss: 0.6865
Train_acc: 0.6429
Epoch: 153--------
Loss: 0.6864
Train_acc: 0.6429
Epoch: 154--------
Loss: 0.6863
Train_acc: 0.6429
Epoch: 155--------
Loss: 0.6861
Train_acc: 0.6429
Epoch: 156--------
Loss: 0.6860
Train_acc: 0.6429
Epoch: 157--------
Loss: 0.6859
Train_acc: 0.6429
Epoch: 158--------
Loss: 0.6857
Train_acc: 0.6429
Epoch: 159--------
Loss: 0.6856
Train_acc: 0.6429
Epoch: 160--------
Loss: 0.6855
Train_acc: 0.6429
Test_acc: 0.6667
Epoch: 161--------
Loss: 0.6853
Train_acc: 0.6429
Epoch: 162--------
Loss: 0.6852
Train_acc: 0.6429
Epoch: 163--------
Loss: 0.6850
Train_acc: 0.6429
Epoch: 164--------
Loss: 0.6849
Train_acc: 0.6429
Epoch: 165--------
Loss: 0.6847
Train_acc: 0.6429
Epoch: 166--------
Loss: 0.6845
Train_acc: 0.6429
Epoch: 167--------
Loss: 0.6844
Train_acc: 0.6429
Epoch: 168--------
Loss: 0.6842
Train_acc: 0.6429
Epoch: 169--------
Loss: 0.6840
Train_acc: 0.6429
Epoch: 170--------
Loss: 0.6839
Train_acc: 0.6429
Epoch: 171--------
Loss: 0.6837
Train_acc: 0.6429
Epoch: 172--------
Loss: 0.6835
Train_acc: 0.6429
Epoch: 173--------
Loss: 0.6833
Train_acc: 0.6429
Epoch: 174--------
Loss: 0.6831
Train_acc: 0.6429
Epoch: 175--------
Loss: 0.6829
Train_acc: 0.6429
Epoch: 176--------
Loss: 0.6827
Train_acc: 0.6429
Epoch: 177--------
Loss: 0.6825
Train_acc: 0.6429
Epoch: 178--------
Loss: 0.6823
Train_acc: 0.6429
Epoch: 179--------
Loss: 0.6821
Train_acc: 0.6429
Epoch: 180--------
Loss: 0.6819
Train_acc: 0.6429
Test_acc: 0.6667
Epoch: 181--------
Loss: 0.6817
Train_acc: 0.6429
Epoch: 182--------
Loss: 0.6815
Train_acc: 0.6429
Epoch: 183--------
Loss: 0.6813
Train_acc: 0.6429
Epoch: 184--------
Loss: 0.6810
Train_acc: 0.6429
Epoch: 185--------
Loss: 0.6808
Train_acc: 0.6429
Epoch: 186--------
Loss: 0.6806
Train_acc: 0.6429
Epoch: 187--------
Loss: 0.6803
Train_acc: 0.6429
Epoch: 188--------
Loss: 0.6801
Train_acc: 0.6429
Epoch: 189--------
Loss: 0.6799
Train_acc: 0.6429
Epoch: 190--------
Loss: 0.6796
Train_acc: 0.6429
Epoch: 191--------
Loss: 0.6793
Train_acc: 0.6429
Epoch: 192--------
Loss: 0.6791
Train_acc: 0.6429
Epoch: 193--------
Loss: 0.6788
Train_acc: 0.6429
Epoch: 194--------
Loss: 0.6785
Train_acc: 0.6429
Epoch: 195--------
Loss: 0.6783
Train_acc: 0.6429
Epoch: 196--------
Loss: 0.6780
Train_acc: 0.6429
Epoch: 197--------
Loss: 0.6777
Train_acc: 0.6429
Epoch: 198--------
Loss: 0.6774
Train_acc: 0.6429
Epoch: 199--------
Loss: 0.6771
Train_acc: 0.6429
Epoch: 200--------
Loss: 0.6768
Train_acc: 0.6429
Test_acc: 0.6667
Epoch: 201--------
Loss: 0.6765
Train_acc: 0.6429
Epoch: 202--------
Loss: 0.6762
Train_acc: 0.6429
Epoch: 203--------
Loss: 0.6758
Train_acc: 0.6429
Epoch: 204--------
Loss: 0.6755
Train_acc: 0.6429
Epoch: 205--------
Loss: 0.6752
Train_acc: 0.6429
Epoch: 206--------
Loss: 0.6748
Train_acc: 0.6429
Epoch: 207--------
Loss: 0.6745
Train_acc: 0.6429
Epoch: 208--------
Loss: 0.6741
Train_acc: 0.6429
Epoch: 209--------
Loss: 0.6738
Train_acc: 0.6429
Epoch: 210--------
Loss: 0.6734
Train_acc: 0.6429
Epoch: 211--------
Loss: 0.6730
Train_acc: 0.6429
Epoch: 212--------
Loss: 0.6726
Train_acc: 0.6429
Epoch: 213--------
Loss: 0.6723
Train_acc: 0.6429
Epoch: 214--------
Loss: 0.6719
Train_acc: 0.6429
Epoch: 215--------
Loss: 0.6715
Train_acc: 0.6429
Epoch: 216--------
Loss: 0.6710
Train_acc: 0.6429
Epoch: 217--------
Loss: 0.6706
Train_acc: 0.6429
Epoch: 218--------
Loss: 0.6702
Train_acc: 0.6786
Epoch: 219--------
Loss: 0.6698
Train_acc: 0.6786
Epoch: 220--------
Loss: 0.6693
Train_acc: 0.6786
Test_acc: 0.6667
Epoch: 221--------
Loss: 0.6689
Train_acc: 0.6786
Epoch: 222--------
Loss: 0.6684
Train_acc: 0.6786
Epoch: 223--------
Loss: 0.6679
Train_acc: 0.6786
Epoch: 224--------
Loss: 0.6675
Train_acc: 0.6786
Epoch: 225--------
Loss: 0.6670
Train_acc: 0.6786
Epoch: 226--------
Loss: 0.6665
Train_acc: 0.6786
Epoch: 227--------
Loss: 0.6660
Train_acc: 0.6786
Epoch: 228--------
Loss: 0.6655
Train_acc: 0.6786
Epoch: 229--------
Loss: 0.6649
Train_acc: 0.6786
Epoch: 230--------
Loss: 0.6644
Train_acc: 0.6786
Epoch: 231--------
Loss: 0.6639
Train_acc: 0.6786
Epoch: 232--------
Loss: 0.6633
Train_acc: 0.6786
Epoch: 233--------
Loss: 0.6628
Train_acc: 0.6786
Epoch: 234--------
Loss: 0.6622
Train_acc: 0.6786
Epoch: 235--------
Loss: 0.6616
Train_acc: 0.6786
Epoch: 236--------
Loss: 0.6610
Train_acc: 0.6786
Epoch: 237--------
Loss: 0.6604
Train_acc: 0.6786
Epoch: 238--------
Loss: 0.6598
Train_acc: 0.6786
Epoch: 239--------
Loss: 0.6592
Train_acc: 0.6786
Epoch: 240--------
Loss: 0.6585
Train_acc: 0.7143
Test_acc: 0.6667
Epoch: 241--------
Loss: 0.6579
Train_acc: 0.7143
Epoch: 242--------
Loss: 0.6572
Train_acc: 0.7143
Epoch: 243--------
Loss: 0.6566
Train_acc: 0.7500
Epoch: 244--------
Loss: 0.6559
Train_acc: 0.7500
Epoch: 245--------
Loss: 0.6552
Train_acc: 0.7500
Epoch: 246--------
Loss: 0.6545
Train_acc: 0.7500
Epoch: 247--------
Loss: 0.6538
Train_acc: 0.7500
Epoch: 248--------
Loss: 0.6531
Train_acc: 0.7500
Epoch: 249--------
Loss: 0.6523
Train_acc: 0.7500
Epoch: 250--------
Loss: 0.6516
Train_acc: 0.8214
Epoch: 251--------
Loss: 0.6508
Train_acc: 0.8214
Epoch: 252--------
Loss: 0.6500
Train_acc: 0.8214
Epoch: 253--------
Loss: 0.6492
Train_acc: 0.8214
Epoch: 254--------
Loss: 0.6484
Train_acc: 0.8214
Epoch: 255--------
Loss: 0.6476
Train_acc: 0.8214
Epoch: 256--------
Loss: 0.6468
Train_acc: 0.8214
Epoch: 257--------
Loss: 0.6459
Train_acc: 0.8214
Epoch: 258--------
Loss: 0.6451
Train_acc: 0.8214
Epoch: 259--------
Loss: 0.6442
Train_acc: 0.8214
Epoch: 260--------
Loss: 0.6433
Train_acc: 0.8214
Test_acc: 0.6667
Epoch: 261--------
Loss: 0.6424
Train_acc: 0.8214
Epoch: 262--------
Loss: 0.6415
Train_acc: 0.8214
Epoch: 263--------
Loss: 0.6406
Train_acc: 0.8214
Epoch: 264--------
Loss: 0.6397
Train_acc: 0.8214
Epoch: 265--------
Loss: 0.6387
Train_acc: 0.8214
Epoch: 266--------
Loss: 0.6378
Train_acc: 0.8214
Epoch: 267--------
Loss: 0.6368
Train_acc: 0.8214
Epoch: 268--------
Loss: 0.6358
Train_acc: 0.8214
Epoch: 269--------
Loss: 0.6348
Train_acc: 0.8214
Epoch: 270--------
Loss: 0.6337
Train_acc: 0.8214
Epoch: 271--------
Loss: 0.6327
Train_acc: 0.8214
Epoch: 272--------
Loss: 0.6317
Train_acc: 0.8214
Epoch: 273--------
Loss: 0.6306
Train_acc: 0.8214
Epoch: 274--------
Loss: 0.6295
Train_acc: 0.8214
Epoch: 275--------
Loss: 0.6284
Train_acc: 0.8214
Epoch: 276--------
Loss: 0.6273
Train_acc: 0.8214
Epoch: 277--------
Loss: 0.6262
Train_acc: 0.8214
Epoch: 278--------
Loss: 0.6250
Train_acc: 0.8214
Epoch: 279--------
Loss: 0.6239
Train_acc: 0.8571
Epoch: 280--------
Loss: 0.6227
Train_acc: 0.8571
Test_acc: 0.6667
Epoch: 281--------
Loss: 0.6215
Train_acc: 0.8571
Epoch: 282--------
Loss: 0.6203
Train_acc: 0.8571
Epoch: 283--------
Loss: 0.6191
Train_acc: 0.8571
Epoch: 284--------
Loss: 0.6178
Train_acc: 0.8571
Epoch: 285--------
Loss: 0.6166
Train_acc: 0.8571
Epoch: 286--------
Loss: 0.6153
Train_acc: 0.8571
Epoch: 287--------
Loss: 0.6140
Train_acc: 0.8571
Epoch: 288--------
Loss: 0.6127
Train_acc: 0.8571
Epoch: 289--------
Loss: 0.6114
Train_acc: 0.8571
Epoch: 290--------
Loss: 0.6101
Train_acc: 0.8571
Epoch: 291--------
Loss: 0.6088
Train_acc: 0.8571
Epoch: 292--------
Loss: 0.6074
Train_acc: 0.8571
Epoch: 293--------
Loss: 0.6060
Train_acc: 0.8571
Epoch: 294--------
Loss: 0.6046
Train_acc: 0.8571
Epoch: 295--------
Loss: 0.6032
Train_acc: 0.8571
Epoch: 296--------
Loss: 0.6018
Train_acc: 0.8571
Epoch: 297--------
Loss: 0.6004
Train_acc: 0.8571
Epoch: 298--------
Loss: 0.5989
Train_acc: 0.8571
Epoch: 299--------
Loss: 0.5975
Train_acc: 0.8571
Epoch: 300--------
Loss: 0.5960
Train_acc: 0.8571
Test_acc: 1.0000
Epoch: 301--------
Loss: 0.5945
Train_acc: 0.8571
Epoch: 302--------
Loss: 0.5930
Train_acc: 0.8571
Epoch: 303--------
Loss: 0.5915
Train_acc: 0.8571
Epoch: 304--------
Loss: 0.5900
Train_acc: 0.8571
Epoch: 305--------
Loss: 0.5884
Train_acc: 0.8571
Epoch: 306--------
Loss: 0.5868
Train_acc: 0.8571
Epoch: 307--------
Loss: 0.5853
Train_acc: 0.8571
Epoch: 308--------
Loss: 0.5837
Train_acc: 0.8571
Epoch: 309--------
Loss: 0.5821
Train_acc: 0.8571
Epoch: 310--------
Loss: 0.5805
Train_acc: 0.8571
Epoch: 311--------
Loss: 0.5788
Train_acc: 0.8571
Epoch: 312--------
Loss: 0.5772
Train_acc: 0.8571
Epoch: 313--------
Loss: 0.5755
Train_acc: 0.8571
Epoch: 314--------
Loss: 0.5739
Train_acc: 0.8571
Epoch: 315--------
Loss: 0.5722
Train_acc: 0.8571
Epoch: 316--------
Loss: 0.5705
Train_acc: 0.8571
Epoch: 317--------
Loss: 0.5688
Train_acc: 0.8571
Epoch: 318--------
Loss: 0.5671
Train_acc: 0.8571
Epoch: 319--------
Loss: 0.5653
Train_acc: 0.8571
Epoch: 320--------
Loss: 0.5636
Train_acc: 0.8571
Test_acc: 1.0000
Epoch: 321--------
Loss: 0.5618
Train_acc: 0.8571
Epoch: 322--------
Loss: 0.5601
Train_acc: 0.8571
Epoch: 323--------
Loss: 0.5583
Train_acc: 0.8571
Epoch: 324--------
Loss: 0.5565
Train_acc: 0.8571
Epoch: 325--------
Loss: 0.5547
Train_acc: 0.8571
Epoch: 326--------
Loss: 0.5529
Train_acc: 0.8571
Epoch: 327--------
Loss: 0.5511
Train_acc: 0.8571
Epoch: 328--------
Loss: 0.5492
Train_acc: 0.8571
Epoch: 329--------
Loss: 0.5474
Train_acc: 0.8571
Epoch: 330--------
Loss: 0.5455
Train_acc: 0.8571
Epoch: 331--------
Loss: 0.5437
Train_acc: 0.8571
Epoch: 332--------
Loss: 0.5418
Train_acc: 0.8571
Epoch: 333--------
Loss: 0.5399
Train_acc: 0.8571
Epoch: 334--------
Loss: 0.5381
Train_acc: 0.8571
Epoch: 335--------
Loss: 0.5362
Train_acc: 0.8571
Epoch: 336--------
Loss: 0.5343
Train_acc: 0.8571
Epoch: 337--------
Loss: 0.5323
Train_acc: 0.8571
Epoch: 338--------
Loss: 0.5304
Train_acc: 0.8571
Epoch: 339--------
Loss: 0.5285
Train_acc: 0.8571
Epoch: 340--------
Loss: 0.5266
Train_acc: 0.8929
Test_acc: 1.0000
Epoch: 341--------
Loss: 0.5246
Train_acc: 0.8929
Epoch: 342--------
Loss: 0.5227
Train_acc: 0.8929
Epoch: 343--------
Loss: 0.5207
Train_acc: 0.8929
Epoch: 344--------
Loss: 0.5188
Train_acc: 0.8929
Epoch: 345--------
Loss: 0.5168
Train_acc: 0.8929
Epoch: 346--------
Loss: 0.5148
Train_acc: 0.8929
Epoch: 347--------
Loss: 0.5129
Train_acc: 0.8929
Epoch: 348--------
Loss: 0.5109
Train_acc: 0.8929
Epoch: 349--------
Loss: 0.5089
Train_acc: 0.8929
Epoch: 350--------
Loss: 0.5069
Train_acc: 0.9286
Epoch: 351--------
Loss: 0.5049
Train_acc: 0.9286
Epoch: 352--------
Loss: 0.5029
Train_acc: 0.9286
Epoch: 353--------
Loss: 0.5009
Train_acc: 0.9286
Epoch: 354--------
Loss: 0.4989
Train_acc: 0.9286
Epoch: 355--------
Loss: 0.4969
Train_acc: 0.9286
Epoch: 356--------
Loss: 0.4949
Train_acc: 0.9286
Epoch: 357--------
Loss: 0.4928
Train_acc: 0.9286
Epoch: 358--------
Loss: 0.4908
Train_acc: 0.9286
Epoch: 359--------
Loss: 0.4888
Train_acc: 0.9286
Epoch: 360--------
Loss: 0.4868
Train_acc: 0.9286
Test_acc: 1.0000
Epoch: 361--------
Loss: 0.4847
Train_acc: 0.9286
Epoch: 362--------
Loss: 0.4827
Train_acc: 0.9286
Epoch: 363--------
Loss: 0.4807
Train_acc: 0.9286
Epoch: 364--------
Loss: 0.4787
Train_acc: 0.9286
Epoch: 365--------
Loss: 0.4766
Train_acc: 0.9286
Epoch: 366--------
Loss: 0.4746
Train_acc: 0.9286
Epoch: 367--------
Loss: 0.4726
Train_acc: 0.9286
Epoch: 368--------
Loss: 0.4705
Train_acc: 0.9286
Epoch: 369--------
Loss: 0.4685
Train_acc: 0.9286
Epoch: 370--------
Loss: 0.4665
Train_acc: 0.9286
Epoch: 371--------
Loss: 0.4644
Train_acc: 0.9286
Epoch: 372--------
Loss: 0.4624
Train_acc: 0.9286
Epoch: 373--------
Loss: 0.4604
Train_acc: 0.9286
Epoch: 374--------
Loss: 0.4583
Train_acc: 0.9286
Epoch: 375--------
Loss: 0.4563
Train_acc: 0.9286
Epoch: 376--------
Loss: 0.4543
Train_acc: 0.9286
Epoch: 377--------
Loss: 0.4523
Train_acc: 0.9286
Epoch: 378--------
Loss: 0.4503
Train_acc: 0.9286
Epoch: 379--------
Loss: 0.4482
Train_acc: 0.9286
Epoch: 380--------
Loss: 0.4462
Train_acc: 0.9286
Test_acc: 1.0000
Epoch: 381--------
Loss: 0.4442
Train_acc: 0.9286
Epoch: 382--------
Loss: 0.4422
Train_acc: 0.9286
Epoch: 383--------
Loss: 0.4402
Train_acc: 0.9286
Epoch: 384--------
Loss: 0.4382
Train_acc: 0.9286
Epoch: 385--------
Loss: 0.4362
Train_acc: 0.9286
Epoch: 386--------
Loss: 0.4342
Train_acc: 0.9286
Epoch: 387--------
Loss: 0.4322
Train_acc: 0.9286
Epoch: 388--------
Loss: 0.4302
Train_acc: 0.9286
Epoch: 389--------
Loss: 0.4282
Train_acc: 0.9286
Epoch: 390--------
Loss: 0.4262
Train_acc: 0.9286
Epoch: 391--------
Loss: 0.4243
Train_acc: 0.9286
Epoch: 392--------
Loss: 0.4223
Train_acc: 0.9286
Epoch: 393--------
Loss: 0.4203
Train_acc: 0.9286
Epoch: 394--------
Loss: 0.4184
Train_acc: 0.9286
Epoch: 395--------
Loss: 0.4164
Train_acc: 0.9286
Epoch: 396--------
Loss: 0.4145
Train_acc: 0.9286
Epoch: 397--------
Loss: 0.4125
Train_acc: 0.9286
Epoch: 398--------
Loss: 0.4106
Train_acc: 0.9286
Epoch: 399--------
Loss: 0.4086
Train_acc: 0.9286
Epoch: 400--------
Loss: 0.4067
Train_acc: 0.9286
Test_acc: 1.0000
Epoch: 401--------
Loss: 0.4048
Train_acc: 0.9286
Epoch: 402--------
Loss: 0.4029
Train_acc: 0.9286
Epoch: 403--------
Loss: 0.4010
Train_acc: 0.9286
Epoch: 404--------
Loss: 0.3991
Train_acc: 0.9286
Epoch: 405--------
Loss: 0.3972
Train_acc: 0.9286
Epoch: 406--------
Loss: 0.3953
Train_acc: 0.9286
Epoch: 407--------
Loss: 0.3934
Train_acc: 0.9286
Epoch: 408--------
Loss: 0.3915
Train_acc: 0.9286
Epoch: 409--------
Loss: 0.3897
Train_acc: 0.9286
Epoch: 410--------
Loss: 0.3878
Train_acc: 0.9286
Epoch: 411--------
Loss: 0.3859
Train_acc: 0.9286
Epoch: 412--------
Loss: 0.3841
Train_acc: 0.9286
Epoch: 413--------
Loss: 0.3823
Train_acc: 0.9286
Epoch: 414--------
Loss: 0.3804
Train_acc: 0.9286
Epoch: 415--------
Loss: 0.3786
Train_acc: 0.9286
Epoch: 416--------
Loss: 0.3768
Train_acc: 0.9286
Epoch: 417--------
Loss: 0.3750
Train_acc: 0.9286
Epoch: 418--------
Loss: 0.3732
Train_acc: 0.9286
Epoch: 419--------
Loss: 0.3714
Train_acc: 0.9286
Epoch: 420--------
Loss: 0.3696
Train_acc: 0.9286
Test_acc: 1.0000
Epoch: 421--------
Loss: 0.3678
Train_acc: 0.9286
Epoch: 422--------
Loss: 0.3661
Train_acc: 0.9286
Epoch: 423--------
Loss: 0.3643
Train_acc: 0.9286
Epoch: 424--------
Loss: 0.3626
Train_acc: 0.9286
Epoch: 425--------
Loss: 0.3608
Train_acc: 0.9286
Epoch: 426--------
Loss: 0.3591
Train_acc: 0.9286
Epoch: 427--------
Loss: 0.3573
Train_acc: 0.9286
Epoch: 428--------
Loss: 0.3556
Train_acc: 0.9286
Epoch: 429--------
Loss: 0.3539
Train_acc: 0.9286
Epoch: 430--------
Loss: 0.3522
Train_acc: 0.9286
Epoch: 431--------
Loss: 0.3505
Train_acc: 0.9286
Epoch: 432--------
Loss: 0.3488
Train_acc: 0.9286
Epoch: 433--------
Loss: 0.3472
Train_acc: 0.9286
Epoch: 434--------
Loss: 0.3455
Train_acc: 0.9286
Epoch: 435--------
Loss: 0.3438
Train_acc: 0.9286
Epoch: 436--------
Loss: 0.3422
Train_acc: 0.9286
Epoch: 437--------
Loss: 0.3406
Train_acc: 0.9286
Epoch: 438--------
Loss: 0.3389
Train_acc: 0.9286
Epoch: 439--------
Loss: 0.3373
Train_acc: 0.9286
Epoch: 440--------
Loss: 0.3357
Train_acc: 0.9286
Test_acc: 1.0000
Epoch: 441--------
Loss: 0.3341
Train_acc: 0.9286
Epoch: 442--------
Loss: 0.3325
Train_acc: 0.9286
Epoch: 443--------
Loss: 0.3309
Train_acc: 0.9286
Epoch: 444--------
Loss: 0.3293
Train_acc: 0.9286
Epoch: 445--------
Loss: 0.3278
Train_acc: 0.9286
Epoch: 446--------
Loss: 0.3262
Train_acc: 0.9286
Epoch: 447--------
Loss: 0.3247
Train_acc: 0.9286
Epoch: 448--------
Loss: 0.3231
Train_acc: 0.9286
Epoch: 449--------
Loss: 0.3216
Train_acc: 0.9286
Epoch: 450--------
Loss: 0.3201
Train_acc: 0.9286
Epoch: 451--------
Loss: 0.3185
Train_acc: 0.9286
Epoch: 452--------
Loss: 0.3170
Train_acc: 0.9286
Epoch: 453--------
Loss: 0.3155
Train_acc: 0.9286
Epoch: 454--------
Loss: 0.3140
Train_acc: 0.9286
Epoch: 455--------
Loss: 0.3126
Train_acc: 0.9286
Epoch: 456--------
Loss: 0.3111
Train_acc: 0.9286
Epoch: 457--------
Loss: 0.3096
Train_acc: 0.9286
Epoch: 458--------
Loss: 0.3082
Train_acc: 0.9286
Epoch: 459--------
Loss: 0.3067
Train_acc: 0.9286
Epoch: 460--------
Loss: 0.3053
Train_acc: 0.9286
Test_acc: 1.0000
Epoch: 461--------
Loss: 0.3039
Train_acc: 0.9286
Epoch: 462--------
Loss: 0.3025
Train_acc: 0.9286
Epoch: 463--------
Loss: 0.3011
Train_acc: 0.9286
Epoch: 464--------
Loss: 0.2997
Train_acc: 0.9286
Epoch: 465--------
Loss: 0.2983
Train_acc: 0.9286
Epoch: 466--------
Loss: 0.2969
Train_acc: 0.9286
Epoch: 467--------
Loss: 0.2955
Train_acc: 0.9286
Epoch: 468--------
Loss: 0.2941
Train_acc: 0.9286
Epoch: 469--------
Loss: 0.2928
Train_acc: 0.9286
Epoch: 470--------
Loss: 0.2914
Train_acc: 0.9286
Epoch: 471--------
Loss: 0.2901
Train_acc: 0.9286
Epoch: 472--------
Loss: 0.2888
Train_acc: 0.9286
Epoch: 473--------
Loss: 0.2874
Train_acc: 0.9286
Epoch: 474--------
Loss: 0.2861
Train_acc: 0.9286
Epoch: 475--------
Loss: 0.2848
Train_acc: 0.9286
Epoch: 476--------
Loss: 0.2835
Train_acc: 0.9286
Epoch: 477--------
Loss: 0.2822
Train_acc: 0.9286
Epoch: 478--------
Loss: 0.2810
Train_acc: 0.9286
Epoch: 479--------
Loss: 0.2797
Train_acc: 0.9286
Epoch: 480--------
Loss: 0.2784
Train_acc: 0.9286
Test_acc: 1.0000
Epoch: 481--------
Loss: 0.2772
Train_acc: 0.9286
Epoch: 482--------
Loss: 0.2759
Train_acc: 0.9286
Epoch: 483--------
Loss: 0.2747
Train_acc: 0.9286
Epoch: 484--------
Loss: 0.2735
Train_acc: 0.9286
Epoch: 485--------
Loss: 0.2722
Train_acc: 0.9286
Epoch: 486--------
Loss: 0.2710
Train_acc: 0.9286
Epoch: 487--------
Loss: 0.2698
Train_acc: 0.9286
Epoch: 488--------
Loss: 0.2686
Train_acc: 0.9286
Epoch: 489--------
Loss: 0.2674
Train_acc: 0.9286
Epoch: 490--------
Loss: 0.2662
Train_acc: 0.9286
Epoch: 491--------
Loss: 0.2651
Train_acc: 0.9286
Epoch: 492--------
Loss: 0.2639
Train_acc: 0.9286
Epoch: 493--------
Loss: 0.2627
Train_acc: 0.9643
Epoch: 494--------
Loss: 0.2616
Train_acc: 0.9643
Epoch: 495--------
Loss: 0.2604
Train_acc: 0.9643
Epoch: 496--------
Loss: 0.2593
Train_acc: 0.9643
Epoch: 497--------
Loss: 0.2582
Train_acc: 0.9643
Epoch: 498--------
Loss: 0.2571
Train_acc: 0.9643
Epoch: 499--------
Loss: 0.2559
Train_acc: 0.9643
Epoch: 500--------
Loss: 0.2548
Train_acc: 0.9643
Test_acc: 1.0000
Epoch: 501--------
Loss: 0.2537
Train_acc: 0.9643
Epoch: 502--------
Loss: 0.2526
Train_acc: 0.9643
Epoch: 503--------
Loss: 0.2516
Train_acc: 0.9643
Epoch: 504--------
Loss: 0.2505
Train_acc: 0.9643
Epoch: 505--------
Loss: 0.2494
Train_acc: 0.9643
Epoch: 506--------
Loss: 0.2484
Train_acc: 0.9643
Epoch: 507--------
Loss: 0.2473
Train_acc: 0.9643
Epoch: 508--------
Loss: 0.2463
Train_acc: 0.9643
Epoch: 509--------
Loss: 0.2452
Train_acc: 0.9643
Epoch: 510--------
Loss: 0.2442
Train_acc: 0.9643
Epoch: 511--------
Loss: 0.2432
Train_acc: 0.9643
Epoch: 512--------
Loss: 0.2421
Train_acc: 0.9643
Epoch: 513--------
Loss: 0.2411
Train_acc: 0.9643
Epoch: 514--------
Loss: 0.2401
Train_acc: 0.9643
Epoch: 515--------
Loss: 0.2391
Train_acc: 0.9643
Epoch: 516--------
Loss: 0.2381
Train_acc: 0.9643
Epoch: 517--------
Loss: 0.2371
Train_acc: 0.9643
Epoch: 518--------
Loss: 0.2362
Train_acc: 0.9643
Epoch: 519--------
Loss: 0.2352
Train_acc: 0.9643
Epoch: 520--------
Loss: 0.2342
Train_acc: 0.9643
Test_acc: 1.0000
Epoch: 521--------
Loss: 0.2333
Train_acc: 0.9643
Epoch: 522--------
Loss: 0.2323
Train_acc: 0.9643
Epoch: 523--------
Loss: 0.2314
Train_acc: 0.9643
Epoch: 524--------
Loss: 0.2304
Train_acc: 0.9643
Epoch: 525--------
Loss: 0.2295
Train_acc: 0.9643
Epoch: 526--------
Loss: 0.2286
Train_acc: 0.9643
Epoch: 527--------
Loss: 0.2276
Train_acc: 0.9643
Epoch: 528--------
Loss: 0.2267
Train_acc: 0.9643
Epoch: 529--------
Loss: 0.2258
Train_acc: 0.9643
Epoch: 530--------
Loss: 0.2249
Train_acc: 0.9643
Epoch: 531--------
Loss: 0.2240
Train_acc: 0.9643
Epoch: 532--------
Loss: 0.2231
Train_acc: 0.9643
Epoch: 533--------
Loss: 0.2223
Train_acc: 0.9643
Epoch: 534--------
Loss: 0.2214
Train_acc: 0.9643
Epoch: 535--------
Loss: 0.2205
Train_acc: 0.9643
Epoch: 536--------
Loss: 0.2196
Train_acc: 0.9643
Epoch: 537--------
Loss: 0.2188
Train_acc: 0.9643
Epoch: 538--------
Loss: 0.2179
Train_acc: 0.9643
Epoch: 539--------
Loss: 0.2171
Train_acc: 0.9643
Epoch: 540--------
Loss: 0.2162
Train_acc: 0.9643
Test_acc: 1.0000
Epoch: 541--------
Loss: 0.2154
Train_acc: 0.9643
Epoch: 542--------
Loss: 0.2146
Train_acc: 0.9643
Epoch: 543--------
Loss: 0.2137
Train_acc: 0.9643
Epoch: 544--------
Loss: 0.2129
Train_acc: 0.9643
Epoch: 545--------
Loss: 0.2121
Train_acc: 0.9643
Epoch: 546--------
Loss: 0.2113
Train_acc: 0.9643
Epoch: 547--------
Loss: 0.2105
Train_acc: 0.9643
Epoch: 548--------
Loss: 0.2097
Train_acc: 0.9643
Epoch: 549--------
Loss: 0.2089
Train_acc: 0.9643
Epoch: 550--------
Loss: 0.2081
Train_acc: 0.9643
Epoch: 551--------
Loss: 0.2073
Train_acc: 0.9643
Epoch: 552--------
Loss: 0.2066
Train_acc: 0.9643
Epoch: 553--------
Loss: 0.2058
Train_acc: 0.9643
Epoch: 554--------
Loss: 0.2050
Train_acc: 0.9643
Epoch: 555--------
Loss: 0.2043
Train_acc: 0.9643
Epoch: 556--------
Loss: 0.2035
Train_acc: 0.9643
Epoch: 557--------
Loss: 0.2028
Train_acc: 0.9643
Epoch: 558--------
Loss: 0.2020
Train_acc: 0.9643
Epoch: 559--------
Loss: 0.2013
Train_acc: 0.9643
Epoch: 560--------
Loss: 0.2005
Train_acc: 0.9643
Test_acc: 1.0000
Epoch: 561--------
Loss: 0.1998
Train_acc: 0.9643
Epoch: 562--------
Loss: 0.1991
Train_acc: 0.9643
Epoch: 563--------
Loss: 0.1983
Train_acc: 0.9643
Epoch: 564--------
Loss: 0.1976
Train_acc: 0.9643
Epoch: 565--------
Loss: 0.1969
Train_acc: 0.9643
Epoch: 566--------
Loss: 0.1962
Train_acc: 0.9643
Epoch: 567--------
Loss: 0.1955
Train_acc: 0.9643
Epoch: 568--------
Loss: 0.1948
Train_acc: 0.9643
Epoch: 569--------
Loss: 0.1941
Train_acc: 0.9643
Epoch: 570--------
Loss: 0.1934
Train_acc: 0.9643
Epoch: 571--------
Loss: 0.1927
Train_acc: 0.9643
Epoch: 572--------
Loss: 0.1921
Train_acc: 0.9643
Epoch: 573--------
Loss: 0.1914
Train_acc: 0.9643
Epoch: 574--------
Loss: 0.1907
Train_acc: 0.9643
Epoch: 575--------
Loss: 0.1900
Train_acc: 0.9643
Epoch: 576--------
Loss: 0.1894
Train_acc: 0.9643
Epoch: 577--------
Loss: 0.1887
Train_acc: 0.9643
Epoch: 578--------
Loss: 0.1881
Train_acc: 0.9643
Epoch: 579--------
Loss: 0.1874
Train_acc: 0.9643
Epoch: 580--------
Loss: 0.1868
Train_acc: 0.9643
Test_acc: 1.0000
Epoch: 581--------
Loss: 0.1861
Train_acc: 0.9643
Epoch: 582--------
Loss: 0.1855
Train_acc: 0.9643
Epoch: 583--------
Loss: 0.1849
Train_acc: 0.9643
Epoch: 584--------
Loss: 0.1842
Train_acc: 0.9643
Epoch: 585--------
Loss: 0.1836
Train_acc: 0.9643
Epoch: 586--------
Loss: 0.1830
Train_acc: 0.9643
Epoch: 587--------
Loss: 0.1824
Train_acc: 0.9643
Epoch: 588--------
Loss: 0.1818
Train_acc: 0.9643
Epoch: 589--------
Loss: 0.1811
Train_acc: 0.9643
Epoch: 590--------
Loss: 0.1805
Train_acc: 0.9643
Epoch: 591--------
Loss: 0.1799
Train_acc: 0.9643
Epoch: 592--------
Loss: 0.1793
Train_acc: 0.9643
Epoch: 593--------
Loss: 0.1788
Train_acc: 0.9643
Epoch: 594--------
Loss: 0.1782
Train_acc: 0.9643
Epoch: 595--------
Loss: 0.1776
Train_acc: 0.9643
Epoch: 596--------
Loss: 0.1770
Train_acc: 0.9643
Epoch: 597--------
Loss: 0.1764
Train_acc: 0.9643
Epoch: 598--------
Loss: 0.1758
Train_acc: 0.9643
Epoch: 599--------
Loss: 0.1753
Train_acc: 0.9643
Epoch: 600--------
Loss: 0.1747
Train_acc: 0.9643
Test_acc: 1.0000
Epoch: 601--------
Loss: 0.1741
Train_acc: 0.9643
Epoch: 602--------
Loss: 0.1736
Train_acc: 0.9643
Epoch: 603--------
Loss: 0.1730
Train_acc: 0.9643
Epoch: 604--------
Loss: 0.1725
Train_acc: 0.9643
Epoch: 605--------
Loss: 0.1719
Train_acc: 0.9643
Epoch: 606--------
Loss: 0.1714
Train_acc: 0.9643
Epoch: 607--------
Loss: 0.1708
Train_acc: 0.9643
Epoch: 608--------
Loss: 0.1703
Train_acc: 0.9643
Epoch: 609--------
Loss: 0.1697
Train_acc: 0.9643
Epoch: 610--------
Loss: 0.1692
Train_acc: 0.9643
Epoch: 611--------
Loss: 0.1687
Train_acc: 0.9643
Epoch: 612--------
Loss: 0.1682
Train_acc: 0.9643
Epoch: 613--------
Loss: 0.1676
Train_acc: 0.9643
Epoch: 614--------
Loss: 0.1671
Train_acc: 0.9643
Epoch: 615--------
Loss: 0.1666
Train_acc: 0.9643
Epoch: 616--------
Loss: 0.1661
Train_acc: 0.9643
Epoch: 617--------
Loss: 0.1656
Train_acc: 0.9643
Epoch: 618--------
Loss: 0.1651
Train_acc: 0.9643
Epoch: 619--------
Loss: 0.1646
Train_acc: 0.9643
Epoch: 620--------
Loss: 0.1640
Train_acc: 0.9643
Test_acc: 1.0000
Epoch: 621--------
Loss: 0.1636
Train_acc: 0.9643
Epoch: 622--------
Loss: 0.1631
Train_acc: 0.9643
Epoch: 623--------
Loss: 0.1626
Train_acc: 0.9643
Epoch: 624--------
Loss: 0.1621
Train_acc: 0.9643
Epoch: 625--------
Loss: 0.1616
Train_acc: 0.9643
Epoch: 626--------
Loss: 0.1611
Train_acc: 0.9643
Epoch: 627--------
Loss: 0.1606
Train_acc: 0.9643
Epoch: 628--------
Loss: 0.1601
Train_acc: 0.9643
Epoch: 629--------
Loss: 0.1597
Train_acc: 0.9643
Epoch: 630--------
Loss: 0.1592
Train_acc: 0.9643
Epoch: 631--------
Loss: 0.1587
Train_acc: 0.9643
Epoch: 632--------
Loss: 0.1583
Train_acc: 0.9643
Epoch: 633--------
Loss: 0.1578
Train_acc: 0.9643
Epoch: 634--------
Loss: 0.1573
Train_acc: 0.9643
Epoch: 635--------
Loss: 0.1569
Train_acc: 0.9643
Epoch: 636--------
Loss: 0.1564
Train_acc: 0.9643
Epoch: 637--------
Loss: 0.1560
Train_acc: 0.9643
Epoch: 638--------
Loss: 0.1555
Train_acc: 0.9643
Epoch: 639--------
Loss: 0.1551
Train_acc: 0.9643
Epoch: 640--------
Loss: 0.1546
Train_acc: 0.9643
Test_acc: 1.0000
Epoch: 641--------
Loss: 0.1542
Train_acc: 0.9643
Epoch: 642--------
Loss: 0.1537
Train_acc: 0.9643
Epoch: 643--------
Loss: 0.1533
Train_acc: 0.9643
Epoch: 644--------
Loss: 0.1529
Train_acc: 0.9643
Epoch: 645--------
Loss: 0.1524
Train_acc: 0.9643
Epoch: 646--------
Loss: 0.1520
Train_acc: 0.9643
Epoch: 647--------
Loss: 0.1516
Train_acc: 0.9643
Epoch: 648--------
Loss: 0.1511
Train_acc: 0.9643
Epoch: 649--------
Loss: 0.1507
Train_acc: 0.9643
Epoch: 650--------
Loss: 0.1503
Train_acc: 0.9643
Epoch: 651--------
Loss: 0.1499
Train_acc: 0.9643
Epoch: 652--------
Loss: 0.1495
Train_acc: 0.9643
Epoch: 653--------
Loss: 0.1491
Train_acc: 0.9643
Epoch: 654--------
Loss: 0.1487
Train_acc: 0.9643
Epoch: 655--------
Loss: 0.1482
Train_acc: 0.9643
Epoch: 656--------
Loss: 0.1478
Train_acc: 0.9643
Epoch: 657--------
Loss: 0.1474
Train_acc: 0.9643
Epoch: 658--------
Loss: 0.1470
Train_acc: 0.9643
Epoch: 659--------
Loss: 0.1466
Train_acc: 0.9643
Epoch: 660--------
Loss: 0.1462
Train_acc: 0.9643
Test_acc: 1.0000
Epoch: 661--------
Loss: 0.1458
Train_acc: 0.9643
Epoch: 662--------
Loss: 0.1454
Train_acc: 0.9643
Epoch: 663--------
Loss: 0.1451
Train_acc: 0.9643
Epoch: 664--------
Loss: 0.1447
Train_acc: 0.9643
Epoch: 665--------
Loss: 0.1443
Train_acc: 0.9643
Epoch: 666--------
Loss: 0.1439
Train_acc: 0.9643
Epoch: 667--------
Loss: 0.1435
Train_acc: 0.9643
Epoch: 668--------
Loss: 0.1431
Train_acc: 0.9643
Epoch: 669--------
Loss: 0.1428
Train_acc: 0.9643
Epoch: 670--------
Loss: 0.1424
Train_acc: 0.9643
Epoch: 671--------
Loss: 0.1420
Train_acc: 0.9643
Epoch: 672--------
Loss: 0.1416
Train_acc: 0.9643
Epoch: 673--------
Loss: 0.1413
Train_acc: 0.9643
Epoch: 674--------
Loss: 0.1409
Train_acc: 0.9643
Epoch: 675--------
Loss: 0.1405
Train_acc: 0.9643
Epoch: 676--------
Loss: 0.1402
Train_acc: 0.9643
Epoch: 677--------
Loss: 0.1398
Train_acc: 0.9643
Epoch: 678--------
Loss: 0.1395
Train_acc: 0.9643
Epoch: 679--------
Loss: 0.1391
Train_acc: 0.9643
Epoch: 680--------
Loss: 0.1387
Train_acc: 0.9643
Test_acc: 1.0000
Epoch: 681--------
Loss: 0.1384
Train_acc: 0.9643
Epoch: 682--------
Loss: 0.1380
Train_acc: 0.9643
Epoch: 683--------
Loss: 0.1377
Train_acc: 0.9643
Epoch: 684--------
Loss: 0.1373
Train_acc: 0.9643
Epoch: 685--------
Loss: 0.1370
Train_acc: 0.9643
Epoch: 686--------
Loss: 0.1367
Train_acc: 0.9643
Epoch: 687--------
Loss: 0.1363
Train_acc: 0.9643
Epoch: 688--------
Loss: 0.1360
Train_acc: 0.9643
Epoch: 689--------
Loss: 0.1356
Train_acc: 0.9643
Epoch: 690--------
Loss: 0.1353
Train_acc: 0.9643
Epoch: 691--------
Loss: 0.1350
Train_acc: 0.9643
Epoch: 692--------
Loss: 0.1346
Train_acc: 0.9643
Epoch: 693--------
Loss: 0.1343
Train_acc: 0.9643
Epoch: 694--------
Loss: 0.1340
Train_acc: 0.9643
Epoch: 695--------
Loss: 0.1336
Train_acc: 0.9643
Epoch: 696--------
Loss: 0.1333
Train_acc: 0.9643
Epoch: 697--------
Loss: 0.1330
Train_acc: 0.9643
Epoch: 698--------
Loss: 0.1327
Train_acc: 0.9643
Epoch: 699--------
Loss: 0.1323
Train_acc: 0.9643
Epoch: 700--------
Loss: 0.1320
Train_acc: 0.9643
Test_acc: 1.0000
Epoch: 701--------
Loss: 0.1317
Train_acc: 0.9643
Epoch: 702--------
Loss: 0.1314
Train_acc: 0.9643
Epoch: 703--------
Loss: 0.1311
Train_acc: 0.9643
Epoch: 704--------
Loss: 0.1308
Train_acc: 0.9643
Epoch: 705--------
Loss: 0.1304
Train_acc: 0.9643
Epoch: 706--------
Loss: 0.1301
Train_acc: 0.9643
Epoch: 707--------
Loss: 0.1298
Train_acc: 0.9643
Epoch: 708--------
Loss: 0.1295
Train_acc: 0.9643
Epoch: 709--------
Loss: 0.1292
Train_acc: 0.9643
Epoch: 710--------
Loss: 0.1289
Train_acc: 0.9643
Epoch: 711--------
Loss: 0.1286
Train_acc: 0.9643
Epoch: 712--------
Loss: 0.1283
Train_acc: 0.9643
Epoch: 713--------
Loss: 0.1280
Train_acc: 0.9643
Epoch: 714--------
Loss: 0.1277
Train_acc: 0.9643
Epoch: 715--------
Loss: 0.1274
Train_acc: 0.9643
Epoch: 716--------
Loss: 0.1271
Train_acc: 0.9643
Epoch: 717--------
Loss: 0.1268
Train_acc: 0.9643
Epoch: 718--------
Loss: 0.1265
Train_acc: 0.9643
Epoch: 719--------
Loss: 0.1263
Train_acc: 0.9643
Epoch: 720--------
Loss: 0.1260
Train_acc: 0.9643
Test_acc: 1.0000
Epoch: 721--------
Loss: 0.1257
Train_acc: 0.9643
Epoch: 722--------
Loss: 0.1254
Train_acc: 0.9643
Epoch: 723--------
Loss: 0.1251
Train_acc: 0.9643
Epoch: 724--------
Loss: 0.1248
Train_acc: 0.9643
Epoch: 725--------
Loss: 0.1245
Train_acc: 0.9643
Epoch: 726--------
Loss: 0.1243
Train_acc: 0.9643
Epoch: 727--------
Loss: 0.1240
Train_acc: 0.9643
Epoch: 728--------
Loss: 0.1237
Train_acc: 0.9643
Epoch: 729--------
Loss: 0.1234
Train_acc: 0.9643
Epoch: 730--------
Loss: 0.1232
Train_acc: 0.9643
Epoch: 731--------
Loss: 0.1229
Train_acc: 0.9643
Epoch: 732--------
Loss: 0.1226
Train_acc: 0.9643
Epoch: 733--------
Loss: 0.1223
Train_acc: 0.9643
Epoch: 734--------
Loss: 0.1221
Train_acc: 0.9643
Epoch: 735--------
Loss: 0.1218
Train_acc: 0.9643
Epoch: 736--------
Loss: 0.1215
Train_acc: 0.9643
Epoch: 737--------
Loss: 0.1213
Train_acc: 0.9643
Epoch: 738--------
Loss: 0.1210
Train_acc: 0.9643
Epoch: 739--------
Loss: 0.1207
Train_acc: 0.9643
Epoch: 740--------
Loss: 0.1205
Train_acc: 0.9643
Test_acc: 1.0000
Epoch: 741--------
Loss: 0.1202
Train_acc: 0.9643
Epoch: 742--------
Loss: 0.1200
Train_acc: 0.9643
Epoch: 743--------
Loss: 0.1197
Train_acc: 0.9643
Epoch: 744--------
Loss: 0.1194
Train_acc: 0.9643
Epoch: 745--------
Loss: 0.1192
Train_acc: 0.9643
Epoch: 746--------
Loss: 0.1189
Train_acc: 0.9643
Epoch: 747--------
Loss: 0.1187
Train_acc: 0.9643
Epoch: 748--------
Loss: 0.1184
Train_acc: 0.9643
Epoch: 749--------
Loss: 0.1182
Train_acc: 0.9643
Epoch: 750--------
Loss: 0.1179
Train_acc: 0.9643
Epoch: 751--------
Loss: 0.1177
Train_acc: 0.9643
Epoch: 752--------
Loss: 0.1174
Train_acc: 0.9643
Epoch: 753--------
Loss: 0.1172
Train_acc: 0.9643
Epoch: 754--------
Loss: 0.1169
Train_acc: 0.9643
Epoch: 755--------
Loss: 0.1167
Train_acc: 0.9643
Epoch: 756--------
Loss: 0.1165
Train_acc: 0.9643
Epoch: 757--------
Loss: 0.1162
Train_acc: 0.9643
Epoch: 758--------
Loss: 0.1160
Train_acc: 0.9643
Epoch: 759--------
Loss: 0.1157
Train_acc: 0.9643
Epoch: 760--------
Loss: 0.1155
Train_acc: 0.9643
Test_acc: 1.0000
Epoch: 761--------
Loss: 0.1153
Train_acc: 0.9643
Epoch: 762--------
Loss: 0.1150
Train_acc: 0.9643
Epoch: 763--------
Loss: 0.1148
Train_acc: 0.9643
Epoch: 764--------
Loss: 0.1146
Train_acc: 0.9643
Epoch: 765--------
Loss: 0.1143
Train_acc: 0.9643
Epoch: 766--------
Loss: 0.1141
Train_acc: 0.9643
Epoch: 767--------
Loss: 0.1139
Train_acc: 0.9643
Epoch: 768--------
Loss: 0.1136
Train_acc: 0.9643
Epoch: 769--------
Loss: 0.1134
Train_acc: 0.9643
Epoch: 770--------
Loss: 0.1132
Train_acc: 0.9643
Epoch: 771--------
Loss: 0.1130
Train_acc: 0.9643
Epoch: 772--------
Loss: 0.1127
Train_acc: 0.9643
Epoch: 773--------
Loss: 0.1125
Train_acc: 0.9643
Epoch: 774--------
Loss: 0.1123
Train_acc: 0.9643
Epoch: 775--------
Loss: 0.1121
Train_acc: 0.9643
Epoch: 776--------
Loss: 0.1118
Train_acc: 0.9643
Epoch: 777--------
Loss: 0.1116
Train_acc: 0.9643
Epoch: 778--------
Loss: 0.1114
Train_acc: 0.9643
Epoch: 779--------
Loss: 0.1112
Train_acc: 0.9643
Epoch: 780--------
Loss: 0.1110
Train_acc: 0.9643
Test_acc: 1.0000
Epoch: 781--------
Loss: 0.1107
Train_acc: 0.9643
Epoch: 782--------
Loss: 0.1105
Train_acc: 0.9643
Epoch: 783--------
Loss: 0.1103
Train_acc: 0.9643
Epoch: 784--------
Loss: 0.1101
Train_acc: 0.9643
Epoch: 785--------
Loss: 0.1099
Train_acc: 0.9643
Epoch: 786--------
Loss: 0.1097
Train_acc: 0.9643
Epoch: 787--------
Loss: 0.1095
Train_acc: 0.9643
Epoch: 788--------
Loss: 0.1093
Train_acc: 0.9643
Epoch: 789--------
Loss: 0.1090
Train_acc: 0.9643
Epoch: 790--------
Loss: 0.1088
Train_acc: 0.9643
Epoch: 791--------
Loss: 0.1086
Train_acc: 0.9643
Epoch: 792--------
Loss: 0.1084
Train_acc: 0.9643
Epoch: 793--------
Loss: 0.1082
Train_acc: 0.9643
Epoch: 794--------
Loss: 0.1080
Train_acc: 0.9643
Epoch: 795--------
Loss: 0.1078
Train_acc: 0.9643
Epoch: 796--------
Loss: 0.1076
Train_acc: 0.9643
Epoch: 797--------
Loss: 0.1074
Train_acc: 0.9643
Epoch: 798--------
Loss: 0.1072
Train_acc: 0.9643
Epoch: 799--------
Loss: 0.1070
Train_acc: 0.9643
Epoch: 800--------
Loss: 0.1068
Train_acc: 0.9643
Test_acc: 1.0000
Epoch: 801--------
Loss: 0.1066
Train_acc: 0.9643
Epoch: 802--------
Loss: 0.1064
Train_acc: 0.9643
Epoch: 803--------
Loss: 0.1062
Train_acc: 0.9643
Epoch: 804--------
Loss: 0.1060
Train_acc: 0.9643
Epoch: 805--------
Loss: 0.1058
Train_acc: 0.9643
Epoch: 806--------
Loss: 0.1056
Train_acc: 0.9643
Epoch: 807--------
Loss: 0.1054
Train_acc: 0.9643
Epoch: 808--------
Loss: 0.1052
Train_acc: 0.9643
Epoch: 809--------
Loss: 0.1050
Train_acc: 0.9643
Epoch: 810--------
Loss: 0.1049
Train_acc: 0.9643
Epoch: 811--------
Loss: 0.1047
Train_acc: 0.9643
Epoch: 812--------
Loss: 0.1045
Train_acc: 0.9643
Epoch: 813--------
Loss: 0.1043
Train_acc: 0.9643
Epoch: 814--------
Loss: 0.1041
Train_acc: 0.9643
Epoch: 815--------
Loss: 0.1039
Train_acc: 0.9643
Epoch: 816--------
Loss: 0.1037
Train_acc: 0.9643
Epoch: 817--------
Loss: 0.1035
Train_acc: 0.9643
Epoch: 818--------
Loss: 0.1033
Train_acc: 0.9643
Epoch: 819--------
Loss: 0.1032
Train_acc: 0.9643
Epoch: 820--------
Loss: 0.1030
Train_acc: 0.9643
Test_acc: 1.0000
Epoch: 821--------
Loss: 0.1028
Train_acc: 0.9643
Epoch: 822--------
Loss: 0.1026
Train_acc: 0.9643
Epoch: 823--------
Loss: 0.1024
Train_acc: 0.9643
Epoch: 824--------
Loss: 0.1023
Train_acc: 0.9643
Epoch: 825--------
Loss: 0.1021
Train_acc: 0.9643
Epoch: 826--------
Loss: 0.1019
Train_acc: 0.9643
Epoch: 827--------
Loss: 0.1017
Train_acc: 0.9643
Epoch: 828--------
Loss: 0.1015
Train_acc: 0.9643
Epoch: 829--------
Loss: 0.1014
Train_acc: 0.9643
Epoch: 830--------
Loss: 0.1012
Train_acc: 0.9643
Epoch: 831--------
Loss: 0.1010
Train_acc: 0.9643
Epoch: 832--------
Loss: 0.1008
Train_acc: 0.9643
Epoch: 833--------
Loss: 0.1007
Train_acc: 0.9643
Epoch: 834--------
Loss: 0.1005
Train_acc: 0.9643
Epoch: 835--------
Loss: 0.1003
Train_acc: 0.9643
Epoch: 836--------
Loss: 0.1001
Train_acc: 0.9643
Epoch: 837--------
Loss: 0.1000
Train_acc: 0.9643
Epoch: 838--------
Loss: 0.0998
Train_acc: 0.9643
Epoch: 839--------
Loss: 0.0996
Train_acc: 0.9643
Epoch: 840--------
Loss: 0.0995
Train_acc: 0.9643
Test_acc: 1.0000
Epoch: 841--------
Loss: 0.0993
Train_acc: 0.9643
Epoch: 842--------
Loss: 0.0991
Train_acc: 0.9643
Epoch: 843--------
Loss: 0.0990
Train_acc: 0.9643
Epoch: 844--------
Loss: 0.0988
Train_acc: 0.9643
Epoch: 845--------
Loss: 0.0986
Train_acc: 0.9643
Epoch: 846--------
Loss: 0.0985
Train_acc: 0.9643
Epoch: 847--------
Loss: 0.0983
Train_acc: 0.9643
Epoch: 848--------
Loss: 0.0981
Train_acc: 0.9643
Epoch: 849--------
Loss: 0.0980
Train_acc: 0.9643
Epoch: 850--------
Loss: 0.0978
Train_acc: 0.9643
Epoch: 851--------
Loss: 0.0976
Train_acc: 0.9643
Epoch: 852--------
Loss: 0.0975
Train_acc: 0.9643
Epoch: 853--------
Loss: 0.0973
Train_acc: 0.9643
Epoch: 854--------
Loss: 0.0972
Train_acc: 0.9643
Epoch: 855--------
Loss: 0.0970
Train_acc: 0.9643
Epoch: 856--------
Loss: 0.0968
Train_acc: 0.9643
Epoch: 857--------
Loss: 0.0967
Train_acc: 0.9643
Epoch: 858--------
Loss: 0.0965
Train_acc: 0.9643
Epoch: 859--------
Loss: 0.0964
Train_acc: 0.9643
Epoch: 860--------
Loss: 0.0962
Train_acc: 0.9643
Test_acc: 1.0000
Epoch: 861--------
Loss: 0.0961
Train_acc: 0.9643
Epoch: 862--------
Loss: 0.0959
Train_acc: 0.9643
Epoch: 863--------
Loss: 0.0957
Train_acc: 0.9643
Epoch: 864--------
Loss: 0.0956
Train_acc: 0.9643
Epoch: 865--------
Loss: 0.0954
Train_acc: 0.9643
Epoch: 866--------
Loss: 0.0953
Train_acc: 0.9643
Epoch: 867--------
Loss: 0.0951
Train_acc: 0.9643
Epoch: 868--------
Loss: 0.0950
Train_acc: 0.9643
Epoch: 869--------
Loss: 0.0948
Train_acc: 0.9643
Epoch: 870--------
Loss: 0.0947
Train_acc: 0.9643
Epoch: 871--------
Loss: 0.0945
Train_acc: 0.9643
Epoch: 872--------
Loss: 0.0944
Train_acc: 0.9643
Epoch: 873--------
Loss: 0.0942
Train_acc: 0.9643
Epoch: 874--------
Loss: 0.0941
Train_acc: 0.9643
Epoch: 875--------
Loss: 0.0939
Train_acc: 0.9643
Epoch: 876--------
Loss: 0.0938
Train_acc: 0.9643
Epoch: 877--------
Loss: 0.0936
Train_acc: 0.9643
Epoch: 878--------
Loss: 0.0935
Train_acc: 0.9643
Epoch: 879--------
Loss: 0.0933
Train_acc: 0.9643
Epoch: 880--------
Loss: 0.0932
Train_acc: 0.9643
Test_acc: 1.0000
Epoch: 881--------
Loss: 0.0930
Train_acc: 0.9643
Epoch: 882--------
Loss: 0.0929
Train_acc: 0.9643
Epoch: 883--------
Loss: 0.0928
Train_acc: 0.9643
Epoch: 884--------
Loss: 0.0926
Train_acc: 0.9643
Epoch: 885--------
Loss: 0.0925
Train_acc: 0.9643
Epoch: 886--------
Loss: 0.0923
Train_acc: 0.9643
Epoch: 887--------
Loss: 0.0922
Train_acc: 0.9643
Epoch: 888--------
Loss: 0.0920
Train_acc: 0.9643
Epoch: 889--------
Loss: 0.0919
Train_acc: 0.9643
Epoch: 890--------
Loss: 0.0918
Train_acc: 0.9643
Epoch: 891--------
Loss: 0.0916
Train_acc: 0.9643
Epoch: 892--------
Loss: 0.0915
Train_acc: 0.9643
Epoch: 893--------
Loss: 0.0913
Train_acc: 0.9643
Epoch: 894--------
Loss: 0.0912
Train_acc: 0.9643
Epoch: 895--------
Loss: 0.0911
Train_acc: 0.9643
Epoch: 896--------
Loss: 0.0909
Train_acc: 0.9643
Epoch: 897--------
Loss: 0.0908
Train_acc: 0.9643
Epoch: 898--------
Loss: 0.0907
Train_acc: 0.9643
Epoch: 899--------
Loss: 0.0905
Train_acc: 0.9643
Epoch: 900--------
Loss: 0.0904
Train_acc: 0.9643
Test_acc: 1.0000
Epoch: 901--------
Loss: 0.0902
Train_acc: 0.9643
Epoch: 902--------
Loss: 0.0901
Train_acc: 0.9643
Epoch: 903--------
Loss: 0.0900
Train_acc: 0.9643
Epoch: 904--------
Loss: 0.0898
Train_acc: 0.9643
Epoch: 905--------
Loss: 0.0897
Train_acc: 0.9643
Epoch: 906--------
Loss: 0.0896
Train_acc: 0.9643
Epoch: 907--------
Loss: 0.0894
Train_acc: 0.9643
Epoch: 908--------
Loss: 0.0893
Train_acc: 0.9643
Epoch: 909--------
Loss: 0.0892
Train_acc: 0.9643
Epoch: 910--------
Loss: 0.0891
Train_acc: 0.9643
Epoch: 911--------
Loss: 0.0889
Train_acc: 0.9643
Epoch: 912--------
Loss: 0.0888
Train_acc: 0.9643
Epoch: 913--------
Loss: 0.0887
Train_acc: 0.9643
Epoch: 914--------
Loss: 0.0885
Train_acc: 0.9643
Epoch: 915--------
Loss: 0.0884
Train_acc: 0.9643
Epoch: 916--------
Loss: 0.0883
Train_acc: 0.9643
Epoch: 917--------
Loss: 0.0881
Train_acc: 0.9643
Epoch: 918--------
Loss: 0.0880
Train_acc: 0.9643
Epoch: 919--------
Loss: 0.0879
Train_acc: 0.9643
Epoch: 920--------
Loss: 0.0878
Train_acc: 0.9643
Test_acc: 1.0000
Epoch: 921--------
Loss: 0.0876
Train_acc: 0.9643
Epoch: 922--------
Loss: 0.0875
Train_acc: 0.9643
Epoch: 923--------
Loss: 0.0874
Train_acc: 0.9643
Epoch: 924--------
Loss: 0.0873
Train_acc: 0.9643
Epoch: 925--------
Loss: 0.0871
Train_acc: 0.9643
Epoch: 926--------
Loss: 0.0870
Train_acc: 0.9643
Epoch: 927--------
Loss: 0.0869
Train_acc: 0.9643
Epoch: 928--------
Loss: 0.0868
Train_acc: 0.9643
Epoch: 929--------
Loss: 0.0866
Train_acc: 0.9643
Epoch: 930--------
Loss: 0.0865
Train_acc: 0.9643
Epoch: 931--------
Loss: 0.0864
Train_acc: 0.9643
Epoch: 932--------
Loss: 0.0863
Train_acc: 0.9643
Epoch: 933--------
Loss: 0.0862
Train_acc: 0.9643
Epoch: 934--------
Loss: 0.0860
Train_acc: 0.9643
Epoch: 935--------
Loss: 0.0859
Train_acc: 0.9643
Epoch: 936--------
Loss: 0.0858
Train_acc: 0.9643
Epoch: 937--------
Loss: 0.0857
Train_acc: 0.9643
Epoch: 938--------
Loss: 0.0856
Train_acc: 0.9643
Epoch: 939--------
Loss: 0.0854
Train_acc: 0.9643
Epoch: 940--------
Loss: 0.0853
Train_acc: 0.9643
Test_acc: 1.0000
Epoch: 941--------
Loss: 0.0852
Train_acc: 0.9643
Epoch: 942--------
Loss: 0.0851
Train_acc: 0.9643
Epoch: 943--------
Loss: 0.0850
Train_acc: 0.9643
Epoch: 944--------
Loss: 0.0848
Train_acc: 0.9643
Epoch: 945--------
Loss: 0.0847
Train_acc: 0.9643
Epoch: 946--------
Loss: 0.0846
Train_acc: 0.9643
Epoch: 947--------
Loss: 0.0845
Train_acc: 0.9643
Epoch: 948--------
Loss: 0.0844
Train_acc: 0.9643
Epoch: 949--------
Loss: 0.0843
Train_acc: 0.9643
Epoch: 950--------
Loss: 0.0842
Train_acc: 0.9643
Epoch: 951--------
Loss: 0.0840
Train_acc: 0.9643
Epoch: 952--------
Loss: 0.0839
Train_acc: 0.9643
Epoch: 953--------
Loss: 0.0838
Train_acc: 0.9643
Epoch: 954--------
Loss: 0.0837
Train_acc: 0.9643
Epoch: 955--------
Loss: 0.0836
Train_acc: 0.9643
Epoch: 956--------
Loss: 0.0835
Train_acc: 0.9643
Epoch: 957--------
Loss: 0.0834
Train_acc: 0.9643
Epoch: 958--------
Loss: 0.0833
Train_acc: 0.9643
Epoch: 959--------
Loss: 0.0831
Train_acc: 0.9643
Epoch: 960--------
Loss: 0.0830
Train_acc: 0.9643
Test_acc: 1.0000
Epoch: 961--------
Loss: 0.0829
Train_acc: 0.9643
Epoch: 962--------
Loss: 0.0828
Train_acc: 0.9643
Epoch: 963--------
Loss: 0.0827
Train_acc: 0.9643
Epoch: 964--------
Loss: 0.0826
Train_acc: 0.9643
Epoch: 965--------
Loss: 0.0825
Train_acc: 0.9643
Epoch: 966--------
Loss: 0.0824
Train_acc: 0.9643
Epoch: 967--------
Loss: 0.0823
Train_acc: 0.9643
Epoch: 968--------
Loss: 0.0822
Train_acc: 0.9643
Epoch: 969--------
Loss: 0.0820
Train_acc: 0.9643
Epoch: 970--------
Loss: 0.0819
Train_acc: 0.9643
Epoch: 971--------
Loss: 0.0818
Train_acc: 0.9643
Epoch: 972--------
Loss: 0.0817
Train_acc: 0.9643
Epoch: 973--------
Loss: 0.0816
Train_acc: 0.9643
Epoch: 974--------
Loss: 0.0815
Train_acc: 0.9643
Epoch: 975--------
Loss: 0.0814
Train_acc: 0.9643
Epoch: 976--------
Loss: 0.0813
Train_acc: 0.9643
Epoch: 977--------
Loss: 0.0812
Train_acc: 0.9643
Epoch: 978--------
Loss: 0.0811
Train_acc: 0.9643
Epoch: 979--------
Loss: 0.0810
Train_acc: 0.9643
Epoch: 980--------
Loss: 0.0809
Train_acc: 0.9643
Test_acc: 1.0000
Epoch: 981--------
Loss: 0.0808
Train_acc: 0.9643
Epoch: 982--------
Loss: 0.0807
Train_acc: 0.9643
Epoch: 983--------
Loss: 0.0806
Train_acc: 0.9643
Epoch: 984--------
Loss: 0.0805
Train_acc: 0.9643
Epoch: 985--------
Loss: 0.0804
Train_acc: 0.9643
Epoch: 986--------
Loss: 0.0803
Train_acc: 0.9643
Epoch: 987--------
Loss: 0.0802
Train_acc: 0.9643
Epoch: 988--------
Loss: 0.0800
Train_acc: 0.9643
Epoch: 989--------
Loss: 0.0799
Train_acc: 0.9643
Epoch: 990--------
Loss: 0.0798
Train_acc: 0.9643
Epoch: 991--------
Loss: 0.0797
Train_acc: 0.9643
Epoch: 992--------
Loss: 0.0796
Train_acc: 0.9643
Epoch: 993--------
Loss: 0.0795
Train_acc: 0.9643
Epoch: 994--------
Loss: 0.0794
Train_acc: 0.9643
Epoch: 995--------
Loss: 0.0793
Train_acc: 0.9643
Epoch: 996--------
Loss: 0.0792
Train_acc: 0.9643
Epoch: 997--------
Loss: 0.0791
Train_acc: 0.9643
Epoch: 998--------
Loss: 0.0790
Train_acc: 0.9643
Epoch: 999--------
Loss: 0.0789
Train_acc: 0.9643
Epoch: 1000--------
Loss: 0.0788
Train_acc: 0.9643
Test_acc: 1.0000
Epoch: 1001--------
Loss: 0.0787
Train_acc: 0.9643
Epoch: 1002--------
Loss: 0.0787
Train_acc: 0.9643
Epoch: 1003--------
Loss: 0.0786
Train_acc: 0.9643
Epoch: 1004--------
Loss: 0.0785
Train_acc: 0.9643
Epoch: 1005--------
Loss: 0.0784
Train_acc: 0.9643
Epoch: 1006--------
Loss: 0.0783
Train_acc: 0.9643
Epoch: 1007--------
Loss: 0.0782
Train_acc: 0.9643
Epoch: 1008--------
Loss: 0.0781
Train_acc: 0.9643
Epoch: 1009--------
Loss: 0.0780
Train_acc: 0.9643
Epoch: 1010--------
Loss: 0.0779
Train_acc: 0.9643
Epoch: 1011--------
Loss: 0.0778
Train_acc: 0.9643
Epoch: 1012--------
Loss: 0.0777
Train_acc: 0.9643
Epoch: 1013--------
Loss: 0.0776
Train_acc: 0.9643
Epoch: 1014--------
Loss: 0.0775
Train_acc: 0.9643
Epoch: 1015--------
Loss: 0.0774
Train_acc: 0.9643
Epoch: 1016--------
Loss: 0.0773
Train_acc: 0.9643
Epoch: 1017--------
Loss: 0.0772
Train_acc: 0.9643
Epoch: 1018--------
Loss: 0.0771
Train_acc: 0.9643
Epoch: 1019--------
Loss: 0.0770
Train_acc: 0.9643
Epoch: 1020--------
Loss: 0.0769
Train_acc: 0.9643
Test_acc: 1.0000
Epoch: 1021--------
Loss: 0.0768
Train_acc: 0.9643
Epoch: 1022--------
Loss: 0.0768
Train_acc: 0.9643
Epoch: 1023--------
Loss: 0.0767
Train_acc: 0.9643
Epoch: 1024--------
Loss: 0.0766
Train_acc: 0.9643
Epoch: 1025--------
Loss: 0.0765
Train_acc: 0.9643
Epoch: 1026--------
Loss: 0.0764
Train_acc: 0.9643
Epoch: 1027--------
Loss: 0.0763
Train_acc: 0.9643
Epoch: 1028--------
Loss: 0.0762
Train_acc: 0.9643
Epoch: 1029--------
Loss: 0.0761
Train_acc: 0.9643
Epoch: 1030--------
Loss: 0.0760
Train_acc: 0.9643
Epoch: 1031--------
Loss: 0.0759
Train_acc: 0.9643
Epoch: 1032--------
Loss: 0.0758
Train_acc: 0.9643
Epoch: 1033--------
Loss: 0.0758
Train_acc: 0.9643
Epoch: 1034--------
Loss: 0.0757
Train_acc: 0.9643
Epoch: 1035--------
Loss: 0.0756
Train_acc: 0.9643
Epoch: 1036--------
Loss: 0.0755
Train_acc: 0.9643
Epoch: 1037--------
Loss: 0.0754
Train_acc: 0.9643
Epoch: 1038--------
Loss: 0.0753
Train_acc: 0.9643
Epoch: 1039--------
Loss: 0.0752
Train_acc: 0.9643
Epoch: 1040--------
Loss: 0.0751
Train_acc: 0.9643
Test_acc: 1.0000
Epoch: 1041--------
Loss: 0.0750
Train_acc: 0.9643
Epoch: 1042--------
Loss: 0.0750
Train_acc: 0.9643
Epoch: 1043--------
Loss: 0.0749
Train_acc: 0.9643
Epoch: 1044--------
Loss: 0.0748
Train_acc: 0.9643
Epoch: 1045--------
Loss: 0.0747
Train_acc: 0.9643
Epoch: 1046--------
Loss: 0.0746
Train_acc: 0.9643
Epoch: 1047--------
Loss: 0.0745
Train_acc: 0.9643
Epoch: 1048--------
Loss: 0.0744
Train_acc: 0.9643
Epoch: 1049--------
Loss: 0.0743
Train_acc: 0.9643
Epoch: 1050--------
Loss: 0.0743
Train_acc: 0.9643
Epoch: 1051--------
Loss: 0.0742
Train_acc: 0.9643
Epoch: 1052--------
Loss: 0.0741
Train_acc: 0.9643
Epoch: 1053--------
Loss: 0.0740
Train_acc: 0.9643
Epoch: 1054--------
Loss: 0.0739
Train_acc: 0.9643
Epoch: 1055--------
Loss: 0.0738
Train_acc: 0.9643
Epoch: 1056--------
Loss: 0.0738
Train_acc: 0.9643
Epoch: 1057--------
Loss: 0.0737
Train_acc: 0.9643
Epoch: 1058--------
Loss: 0.0736
Train_acc: 0.9643
Epoch: 1059--------
Loss: 0.0735
Train_acc: 0.9643
Epoch: 1060--------
Loss: 0.0734
Train_acc: 0.9643
Test_acc: 1.0000
Epoch: 1061--------
Loss: 0.0733
Train_acc: 0.9643
Epoch: 1062--------
Loss: 0.0733
Train_acc: 0.9643
Epoch: 1063--------
Loss: 0.0732
Train_acc: 0.9643
Epoch: 1064--------
Loss: 0.0731
Train_acc: 0.9643
Epoch: 1065--------
Loss: 0.0730
Train_acc: 0.9643
Epoch: 1066--------
Loss: 0.0729
Train_acc: 0.9643
Epoch: 1067--------
Loss: 0.0728
Train_acc: 0.9643
Epoch: 1068--------
Loss: 0.0728
Train_acc: 0.9643
Epoch: 1069--------
Loss: 0.0727
Train_acc: 0.9643
Epoch: 1070--------
Loss: 0.0726
Train_acc: 0.9643
Epoch: 1071--------
Loss: 0.0725
Train_acc: 0.9643
Epoch: 1072--------
Loss: 0.0724
Train_acc: 0.9643
Epoch: 1073--------
Loss: 0.0724
Train_acc: 0.9643
Epoch: 1074--------
Loss: 0.0723
Train_acc: 0.9643
Epoch: 1075--------
Loss: 0.0722
Train_acc: 0.9643
Epoch: 1076--------
Loss: 0.0721
Train_acc: 0.9643
Epoch: 1077--------
Loss: 0.0720
Train_acc: 0.9643
Epoch: 1078--------
Loss: 0.0720
Train_acc: 0.9643
Epoch: 1079--------
Loss: 0.0719
Train_acc: 0.9643
Epoch: 1080--------
Loss: 0.0718
Train_acc: 0.9643
Test_acc: 1.0000
Epoch: 1081--------
Loss: 0.0717
Train_acc: 0.9643
Epoch: 1082--------
Loss: 0.0716
Train_acc: 0.9643
Epoch: 1083--------
Loss: 0.0716
Train_acc: 0.9643
Epoch: 1084--------
Loss: 0.0715
Train_acc: 0.9643
Epoch: 1085--------
Loss: 0.0714
Train_acc: 0.9643
Epoch: 1086--------
Loss: 0.0713
Train_acc: 0.9643
Epoch: 1087--------
Loss: 0.0712
Train_acc: 0.9643
Epoch: 1088--------
Loss: 0.0712
Train_acc: 0.9643
Epoch: 1089--------
Loss: 0.0711
Train_acc: 0.9643
Epoch: 1090--------
Loss: 0.0710
Train_acc: 0.9643
Epoch: 1091--------
Loss: 0.0709
Train_acc: 0.9643
Epoch: 1092--------
Loss: 0.0709
Train_acc: 0.9643
Epoch: 1093--------
Loss: 0.0708
Train_acc: 0.9643
Epoch: 1094--------
Loss: 0.0707
Train_acc: 0.9643
Epoch: 1095--------
Loss: 0.0706
Train_acc: 0.9643
Epoch: 1096--------
Loss: 0.0706
Train_acc: 0.9643
Epoch: 1097--------
Loss: 0.0705
Train_acc: 0.9643
Epoch: 1098--------
Loss: 0.0704
Train_acc: 0.9643
Epoch: 1099--------
Loss: 0.0703
Train_acc: 0.9643
Epoch: 1100--------
Loss: 0.0703
Train_acc: 0.9643
Test_acc: 1.0000
Epoch: 1101--------
Loss: 0.0702
Train_acc: 0.9643
Epoch: 1102--------
Loss: 0.0701
Train_acc: 0.9643
Epoch: 1103--------
Loss: 0.0700
Train_acc: 0.9643
Epoch: 1104--------
Loss: 0.0700
Train_acc: 0.9643
Epoch: 1105--------
Loss: 0.0699
Train_acc: 0.9643
Epoch: 1106--------
Loss: 0.0698
Train_acc: 0.9643
Epoch: 1107--------
Loss: 0.0697
Train_acc: 0.9643
Epoch: 1108--------
Loss: 0.0697
Train_acc: 0.9643
Epoch: 1109--------
Loss: 0.0696
Train_acc: 0.9643
Epoch: 1110--------
Loss: 0.0695
Train_acc: 0.9643
Epoch: 1111--------
Loss: 0.0694
Train_acc: 0.9643
Epoch: 1112--------
Loss: 0.0694
Train_acc: 0.9643
Epoch: 1113--------
Loss: 0.0693
Train_acc: 0.9643
Epoch: 1114--------
Loss: 0.0692
Train_acc: 0.9643
Epoch: 1115--------
Loss: 0.0691
Train_acc: 0.9643
Epoch: 1116--------
Loss: 0.0691
Train_acc: 0.9643
Epoch: 1117--------
Loss: 0.0690
Train_acc: 0.9643
Epoch: 1118--------
Loss: 0.0689
Train_acc: 0.9643
Epoch: 1119--------
Loss: 0.0689
Train_acc: 0.9643
Epoch: 1120--------
Loss: 0.0688
Train_acc: 0.9643
Test_acc: 1.0000
Epoch: 1121--------
Loss: 0.0687
Train_acc: 0.9643
Epoch: 1122--------
Loss: 0.0686
Train_acc: 0.9643
Epoch: 1123--------
Loss: 0.0686
Train_acc: 0.9643
Epoch: 1124--------
Loss: 0.0685
Train_acc: 0.9643
Epoch: 1125--------
Loss: 0.0684
Train_acc: 0.9643
Epoch: 1126--------
Loss: 0.0684
Train_acc: 0.9643
Epoch: 1127--------
Loss: 0.0683
Train_acc: 0.9643
Epoch: 1128--------
Loss: 0.0682
Train_acc: 0.9643
Epoch: 1129--------
Loss: 0.0681
Train_acc: 0.9643
Epoch: 1130--------
Loss: 0.0681
Train_acc: 0.9643
Epoch: 1131--------
Loss: 0.0680
Train_acc: 0.9643
Epoch: 1132--------
Loss: 0.0679
Train_acc: 0.9643
Epoch: 1133--------
Loss: 0.0679
Train_acc: 0.9643
Epoch: 1134--------
Loss: 0.0678
Train_acc: 0.9643
Epoch: 1135--------
Loss: 0.0677
Train_acc: 0.9643
Epoch: 1136--------
Loss: 0.0677
Train_acc: 0.9643
Epoch: 1137--------
Loss: 0.0676
Train_acc: 0.9643
Epoch: 1138--------
Loss: 0.0675
Train_acc: 0.9643
Epoch: 1139--------
Loss: 0.0674
Train_acc: 0.9643
Epoch: 1140--------
Loss: 0.0674
Train_acc: 0.9643
Test_acc: 1.0000
Epoch: 1141--------
Loss: 0.0673
Train_acc: 0.9643
Epoch: 1142--------
Loss: 0.0672
Train_acc: 0.9643
Epoch: 1143--------
Loss: 0.0672
Train_acc: 0.9643
Epoch: 1144--------
Loss: 0.0671
Train_acc: 0.9643
Epoch: 1145--------
Loss: 0.0670
Train_acc: 0.9643
Epoch: 1146--------
Loss: 0.0670
Train_acc: 0.9643
Epoch: 1147--------
Loss: 0.0669
Train_acc: 0.9643
Epoch: 1148--------
Loss: 0.0668
Train_acc: 0.9643
Epoch: 1149--------
Loss: 0.0668
Train_acc: 0.9643
Epoch: 1150--------
Loss: 0.0667
Train_acc: 0.9643
Epoch: 1151--------
Loss: 0.0666
Train_acc: 0.9643
Epoch: 1152--------
Loss: 0.0666
Train_acc: 0.9643
Epoch: 1153--------
Loss: 0.0665
Train_acc: 0.9643
Epoch: 1154--------
Loss: 0.0664
Train_acc: 0.9643
Epoch: 1155--------
Loss: 0.0664
Train_acc: 0.9643
Epoch: 1156--------
Loss: 0.0663
Train_acc: 0.9643
Epoch: 1157--------
Loss: 0.0662
Train_acc: 0.9643
Epoch: 1158--------
Loss: 0.0662
Train_acc: 0.9643
Epoch: 1159--------
Loss: 0.0661
Train_acc: 0.9643
Epoch: 1160--------
Loss: 0.0660
Train_acc: 0.9643
Test_acc: 1.0000
Epoch: 1161--------
Loss: 0.0660
Train_acc: 0.9643
Epoch: 1162--------
Loss: 0.0659
Train_acc: 0.9643
Epoch: 1163--------
Loss: 0.0658
Train_acc: 0.9643
Epoch: 1164--------
Loss: 0.0658
Train_acc: 0.9643
Epoch: 1165--------
Loss: 0.0657
Train_acc: 0.9643
Epoch: 1166--------
Loss: 0.0657
Train_acc: 0.9643
Epoch: 1167--------
Loss: 0.0656
Train_acc: 0.9643
Epoch: 1168--------
Loss: 0.0655
Train_acc: 0.9643
Epoch: 1169--------
Loss: 0.0655
Train_acc: 0.9643
Epoch: 1170--------
Loss: 0.0654
Train_acc: 0.9643
Epoch: 1171--------
Loss: 0.0653
Train_acc: 0.9643
Epoch: 1172--------
Loss: 0.0653
Train_acc: 0.9643
Epoch: 1173--------
Loss: 0.0652
Train_acc: 0.9643
Epoch: 1174--------
Loss: 0.0651
Train_acc: 0.9643
Epoch: 1175--------
Loss: 0.0651
Train_acc: 0.9643
Epoch: 1176--------
Loss: 0.0650
Train_acc: 0.9643
Epoch: 1177--------
Loss: 0.0650
Train_acc: 0.9643
Epoch: 1178--------
Loss: 0.0649
Train_acc: 0.9643
Epoch: 1179--------
Loss: 0.0648
Train_acc: 0.9643
Epoch: 1180--------
Loss: 0.0648
Train_acc: 0.9643
Test_acc: 1.0000
Epoch: 1181--------
Loss: 0.0647
Train_acc: 0.9643
Epoch: 1182--------
Loss: 0.0646
Train_acc: 0.9643
Epoch: 1183--------
Loss: 0.0646
Train_acc: 0.9643
Epoch: 1184--------
Loss: 0.0645
Train_acc: 0.9643
Epoch: 1185--------
Loss: 0.0645
Train_acc: 0.9643
Epoch: 1186--------
Loss: 0.0644
Train_acc: 0.9643
Epoch: 1187--------
Loss: 0.0643
Train_acc: 0.9643
Epoch: 1188--------
Loss: 0.0643
Train_acc: 0.9643
Epoch: 1189--------
Loss: 0.0642
Train_acc: 0.9643
Epoch: 1190--------
Loss: 0.0641
Train_acc: 0.9643
Epoch: 1191--------
Loss: 0.0641
Train_acc: 0.9643
Epoch: 1192--------
Loss: 0.0640
Train_acc: 0.9643
Epoch: 1193--------
Loss: 0.0640
Train_acc: 0.9643
Epoch: 1194--------
Loss: 0.0639
Train_acc: 0.9643
Epoch: 1195--------
Loss: 0.0638
Train_acc: 0.9643
Epoch: 1196--------
Loss: 0.0638
Train_acc: 0.9643
Epoch: 1197--------
Loss: 0.0637
Train_acc: 0.9643
Epoch: 1198--------
Loss: 0.0637
Train_acc: 0.9643
Epoch: 1199--------
Loss: 0.0636
Train_acc: 0.9643
Epoch: 1200--------
Loss: 0.0635
Train_acc: 0.9643
Test_acc: 1.0000
Epoch: 1201--------
Loss: 0.0635
Train_acc: 0.9643
Epoch: 1202--------
Loss: 0.0634
Train_acc: 0.9643
Epoch: 1203--------
Loss: 0.0634
Train_acc: 0.9643
Epoch: 1204--------
Loss: 0.0633
Train_acc: 0.9643
Epoch: 1205--------
Loss: 0.0632
Train_acc: 0.9643
Epoch: 1206--------
Loss: 0.0632
Train_acc: 0.9643
Epoch: 1207--------
Loss: 0.0631
Train_acc: 0.9643
Epoch: 1208--------
Loss: 0.0631
Train_acc: 0.9643
Epoch: 1209--------
Loss: 0.0630
Train_acc: 0.9643
Epoch: 1210--------
Loss: 0.0629
Train_acc: 0.9643
Epoch: 1211--------
Loss: 0.0629
Train_acc: 0.9643
Epoch: 1212--------
Loss: 0.0628
Train_acc: 0.9643
Epoch: 1213--------
Loss: 0.0628
Train_acc: 0.9643
Epoch: 1214--------
Loss: 0.0627
Train_acc: 0.9643
Epoch: 1215--------
Loss: 0.0627
Train_acc: 0.9643
Epoch: 1216--------
Loss: 0.0626
Train_acc: 0.9643
Epoch: 1217--------
Loss: 0.0625
Train_acc: 0.9643
Epoch: 1218--------
Loss: 0.0625
Train_acc: 0.9643
Epoch: 1219--------
Loss: 0.0624
Train_acc: 0.9643
Epoch: 1220--------
Loss: 0.0624
Train_acc: 0.9643
Test_acc: 1.0000
Epoch: 1221--------
Loss: 0.0623
Train_acc: 0.9643
Epoch: 1222--------
Loss: 0.0622
Train_acc: 0.9643
Epoch: 1223--------
Loss: 0.0622
Train_acc: 0.9643
Epoch: 1224--------
Loss: 0.0621
Train_acc: 0.9643
Epoch: 1225--------
Loss: 0.0621
Train_acc: 0.9643
Epoch: 1226--------
Loss: 0.0620
Train_acc: 0.9643
Epoch: 1227--------
Loss: 0.0620
Train_acc: 0.9643
Epoch: 1228--------
Loss: 0.0619
Train_acc: 0.9643
Epoch: 1229--------
Loss: 0.0619
Train_acc: 0.9643
Epoch: 1230--------
Loss: 0.0618
Train_acc: 0.9643
Epoch: 1231--------
Loss: 0.0617
Train_acc: 0.9643
Epoch: 1232--------
Loss: 0.0617
Train_acc: 0.9643
Epoch: 1233--------
Loss: 0.0616
Train_acc: 0.9643
Epoch: 1234--------
Loss: 0.0616
Train_acc: 0.9643
Epoch: 1235--------
Loss: 0.0615
Train_acc: 0.9643
Epoch: 1236--------
Loss: 0.0615
Train_acc: 0.9643
Epoch: 1237--------
Loss: 0.0614
Train_acc: 0.9643
Epoch: 1238--------
Loss: 0.0613
Train_acc: 0.9643
Epoch: 1239--------
Loss: 0.0613
Train_acc: 0.9643
Epoch: 1240--------
Loss: 0.0612
Train_acc: 0.9643
Test_acc: 1.0000
Epoch: 1241--------
Loss: 0.0612
Train_acc: 0.9643
Epoch: 1242--------
Loss: 0.0611
Train_acc: 0.9643
Epoch: 1243--------
Loss: 0.0611
Train_acc: 0.9643
Epoch: 1244--------
Loss: 0.0610
Train_acc: 0.9643
Epoch: 1245--------
Loss: 0.0610
Train_acc: 0.9643
Epoch: 1246--------
Loss: 0.0609
Train_acc: 0.9643
Epoch: 1247--------
Loss: 0.0609
Train_acc: 0.9643
Epoch: 1248--------
Loss: 0.0608
Train_acc: 0.9643
Epoch: 1249--------
Loss: 0.0607
Train_acc: 0.9643
Epoch: 1250--------
Loss: 0.0607
Train_acc: 0.9643
Epoch: 1251--------
Loss: 0.0606
Train_acc: 0.9643
Epoch: 1252--------
Loss: 0.0606
Train_acc: 0.9643
Epoch: 1253--------
Loss: 0.0605
Train_acc: 0.9643
Epoch: 1254--------
Loss: 0.0605
Train_acc: 0.9643
Epoch: 1255--------
Loss: 0.0604
Train_acc: 0.9643
Epoch: 1256--------
Loss: 0.0604
Train_acc: 0.9643
Epoch: 1257--------
Loss: 0.0603
Train_acc: 0.9643
Epoch: 1258--------
Loss: 0.0603
Train_acc: 0.9643
Epoch: 1259--------
Loss: 0.0602
Train_acc: 0.9643
Epoch: 1260--------
Loss: 0.0602
Train_acc: 0.9643
Test_acc: 1.0000
Epoch: 1261--------
Loss: 0.0601
Train_acc: 0.9643
Epoch: 1262--------
Loss: 0.0600
Train_acc: 0.9643
Epoch: 1263--------
Loss: 0.0600
Train_acc: 0.9643
Epoch: 1264--------
Loss: 0.0599
Train_acc: 0.9643
Epoch: 1265--------
Loss: 0.0599
Train_acc: 0.9643
Epoch: 1266--------
Loss: 0.0598
Train_acc: 0.9643
Epoch: 1267--------
Loss: 0.0598
Train_acc: 0.9643
Epoch: 1268--------
Loss: 0.0597
Train_acc: 0.9643
Epoch: 1269--------
Loss: 0.0597
Train_acc: 0.9643
Epoch: 1270--------
Loss: 0.0596
Train_acc: 0.9643
Epoch: 1271--------
Loss: 0.0596
Train_acc: 0.9643
Epoch: 1272--------
Loss: 0.0595
Train_acc: 0.9643
Epoch: 1273--------
Loss: 0.0595
Train_acc: 0.9643
Epoch: 1274--------
Loss: 0.0594
Train_acc: 0.9643
Epoch: 1275--------
Loss: 0.0594
Train_acc: 0.9643
Epoch: 1276--------
Loss: 0.0593
Train_acc: 0.9643
Epoch: 1277--------
Loss: 0.0593
Train_acc: 0.9643
Epoch: 1278--------
Loss: 0.0592
Train_acc: 0.9643
Epoch: 1279--------
Loss: 0.0592
Train_acc: 0.9643
Epoch: 1280--------
Loss: 0.0591
Train_acc: 0.9643
Test_acc: 1.0000
Epoch: 1281--------
Loss: 0.0591
Train_acc: 0.9643
Epoch: 1282--------
Loss: 0.0590
Train_acc: 0.9643
Epoch: 1283--------
Loss: 0.0590
Train_acc: 1.0000
Epoch: 1284--------
Loss: 0.0589
Train_acc: 0.9643
Epoch: 1285--------
Loss: 0.0589
Train_acc: 1.0000
Epoch: 1286--------
Loss: 0.0588
Train_acc: 0.9643
Epoch: 1287--------
Loss: 0.0587
Train_acc: 0.9643
Epoch: 1288--------
Loss: 0.0587
Train_acc: 1.0000
Epoch: 1289--------
Loss: 0.0586
Train_acc: 0.9643
Epoch: 1290--------
Loss: 0.0586
Train_acc: 1.0000
Epoch: 1291--------
Loss: 0.0585
Train_acc: 0.9643
Epoch: 1292--------
Loss: 0.0585
Train_acc: 0.9643
Epoch: 1293--------
Loss: 0.0584
Train_acc: 1.0000
Epoch: 1294--------
Loss: 0.0584
Train_acc: 0.9643
Epoch: 1295--------
Loss: 0.0583
Train_acc: 1.0000
Epoch: 1296--------
Loss: 0.0583
Train_acc: 0.9643
Epoch: 1297--------
Loss: 0.0582
Train_acc: 1.0000
Epoch: 1298--------
Loss: 0.0582
Train_acc: 1.0000
Epoch: 1299--------
Loss: 0.0581
Train_acc: 0.9643
Epoch: 1300--------
Loss: 0.0581
Train_acc: 1.0000
Test_acc: 1.0000
Epoch: 1301--------
Loss: 0.0580
Train_acc: 0.9643
Epoch: 1302--------
Loss: 0.0580
Train_acc: 1.0000
Epoch: 1303--------
Loss: 0.0580
Train_acc: 1.0000
Epoch: 1304--------
Loss: 0.0579
Train_acc: 1.0000
Epoch: 1305--------
Loss: 0.0579
Train_acc: 1.0000
Epoch: 1306--------
Loss: 0.0578
Train_acc: 1.0000
Epoch: 1307--------
Loss: 0.0578
Train_acc: 1.0000
Epoch: 1308--------
Loss: 0.0577
Train_acc: 1.0000
Epoch: 1309--------
Loss: 0.0577
Train_acc: 1.0000
Epoch: 1310--------
Loss: 0.0576
Train_acc: 1.0000
Epoch: 1311--------
Loss: 0.0576
Train_acc: 1.0000
Epoch: 1312--------
Loss: 0.0575
Train_acc: 1.0000
Epoch: 1313--------
Loss: 0.0575
Train_acc: 1.0000
Epoch: 1314--------
Loss: 0.0574
Train_acc: 1.0000
Epoch: 1315--------
Loss: 0.0574
Train_acc: 1.0000
Epoch: 1316--------
Loss: 0.0573
Train_acc: 1.0000
Epoch: 1317--------
Loss: 0.0573
Train_acc: 1.0000
Epoch: 1318--------
Loss: 0.0572
Train_acc: 1.0000
Epoch: 1319--------
Loss: 0.0572
Train_acc: 1.0000
Epoch: 1320--------
Loss: 0.0571
Train_acc: 1.0000
Test_acc: 1.0000
Epoch: 1321--------
Loss: 0.0571
Train_acc: 1.0000
Epoch: 1322--------
Loss: 0.0570
Train_acc: 1.0000
Epoch: 1323--------
Loss: 0.0570
Train_acc: 1.0000
Epoch: 1324--------
Loss: 0.0569
Train_acc: 1.0000
Epoch: 1325--------
Loss: 0.0569
Train_acc: 1.0000
Epoch: 1326--------
Loss: 0.0568
Train_acc: 1.0000
Epoch: 1327--------
Loss: 0.0568
Train_acc: 1.0000
Epoch: 1328--------
Loss: 0.0567
Train_acc: 1.0000
Epoch: 1329--------
Loss: 0.0567
Train_acc: 1.0000
Epoch: 1330--------
Loss: 0.0567
Train_acc: 1.0000
Epoch: 1331--------
Loss: 0.0566
Train_acc: 1.0000
Epoch: 1332--------
Loss: 0.0566
Train_acc: 1.0000
Epoch: 1333--------
Loss: 0.0565
Train_acc: 1.0000
Epoch: 1334--------
Loss: 0.0565
Train_acc: 1.0000
Epoch: 1335--------
Loss: 0.0564
Train_acc: 1.0000
Epoch: 1336--------
Loss: 0.0564
Train_acc: 1.0000
Epoch: 1337--------
Loss: 0.0563
Train_acc: 1.0000
Epoch: 1338--------
Loss: 0.0563
Train_acc: 1.0000
Epoch: 1339--------
Loss: 0.0562
Train_acc: 1.0000
Epoch: 1340--------
Loss: 0.0562
Train_acc: 1.0000
Test_acc: 1.0000
Epoch: 1341--------
Loss: 0.0561
Train_acc: 1.0000
Epoch: 1342--------
Loss: 0.0561
Train_acc: 1.0000
Epoch: 1343--------
Loss: 0.0561
Train_acc: 1.0000
Epoch: 1344--------
Loss: 0.0560
Train_acc: 1.0000
Epoch: 1345--------
Loss: 0.0560
Train_acc: 1.0000
Epoch: 1346--------
Loss: 0.0559
Train_acc: 1.0000
Epoch: 1347--------
Loss: 0.0559
Train_acc: 1.0000
Epoch: 1348--------
Loss: 0.0558
Train_acc: 1.0000
Epoch: 1349--------
Loss: 0.0558
Train_acc: 1.0000
Epoch: 1350--------
Loss: 0.0557
Train_acc: 1.0000
Epoch: 1351--------
Loss: 0.0557
Train_acc: 1.0000
Epoch: 1352--------
Loss: 0.0556
Train_acc: 1.0000
Epoch: 1353--------
Loss: 0.0556
Train_acc: 1.0000
Epoch: 1354--------
Loss: 0.0556
Train_acc: 1.0000
Epoch: 1355--------
Loss: 0.0555
Train_acc: 1.0000
Epoch: 1356--------
Loss: 0.0555
Train_acc: 1.0000
Epoch: 1357--------
Loss: 0.0554
Train_acc: 1.0000
Epoch: 1358--------
Loss: 0.0554
Train_acc: 1.0000
Epoch: 1359--------
Loss: 0.0553
Train_acc: 1.0000
Epoch: 1360--------
Loss: 0.0553
Train_acc: 1.0000
Test_acc: 1.0000
Epoch: 1361--------
Loss: 0.0552
Train_acc: 1.0000
Epoch: 1362--------
Loss: 0.0552
Train_acc: 1.0000
Epoch: 1363--------
Loss: 0.0551
Train_acc: 1.0000
Epoch: 1364--------
Loss: 0.0551
Train_acc: 1.0000
Epoch: 1365--------
Loss: 0.0551
Train_acc: 1.0000
Epoch: 1366--------
Loss: 0.0550
Train_acc: 1.0000
Epoch: 1367--------
Loss: 0.0550
Train_acc: 1.0000
Epoch: 1368--------
Loss: 0.0549
Train_acc: 1.0000
Epoch: 1369--------
Loss: 0.0549
Train_acc: 1.0000
Epoch: 1370--------
Loss: 0.0548
Train_acc: 1.0000
Epoch: 1371--------
Loss: 0.0548
Train_acc: 1.0000
Epoch: 1372--------
Loss: 0.0548
Train_acc: 1.0000
Epoch: 1373--------
Loss: 0.0547
Train_acc: 1.0000
Epoch: 1374--------
Loss: 0.0547
Train_acc: 1.0000
Epoch: 1375--------
Loss: 0.0546
Train_acc: 1.0000
Epoch: 1376--------
Loss: 0.0546
Train_acc: 1.0000
Epoch: 1377--------
Loss: 0.0545
Train_acc: 1.0000
Epoch: 1378--------
Loss: 0.0545
Train_acc: 1.0000
Epoch: 1379--------
Loss: 0.0544
Train_acc: 1.0000
Epoch: 1380--------
Loss: 0.0544
Train_acc: 1.0000
Test_acc: 1.0000
Epoch: 1381--------
Loss: 0.0544
Train_acc: 1.0000
Epoch: 1382--------
Loss: 0.0543
Train_acc: 1.0000
Epoch: 1383--------
Loss: 0.0543
Train_acc: 1.0000
Epoch: 1384--------
Loss: 0.0542
Train_acc: 1.0000
Epoch: 1385--------
Loss: 0.0542
Train_acc: 1.0000
Epoch: 1386--------
Loss: 0.0541
Train_acc: 1.0000
Epoch: 1387--------
Loss: 0.0541
Train_acc: 1.0000
Epoch: 1388--------
Loss: 0.0541
Train_acc: 1.0000
Epoch: 1389--------
Loss: 0.0540
Train_acc: 1.0000
Epoch: 1390--------
Loss: 0.0540
Train_acc: 1.0000
Epoch: 1391--------
Loss: 0.0539
Train_acc: 1.0000
Epoch: 1392--------
Loss: 0.0539
Train_acc: 1.0000
Epoch: 1393--------
Loss: 0.0539
Train_acc: 1.0000
Epoch: 1394--------
Loss: 0.0538
Train_acc: 1.0000
Epoch: 1395--------
Loss: 0.0538
Train_acc: 1.0000
Epoch: 1396--------
Loss: 0.0537
Train_acc: 1.0000
Epoch: 1397--------
Loss: 0.0537
Train_acc: 1.0000
Epoch: 1398--------
Loss: 0.0536
Train_acc: 1.0000
Epoch: 1399--------
Loss: 0.0536
Train_acc: 1.0000
Epoch: 1400--------
Loss: 0.0536
Train_acc: 1.0000
Test_acc: 1.0000
Epoch: 1401--------
Loss: 0.0535
Train_acc: 1.0000
Epoch: 1402--------
Loss: 0.0535
Train_acc: 1.0000
Epoch: 1403--------
Loss: 0.0534
Train_acc: 1.0000
Epoch: 1404--------
Loss: 0.0534
Train_acc: 1.0000
Epoch: 1405--------
Loss: 0.0533
Train_acc: 1.0000
Epoch: 1406--------
Loss: 0.0533
Train_acc: 1.0000
Epoch: 1407--------
Loss: 0.0533
Train_acc: 1.0000
Epoch: 1408--------
Loss: 0.0532
Train_acc: 1.0000
Epoch: 1409--------
Loss: 0.0532
Train_acc: 1.0000
Epoch: 1410--------
Loss: 0.0531
Train_acc: 1.0000
Epoch: 1411--------
Loss: 0.0531
Train_acc: 1.0000
Epoch: 1412--------
Loss: 0.0531
Train_acc: 1.0000
Epoch: 1413--------
Loss: 0.0530
Train_acc: 1.0000
Epoch: 1414--------
Loss: 0.0530
Train_acc: 1.0000
Epoch: 1415--------
Loss: 0.0529
Train_acc: 1.0000
Epoch: 1416--------
Loss: 0.0529
Train_acc: 1.0000
Epoch: 1417--------
Loss: 0.0529
Train_acc: 1.0000
Epoch: 1418--------
Loss: 0.0528
Train_acc: 1.0000
Epoch: 1419--------
Loss: 0.0528
Train_acc: 1.0000
Epoch: 1420--------
Loss: 0.0527
Train_acc: 1.0000
Test_acc: 1.0000
Epoch: 1421--------
Loss: 0.0527
Train_acc: 1.0000
Epoch: 1422--------
Loss: 0.0527
Train_acc: 1.0000
Epoch: 1423--------
Loss: 0.0526
Train_acc: 1.0000
Epoch: 1424--------
Loss: 0.0526
Train_acc: 1.0000
Epoch: 1425--------
Loss: 0.0525
Train_acc: 1.0000
Epoch: 1426--------
Loss: 0.0525
Train_acc: 1.0000
Epoch: 1427--------
Loss: 0.0524
Train_acc: 1.0000
Epoch: 1428--------
Loss: 0.0524
Train_acc: 1.0000
Epoch: 1429--------
Loss: 0.0524
Train_acc: 1.0000
Epoch: 1430--------
Loss: 0.0523
Train_acc: 1.0000
Epoch: 1431--------
Loss: 0.0523
Train_acc: 1.0000
Epoch: 1432--------
Loss: 0.0522
Train_acc: 1.0000
Epoch: 1433--------
Loss: 0.0522
Train_acc: 1.0000
Epoch: 1434--------
Loss: 0.0522
Train_acc: 1.0000
Epoch: 1435--------
Loss: 0.0521
Train_acc: 1.0000
Epoch: 1436--------
Loss: 0.0521
Train_acc: 1.0000
Epoch: 1437--------
Loss: 0.0521
Train_acc: 1.0000
Epoch: 1438--------
Loss: 0.0520
Train_acc: 1.0000
Epoch: 1439--------
Loss: 0.0520
Train_acc: 1.0000
Epoch: 1440--------
Loss: 0.0519
Train_acc: 1.0000
Test_acc: 1.0000
Epoch: 1441--------
Loss: 0.0519
Train_acc: 1.0000
Epoch: 1442--------
Loss: 0.0519
Train_acc: 1.0000
Epoch: 1443--------
Loss: 0.0518
Train_acc: 1.0000
Epoch: 1444--------
Loss: 0.0518
Train_acc: 1.0000
Epoch: 1445--------
Loss: 0.0517
Train_acc: 1.0000
Epoch: 1446--------
Loss: 0.0517
Train_acc: 1.0000
Epoch: 1447--------
Loss: 0.0517
Train_acc: 1.0000
Epoch: 1448--------
Loss: 0.0516
Train_acc: 1.0000
Epoch: 1449--------
Loss: 0.0516
Train_acc: 1.0000
Epoch: 1450--------
Loss: 0.0515
Train_acc: 1.0000
Epoch: 1451--------
Loss: 0.0515
Train_acc: 1.0000
Epoch: 1452--------
Loss: 0.0515
Train_acc: 1.0000
Epoch: 1453--------
Loss: 0.0514
Train_acc: 1.0000
Epoch: 1454--------
Loss: 0.0514
Train_acc: 1.0000
Epoch: 1455--------
Loss: 0.0513
Train_acc: 1.0000
Epoch: 1456--------
Loss: 0.0513
Train_acc: 1.0000
Epoch: 1457--------
Loss: 0.0513
Train_acc: 1.0000
Epoch: 1458--------
Loss: 0.0512
Train_acc: 1.0000
Epoch: 1459--------
Loss: 0.0512
Train_acc: 1.0000
Epoch: 1460--------
Loss: 0.0512
Train_acc: 1.0000
Test_acc: 1.0000
Epoch: 1461--------
Loss: 0.0511
Train_acc: 1.0000
Epoch: 1462--------
Loss: 0.0511
Train_acc: 1.0000
Epoch: 1463--------
Loss: 0.0510
Train_acc: 1.0000
Epoch: 1464--------
Loss: 0.0510
Train_acc: 1.0000
Epoch: 1465--------
Loss: 0.0510
Train_acc: 1.0000
Epoch: 1466--------
Loss: 0.0509
Train_acc: 1.0000
Epoch: 1467--------
Loss: 0.0509
Train_acc: 1.0000
Epoch: 1468--------
Loss: 0.0509
Train_acc: 1.0000
Epoch: 1469--------
Loss: 0.0508
Train_acc: 1.0000
Epoch: 1470--------
Loss: 0.0508
Train_acc: 1.0000
Epoch: 1471--------
Loss: 0.0507
Train_acc: 1.0000
Epoch: 1472--------
Loss: 0.0507
Train_acc: 1.0000
Epoch: 1473--------
Loss: 0.0507
Train_acc: 1.0000
Epoch: 1474--------
Loss: 0.0506
Train_acc: 1.0000
Epoch: 1475--------
Loss: 0.0506
Train_acc: 1.0000
Epoch: 1476--------
Loss: 0.0505
Train_acc: 1.0000
Epoch: 1477--------
Loss: 0.0505
Train_acc: 1.0000
Epoch: 1478--------
Loss: 0.0505
Train_acc: 1.0000
Epoch: 1479--------
Loss: 0.0504
Train_acc: 1.0000
Epoch: 1480--------
Loss: 0.0504
Train_acc: 1.0000
Test_acc: 1.0000
Epoch: 1481--------
Loss: 0.0504
Train_acc: 1.0000
Epoch: 1482--------
Loss: 0.0503
Train_acc: 1.0000
Epoch: 1483--------
Loss: 0.0503
Train_acc: 1.0000
Epoch: 1484--------
Loss: 0.0503
Train_acc: 1.0000
Epoch: 1485--------
Loss: 0.0502
Train_acc: 1.0000
Epoch: 1486--------
Loss: 0.0502
Train_acc: 1.0000
Epoch: 1487--------
Loss: 0.0501
Train_acc: 1.0000
Epoch: 1488--------
Loss: 0.0501
Train_acc: 1.0000
Epoch: 1489--------
Loss: 0.0501
Train_acc: 1.0000
Epoch: 1490--------
Loss: 0.0500
Train_acc: 1.0000
Epoch: 1491--------
Loss: 0.0500
Train_acc: 1.0000
Epoch: 1492--------
Loss: 0.0500
Train_acc: 1.0000
Epoch: 1493--------
Loss: 0.0499
Train_acc: 1.0000
Epoch: 1494--------
Loss: 0.0499
Train_acc: 1.0000
Epoch: 1495--------
Loss: 0.0498
Train_acc: 1.0000
Epoch: 1496--------
Loss: 0.0498
Train_acc: 1.0000
Epoch: 1497--------
Loss: 0.0498
Train_acc: 1.0000
Epoch: 1498--------
Loss: 0.0497
Train_acc: 1.0000
Epoch: 1499--------
Loss: 0.0497
Train_acc: 1.0000
Epoch: 1500--------
Loss: 0.0497
Train_acc: 1.0000
Test_acc: 1.0000
Epoch: 1501--------
Loss: 0.0496
Train_acc: 1.0000
Epoch: 1502--------
Loss: 0.0496
Train_acc: 1.0000
Epoch: 1503--------
Loss: 0.0496
Train_acc: 1.0000
Epoch: 1504--------
Loss: 0.0495
Train_acc: 1.0000
Epoch: 1505--------
Loss: 0.0495
Train_acc: 1.0000
Epoch: 1506--------
Loss: 0.0494
Train_acc: 1.0000
Epoch: 1507--------
Loss: 0.0494
Train_acc: 1.0000
Epoch: 1508--------
Loss: 0.0494
Train_acc: 1.0000
Epoch: 1509--------
Loss: 0.0493
Train_acc: 1.0000
Epoch: 1510--------
Loss: 0.0493
Train_acc: 1.0000
Epoch: 1511--------
Loss: 0.0493
Train_acc: 1.0000
Epoch: 1512--------
Loss: 0.0492
Train_acc: 1.0000
Epoch: 1513--------
Loss: 0.0492
Train_acc: 1.0000
Epoch: 1514--------
Loss: 0.0492
Train_acc: 1.0000
Epoch: 1515--------
Loss: 0.0491
Train_acc: 1.0000
Epoch: 1516--------
Loss: 0.0491
Train_acc: 1.0000
Epoch: 1517--------
Loss: 0.0491
Train_acc: 1.0000
Epoch: 1518--------
Loss: 0.0490
Train_acc: 1.0000
Epoch: 1519--------
Loss: 0.0490
Train_acc: 1.0000
Epoch: 1520--------
Loss: 0.0489
Train_acc: 1.0000
Test_acc: 1.0000
Epoch: 1521--------
Loss: 0.0489
Train_acc: 1.0000
Epoch: 1522--------
Loss: 0.0489
Train_acc: 1.0000
Epoch: 1523--------
Loss: 0.0488
Train_acc: 1.0000
Epoch: 1524--------
Loss: 0.0488
Train_acc: 1.0000
Epoch: 1525--------
Loss: 0.0488
Train_acc: 1.0000
Epoch: 1526--------
Loss: 0.0487
Train_acc: 1.0000
Epoch: 1527--------
Loss: 0.0487
Train_acc: 1.0000
Epoch: 1528--------
Loss: 0.0487
Train_acc: 1.0000
Epoch: 1529--------
Loss: 0.0486
Train_acc: 1.0000
Epoch: 1530--------
Loss: 0.0486
Train_acc: 1.0000
Epoch: 1531--------
Loss: 0.0486
Train_acc: 1.0000
Epoch: 1532--------
Loss: 0.0485
Train_acc: 1.0000
Epoch: 1533--------
Loss: 0.0485
Train_acc: 1.0000
Epoch: 1534--------
Loss: 0.0485
Train_acc: 1.0000
Epoch: 1535--------
Loss: 0.0484
Train_acc: 1.0000
Epoch: 1536--------
Loss: 0.0484
Train_acc: 1.0000
Epoch: 1537--------
Loss: 0.0484
Train_acc: 1.0000
Epoch: 1538--------
Loss: 0.0483
Train_acc: 1.0000
Epoch: 1539--------
Loss: 0.0483
Train_acc: 1.0000
Epoch: 1540--------
Loss: 0.0483
Train_acc: 1.0000
Test_acc: 1.0000
Epoch: 1541--------
Loss: 0.0482
Train_acc: 1.0000
Epoch: 1542--------
Loss: 0.0482
Train_acc: 1.0000
Epoch: 1543--------
Loss: 0.0482
Train_acc: 1.0000
Epoch: 1544--------
Loss: 0.0481
Train_acc: 1.0000
Epoch: 1545--------
Loss: 0.0481
Train_acc: 1.0000
Epoch: 1546--------
Loss: 0.0480
Train_acc: 1.0000
Epoch: 1547--------
Loss: 0.0480
Train_acc: 1.0000
Epoch: 1548--------
Loss: 0.0480
Train_acc: 1.0000
Epoch: 1549--------
Loss: 0.0479
Train_acc: 1.0000
Epoch: 1550--------
Loss: 0.0479
Train_acc: 1.0000
Epoch: 1551--------
Loss: 0.0479
Train_acc: 1.0000
Epoch: 1552--------
Loss: 0.0478
Train_acc: 1.0000
Epoch: 1553--------
Loss: 0.0478
Train_acc: 1.0000
Epoch: 1554--------
Loss: 0.0478
Train_acc: 1.0000
Epoch: 1555--------
Loss: 0.0477
Train_acc: 1.0000
Epoch: 1556--------
Loss: 0.0477
Train_acc: 1.0000
Epoch: 1557--------
Loss: 0.0477
Train_acc: 1.0000
Epoch: 1558--------
Loss: 0.0476
Train_acc: 1.0000
Epoch: 1559--------
Loss: 0.0476
Train_acc: 1.0000
Epoch: 1560--------
Loss: 0.0476
Train_acc: 1.0000
Test_acc: 1.0000
Epoch: 1561--------
Loss: 0.0475
Train_acc: 1.0000
Epoch: 1562--------
Loss: 0.0475
Train_acc: 1.0000
Epoch: 1563--------
Loss: 0.0475
Train_acc: 1.0000
Epoch: 1564--------
Loss: 0.0474
Train_acc: 1.0000
Epoch: 1565--------
Loss: 0.0474
Train_acc: 1.0000
Epoch: 1566--------
Loss: 0.0474
Train_acc: 1.0000
Epoch: 1567--------
Loss: 0.0473
Train_acc: 1.0000
Epoch: 1568--------
Loss: 0.0473
Train_acc: 1.0000
Epoch: 1569--------
Loss: 0.0473
Train_acc: 1.0000
Epoch: 1570--------
Loss: 0.0472
Train_acc: 1.0000
Epoch: 1571--------
Loss: 0.0472
Train_acc: 1.0000
Epoch: 1572--------
Loss: 0.0472
Train_acc: 1.0000
Epoch: 1573--------
Loss: 0.0471
Train_acc: 1.0000
Epoch: 1574--------
Loss: 0.0471
Train_acc: 1.0000
Epoch: 1575--------
Loss: 0.0471
Train_acc: 1.0000
Epoch: 1576--------
Loss: 0.0470
Train_acc: 1.0000
Epoch: 1577--------
Loss: 0.0470
Train_acc: 1.0000
Epoch: 1578--------
Loss: 0.0470
Train_acc: 1.0000
Epoch: 1579--------
Loss: 0.0469
Train_acc: 1.0000
Epoch: 1580--------
Loss: 0.0469
Train_acc: 1.0000
Test_acc: 1.0000
Epoch: 1581--------
Loss: 0.0469
Train_acc: 1.0000
Epoch: 1582--------
Loss: 0.0468
Train_acc: 1.0000
Epoch: 1583--------
Loss: 0.0468
Train_acc: 1.0000
Epoch: 1584--------
Loss: 0.0468
Train_acc: 1.0000
Epoch: 1585--------
Loss: 0.0468
Train_acc: 1.0000
Epoch: 1586--------
Loss: 0.0467
Train_acc: 1.0000
Epoch: 1587--------
Loss: 0.0467
Train_acc: 1.0000
Epoch: 1588--------
Loss: 0.0467
Train_acc: 1.0000
Epoch: 1589--------
Loss: 0.0466
Train_acc: 1.0000
Epoch: 1590--------
Loss: 0.0466
Train_acc: 1.0000
Epoch: 1591--------
Loss: 0.0466
Train_acc: 1.0000
Epoch: 1592--------
Loss: 0.0465
Train_acc: 1.0000
Epoch: 1593--------
Loss: 0.0465
Train_acc: 1.0000
Epoch: 1594--------
Loss: 0.0465
Train_acc: 1.0000
Epoch: 1595--------
Loss: 0.0464
Train_acc: 1.0000
Epoch: 1596--------
Loss: 0.0464
Train_acc: 1.0000
Epoch: 1597--------
Loss: 0.0464
Train_acc: 1.0000
Epoch: 1598--------
Loss: 0.0463
Train_acc: 1.0000
Epoch: 1599--------
Loss: 0.0463
Train_acc: 1.0000
Epoch: 1600--------
Loss: 0.0463
Train_acc: 1.0000
Test_acc: 1.0000
Epoch: 1601--------
Loss: 0.0462
Train_acc: 1.0000
Epoch: 1602--------
Loss: 0.0462
Train_acc: 1.0000
Epoch: 1603--------
Loss: 0.0462
Train_acc: 1.0000
Epoch: 1604--------
Loss: 0.0461
Train_acc: 1.0000
Epoch: 1605--------
Loss: 0.0461
Train_acc: 1.0000
Epoch: 1606--------
Loss: 0.0461
Train_acc: 1.0000
Epoch: 1607--------
Loss: 0.0460
Train_acc: 1.0000
Epoch: 1608--------
Loss: 0.0460
Train_acc: 1.0000
Epoch: 1609--------
Loss: 0.0460
Train_acc: 1.0000
Epoch: 1610--------
Loss: 0.0459
Train_acc: 1.0000
Epoch: 1611--------
Loss: 0.0459
Train_acc: 1.0000
Epoch: 1612--------
Loss: 0.0459
Train_acc: 1.0000
Epoch: 1613--------
Loss: 0.0459
Train_acc: 1.0000
Epoch: 1614--------
Loss: 0.0458
Train_acc: 1.0000
Epoch: 1615--------
Loss: 0.0458
Train_acc: 1.0000
Epoch: 1616--------
Loss: 0.0458
Train_acc: 1.0000
Epoch: 1617--------
Loss: 0.0457
Train_acc: 1.0000
Epoch: 1618--------
Loss: 0.0457
Train_acc: 1.0000
Epoch: 1619--------
Loss: 0.0457
Train_acc: 1.0000
Epoch: 1620--------
Loss: 0.0456
Train_acc: 1.0000
Test_acc: 1.0000
Epoch: 1621--------
Loss: 0.0456
Train_acc: 1.0000
Epoch: 1622--------
Loss: 0.0456
Train_acc: 1.0000
Epoch: 1623--------
Loss: 0.0455
Train_acc: 1.0000
Epoch: 1624--------
Loss: 0.0455
Train_acc: 1.0000
Epoch: 1625--------
Loss: 0.0455
Train_acc: 1.0000
Epoch: 1626--------
Loss: 0.0454
Train_acc: 1.0000
Epoch: 1627--------
Loss: 0.0454
Train_acc: 1.0000
Epoch: 1628--------
Loss: 0.0454
Train_acc: 1.0000
Epoch: 1629--------
Loss: 0.0454
Train_acc: 1.0000
Epoch: 1630--------
Loss: 0.0453
Train_acc: 1.0000
Epoch: 1631--------
Loss: 0.0453
Train_acc: 1.0000
Epoch: 1632--------
Loss: 0.0453
Train_acc: 1.0000
Epoch: 1633--------
Loss: 0.0452
Train_acc: 1.0000
Epoch: 1634--------
Loss: 0.0452
Train_acc: 1.0000
Epoch: 1635--------
Loss: 0.0452
Train_acc: 1.0000
Epoch: 1636--------
Loss: 0.0451
Train_acc: 1.0000
Epoch: 1637--------
Loss: 0.0451
Train_acc: 1.0000
Epoch: 1638--------
Loss: 0.0451
Train_acc: 1.0000
Epoch: 1639--------
Loss: 0.0450
Train_acc: 1.0000
Epoch: 1640--------
Loss: 0.0450
Train_acc: 1.0000
Test_acc: 1.0000
Epoch: 1641--------
Loss: 0.0450
Train_acc: 1.0000
Epoch: 1642--------
Loss: 0.0450
Train_acc: 1.0000
Epoch: 1643--------
Loss: 0.0449
Train_acc: 1.0000
Epoch: 1644--------
Loss: 0.0449
Train_acc: 1.0000
Epoch: 1645--------
Loss: 0.0449
Train_acc: 1.0000
Epoch: 1646--------
Loss: 0.0448
Train_acc: 1.0000
Epoch: 1647--------
Loss: 0.0448
Train_acc: 1.0000
Epoch: 1648--------
Loss: 0.0448
Train_acc: 1.0000
Epoch: 1649--------
Loss: 0.0447
Train_acc: 1.0000
Epoch: 1650--------
Loss: 0.0447
Train_acc: 1.0000
Epoch: 1651--------
Loss: 0.0447
Train_acc: 1.0000
Epoch: 1652--------
Loss: 0.0446
Train_acc: 1.0000
Epoch: 1653--------
Loss: 0.0446
Train_acc: 1.0000
Epoch: 1654--------
Loss: 0.0446
Train_acc: 1.0000
Epoch: 1655--------
Loss: 0.0446
Train_acc: 1.0000
Epoch: 1656--------
Loss: 0.0445
Train_acc: 1.0000
Epoch: 1657--------
Loss: 0.0445
Train_acc: 1.0000
Epoch: 1658--------
Loss: 0.0445
Train_acc: 1.0000
Epoch: 1659--------
Loss: 0.0444
Train_acc: 1.0000
Epoch: 1660--------
Loss: 0.0444
Train_acc: 1.0000
Test_acc: 1.0000
Epoch: 1661--------
Loss: 0.0444
Train_acc: 1.0000
Epoch: 1662--------
Loss: 0.0444
Train_acc: 1.0000
Epoch: 1663--------
Loss: 0.0443
Train_acc: 1.0000
Epoch: 1664--------
Loss: 0.0443
Train_acc: 1.0000
Epoch: 1665--------
Loss: 0.0443
Train_acc: 1.0000
Epoch: 1666--------
Loss: 0.0442
Train_acc: 1.0000
Epoch: 1667--------
Loss: 0.0442
Train_acc: 1.0000
Epoch: 1668--------
Loss: 0.0442
Train_acc: 1.0000
Epoch: 1669--------
Loss: 0.0441
Train_acc: 1.0000
Epoch: 1670--------
Loss: 0.0441
Train_acc: 1.0000
Epoch: 1671--------
Loss: 0.0441
Train_acc: 1.0000
Epoch: 1672--------
Loss: 0.0441
Train_acc: 1.0000
Epoch: 1673--------
Loss: 0.0440
Train_acc: 1.0000
Epoch: 1674--------
Loss: 0.0440
Train_acc: 1.0000
Epoch: 1675--------
Loss: 0.0440
Train_acc: 1.0000
Epoch: 1676--------
Loss: 0.0439
Train_acc: 1.0000
Epoch: 1677--------
Loss: 0.0439
Train_acc: 1.0000
Epoch: 1678--------
Loss: 0.0439
Train_acc: 1.0000
Epoch: 1679--------
Loss: 0.0438
Train_acc: 1.0000
Epoch: 1680--------
Loss: 0.0438
Train_acc: 1.0000
Test_acc: 1.0000
Epoch: 1681--------
Loss: 0.0438
Train_acc: 1.0000
Epoch: 1682--------
Loss: 0.0438
Train_acc: 1.0000
Epoch: 1683--------
Loss: 0.0437
Train_acc: 1.0000
Epoch: 1684--------
Loss: 0.0437
Train_acc: 1.0000
Epoch: 1685--------
Loss: 0.0437
Train_acc: 1.0000
Epoch: 1686--------
Loss: 0.0436
Train_acc: 1.0000
Epoch: 1687--------
Loss: 0.0436
Train_acc: 1.0000
Epoch: 1688--------
Loss: 0.0436
Train_acc: 1.0000
Epoch: 1689--------
Loss: 0.0436
Train_acc: 1.0000
Epoch: 1690--------
Loss: 0.0435
Train_acc: 1.0000
Epoch: 1691--------
Loss: 0.0435
Train_acc: 1.0000
Epoch: 1692--------
Loss: 0.0435
Train_acc: 1.0000
Epoch: 1693--------
Loss: 0.0434
Train_acc: 1.0000
Epoch: 1694--------
Loss: 0.0434
Train_acc: 1.0000
Epoch: 1695--------
Loss: 0.0434
Train_acc: 1.0000
Epoch: 1696--------
Loss: 0.0434
Train_acc: 1.0000
Epoch: 1697--------
Loss: 0.0433
Train_acc: 1.0000
Epoch: 1698--------
Loss: 0.0433
Train_acc: 1.0000
Epoch: 1699--------
Loss: 0.0433
Train_acc: 1.0000
Epoch: 1700--------
Loss: 0.0432
Train_acc: 1.0000
Test_acc: 1.0000
Epoch: 1701--------
Loss: 0.0432
Train_acc: 1.0000
Epoch: 1702--------
Loss: 0.0432
Train_acc: 1.0000
Epoch: 1703--------
Loss: 0.0432
Train_acc: 1.0000
Epoch: 1704--------
Loss: 0.0431
Train_acc: 1.0000
Epoch: 1705--------
Loss: 0.0431
Train_acc: 1.0000
Epoch: 1706--------
Loss: 0.0431
Train_acc: 1.0000
Epoch: 1707--------
Loss: 0.0430
Train_acc: 1.0000
Epoch: 1708--------
Loss: 0.0430
Train_acc: 1.0000
Epoch: 1709--------
Loss: 0.0430
Train_acc: 1.0000
Epoch: 1710--------
Loss: 0.0430
Train_acc: 1.0000
Epoch: 1711--------
Loss: 0.0429
Train_acc: 1.0000
Epoch: 1712--------
Loss: 0.0429
Train_acc: 1.0000
Epoch: 1713--------
Loss: 0.0429
Train_acc: 1.0000
Epoch: 1714--------
Loss: 0.0428
Train_acc: 1.0000
Epoch: 1715--------
Loss: 0.0428
Train_acc: 1.0000
Epoch: 1716--------
Loss: 0.0428
Train_acc: 1.0000
Epoch: 1717--------
Loss: 0.0428
Train_acc: 1.0000
Epoch: 1718--------
Loss: 0.0427
Train_acc: 1.0000
Epoch: 1719--------
Loss: 0.0427
Train_acc: 1.0000
Epoch: 1720--------
Loss: 0.0427
Train_acc: 1.0000
Test_acc: 1.0000
Epoch: 1721--------
Loss: 0.0426
Train_acc: 1.0000
Epoch: 1722--------
Loss: 0.0426
Train_acc: 1.0000
Epoch: 1723--------
Loss: 0.0426
Train_acc: 1.0000
Epoch: 1724--------
Loss: 0.0426
Train_acc: 1.0000
Epoch: 1725--------
Loss: 0.0425
Train_acc: 1.0000
Epoch: 1726--------
Loss: 0.0425
Train_acc: 1.0000
Epoch: 1727--------
Loss: 0.0425
Train_acc: 1.0000
Epoch: 1728--------
Loss: 0.0424
Train_acc: 1.0000
Epoch: 1729--------
Loss: 0.0424
Train_acc: 1.0000
Epoch: 1730--------
Loss: 0.0424
Train_acc: 1.0000
Epoch: 1731--------
Loss: 0.0424
Train_acc: 1.0000
Epoch: 1732--------
Loss: 0.0423
Train_acc: 1.0000
Epoch: 1733--------
Loss: 0.0423
Train_acc: 1.0000
Epoch: 1734--------
Loss: 0.0423
Train_acc: 1.0000
Epoch: 1735--------
Loss: 0.0423
Train_acc: 1.0000
Epoch: 1736--------
Loss: 0.0422
Train_acc: 1.0000
Epoch: 1737--------
Loss: 0.0422
Train_acc: 1.0000
Epoch: 1738--------
Loss: 0.0422
Train_acc: 1.0000
Epoch: 1739--------
Loss: 0.0421
Train_acc: 1.0000
Epoch: 1740--------
Loss: 0.0421
Train_acc: 1.0000
Test_acc: 1.0000
Epoch: 1741--------
Loss: 0.0421
Train_acc: 1.0000
Epoch: 1742--------
Loss: 0.0421
Train_acc: 1.0000
Epoch: 1743--------
Loss: 0.0420
Train_acc: 1.0000
Epoch: 1744--------
Loss: 0.0420
Train_acc: 1.0000
Epoch: 1745--------
Loss: 0.0420
Train_acc: 1.0000
Epoch: 1746--------
Loss: 0.0420
Train_acc: 1.0000
Epoch: 1747--------
Loss: 0.0419
Train_acc: 1.0000
Epoch: 1748--------
Loss: 0.0419
Train_acc: 1.0000
Epoch: 1749--------
Loss: 0.0419
Train_acc: 1.0000
Epoch: 1750--------
Loss: 0.0418
Train_acc: 1.0000
Epoch: 1751--------
Loss: 0.0418
Train_acc: 1.0000
Epoch: 1752--------
Loss: 0.0418
Train_acc: 1.0000
Epoch: 1753--------
Loss: 0.0418
Train_acc: 1.0000
Epoch: 1754--------
Loss: 0.0417
Train_acc: 1.0000
Epoch: 1755--------
Loss: 0.0417
Train_acc: 1.0000
Epoch: 1756--------
Loss: 0.0417
Train_acc: 1.0000
Epoch: 1757--------
Loss: 0.0417
Train_acc: 1.0000
Epoch: 1758--------
Loss: 0.0416
Train_acc: 1.0000
Epoch: 1759--------
Loss: 0.0416
Train_acc: 1.0000
Epoch: 1760--------
Loss: 0.0416
Train_acc: 1.0000
Test_acc: 1.0000
Epoch: 1761--------
Loss: 0.0415
Train_acc: 1.0000
Epoch: 1762--------
Loss: 0.0415
Train_acc: 1.0000
Epoch: 1763--------
Loss: 0.0415
Train_acc: 1.0000
Epoch: 1764--------
Loss: 0.0415
Train_acc: 1.0000
Epoch: 1765--------
Loss: 0.0414
Train_acc: 1.0000
Epoch: 1766--------
Loss: 0.0414
Train_acc: 1.0000
Epoch: 1767--------
Loss: 0.0414
Train_acc: 1.0000
Epoch: 1768--------
Loss: 0.0414
Train_acc: 1.0000
Epoch: 1769--------
Loss: 0.0413
Train_acc: 1.0000
Epoch: 1770--------
Loss: 0.0413
Train_acc: 1.0000
Epoch: 1771--------
Loss: 0.0413
Train_acc: 1.0000
Epoch: 1772--------
Loss: 0.0412
Train_acc: 1.0000
Epoch: 1773--------
Loss: 0.0412
Train_acc: 1.0000
Epoch: 1774--------
Loss: 0.0412
Train_acc: 1.0000
Epoch: 1775--------
Loss: 0.0412
Train_acc: 1.0000
Epoch: 1776--------
Loss: 0.0411
Train_acc: 1.0000
Epoch: 1777--------
Loss: 0.0411
Train_acc: 1.0000
Epoch: 1778--------
Loss: 0.0411
Train_acc: 1.0000
Epoch: 1779--------
Loss: 0.0411
Train_acc: 1.0000
Epoch: 1780--------
Loss: 0.0410
Train_acc: 1.0000
Test_acc: 1.0000
Epoch: 1781--------
Loss: 0.0410
Train_acc: 1.0000
Epoch: 1782--------
Loss: 0.0410
Train_acc: 1.0000
Epoch: 1783--------
Loss: 0.0410
Train_acc: 1.0000
Epoch: 1784--------
Loss: 0.0409
Train_acc: 1.0000
Epoch: 1785--------
Loss: 0.0409
Train_acc: 1.0000
Epoch: 1786--------
Loss: 0.0409
Train_acc: 1.0000
Epoch: 1787--------
Loss: 0.0408
Train_acc: 1.0000
Epoch: 1788--------
Loss: 0.0408
Train_acc: 1.0000
Epoch: 1789--------
Loss: 0.0408
Train_acc: 1.0000
Epoch: 1790--------
Loss: 0.0408
Train_acc: 1.0000
Epoch: 1791--------
Loss: 0.0407
Train_acc: 1.0000
Epoch: 1792--------
Loss: 0.0407
Train_acc: 1.0000
Epoch: 1793--------
Loss: 0.0407
Train_acc: 1.0000
Epoch: 1794--------
Loss: 0.0407
Train_acc: 1.0000
Epoch: 1795--------
Loss: 0.0406
Train_acc: 1.0000
Epoch: 1796--------
Loss: 0.0406
Train_acc: 1.0000
Epoch: 1797--------
Loss: 0.0406
Train_acc: 1.0000
Epoch: 1798--------
Loss: 0.0406
Train_acc: 1.0000
Epoch: 1799--------
Loss: 0.0405
Train_acc: 1.0000
Epoch: 1800--------
Loss: 0.0405
Train_acc: 1.0000
Test_acc: 1.0000
Epoch: 1801--------
Loss: 0.0405
Train_acc: 1.0000
Epoch: 1802--------
Loss: 0.0405
Train_acc: 1.0000
Epoch: 1803--------
Loss: 0.0404
Train_acc: 1.0000
Epoch: 1804--------
Loss: 0.0404
Train_acc: 1.0000
Epoch: 1805--------
Loss: 0.0404
Train_acc: 1.0000
Epoch: 1806--------
Loss: 0.0404
Train_acc: 1.0000
Epoch: 1807--------
Loss: 0.0403
Train_acc: 1.0000
Epoch: 1808--------
Loss: 0.0403
Train_acc: 1.0000
Epoch: 1809--------
Loss: 0.0403
Train_acc: 1.0000
Epoch: 1810--------
Loss: 0.0403
Train_acc: 1.0000
Epoch: 1811--------
Loss: 0.0402
Train_acc: 1.0000
Epoch: 1812--------
Loss: 0.0402
Train_acc: 1.0000
Epoch: 1813--------
Loss: 0.0402
Train_acc: 1.0000
Epoch: 1814--------
Loss: 0.0401
Train_acc: 1.0000
Epoch: 1815--------
Loss: 0.0401
Train_acc: 1.0000
Epoch: 1816--------
Loss: 0.0401
Train_acc: 1.0000
Epoch: 1817--------
Loss: 0.0401
Train_acc: 1.0000
Epoch: 1818--------
Loss: 0.0400
Train_acc: 1.0000
Epoch: 1819--------
Loss: 0.0400
Train_acc: 1.0000
Epoch: 1820--------
Loss: 0.0400
Train_acc: 1.0000
Test_acc: 1.0000
Epoch: 1821--------
Loss: 0.0400
Train_acc: 1.0000
Epoch: 1822--------
Loss: 0.0399
Train_acc: 1.0000
Epoch: 1823--------
Loss: 0.0399
Train_acc: 1.0000
Epoch: 1824--------
Loss: 0.0399
Train_acc: 1.0000
Epoch: 1825--------
Loss: 0.0399
Train_acc: 1.0000
Epoch: 1826--------
Loss: 0.0398
Train_acc: 1.0000
Epoch: 1827--------
Loss: 0.0398
Train_acc: 1.0000
Epoch: 1828--------
Loss: 0.0398
Train_acc: 1.0000
Epoch: 1829--------
Loss: 0.0398
Train_acc: 1.0000
Epoch: 1830--------
Loss: 0.0397
Train_acc: 1.0000
Epoch: 1831--------
Loss: 0.0397
Train_acc: 1.0000
Epoch: 1832--------
Loss: 0.0397
Train_acc: 1.0000
Epoch: 1833--------
Loss: 0.0397
Train_acc: 1.0000
Epoch: 1834--------
Loss: 0.0396
Train_acc: 1.0000
Epoch: 1835--------
Loss: 0.0396
Train_acc: 1.0000
Epoch: 1836--------
Loss: 0.0396
Train_acc: 1.0000
Epoch: 1837--------
Loss: 0.0396
Train_acc: 1.0000
Epoch: 1838--------
Loss: 0.0395
Train_acc: 1.0000
Epoch: 1839--------
Loss: 0.0395
Train_acc: 1.0000
Epoch: 1840--------
Loss: 0.0395
Train_acc: 1.0000
Test_acc: 1.0000
Epoch: 1841--------
Loss: 0.0395
Train_acc: 1.0000
Epoch: 1842--------
Loss: 0.0394
Train_acc: 1.0000
Epoch: 1843--------
Loss: 0.0394
Train_acc: 1.0000
Epoch: 1844--------
Loss: 0.0394
Train_acc: 1.0000
Epoch: 1845--------
Loss: 0.0394
Train_acc: 1.0000
Epoch: 1846--------
Loss: 0.0393
Train_acc: 1.0000
Epoch: 1847--------
Loss: 0.0393
Train_acc: 1.0000
Epoch: 1848--------
Loss: 0.0393
Train_acc: 1.0000
Epoch: 1849--------
Loss: 0.0393
Train_acc: 1.0000
Epoch: 1850--------
Loss: 0.0392
Train_acc: 1.0000
Epoch: 1851--------
Loss: 0.0392
Train_acc: 1.0000
Epoch: 1852--------
Loss: 0.0392
Train_acc: 1.0000
Epoch: 1853--------
Loss: 0.0392
Train_acc: 1.0000
Epoch: 1854--------
Loss: 0.0391
Train_acc: 1.0000
Epoch: 1855--------
Loss: 0.0391
Train_acc: 1.0000
Epoch: 1856--------
Loss: 0.0391
Train_acc: 1.0000
Epoch: 1857--------
Loss: 0.0391
Train_acc: 1.0000
Epoch: 1858--------
Loss: 0.0390
Train_acc: 1.0000
Epoch: 1859--------
Loss: 0.0390
Train_acc: 1.0000
Epoch: 1860--------
Loss: 0.0390
Train_acc: 1.0000
Test_acc: 1.0000
Epoch: 1861--------
Loss: 0.0390
Train_acc: 1.0000
Epoch: 1862--------
Loss: 0.0389
Train_acc: 1.0000
Epoch: 1863--------
Loss: 0.0389
Train_acc: 1.0000
Epoch: 1864--------
Loss: 0.0389
Train_acc: 1.0000
Epoch: 1865--------
Loss: 0.0389
Train_acc: 1.0000
Epoch: 1866--------
Loss: 0.0388
Train_acc: 1.0000
Epoch: 1867--------
Loss: 0.0388
Train_acc: 1.0000
Epoch: 1868--------
Loss: 0.0388
Train_acc: 1.0000
Epoch: 1869--------
Loss: 0.0388
Train_acc: 1.0000
Epoch: 1870--------
Loss: 0.0387
Train_acc: 1.0000
Epoch: 1871--------
Loss: 0.0387
Train_acc: 1.0000
Epoch: 1872--------
Loss: 0.0387
Train_acc: 1.0000
Epoch: 1873--------
Loss: 0.0387
Train_acc: 1.0000
Epoch: 1874--------
Loss: 0.0386
Train_acc: 1.0000
Epoch: 1875--------
Loss: 0.0386
Train_acc: 1.0000
Epoch: 1876--------
Loss: 0.0386
Train_acc: 1.0000
Epoch: 1877--------
Loss: 0.0386
Train_acc: 1.0000
Epoch: 1878--------
Loss: 0.0385
Train_acc: 1.0000
Epoch: 1879--------
Loss: 0.0385
Train_acc: 1.0000
Epoch: 1880--------
Loss: 0.0385
Train_acc: 1.0000
Test_acc: 1.0000
Epoch: 1881--------
Loss: 0.0385
Train_acc: 1.0000
Epoch: 1882--------
Loss: 0.0384
Train_acc: 1.0000
Epoch: 1883--------
Loss: 0.0384
Train_acc: 1.0000
Epoch: 1884--------
Loss: 0.0384
Train_acc: 1.0000
Epoch: 1885--------
Loss: 0.0384
Train_acc: 1.0000
Epoch: 1886--------
Loss: 0.0384
Train_acc: 1.0000
Epoch: 1887--------
Loss: 0.0383
Train_acc: 1.0000
Epoch: 1888--------
Loss: 0.0383
Train_acc: 1.0000
Epoch: 1889--------
Loss: 0.0383
Train_acc: 1.0000
Epoch: 1890--------
Loss: 0.0383
Train_acc: 1.0000
Epoch: 1891--------
Loss: 0.0382
Train_acc: 1.0000
Epoch: 1892--------
Loss: 0.0382
Train_acc: 1.0000
Epoch: 1893--------
Loss: 0.0382
Train_acc: 1.0000
Epoch: 1894--------
Loss: 0.0382
Train_acc: 1.0000
Epoch: 1895--------
Loss: 0.0381
Train_acc: 1.0000
Epoch: 1896--------
Loss: 0.0381
Train_acc: 1.0000
Epoch: 1897--------
Loss: 0.0381
Train_acc: 1.0000
Epoch: 1898--------
Loss: 0.0381
Train_acc: 1.0000
Epoch: 1899--------
Loss: 0.0380
Train_acc: 1.0000
Epoch: 1900--------
Loss: 0.0380
Train_acc: 1.0000
Test_acc: 1.0000
Epoch: 1901--------
Loss: 0.0380
Train_acc: 1.0000
Epoch: 1902--------
Loss: 0.0380
Train_acc: 1.0000
Epoch: 1903--------
Loss: 0.0379
Train_acc: 1.0000
Epoch: 1904--------
Loss: 0.0379
Train_acc: 1.0000
Epoch: 1905--------
Loss: 0.0379
Train_acc: 1.0000
Epoch: 1906--------
Loss: 0.0379
Train_acc: 1.0000
Epoch: 1907--------
Loss: 0.0378
Train_acc: 1.0000
Epoch: 1908--------
Loss: 0.0378
Train_acc: 1.0000
Epoch: 1909--------
Loss: 0.0378
Train_acc: 1.0000
Epoch: 1910--------
Loss: 0.0378
Train_acc: 1.0000
Epoch: 1911--------
Loss: 0.0378
Train_acc: 1.0000
Epoch: 1912--------
Loss: 0.0377
Train_acc: 1.0000
Epoch: 1913--------
Loss: 0.0377
Train_acc: 1.0000
Epoch: 1914--------
Loss: 0.0377
Train_acc: 1.0000
Epoch: 1915--------
Loss: 0.0377
Train_acc: 1.0000
Epoch: 1916--------
Loss: 0.0376
Train_acc: 1.0000
Epoch: 1917--------
Loss: 0.0376
Train_acc: 1.0000
Epoch: 1918--------
Loss: 0.0376
Train_acc: 1.0000
Epoch: 1919--------
Loss: 0.0376
Train_acc: 1.0000
Epoch: 1920--------
Loss: 0.0375
Train_acc: 1.0000
Test_acc: 1.0000
Epoch: 1921--------
Loss: 0.0375
Train_acc: 1.0000
Epoch: 1922--------
Loss: 0.0375
Train_acc: 1.0000
Epoch: 1923--------
Loss: 0.0375
Train_acc: 1.0000
Epoch: 1924--------
Loss: 0.0374
Train_acc: 1.0000
Epoch: 1925--------
Loss: 0.0374
Train_acc: 1.0000
Epoch: 1926--------
Loss: 0.0374
Train_acc: 1.0000
Epoch: 1927--------
Loss: 0.0374
Train_acc: 1.0000
Epoch: 1928--------
Loss: 0.0374
Train_acc: 1.0000
Epoch: 1929--------
Loss: 0.0373
Train_acc: 1.0000
Epoch: 1930--------
Loss: 0.0373
Train_acc: 1.0000
Epoch: 1931--------
Loss: 0.0373
Train_acc: 1.0000
Epoch: 1932--------
Loss: 0.0373
Train_acc: 1.0000
Epoch: 1933--------
Loss: 0.0372
Train_acc: 1.0000
Epoch: 1934--------
Loss: 0.0372
Train_acc: 1.0000
Epoch: 1935--------
Loss: 0.0372
Train_acc: 1.0000
Epoch: 1936--------
Loss: 0.0372
Train_acc: 1.0000
Epoch: 1937--------
Loss: 0.0371
Train_acc: 1.0000
Epoch: 1938--------
Loss: 0.0371
Train_acc: 1.0000
Epoch: 1939--------
Loss: 0.0371
Train_acc: 1.0000
Epoch: 1940--------
Loss: 0.0371
Train_acc: 1.0000
Test_acc: 1.0000
Epoch: 1941--------
Loss: 0.0371
Train_acc: 1.0000
Epoch: 1942--------
Loss: 0.0370
Train_acc: 1.0000
Epoch: 1943--------
Loss: 0.0370
Train_acc: 1.0000
Epoch: 1944--------
Loss: 0.0370
Train_acc: 1.0000
Epoch: 1945--------
Loss: 0.0370
Train_acc: 1.0000
Epoch: 1946--------
Loss: 0.0369
Train_acc: 1.0000
Epoch: 1947--------
Loss: 0.0369
Train_acc: 1.0000
Epoch: 1948--------
Loss: 0.0369
Train_acc: 1.0000
Epoch: 1949--------
Loss: 0.0369
Train_acc: 1.0000
Epoch: 1950--------
Loss: 0.0368
Train_acc: 1.0000
Epoch: 1951--------
Loss: 0.0368
Train_acc: 1.0000
Epoch: 1952--------
Loss: 0.0368
Train_acc: 1.0000
Epoch: 1953--------
Loss: 0.0368
Train_acc: 1.0000
Epoch: 1954--------
Loss: 0.0368
Train_acc: 1.0000
Epoch: 1955--------
Loss: 0.0367
Train_acc: 1.0000
Epoch: 1956--------
Loss: 0.0367
Train_acc: 1.0000
Epoch: 1957--------
Loss: 0.0367
Train_acc: 1.0000
Epoch: 1958--------
Loss: 0.0367
Train_acc: 1.0000
Epoch: 1959--------
Loss: 0.0366
Train_acc: 1.0000
Epoch: 1960--------
Loss: 0.0366
Train_acc: 1.0000
Test_acc: 1.0000
Epoch: 1961--------
Loss: 0.0366
Train_acc: 1.0000
Epoch: 1962--------
Loss: 0.0366
Train_acc: 1.0000
Epoch: 1963--------
Loss: 0.0365
Train_acc: 1.0000
Epoch: 1964--------
Loss: 0.0365
Train_acc: 1.0000
Epoch: 1965--------
Loss: 0.0365
Train_acc: 1.0000
Epoch: 1966--------
Loss: 0.0365
Train_acc: 1.0000
Epoch: 1967--------
Loss: 0.0365
Train_acc: 1.0000
Epoch: 1968--------
Loss: 0.0364
Train_acc: 1.0000
Epoch: 1969--------
Loss: 0.0364
Train_acc: 1.0000
Epoch: 1970--------
Loss: 0.0364
Train_acc: 1.0000
Epoch: 1971--------
Loss: 0.0364
Train_acc: 1.0000
Epoch: 1972--------
Loss: 0.0363
Train_acc: 1.0000
Epoch: 1973--------
Loss: 0.0363
Train_acc: 1.0000
Epoch: 1974--------
Loss: 0.0363
Train_acc: 1.0000
Epoch: 1975--------
Loss: 0.0363
Train_acc: 1.0000
Epoch: 1976--------
Loss: 0.0363
Train_acc: 1.0000
Epoch: 1977--------
Loss: 0.0362
Train_acc: 1.0000
Epoch: 1978--------
Loss: 0.0362
Train_acc: 1.0000
Epoch: 1979--------
Loss: 0.0362
Train_acc: 1.0000
Epoch: 1980--------
Loss: 0.0362
Train_acc: 1.0000
Test_acc: 1.0000
Epoch: 1981--------
Loss: 0.0361
Train_acc: 1.0000
Epoch: 1982--------
Loss: 0.0361
Train_acc: 1.0000
Epoch: 1983--------
Loss: 0.0361
Train_acc: 1.0000
Epoch: 1984--------
Loss: 0.0361
Train_acc: 1.0000
Epoch: 1985--------
Loss: 0.0360
Train_acc: 1.0000
Epoch: 1986--------
Loss: 0.0360
Train_acc: 1.0000
Epoch: 1987--------
Loss: 0.0360
Train_acc: 1.0000
Epoch: 1988--------
Loss: 0.0360
Train_acc: 1.0000
Epoch: 1989--------
Loss: 0.0360
Train_acc: 1.0000
Epoch: 1990--------
Loss: 0.0359
Train_acc: 1.0000
Epoch: 1991--------
Loss: 0.0359
Train_acc: 1.0000
Epoch: 1992--------
Loss: 0.0359
Train_acc: 1.0000
Epoch: 1993--------
Loss: 0.0359
Train_acc: 1.0000
Epoch: 1994--------
Loss: 0.0358
Train_acc: 1.0000
Epoch: 1995--------
Loss: 0.0358
Train_acc: 1.0000
Epoch: 1996--------
Loss: 0.0358
Train_acc: 1.0000
Epoch: 1997--------
Loss: 0.0358
Train_acc: 1.0000
Epoch: 1998--------
Loss: 0.0358
Train_acc: 1.0000
Epoch: 1999--------
Loss: 0.0357
Train_acc: 1.0000
Test_acc: 1.0000
</pre></div></div>
</div>
<p>Finally, we plot the training results</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span> In [24]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">dpi</span><span class="o">=</span><span class="mi">300</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">losses</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Training loss&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">train_accs</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Training accuracy&quot;</span><span class="p">)</span>
<span class="n">test_epochs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">isfinite</span><span class="p">(</span><span class="n">test_accs</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span>
    <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">num_epochs</span><span class="p">)[</span><span class="n">test_epochs</span><span class="p">],</span>
    <span class="n">test_accs</span><span class="p">[</span><span class="n">test_epochs</span><span class="p">],</span>
    <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Test accuracy&quot;</span><span class="p">,</span>
    <span class="n">linestyle</span><span class="o">=</span><span class="s2">&quot;-&quot;</span><span class="p">,</span>
    <span class="n">marker</span><span class="o">=</span><span class="s2">&quot;o&quot;</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Epochs&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Training results&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span> Out [24]:
</pre></div>
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Text(0.5, 1.0, &#39;Training results&#39;)
</pre></div></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../../_images/notebooks_hypergraph_hnhn_train_21_1.png" src="../../_images/notebooks_hypergraph_hnhn_train_21_1.png" />
</div>
</div>
</section>


                </article>
              
              
              
              
              
                <footer class="prev-next-footer">
                  <!-- Previous / next buttons -->
<div class="prev-next-area">
    <a class="left-prev"
       href="hmpnn_train.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Train a Hypergraph Message Passing Neural Network (HMPNN)</p>
      </div>
    </a>
    <a class="right-next"
       href="hnhn_train_bis.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Train a Hypergraph Network with Hyperedge Neurons (HNHN)</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> On this page
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#">Train a Hypergraph Networks with Hyperedge Neurons (HNHN)</a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#Pre-processing">Pre-processing</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#Import-dataset">Import dataset</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#Define-neighborhood-structures.">Define neighborhood structures.</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#Import-signal">Import signal</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#Create-the-Neural-Network">Create the Neural Network</a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#Train-the-Neural-Network">Train the Neural Network</a></li>
</ul>

  </nav></div>

  <div class="sidebar-secondary-item">
  <div class="tocsection sourcelink">
    <a href="../../_sources/notebooks/hypergraph/hnhn_train.ipynb.txt">
      <i class="fa-solid fa-file-lines"></i> Show Source
    </a>
  </div>
</div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
          </footer>
        
      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../_static/scripts/bootstrap.js?digest=ac02cc09edc035673794"></script>
<script src="../../_static/scripts/pydata-sphinx-theme.js?digest=ac02cc09edc035673794"></script>

  <footer class="bd-footer">
<div class="bd-footer__inner bd-page-width">
  
    <div class="footer-items__start">
      
        <div class="footer-item">
  <p class="copyright">
    
      Â© Copyright 2022-2023, PyT-Team, Inc..
      <br/>
    
  </p>
</div>
      
        <div class="footer-item">
  <p class="sphinx-version">
    Created using <a href="https://www.sphinx-doc.org/">Sphinx</a> 7.2.6.
    <br/>
  </p>
</div>
      
    </div>
  
  
  
    <div class="footer-items__end">
      
        <div class="footer-item"><p class="theme-version">
  Built with the <a href="https://pydata-sphinx-theme.readthedocs.io/en/stable/index.html">PyData Sphinx Theme</a> 0.14.1.
</p></div>
      
    </div>
  
</div>

  </footer>
  </body>
</html>