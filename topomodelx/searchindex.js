Search.setIndex({"docnames": ["api/base/aggregation", "api/base/conv", "api/base/index", "api/base/message_passing", "api/index", "api/nn/cell/can", "api/nn/cell/can_layer", "api/nn/cell/ccxn", "api/nn/cell/ccxn_layer", "api/nn/cell/cwn", "api/nn/cell/cwn_layer", "api/nn/cell/index", "api/nn/hypergraph/allset", "api/nn/hypergraph/allset_layer", "api/nn/hypergraph/allset_transformer", "api/nn/hypergraph/allset_transformer_layer", "api/nn/hypergraph/dhgcn", "api/nn/hypergraph/dhgcn_layer", "api/nn/hypergraph/hmpnn", "api/nn/hypergraph/hmpnn_layer", "api/nn/hypergraph/hnhn", "api/nn/hypergraph/hnhn_layer", "api/nn/hypergraph/hnhn_layer_bis", "api/nn/hypergraph/hypergat", "api/nn/hypergraph/hypergat_layer", "api/nn/hypergraph/hypersage", "api/nn/hypergraph/hypersage_layer", "api/nn/hypergraph/index", "api/nn/hypergraph/template_layer", "api/nn/hypergraph/unigcn", "api/nn/hypergraph/unigcn_layer", "api/nn/hypergraph/unigcnii", "api/nn/hypergraph/unigcnii_layer", "api/nn/hypergraph/unigin", "api/nn/hypergraph/unigin_layer", "api/nn/hypergraph/unisage", "api/nn/hypergraph/unisage_layer", "api/nn/index", "api/nn/simplicial/dist2cycle", "api/nn/simplicial/dist2cycle_layer", "api/nn/simplicial/hsn", "api/nn/simplicial/hsn_layer", "api/nn/simplicial/index", "api/nn/simplicial/san", "api/nn/simplicial/san_layer", "api/nn/simplicial/sca_cmps", "api/nn/simplicial/sca_cmps_layer", "api/nn/simplicial/sccn", "api/nn/simplicial/sccn_layer", "api/nn/simplicial/sccnn", "api/nn/simplicial/sccnn_layer", "api/nn/simplicial/scconv", "api/nn/simplicial/scconv_layer", "api/nn/simplicial/scn2", "api/nn/simplicial/scn2_layer", "api/nn/simplicial/scnn", "api/nn/simplicial/scnn_layer", "api/nn/simplicial/scone", "api/nn/simplicial/scone_layer", "api/utils/index", "challenge/index", "contributing/index", "index", "notebooks/cell/can_train", "notebooks/cell/ccxn_train", "notebooks/cell/cwn_train", "notebooks/hypergraph/allset_train", "notebooks/hypergraph/allset_transformer_train", "notebooks/hypergraph/dhgcn_train", "notebooks/hypergraph/hmpnn_train", "notebooks/hypergraph/hnhn_train", "notebooks/hypergraph/hnhn_train_bis", "notebooks/hypergraph/hypergat_train", "notebooks/hypergraph/hypersage_train", "notebooks/hypergraph/template_train", "notebooks/hypergraph/unigcn_train", "notebooks/hypergraph/unigcnii_train", "notebooks/hypergraph/unigin_train", "notebooks/hypergraph/unisage_train", "notebooks/simplicial/dist2cycle_train", "notebooks/simplicial/hsn_train", "notebooks/simplicial/san_train", "notebooks/simplicial/sca_cmps_train", "notebooks/simplicial/sccn_train", "notebooks/simplicial/sccnn_train", "notebooks/simplicial/scconv_train", "notebooks/simplicial/scn2_train", "notebooks/simplicial/scnn_train", "notebooks/simplicial/scone_train", "tutorials/index"], "filenames": ["api/base/aggregation.rst", "api/base/conv.rst", "api/base/index.rst", "api/base/message_passing.rst", "api/index.rst", "api/nn/cell/can.rst", "api/nn/cell/can_layer.rst", "api/nn/cell/ccxn.rst", "api/nn/cell/ccxn_layer.rst", "api/nn/cell/cwn.rst", "api/nn/cell/cwn_layer.rst", "api/nn/cell/index.rst", "api/nn/hypergraph/allset.rst", "api/nn/hypergraph/allset_layer.rst", "api/nn/hypergraph/allset_transformer.rst", "api/nn/hypergraph/allset_transformer_layer.rst", "api/nn/hypergraph/dhgcn.rst", "api/nn/hypergraph/dhgcn_layer.rst", "api/nn/hypergraph/hmpnn.rst", "api/nn/hypergraph/hmpnn_layer.rst", "api/nn/hypergraph/hnhn.rst", "api/nn/hypergraph/hnhn_layer.rst", "api/nn/hypergraph/hnhn_layer_bis.rst", "api/nn/hypergraph/hypergat.rst", "api/nn/hypergraph/hypergat_layer.rst", "api/nn/hypergraph/hypersage.rst", "api/nn/hypergraph/hypersage_layer.rst", "api/nn/hypergraph/index.rst", "api/nn/hypergraph/template_layer.rst", "api/nn/hypergraph/unigcn.rst", "api/nn/hypergraph/unigcn_layer.rst", "api/nn/hypergraph/unigcnii.rst", "api/nn/hypergraph/unigcnii_layer.rst", "api/nn/hypergraph/unigin.rst", "api/nn/hypergraph/unigin_layer.rst", "api/nn/hypergraph/unisage.rst", "api/nn/hypergraph/unisage_layer.rst", "api/nn/index.rst", "api/nn/simplicial/dist2cycle.rst", "api/nn/simplicial/dist2cycle_layer.rst", "api/nn/simplicial/hsn.rst", "api/nn/simplicial/hsn_layer.rst", "api/nn/simplicial/index.rst", "api/nn/simplicial/san.rst", "api/nn/simplicial/san_layer.rst", "api/nn/simplicial/sca_cmps.rst", "api/nn/simplicial/sca_cmps_layer.rst", "api/nn/simplicial/sccn.rst", "api/nn/simplicial/sccn_layer.rst", "api/nn/simplicial/sccnn.rst", "api/nn/simplicial/sccnn_layer.rst", "api/nn/simplicial/scconv.rst", "api/nn/simplicial/scconv_layer.rst", "api/nn/simplicial/scn2.rst", "api/nn/simplicial/scn2_layer.rst", "api/nn/simplicial/scnn.rst", "api/nn/simplicial/scnn_layer.rst", "api/nn/simplicial/scone.rst", "api/nn/simplicial/scone_layer.rst", "api/utils/index.rst", "challenge/index.rst", "contributing/index.rst", "index.rst", "notebooks/cell/can_train.ipynb", "notebooks/cell/ccxn_train.ipynb", "notebooks/cell/cwn_train.ipynb", "notebooks/hypergraph/allset_train.ipynb", "notebooks/hypergraph/allset_transformer_train.ipynb", "notebooks/hypergraph/dhgcn_train.ipynb", "notebooks/hypergraph/hmpnn_train.ipynb", "notebooks/hypergraph/hnhn_train.ipynb", "notebooks/hypergraph/hnhn_train_bis.ipynb", "notebooks/hypergraph/hypergat_train.ipynb", "notebooks/hypergraph/hypersage_train.ipynb", "notebooks/hypergraph/template_train.ipynb", "notebooks/hypergraph/unigcn_train.ipynb", "notebooks/hypergraph/unigcnii_train.ipynb", "notebooks/hypergraph/unigin_train.ipynb", "notebooks/hypergraph/unisage_train.ipynb", "notebooks/simplicial/dist2cycle_train.ipynb", "notebooks/simplicial/hsn_train.ipynb", "notebooks/simplicial/san_train.ipynb", "notebooks/simplicial/sca_cmps_train.ipynb", "notebooks/simplicial/sccn_train.ipynb", "notebooks/simplicial/sccnn_train.ipynb", "notebooks/simplicial/scconv_train.ipynb", "notebooks/simplicial/scn2_train.ipynb", "notebooks/simplicial/scnn_train.ipynb", "notebooks/simplicial/scone_train.ipynb", "tutorials/index.rst"], "titles": ["Aggregation", "Conv", "Base", "Message Passing", "API Reference", "CAN", "Can_Layer", "CCXN", "CCXN_Layer", "CWN", "Cwn_Layer", "Cell", "AllSet", "AllSet_Layer", "AllSet_Transformer", "AllSet_Transformer_Layer", "DHGCN", "DHGCN_Layer", "HMPNN", "HMPNN_Layer", "HNHN", "HNHN_Layer", "HNHN_Layer_Bis", "Hypergat", "Hypergat_Layer", "Hypersage", "Hypersage_Layer", "Hypergraph", "Template_Layer", "Unigcn", "Unigcn_Layer", "Unigcnii", "Unigcnii_Layer", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "Neural Networks", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "Simplicial", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "Utils", "ICML 2023 Topological Deep Learning Challenge", "Contributing", "\ud83c\udf10 TopoModelX (TMX) \ud83c\udf69", "Train a Cell Attention Network (CAN)", "Train a Convolutional Cell Complex Network (CCXN)", "Train a CW Network (CWN)", "Train an All-Set TNN", "Train an All-Set-Transformer TNN", "Train a DHGCN TNN", "Train a Hypergraph Message Passing Neural Network (HMPNN)", "Train a Hypergraph Networks with Hyperedge Neurons (HNHN)", "Train a Hypergraph Network with Hyperedge Neurons (HNHN)", "Train a Hypergraph Neural Network", "Train a Hypersage TNN", "Train a (template) Hypergraph Neural Network", "Train a UNIGCN TNN", "Train a hypergraph neural network using UniGCNII layers", "Train a UNIGIN TNN", "Train a Uni-sage TNN", "Train a Simplicial Neural Network for Homology Localization (Dist2Cycle)", "Train a Simplicial High-Skip Network (HSN)", "Train a Simplicial Attention Network (SAN)", "Train a Simplicial Complex Autoencoder (SCA) with Coadjacency Message Passing Scheme (CMPS)", "Train a Simplicial Complex Convolutional Network (SCCN)", "Train a SCCNN", "Train a Simplicial 2-complex convolutional neural network (SCConv)", "Train a Simplex Convolutional Network (SCN) of Rank 2", "Train a Simplicial Convolutional Neural Network (SCNN)", "Train a Simplicial Complex Net (SCoNe)", "Tutorials"], "terms": {"modul": [0, 3, 5, 6, 10, 12, 13, 14, 15, 19, 34, 37, 60, 61, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 84, 87], "class": [0, 1, 2, 3, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 87, 88], "topomodelx": [0, 1, 3, 4, 5, 6, 7, 8, 9, 10, 12, 13, 14, 15, 18, 19, 20, 21, 22, 23, 24, 25, 26, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88], "base": [0, 1, 3, 4, 10, 11, 15, 27, 42, 46, 60, 67, 84, 85, 87, 88], "aggr_func": [0, 3, 6, 19, 48, 63], "liter": [0, 1, 3, 6, 15, 19, 21, 24, 26, 36, 44, 48, 58], "mean": [0, 3, 6, 19, 26, 36, 48, 50, 59, 61, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 76, 77, 79, 80, 81, 83, 84, 85, 86, 87, 88], "sum": [0, 3, 6, 19, 30, 32, 34, 36, 48, 50, 63, 64, 65, 70, 72, 81, 83, 85, 88], "update_func": [0, 1, 6, 15, 24, 26, 47, 48, 49, 50, 51, 55, 56, 58, 63, 83, 87, 88], "relu": [0, 1, 6, 13, 15, 22, 24, 26, 48, 58, 63, 72, 77, 88], "sigmoid": [0, 1, 6, 19, 26, 47, 48, 58, 63, 66, 67, 72, 73, 74, 75, 77, 78, 83, 88], "tanh": [0, 6, 48, 58], "none": [0, 1, 3, 6, 8, 10, 12, 13, 15, 19, 21, 24, 26, 28, 30, 32, 36, 39, 41, 44, 46, 48, 49, 50, 52, 54, 55, 56, 58, 59, 61, 66, 87, 88], "sourc": [0, 1, 3, 5, 6, 7, 8, 9, 10, 12, 13, 14, 15, 18, 19, 20, 21, 22, 23, 24, 25, 26, 28, 29, 30, 31, 32, 33, 34, 35, 36, 38, 39, 40, 41, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60], "messag": [0, 1, 2, 4, 6, 8, 10, 15, 18, 19, 20, 21, 23, 24, 25, 26, 28, 29, 30, 31, 32, 33, 34, 35, 36, 38, 40, 43, 44, 46, 47, 48, 51, 53, 54, 60, 63, 64, 65, 66, 67, 68, 70, 72, 73, 74, 75, 76, 77, 78, 81, 83], "pass": [0, 1, 2, 4, 6, 8, 10, 15, 18, 19, 20, 21, 23, 24, 25, 26, 28, 29, 30, 31, 32, 33, 34, 35, 36, 38, 39, 40, 41, 43, 44, 46, 47, 48, 51, 52, 53, 54, 55, 57, 58, 60, 63, 64, 66, 67, 68, 70, 72, 73, 74, 75, 76, 77, 78, 81, 83, 87, 88], "layer": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 12, 13, 14, 15, 18, 19, 20, 21, 22, 23, 24, 25, 26, 28, 29, 30, 31, 32, 33, 34, 35, 36, 38, 39, 40, 41, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 58, 60, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88], "paramet": [0, 1, 3, 5, 6, 7, 8, 9, 10, 12, 13, 14, 15, 18, 19, 20, 21, 22, 23, 24, 25, 26, 28, 29, 30, 31, 32, 33, 34, 35, 36, 38, 39, 40, 41, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 58, 61, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88], "default": [0, 1, 3, 5, 6, 8, 10, 12, 13, 14, 15, 19, 21, 24, 25, 26, 31, 36, 43, 44, 48, 61, 63, 66, 67, 71, 73, 76], "method": [0, 1, 3, 6, 15, 21, 23, 24, 26, 44, 50, 56, 60, 61, 69, 71], "inter": [0, 26, 73, 84], "neighborhood": [0, 1, 2, 3, 6, 8, 10, 15, 30, 44, 60, 63, 64, 65, 66, 67, 73, 81, 82, 88], "updat": [0, 1, 2, 3, 6, 10, 15, 24, 26, 30, 32, 34, 36, 42, 44, 50, 56, 58, 63, 66, 67, 71, 81, 88], "appli": [0, 1, 3, 5, 6, 10, 12, 14, 15, 18, 19, 24, 26, 48, 58, 63, 66, 67, 69, 72, 81, 85, 88], "merg": 0, "forward": [0, 1, 2, 3, 5, 6, 7, 8, 9, 10, 12, 13, 14, 15, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 87, 88], "x": [0, 1, 3, 6, 8, 10, 12, 13, 14, 15, 19, 21, 24, 25, 26, 30, 32, 34, 36, 39, 41, 43, 44, 46, 48, 49, 50, 52, 54, 55, 56, 57, 58, 61, 63, 64, 65, 66, 67, 69, 70, 71, 72, 73, 75, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88], "list": [0, 12, 13, 14, 15, 45, 46, 57, 61, 66, 67, 68, 76, 88], "len": [0, 69, 70, 71, 75, 76, 77, 78, 79, 80, 81, 83, 84, 85, 87, 88], "n_messages_to_merg": 0, "each": [0, 1, 3, 6, 24, 26, 41, 45, 46, 47, 48, 49, 50, 56, 58, 60, 61, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 83, 84, 88], "ha": [0, 1, 6, 61, 64, 65, 66, 67, 68, 70, 72, 73, 74, 76, 79, 80, 81, 82, 83, 84, 85, 86, 87], "shape": [0, 1, 3, 5, 6, 7, 8, 9, 10, 13, 15, 18, 19, 20, 21, 22, 23, 24, 25, 26, 28, 29, 30, 31, 32, 33, 34, 35, 36, 38, 39, 40, 41, 43, 44, 45, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 58, 59, 61, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87], "n_skeleton_in": 0, "channel": [0, 5, 6, 8, 13, 15, 32, 38, 39, 40, 41, 44, 47, 48, 49, 51, 52, 53, 54, 55, 56, 63, 76, 79, 80, 81, 83, 87], "input": [0, 1, 3, 5, 6, 7, 8, 9, 10, 12, 13, 14, 15, 18, 19, 21, 22, 24, 26, 28, 30, 31, 32, 34, 36, 39, 41, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 58, 61, 63, 64, 65, 66, 67, 68, 69, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 83, 84, 87], "step": [0, 1, 3, 6, 8, 10, 21, 24, 26, 28, 30, 34, 36, 50, 56, 60, 61, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88], "4": [0, 1, 10, 14, 15, 24, 26, 50, 56, 61, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 83, 84, 85, 86, 87, 88], "h": [0, 3, 24, 32, 41, 48, 52, 63, 67, 72, 73, 76, 80, 81, 83, 84, 85, 87], "arrai": [0, 61, 64, 65, 66, 67, 68, 70, 72, 73, 74, 76, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88], "like": [0, 50, 56, 58, 61, 63, 64, 81, 83, 84, 87], "n_skeleton_out": 0, "out_channel": [0, 1, 3, 5, 6, 10, 12, 14, 15, 23, 24, 25, 26, 28, 30, 33, 34, 36, 43, 44, 50, 55, 56, 58, 63, 65, 66, 67, 68, 72, 73, 74, 75, 77, 78, 81, 87], "featur": [0, 1, 3, 5, 6, 7, 8, 9, 10, 12, 13, 14, 15, 18, 19, 20, 21, 22, 23, 24, 25, 26, 28, 29, 30, 31, 32, 33, 34, 35, 36, 38, 39, 40, 41, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 58, 60, 61, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 80, 81, 82, 83, 84, 85, 86, 87, 88], "skeleton": [0, 46, 63, 88], "out": [0, 6, 59, 60, 61, 62, 63, 68, 70, 71, 74, 81, 82, 83, 88], "return": [0, 1, 3, 5, 6, 7, 8, 9, 10, 12, 13, 14, 15, 18, 19, 20, 21, 22, 23, 24, 25, 26, 28, 29, 30, 31, 32, 33, 34, 35, 36, 38, 39, 40, 41, 43, 44, 45, 46, 47, 48, 49, 50, 51, 53, 54, 56, 58, 59, 61, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 83, 84, 85, 87, 88], "convolut": [1, 6, 8, 9, 10, 21, 43, 44, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 60, 65, 81], "in_channel": [1, 3, 6, 12, 13, 14, 15, 23, 24, 25, 26, 28, 30, 31, 32, 34, 36, 43, 44, 50, 55, 56, 58, 63, 66, 67, 68, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 83, 84, 87], "aggr_norm": [1, 15, 30, 49, 50, 55, 56, 87], "bool": [1, 3, 5, 6, 7, 8, 12, 13, 14, 15, 21, 30, 34, 36, 45, 46, 50, 56, 61, 63, 64, 66, 67], "fals": [1, 3, 6, 7, 8, 12, 13, 14, 15, 30, 34, 36, 45, 46, 49, 50, 55, 56, 63, 64, 65, 66, 67, 68, 70, 72, 73, 74, 75, 77, 78, 82, 84, 85, 86, 87], "att": [1, 3, 7, 8, 24, 44, 45, 46, 50, 64, 72, 82], "initi": [1, 3, 6, 8, 10, 15, 21, 24, 26, 30, 34, 36, 41, 44, 46, 48, 50, 56, 58, 63, 64, 66, 67, 69, 70, 71, 73, 81, 84, 85, 87], "xavier_uniform": [1, 3, 6, 15, 21, 24, 26, 44, 56, 73], "xavier_norm": [1, 3, 6, 15, 21, 24, 26, 44, 50], "initialization_gain": [1, 3, 15, 24], "float": [1, 3, 5, 6, 12, 13, 14, 15, 19, 21, 22, 24, 31, 32, 34, 43, 50, 56, 58, 63, 64, 65, 66, 67, 68, 70, 71, 72, 73, 74, 75, 76, 78, 79, 80, 81, 83, 84, 85, 86, 87, 88], "1": [1, 3, 5, 6, 7, 8, 9, 10, 13, 14, 15, 19, 20, 21, 23, 24, 25, 26, 29, 30, 31, 32, 33, 34, 35, 36, 38, 39, 40, 41, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 54, 55, 56, 58, 59, 60, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 85, 86, 88], "414": [1, 3, 15, 21, 24, 50, 56, 70, 75], "with_linear_transform": 1, "true": [1, 5, 6, 21, 63, 64, 65, 66, 67, 68, 70, 72, 73, 74, 75, 77, 78, 79, 80, 81, 83, 84, 85, 86, 87, 88], "2": [1, 3, 5, 6, 7, 8, 9, 10, 12, 13, 14, 18, 20, 21, 23, 25, 26, 29, 30, 31, 32, 33, 35, 36, 38, 40, 41, 43, 44, 45, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 60, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 88], "3": [1, 5, 10, 30, 32, 36, 61, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88], "build": [1, 2, 22, 57, 60, 61, 88], "rout": 1, "given": [1, 3, 8, 10, 19, 21, 22, 30, 34, 36, 41, 46, 48, 58, 60, 63, 64, 65, 66, 67, 70, 72, 73, 76, 79, 80, 81, 83, 84, 85, 87, 88], "one": [1, 3, 15, 21, 24, 48, 54, 58, 60, 61, 64, 70, 76, 79, 80, 81, 82, 83, 84, 85, 88], "matrix": [1, 3, 6, 7, 8, 9, 10, 13, 15, 18, 19, 20, 21, 22, 23, 24, 25, 26, 28, 29, 30, 31, 32, 33, 34, 35, 36, 38, 39, 40, 41, 43, 44, 45, 50, 51, 52, 54, 55, 56, 58, 61, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 86, 87, 88], "includ": [1, 60, 61, 88], "an": [1, 4, 6, 8, 10, 50, 56, 60, 61, 63, 64, 65, 68, 70, 72, 73, 74, 75, 77, 78, 79, 80, 81, 83, 84, 85, 86, 87, 88], "option": [1, 3, 5, 6, 7, 8, 10, 12, 13, 14, 15, 32, 43, 44, 60, 61, 63, 64, 66, 67], "specif": [1, 9, 60, 61, 65, 70, 72, 76], "function": [1, 3, 5, 6, 12, 13, 14, 15, 19, 22, 26, 32, 36, 46, 47, 48, 50, 51, 52, 56, 58, 59, 60, 61, 63, 65, 66, 67, 73, 76, 79, 80, 81, 83, 84, 87, 88], "int": [1, 5, 6, 7, 8, 9, 10, 12, 13, 14, 15, 18, 19, 20, 21, 22, 23, 24, 25, 26, 28, 29, 30, 31, 32, 33, 34, 35, 36, 38, 40, 41, 43, 44, 45, 46, 47, 48, 49, 50, 51, 53, 54, 55, 56, 57, 58, 59, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 82, 87, 88], "dimens": [1, 6, 7, 8, 9, 10, 12, 13, 14, 15, 18, 19, 20, 21, 22, 23, 24, 25, 26, 28, 29, 30, 33, 34, 35, 36, 38, 40, 41, 43, 45, 46, 47, 48, 49, 50, 51, 53, 54, 55, 56, 57, 58, 61, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88], "output": [1, 3, 5, 6, 8, 10, 12, 13, 14, 15, 19, 21, 22, 24, 26, 28, 30, 32, 34, 36, 39, 41, 43, 44, 46, 48, 49, 50, 54, 55, 56, 58, 60, 61, 63, 66, 67, 68, 72, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 87, 88], "whether": [1, 3, 5, 6, 7, 8, 12, 13, 14, 15, 21, 30, 34, 36, 45, 46, 63, 64, 66, 67], "normal": [1, 6, 12, 13, 14, 15, 21, 22, 30, 32, 48, 50, 51, 52, 54, 56, 66, 67, 83, 86], "aggreg": [1, 2, 3, 4, 6, 10, 15, 19, 26, 30, 36, 46, 47, 48, 50, 51, 56, 60, 63, 72, 73, 81, 85, 87], "size": [1, 12, 14, 15, 30, 63, 64, 65, 66, 67, 68, 70, 72, 73, 74, 76, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88], "us": [1, 3, 5, 6, 7, 8, 10, 19, 21, 22, 30, 31, 32, 34, 36, 43, 45, 46, 47, 48, 51, 52, 57, 58, 60, 61, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 77, 78, 79, 80, 81, 82, 83, 84, 86, 87, 88], "attent": [1, 2, 3, 5, 6, 7, 8, 15, 23, 24, 43, 44, 45, 46, 60, 67, 72], "learnabl": [1, 3, 6, 13, 15, 21, 28, 30, 36, 39, 41, 44, 48, 50, 54, 56, 58, 63, 67, 81, 87], "linear": [1, 6, 7, 9, 19, 23, 25, 29, 30, 32, 33, 35, 36, 45, 55, 60, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 83, 84, 87, 88], "transform": [1, 6, 14, 15, 30, 32, 36, 61, 71, 76, 81], "nb": 1, "equal": [1, 61, 72, 88], "x_sourc": [1, 3, 6, 15, 24, 44], "x_target": [1, 3, 6, 24, 26], "tensor": [1, 3, 4, 5, 6, 7, 8, 9, 10, 12, 13, 14, 15, 18, 19, 20, 21, 22, 23, 24, 25, 26, 28, 29, 30, 31, 32, 33, 34, 35, 36, 38, 39, 40, 41, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 83, 84, 85, 86, 87, 88], "thi": [1, 2, 3, 6, 8, 10, 19, 21, 22, 26, 30, 34, 36, 41, 44, 46, 48, 49, 50, 54, 55, 56, 58, 59, 60, 61, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 76, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88], "implement": [1, 3, 4, 6, 8, 9, 10, 15, 18, 20, 21, 23, 24, 25, 26, 29, 30, 32, 33, 34, 35, 36, 38, 40, 41, 43, 44, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 60, 63, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 77, 78, 81, 82, 83, 87, 88], "from": [1, 3, 5, 6, 8, 10, 15, 18, 19, 21, 26, 30, 34, 36, 44, 46, 57, 58, 59, 60, 61, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88], "cell": [1, 3, 5, 6, 7, 8, 9, 10, 15, 24, 26, 37, 41, 44, 47, 48, 50, 51, 54, 56, 60, 65, 66, 67, 68, 73, 74, 75, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 88], "via": [1, 3, 39, 41, 44, 47, 48, 87], "defin": [1, 2, 3, 34, 44, 57, 60, 61, 63, 64, 65, 66, 76, 81, 82, 88], "where": [1, 3, 44, 46, 50, 56, 60, 61, 63, 64, 65, 67, 70, 72, 73, 79, 80, 81, 82, 83, 84, 85, 87, 88], "can": [1, 3, 6, 8, 11, 21, 44, 48, 49, 56, 58, 60, 61, 62, 72, 73, 76, 79, 80, 81, 83, 85, 87, 88], "target": [1, 3, 6, 15, 24, 26, 44, 50, 56, 64, 84, 87], "In": [1, 3, 6, 14, 15, 19, 22, 23, 24, 31, 32, 33, 36, 44, 48, 49, 54, 60, 61, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88], "practic": [1, 3, 44, 66, 67], "If": [1, 3, 6, 10, 19, 24, 26, 32, 47, 61, 63, 64, 65, 66, 67, 68, 69, 70, 71, 74, 76, 81, 82], "provid": [1, 3, 6, 32, 50, 56, 60, 61, 82, 84, 87], "i": [1, 2, 3, 5, 6, 8, 10, 11, 19, 21, 24, 25, 26, 27, 30, 31, 32, 34, 36, 39, 41, 42, 43, 46, 48, 54, 56, 58, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88], "assum": [1, 3, 15, 24, 26, 44], "e": [1, 3, 6, 12, 14, 15, 19, 34, 50, 55, 56, 60, 61, 62, 63, 64, 65, 66, 67, 68, 70, 72, 73, 74, 76, 79, 80, 81, 84, 86, 87, 88], "send": [1, 3, 8, 10, 21, 60, 63, 64, 65, 66, 67, 68, 70, 72, 73, 74, 79, 80, 81, 86], "themselv": [1, 3, 83], "n_source_cel": [1, 3, 15, 24, 44], "all": [1, 3, 6, 15, 24, 26, 31, 32, 44, 49, 59, 60, 61, 63, 68, 72, 73, 74, 76, 79, 80, 81, 83, 84, 87, 88], "have": [1, 3, 15, 19, 24, 26, 44, 50, 56, 60, 61, 64, 66, 67, 68, 70, 72, 73, 74, 79, 80, 83, 84, 87, 88], "same": [1, 3, 6, 15, 24, 26, 44, 48, 49, 50, 54, 60, 61, 63, 64, 81, 82, 84, 87, 88], "rank": [1, 3, 7, 9, 15, 20, 23, 24, 25, 26, 29, 32, 33, 35, 38, 40, 43, 44, 48, 51, 52, 54, 60, 63, 64, 65, 66, 67, 68, 70, 72, 73, 74, 75, 77, 78, 79, 80, 81, 82, 83, 84, 85, 87], "r": [1, 3, 6, 8, 10, 15, 24, 43, 44, 46, 47, 48, 54, 61, 63, 64, 65, 66, 67, 72, 81, 82, 83, 88], "torch": [1, 3, 5, 6, 7, 8, 9, 10, 12, 13, 14, 15, 18, 19, 20, 21, 22, 24, 26, 28, 30, 31, 32, 34, 36, 39, 41, 44, 45, 46, 47, 48, 50, 51, 52, 54, 56, 58, 60, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88], "spars": [1, 3, 4, 6, 8, 10, 13, 15, 18, 19, 20, 21, 22, 24, 28, 30, 34, 36, 39, 41, 44, 47, 48, 50, 54, 56, 58, 61, 66, 67, 69, 70, 71, 72, 73, 76, 79, 81, 83, 84, 85], "n_target_cel": [1, 3, 15, 24, 26, 44, 50, 56], "": [1, 3, 15, 18, 19, 26, 44, 60, 61, 63, 64, 65, 66, 67, 68, 69, 71, 74, 76, 79, 80, 82, 83, 84, 88], "x_message_on_target": [1, 24, 26, 50, 56], "embed": [1, 24, 26, 45, 50, 56], "The": [2, 3, 4, 6, 8, 10, 11, 19, 20, 21, 22, 26, 27, 30, 32, 34, 36, 37, 41, 42, 43, 46, 48, 54, 56, 58, 60, 66, 67, 68, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 82, 83, 84, 85, 86, 87, 88], "compos": [2, 8, 10, 11, 21, 27, 30, 34, 36, 42], "primarili": [2, 11, 27, 37, 42], "three": [2, 30, 34, 36, 37, 42, 60, 61, 76], "conv": [2, 4, 21, 28], "structur": [2, 63, 64, 65, 66, 67, 73, 81, 84, 87], "messagepass": [2, 3, 50, 56], "reset_paramet": [2, 3, 6, 13, 15, 21, 24, 27, 28, 30, 32, 36, 39, 41, 42, 44, 46, 48, 50, 52, 54, 56, 58], "message_pass": 3, "add": [3, 6, 13, 15, 19, 59, 61, 76], "uniform": [3, 6, 26, 88], "through": [3, 5, 7, 9, 10, 18, 23, 25, 29, 31, 33, 35, 45, 57, 60, 64, 65, 68, 69, 71, 72, 73, 74, 75, 76, 77, 78, 87, 88], "singl": [3, 26, 60, 65], "n": [3, 6, 10, 44, 46, 57, 60, 61, 62, 63, 65, 72, 73, 81, 82, 88], "decompos": 3, "creat": [3, 10, 57, 60, 61, 67, 72, 73], "go": [3, 9, 10, 36, 62, 63, 65, 81, 88], "come": 3, "differ": [3, 22, 48, 54, 61, 64, 70, 79, 80, 81, 83, 84, 87, 88], "onto": [3, 81], "should": [3, 6, 60, 61, 88], "instanti": [3, 65], "directli": [3, 76], "rather": [3, 60, 61], "inherit": [3, 60], "subclass": [3, 50, 56], "effect": [3, 19], "doe": [3, 60, 61, 84, 87, 88], "trainabl": [3, 34, 50, 56, 72], "weight": [3, 6, 15, 22, 24, 44, 46, 50, 56, 61, 67, 72, 81, 83, 84, 88], "its": [3, 19, 30, 32, 34, 36, 60, 61, 69, 71, 72], "gain": [3, 15, 21, 50, 56, 58], "refer": [3, 5, 6, 7, 8, 9, 10, 12, 13, 14, 15, 18, 19, 20, 21, 23, 24, 25, 26, 29, 30, 31, 32, 33, 34, 35, 36, 39, 41, 44, 46, 48, 50, 52, 54, 56, 58, 61, 63, 73, 81], "h23": 3, "hajij": [3, 7, 8, 10, 21, 30, 34, 36, 41, 46, 48, 52, 58, 60, 62, 63, 64, 80, 81], "zamzmi": [3, 7, 8, 36, 41, 46, 62], "papamark": [3, 36, 46, 60, 62], "miolan": [3, 8, 10, 21, 30, 34, 36, 41, 46, 48, 52, 58, 60, 62], "guzm\u00e1n": [3, 36, 41, 60, 62], "s\u00e1enz": [3, 36, 41, 60, 62], "ramamurthi": [3, 36, 41, 60, 62], "birdal": [3, 36, 60, 62], "dei": [3, 36, 60, 62], "mukherje": [3, 36, 60, 62], "samaga": [3, 36, 60, 62], "livesai": [3, 36, 60, 62], "walter": [3, 36, 60, 62], "rosen": [3, 36, 60, 62], "schaub": [3, 36, 60, 62], "topolog": [3, 4, 7, 8, 10, 19, 21, 24, 26, 30, 34, 36, 39, 41, 44, 46, 48, 50, 52, 54, 56, 58, 62, 63, 64, 65, 70, 79, 80, 81, 82, 83, 85, 87, 88], "deep": [3, 8, 10, 21, 30, 34, 36, 41, 46, 48, 52, 58, 62, 63, 64, 65, 67, 70, 79, 80, 81, 82, 83, 85, 87, 88], "learn": [3, 8, 10, 14, 15, 20, 21, 25, 26, 30, 32, 34, 36, 41, 46, 48, 52, 54, 58, 61, 62, 63, 64, 65, 70, 71, 72, 73, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88], "beyond": [3, 7, 8, 20, 21, 36, 52, 62, 63, 81], "graph": [3, 5, 6, 13, 15, 20, 21, 30, 31, 32, 33, 34, 36, 48, 49, 50, 54, 60, 62, 63, 64, 65, 66, 67, 68, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 86, 87, 88], "data": [3, 7, 8, 36, 48, 54, 61, 62, 63, 69, 71, 81, 83, 86], "2023": [3, 8, 10, 21, 30, 34, 36, 41, 46, 48, 52, 58, 62, 63, 64, 65, 70, 76, 79, 80, 81, 82, 83, 84, 85, 87, 88], "http": [3, 5, 6, 7, 8, 9, 10, 12, 13, 14, 15, 18, 19, 20, 21, 23, 24, 30, 31, 32, 33, 34, 36, 41, 46, 48, 52, 54, 56, 58, 63, 70, 71, 76, 78, 79, 80, 81, 83, 84, 86], "arxiv": [3, 5, 6, 7, 8, 9, 10, 12, 13, 14, 15, 18, 19, 21, 25, 26, 30, 31, 32, 33, 34, 36, 41, 44, 46, 48, 52, 56, 58, 62], "org": [3, 5, 6, 7, 8, 9, 10, 12, 13, 14, 15, 18, 19, 21, 23, 24, 30, 31, 32, 33, 34, 36, 41, 46, 48, 52, 56, 58, 70, 79, 80, 81, 83, 84], "ab": [3, 8, 9, 10, 12, 13, 18, 19, 21, 30, 34, 36, 41, 46, 48, 52, 58, 85], "2206": [3, 36, 62, 79], "00606": [3, 36, 62], "pshm23": [3, 8, 10, 21, 30, 34, 36, 41, 46, 48, 52, 54, 58], "papillon": [3, 8, 10, 21, 30, 34, 36, 41, 46, 48, 52, 58, 60, 62, 63, 64, 65, 70, 79, 80, 81, 82, 83, 85, 87, 88], "sanborn": [3, 8, 10, 21, 30, 34, 36, 41, 46, 48, 52, 58, 60, 62], "architectur": [3, 8, 10, 15, 21, 30, 34, 36, 41, 44, 46, 48, 52, 54, 58, 60, 62, 63, 64, 65, 66, 67, 68, 70, 72, 73, 74, 79, 80, 81, 82, 83, 85, 87, 88], "A": [3, 8, 10, 12, 13, 14, 15, 18, 21, 26, 28, 30, 34, 36, 39, 41, 46, 48, 52, 58, 60, 61, 62, 63, 64, 65, 66, 67, 69, 70, 79, 80, 81, 82, 83, 84, 85, 87, 88], "survei": [3, 8, 10, 21, 30, 34, 36, 41, 46, 48, 52, 58, 60, 62, 63, 64, 65, 70, 79, 80, 81, 82, 83, 85, 87, 88], "neural": [3, 4, 7, 8, 9, 10, 12, 13, 14, 15, 18, 19, 21, 23, 24, 25, 26, 29, 30, 31, 32, 33, 34, 35, 36, 39, 41, 44, 46, 48, 50, 52, 54, 55, 56, 57, 58, 60, 62], "network": [3, 4, 5, 6, 7, 8, 9, 10, 12, 13, 14, 15, 18, 19, 20, 21, 22, 23, 24, 25, 26, 29, 30, 31, 32, 33, 34, 35, 36, 38, 39, 40, 41, 43, 44, 46, 47, 48, 50, 51, 52, 53, 54, 55, 56, 57, 58, 60, 62], "2304": [3, 8, 10, 21, 30, 34, 36, 41, 46, 48, 52, 58, 62, 70], "10031": [3, 8, 10, 21, 30, 34, 36, 41, 46, 48, 52, 58, 62], "x_messag": [3, 26], "receiv": [3, 10, 19, 26, 60], "sever": [3, 4, 26, 60], "per": [3, 15, 24, 26, 60, 61], "correspond": [3, 10, 26, 48, 54, 56, 63, 67, 69, 71, 76], "within": [3, 61, 63, 69, 71, 81], "n_messag": [3, 24, 26], "associ": [3, 26, 61, 63, 64, 65, 66, 67, 68, 72, 73, 74, 75, 77, 78, 79, 80, 81, 82, 83, 84], "One": [3, 26, 38, 40, 43, 49, 53, 60, 83, 87, 88], "sent": [3, 26, 76], "comput": [3, 4, 5, 6, 7, 9, 12, 13, 14, 15, 18, 19, 20, 21, 22, 23, 24, 25, 26, 28, 29, 32, 33, 35, 38, 40, 43, 45, 47, 49, 50, 51, 53, 55, 56, 59, 60, 61, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 81, 87, 88], "scheme": [3, 6, 8, 46, 60, 64, 81], "altern": [3, 60], "user": [3, 64, 79, 83, 84, 87], "overwrit": 3, "order": [3, 41, 43, 44, 48, 49, 50, 54, 55, 56, 58, 60, 63, 80, 81, 83, 84, 86, 87, 88], "replac": 3, "own": 3, "mechan": [3, 5, 6, 7, 8, 15, 24, 63, 64, 67, 72, 81], "detail": [3, 10, 60, 61, 63, 67, 81], "definit": [3, 87], "higher": [3, 41, 48, 54, 60, 63, 80, 83, 86, 87], "scalar": [3, 15, 21, 24, 64, 65, 82, 88], "between": [3, 6, 7, 8, 10, 13, 15, 19, 24, 26, 48, 54, 63, 64, 81, 83], "two": [3, 8, 10, 19, 21, 28, 32, 50, 63, 66, 67, 68, 70, 72, 73, 74, 79, 80, 81, 83, 84, 87, 88], "m_": [3, 6, 8, 10, 13, 15, 19, 21, 24, 26, 30, 32, 34, 36, 39, 41, 44, 46, 48, 50, 52, 54, 56, 58, 63, 64, 65, 66, 67, 70, 72, 73, 79, 80, 81, 82, 83, 85, 87, 88], "y": [3, 6, 8, 10, 13, 15, 19, 21, 24, 26, 30, 32, 34, 36, 39, 41, 44, 46, 48, 50, 52, 54, 56, 58, 61, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88], "rightarrow": [3, 6, 8, 10, 13, 15, 19, 21, 24, 26, 30, 32, 34, 36, 39, 41, 44, 46, 48, 50, 52, 54, 56, 58, 63, 64, 65, 66, 67, 70, 72, 73, 79, 80, 81, 82, 83, 85, 87, 88], "left": [3, 10, 26, 46, 57, 61, 65, 72, 73, 88], "right": [3, 10, 26, 57, 61, 65, 72, 73, 88], "travel": 3, "denot": [3, 66, 67, 72, 73, 87, 88], "mathcal": [3, 6, 8, 10, 13, 15, 19, 21, 24, 26, 30, 32, 34, 36, 39, 41, 44, 46, 48, 50, 52, 54, 56, 58, 63, 64, 65, 66, 67, 70, 72, 73, 79, 80, 81, 82, 83, 85, 87, 88], "mathbf": [3, 84, 87], "_x": [3, 39, 52, 54, 63, 79, 81, 85], "_y": [3, 32, 41, 48, 80, 83], "theta": [3, 6, 8, 21, 24, 26, 30, 34, 39, 41, 44, 46, 48, 50, 52, 54, 58, 63, 64, 67, 70, 72, 73, 79, 80, 82, 83, 84, 85, 87, 88], "ar": [3, 6, 8, 10, 12, 13, 14, 15, 19, 21, 22, 30, 34, 36, 41, 44, 46, 48, 50, 58, 60, 61, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 74, 76, 79, 80, 81, 82, 83, 84, 85, 87, 88], "call": [3, 19, 22, 50, 56, 61, 67, 88], "leftarrow": [3, 73], "across": [3, 22], "belong": [3, 61, 70, 79, 80, 81, 83, 84], "m_x": [3, 6, 8, 10, 19, 21, 24, 26, 30, 32, 34, 36, 39, 41, 44, 46, 48, 50, 52, 54, 56, 58, 64, 65, 70, 72, 73, 79, 80, 82, 83, 85, 87, 88], "text": [3, 8, 10, 15, 23, 24, 46, 50, 61, 66, 67, 68, 69, 70, 71, 72, 73, 74, 79, 80, 81, 82, 83, 84], "agg": [3, 8, 10, 19, 46, 82], "_": [3, 6, 7, 8, 9, 10, 21, 23, 24, 25, 26, 29, 32, 33, 35, 36, 38, 39, 40, 41, 43, 44, 45, 46, 47, 48, 49, 51, 52, 53, 54, 56, 58, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88], "result": [3, 61, 63, 64, 67, 70, 84, 87], "construct": [3, 6, 69, 71, 76, 88], "reset": [3, 6, 13, 15, 21, 24, 28, 30, 32, 36, 39, 41, 44, 46, 48, 50, 52, 54, 56, 58], "note": [3, 6, 8, 10, 21, 41, 44, 46, 49, 50, 52, 54, 55, 56, 58, 60, 61, 66, 67, 68, 72, 73, 74, 79, 80, 84, 87, 88], "give": [4, 63, 67, 81], "overview": 4, "which": [4, 6, 19, 26, 44, 45, 49, 60, 61, 66, 67, 69, 71, 72, 76, 79, 80, 81, 83, 84, 87, 88], "consist": [4, 32, 37, 59, 60, 61, 76, 88], "core": 4, "mathemat": 4, "concept": 4, "nn": [4, 5, 6, 7, 8, 9, 10, 12, 13, 14, 15, 18, 19, 20, 21, 22, 23, 24, 25, 26, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 60, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88], "organ": [4, 60, 83], "domain": [4, 60, 62, 63, 64, 65, 66, 67, 69, 70, 71, 73, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88], "util": [4, 6, 31, 63, 66, 67, 75, 76, 77, 78, 81, 88], "broadcast": [4, 59, 64, 84, 87], "scatter": [4, 59, 71, 88], "scatter_add": [4, 59], "scatter_mean": [4, 59], "scatter_sum": [4, 59], "in_channels_0": [5, 6, 7, 8, 9, 10, 50, 53, 54, 63, 64, 65, 84, 86, 87], "in_channels_1": [5, 6, 7, 8, 9, 10, 50, 53, 54, 63, 64, 65, 84, 86, 87], "num_class": [5, 7, 8, 9, 18, 31, 49, 53, 63, 64, 65, 69, 71, 76, 84, 86], "dropout": [5, 6, 12, 13, 14, 15, 18, 19, 63, 66, 67, 69, 71], "0": [5, 6, 7, 8, 9, 12, 13, 14, 15, 18, 19, 21, 22, 24, 26, 30, 31, 32, 34, 36, 38, 40, 41, 43, 44, 46, 50, 51, 52, 54, 56, 58, 60, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88], "5": [5, 18, 19, 21, 31, 43, 61, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 83, 84, 85, 86, 87, 88], "head": [5, 6, 14, 15, 63, 67], "concat": [5, 6, 50, 56, 63], "skip_connect": [5, 6, 63], "att_activ": [5, 6, 63], "leakyrelu": [5, 6, 63, 72, 81], "negative_slop": [5, 6, 63], "n_layer": [5, 7, 9, 12, 14, 18, 20, 23, 25, 29, 31, 33, 35, 38, 40, 43, 45, 47, 49, 51, 53, 55, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 87], "att_lift": [5, 63], "classif": [5, 8, 10, 20, 21, 23, 24, 25, 29, 31, 33, 35, 38, 40, 41, 43, 44, 46, 47, 49, 51, 53, 54, 55, 57, 60, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 85, 86], "number": [5, 6, 7, 9, 12, 13, 14, 15, 18, 19, 20, 31, 32, 43, 44, 47, 49, 51, 53, 60, 63, 64, 65, 66, 67, 69, 70, 71, 76, 79, 80, 81, 82, 83, 85, 86, 88], "node": [5, 6, 7, 8, 9, 10, 13, 15, 18, 19, 20, 21, 22, 23, 24, 25, 26, 28, 29, 30, 31, 32, 33, 34, 35, 36, 38, 39, 40, 41, 43, 45, 46, 47, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 60, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 85, 86, 88], "level": [5, 22, 24, 31, 46, 60, 63, 66, 67, 68, 70, 72, 73, 74, 76, 79, 80, 81, 82, 83, 84, 87], "edg": [5, 6, 7, 8, 9, 10, 12, 13, 14, 15, 20, 21, 23, 24, 25, 28, 29, 30, 32, 33, 34, 35, 36, 39, 41, 43, 45, 49, 50, 51, 52, 53, 54, 55, 56, 58, 60, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88], "num_classest": 5, "probabl": [5, 6, 12, 13, 14, 15, 63, 66, 67], "concaten": [5, 6, 19, 63, 67, 72, 88], "skip": [5, 6, 32, 38, 40, 41, 63, 76, 88], "connect": [5, 6, 32, 41, 57, 63, 72, 76, 80, 84, 88], "activ": [5, 6, 13, 15, 47, 48, 51, 63, 67], "lift": [5, 6, 60, 63, 64, 65, 66, 67, 70, 73, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 87], "signal": [5, 6, 63, 64, 65, 66, 67, 68, 72, 73, 74, 75, 77, 78, 81, 82], "can22": [5, 6], "giusti": [5, 6, 44, 60, 63, 81], "battiloro": [5, 6, 44, 60, 81], "testa": [5, 6], "di": [5, 6, 44], "lorenzo": [5, 6, 44], "sardellitti": [5, 6, 44], "barbarossa": [5, 6, 44], "2022": [5, 6, 14, 15, 18, 19, 41, 44, 48, 54, 56, 60, 63, 69, 79, 80, 81, 83, 87], "paper": [5, 6, 15, 19, 20, 21, 22, 60, 63, 69, 70, 71, 79, 80, 81, 84, 85, 86, 87, 88], "pdf": [5, 6, 7, 8, 14, 15, 20, 21, 23, 24, 31, 32, 33, 36, 41, 46, 56, 61], "2209": [5, 6], "08179": [5, 6], "repositori": [5, 6, 60, 61], "lrnzgiusti": [5, 6], "x_0": [5, 6, 7, 8, 9, 10, 12, 14, 18, 19, 20, 21, 22, 30, 31, 32, 33, 34, 36, 38, 40, 41, 50, 51, 52, 53, 54, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 76, 77, 79, 80, 81, 82, 83, 84, 85, 86, 87], "x_1": [5, 6, 7, 8, 9, 10, 18, 19, 20, 21, 22, 23, 28, 29, 35, 50, 51, 52, 53, 54, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87], "neighborhood_0_to_0": [5, 6, 7, 8, 63, 64], "lower_neighborhood": [5, 6, 63], "upper_neighborhood": [5, 6, 63], "n_node": [5, 7, 9, 13, 15, 18, 19, 20, 21, 22, 23, 25, 28, 29, 30, 33, 34, 35, 36, 38, 39, 40, 41, 43, 47, 49, 50, 51, 52, 53, 54, 55, 58, 64, 65, 68, 69, 70, 71, 72, 73, 74, 75, 77, 78, 87], "n_edg": [5, 7, 9, 19, 20, 21, 22, 23, 25, 28, 29, 30, 33, 34, 35, 36, 38, 39, 40, 41, 43, 49, 50, 51, 52, 54, 55, 56, 58, 64, 65, 70, 72, 73, 74, 75, 77, 78, 87], "canlay": [6, 11, 63], "01": [6, 69, 76, 79, 84, 87, 88], "add_self_loop": [6, 11], "version": [6, 8, 9, 21, 64, 65, 84, 87], "v1": 6, "v2": 6, "share_weight": 6, "kwarg": [6, 25, 26, 73], "model": [6, 31, 38, 40, 60, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 83, 85, 86], "consid": [6, 43, 49, 50, 55, 56, 63, 81, 84, 87], "though": 6, "upper": [6, 9, 10, 19, 43, 44, 47, 48, 49, 50, 51, 52, 55, 56, 57, 63, 65, 81, 84, 87, 88], "lower": [6, 44, 47, 48, 50, 55, 56, 57, 63, 81, 82, 84, 87, 88], "addition": [6, 66, 67, 83], "ad": [6, 19, 60, 61], "prefer": [6, 60, 61], "necessari": [6, 59, 60, 63, 81], "self": [6, 31, 32, 61, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 81, 83, 84, 87, 88], "loop": [6, 31, 32, 60, 66, 67, 68, 69, 71, 73, 74, 75, 76, 78, 79, 80, 81, 83, 85, 86, 88], "preprocess": 6, "coeffici": [6, 63, 72], "otherwis": [6, 63, 64, 65, 66, 67, 68, 69, 70, 71, 74, 76, 82, 88], "averag": [6, 9, 55, 64, 65, 68, 88], "callabl": [6, 13, 15, 19, 22], "origin": [6, 32, 60, 61, 63, 64, 65, 76, 81, 83, 84, 86, 87, 88], "while": [6, 60, 61, 81], "attet": 6, "gatv2": [6, 63], "valid": [6, 61, 75, 76, 77, 78, 88], "onli": [6, 8, 19, 54, 60, 61, 63, 68, 76, 81, 84, 86, 88], "share": [6, 19, 60, 63], "n_1": [6, 44, 63, 81], "n_2": [6, 44, 63, 81], "a_": [6, 10, 41, 44, 63, 64, 65, 72, 79, 80, 81], "uparrow": [6, 8, 10, 39, 41, 44, 48, 52, 54, 56, 58, 63, 64, 65, 79, 80, 81, 83, 84, 85, 87, 88], "downarrow": [6, 39, 44, 48, 52, 54, 56, 58, 63, 79, 81, 82, 83, 84, 85, 87, 88], "begin": [6, 8, 10, 19, 21, 24, 26, 30, 32, 34, 36, 39, 41, 44, 46, 48, 50, 52, 54, 56, 58, 61, 63, 81], "align": [6, 8, 10, 19, 21, 24, 26, 30, 32, 34, 36, 39, 41, 44, 46, 48, 50, 52, 54, 56, 58, 63, 81, 87], "quad": [6, 8, 10, 13, 15, 19, 21, 24, 26, 30, 32, 34, 36, 39, 41, 44, 46, 48, 50, 52, 54, 56, 58, 63, 64, 65, 66, 67, 70, 72, 73, 79, 80, 81, 82, 83, 85, 87, 88], "k": [6, 10, 19, 25, 26, 44, 46, 50, 56, 62, 63, 65, 67, 70, 72, 79, 80, 81, 82, 83, 85, 87, 88], "alpha_k": [6, 44, 63, 81], "h_x": [6, 8, 10, 13, 15, 19, 21, 24, 26, 30, 32, 34, 36, 39, 41, 44, 46, 48, 50, 54, 56, 58, 63, 64, 65, 66, 67, 70, 72, 73, 79, 80, 81, 82, 83, 87, 88], "t": [6, 8, 10, 13, 15, 19, 21, 24, 26, 30, 32, 34, 36, 39, 41, 44, 46, 48, 50, 52, 54, 56, 58, 62, 63, 64, 65, 66, 67, 70, 71, 72, 73, 76, 79, 80, 81, 82, 83, 84, 85, 87, 88], "h_y": [6, 8, 10, 13, 15, 19, 21, 26, 30, 34, 36, 41, 44, 50, 52, 56, 58, 63, 64, 65, 66, 67, 70, 73, 80, 81, 85, 87, 88], "a_k": [6, 44, 63, 81, 88], "cdot": [6, 21, 24, 26, 30, 32, 34, 36, 39, 41, 44, 48, 50, 52, 54, 56, 58, 63, 70, 72, 73, 79, 80, 81, 83, 85, 87, 88], "psi_k": [6, 44, 63, 81], "foral": [6, 44, 63, 72, 81], "n_k": [6, 44, 63, 81], "bigoplus_": [6, 44, 63, 81], "_k": [6, 10, 44, 46, 63, 65, 81, 82], "m": [6, 8, 39, 44, 46, 52, 54, 56, 58, 61, 64, 72, 79, 82, 85, 87, 88], "bigotimes_": [6, 44, 63, 81], "phi": [6, 44, 63, 81], "end": [6, 8, 10, 19, 21, 24, 26, 30, 32, 34, 36, 39, 41, 44, 46, 48, 50, 52, 54, 56, 58, 61, 63, 79, 80, 81, 83, 88], "n_k_cell": 6, "complex": [6, 7, 8, 9, 10, 23, 25, 28, 29, 33, 35, 39, 41, 45, 46, 47, 48, 49, 50, 51, 52, 54, 55, 56, 57, 58, 60, 61, 63, 65, 66, 67, 68, 72, 73, 74, 75, 77, 78, 79, 80, 81, 86, 87], "map": [6, 8, 10, 13, 15, 19, 20, 21, 22, 28, 30, 34, 36, 39, 41, 47, 48, 58, 70, 85, 88], "a_k_low": 6, "a_k_up": 6, "liftlay": [6, 11, 63], "signal_lift_activ": 6, "signal_lift_dropout": [6, 63], "adapt": [6, 59], "offici": [6, 71], "rate": [6, 18, 19, 69, 71], "num_nod": [6, 31, 32, 76], "num_edg": [6, 31, 32, 76], "reiniti": 6, "xavier": 6, "multiheadcellattent": [6, 11, 63], "follow": [6, 10, 15, 57, 60, 61, 62, 63, 66, 67, 68, 73, 74, 75, 76, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88], "propos": [6, 8, 10, 15, 21, 24, 26, 30, 34, 36, 41, 44, 46, 48, 52, 54, 58, 63, 64, 65, 70, 73, 79, 80, 81, 84, 85, 86, 87, 88], "gat": [6, 63, 81], "gat17": 6, "adjac": [6, 7, 8, 9, 10, 18, 19, 38, 39, 40, 41, 47, 48, 50, 51, 52, 56, 63, 64, 65, 69, 79, 80, 81, 83, 84, 88], "non": [6, 63, 81], "zero": [6, 63, 66, 67, 76, 79, 80, 81, 84, 87, 88], "valu": [6, 43, 59, 64, 65, 66, 67, 68, 69, 71, 72, 73, 74, 76, 79, 82, 84, 85, 86, 87, 88], "empti": 6, "preprint": [6, 25, 26, 44], "veli\u010dkovi\u0107": 6, "cucurul": 6, "casanova": 6, "romero": 6, "li\u00f2": 6, "bengio": [6, 20, 21], "1710": [6, 70], "10903": 6, "2017": [6, 67, 79], "up": [6, 7, 10, 30, 32, 34, 36, 38, 40, 44, 56, 61, 81, 83, 88], "down": [6, 43, 44, 45, 46, 49, 51, 52, 56, 81], "multiheadcellattention_v2": [6, 11], "gatv2_22": 6, "brodi": 6, "alon": 6, "yahav": 6, "how": [6, 61, 67, 79, 80, 81], "2105": [6, 31, 32, 33, 36, 70], "14491": 6, "alpha": [6, 21, 31, 32, 56, 63, 73, 76, 81, 88], "multiheadliftlay": [6, 11, 63], "type": [6, 13, 15, 61], "built": [6, 60, 61], "object": [6, 19, 61], "signal_lift_readout": 6, "str": [6, 13, 15, 24, 26, 47, 50, 51, 56, 59, 61], "cat": 6, "multi": [6, 13, 15, 61, 67], "readout": [6, 49, 63], "z": [6, 8, 10, 13, 15, 19, 24, 26, 30, 32, 34, 36, 41, 44, 50, 56, 58, 63, 64, 65, 66, 67, 72, 73, 80, 87, 88], "h_z": [6, 10, 13, 15, 19, 44, 63, 65, 66, 67], "index": [6, 59, 61, 83, 88], "poollay": [6, 11, 63], "k_pool": [6, 63], "signal_pool_activ": [6, 63], "pool": [6, 7, 9, 23, 25, 29, 33, 35, 45, 63, 64, 65, 68, 72, 73, 74, 75, 77, 78], "ratio": 6, "fraction": [6, 70], "keep": [6, 61, 63, 64, 65, 66, 67, 68, 69, 71, 73, 74, 75, 78, 79, 80, 81, 83, 84], "after": [6, 30, 36, 61, 63, 71, 77], "oper": [6, 13, 15, 58, 63, 67, 72, 81, 83, 84, 88], "tupl": [6, 18, 44, 49, 50, 57, 69, 88], "gamma": [6, 63, 71], "tau": [6, 63], "c_r": [6, 63], "num_pooled_nod": 6, "file": [6, 60, 61], "sparse_coo_tensor": [6, 69, 71], "softmax": [6, 11, 58, 70, 76, 81, 87, 88], "src": [6, 59, 76], "num_cel": 6, "There": [6, 60, 61, 69, 70, 71, 79, 80, 81, 83, 87], "subtract": 6, "maximum": [6, 48, 83, 88], "element": [6, 61, 67], "avoid": [6, 50, 56], "overflow": 6, "underflow": 6, "indic": [6, 44, 59, 71, 79, 80, 81, 83, 84, 88], "batch": [6, 88], "in_channels_2": [7, 8, 9, 10, 50, 53, 54, 63, 64, 65, 84, 86, 87], "face": [7, 8, 9, 10, 45, 49, 50, 51, 52, 53, 54, 55, 60, 63, 64, 65, 66, 67, 68, 72, 73, 74, 79, 80, 81, 82, 83, 84, 85, 86, 87], "hiz20": [7, 8], "istvan": [7, 8], "analysi": [7, 8], "workshop": [7, 8, 20, 21, 41, 52, 60], "neurip": [7, 8, 52], "2020": [7, 8, 20, 21, 22, 23, 24, 25, 26, 52, 60, 64, 69, 70, 72, 73, 85], "2010": [7, 8, 25, 26], "00743": [7, 8], "neighborhood_1_to_2": [7, 8, 64], "avg": [7, 45, 64], "n_face": [7, 9, 49, 50, 51, 52, 54, 64, 65], "transpos": [7, 45, 46, 64, 82, 85], "boundari": [7, 9, 10, 20, 23, 25, 29, 33, 35, 38, 40, 58, 64, 65, 66, 67, 68, 70, 72, 73, 74, 75, 77, 78, 79, 80, 83, 88], "x_2": [7, 8, 9, 10, 50, 51, 52, 53, 54, 64, 65, 66, 67, 68, 72, 73, 74, 79, 80, 81, 82, 83, 84, 85, 86, 87], "label": [7, 9, 23, 25, 29, 33, 35, 38, 40, 43, 45, 48, 49, 51, 53, 54, 61, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 81, 82, 85, 86, 88], "assign": [7, 9, 23, 25, 29, 33, 35, 38, 40, 43, 45, 47, 49, 51, 53, 61, 64, 65, 68, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 83, 84, 88], "whole": [7, 9, 23, 25, 29, 33, 35, 45, 49, 51, 64, 65, 68, 72, 73, 74, 75, 77, 78], "simplifi": [8, 21, 64], "ccxn": [8, 11], "et": [8, 9, 10, 19, 22, 30, 34, 36, 48, 54, 60, 62, 63, 64, 65, 66, 67, 69, 70, 71, 72, 73, 79, 80, 81, 82, 83, 84, 85, 87, 88], "al": [8, 9, 10, 22, 30, 34, 36, 48, 54, 60, 62, 63, 64, 65, 66, 67, 70, 71, 72, 73, 79, 80, 81, 82, 83, 84, 85, 87, 88], "ccxnlayer": [8, 11, 64], "entir": [8, 10, 63, 64, 65], "wa": [8, 10, 21, 41, 46, 48, 58, 82, 87], "Its": [8, 10, 21, 30, 34, 36, 41, 46, 48, 58], "equat": [8, 10, 19, 21, 24, 26, 30, 34, 36, 39, 41, 44, 46, 48, 50, 52, 54, 56, 58, 61, 62, 63, 64, 65, 70, 79, 80, 81, 83, 85, 88], "tnn23": [8, 10, 19, 21, 24, 26, 30, 34, 36, 39, 41, 44, 46, 48, 50, 52, 54, 56, 58], "graphic": [8, 10, 21, 30, 34, 36, 41, 46, 48, 58, 62], "illustr": [8, 10, 21, 30, 34, 36, 41, 46, 48, 58, 86], "amp": [8, 64], "l": [8, 10, 18, 19, 39, 41, 43, 44, 46, 48, 52, 54, 56, 58, 63, 64, 65, 72, 73, 79, 80, 81, 82, 83, 84, 85, 87, 88], "u": [8, 10, 19, 44, 46, 56, 60, 64, 65, 66, 67, 72, 82, 88], "cohomologi": [8, 64], "coboundari": [8, 9, 64, 65], "t_": [8, 26, 64, 73], "c": [8, 13, 14, 15, 19, 21, 24, 26, 30, 32, 34, 36, 41, 48, 50, 52, 62, 63, 64, 66, 67, 70, 71, 72, 73, 79, 80, 83, 85, 88], "h_": [8, 24, 39, 46, 48, 54, 64, 72, 79, 82, 83], "awesom": [8, 10, 15, 19, 21, 24, 26, 30, 34, 36, 39, 41, 44, 46, 48, 50, 52, 54, 56, 58, 62, 67, 73], "tnn": [8, 10, 15, 19, 21, 24, 26, 30, 34, 36, 39, 41, 44, 46, 48, 50, 52, 54, 56, 58, 60, 62], "n_0_cell": 8, "n_1_cell": 8, "a_0_up": 8, "n_2_cell": 8, "b_2": [8, 50, 51, 52, 58, 64, 81, 85], "requir": [8, 61, 63, 82], "predict": [8, 12, 14, 18, 20, 58, 61, 63, 66, 67, 69, 70, 71, 87, 88], "hid_channel": [9, 65], "cw": [9, 10, 60], "hidden": [9, 12, 13, 14, 15, 18, 43, 65, 66, 67, 69, 71, 72, 81, 88], "b21": [9, 10], "bodnar": [9, 10, 60, 65], "weisfeil": [9, 10, 65], "lehman": [9, 10, 65], "cellular": [9, 10, 60, 65], "confer": [9, 10, 14, 15, 18, 19, 23, 24, 31, 32, 33, 36, 48, 54, 58, 86, 87], "inform": [9, 10, 61, 66, 67, 69, 72, 73, 87], "process": [9, 10, 23, 24, 60, 61, 76], "system": [9, 10], "2021": [9, 10, 12, 13, 31, 32, 33, 36, 60, 65, 66, 67, 87, 88], "2106": [9, 10, 12, 13, 14, 15], "12575": [9, 10], "neighborhood_1_to_1": [9, 10, 65], "neighborhood_2_to_1": [9, 10, 65], "neighborhood_0_to_1": [9, 10, 65], "project": [9, 43, 45, 61, 65, 81, 87], "cwn": [10, 11, 60], "cwnlayer": [10, 11, 65], "conv_1_to_1": 10, "conv_0_to_1": 10, "aggregate_fn": 10, "update_fn": 10, "represent": [10, 14, 15, 18, 19, 20, 21, 22, 25, 26, 41, 48, 54, 57, 60, 63, 66, 67, 69, 70, 71, 72, 83, 86, 88], "case": [10, 60, 63, 64, 65, 66, 67, 68, 70, 72, 73, 74, 79, 80, 81, 86, 88], "convolv": 10, "neighbor": [10, 19, 22, 63, 73, 87, 88], "co": [10, 60, 76], "check": [10, 60, 61, 62, 70, 79, 80], "docstr": [10, 60], "_cwndefaultfirstconv": 10, "more": [10, 19, 23, 24, 60, 61, 62, 79, 83, 84, 88], "_cwndefaultsecondconv": 10, "obtain": [10, 55, 73, 76, 81, 84, 87, 88], "_cwndefaultaggreg": 10, "_cwndefaultupd": 10, "final": [10, 19, 22, 55, 58, 60, 69, 70, 71, 76, 81, 87, 88], "first": [10, 48, 54, 61, 63, 64, 65, 66, 67, 68, 70, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88], "exploit": [10, 63], "second": [10, 32, 60, 66, 67, 82], "b": [10, 13, 15, 19, 21, 24, 26, 30, 32, 34, 36, 46, 48, 50, 51, 52, 61, 65, 66, 67, 70, 72, 73, 82, 83, 84, 85], "Then": [10, 61, 64, 65], "agg_": [10, 13, 15, 19, 46, 64, 65, 66, 67, 82], "n_": [10, 44, 66, 67, 68, 69, 70, 71, 72, 73, 74, 79, 80, 81, 83, 84, 87], "_cell": 10, "in_channels_": 10, "b_": [10, 48, 65, 82, 83], "t_r": 10, "six": 11, "can_lay": [11, 63], "ccxn_layer": [11, 64], "cwn_layer": [11, 65], "hypergraph": [12, 13, 14, 15, 18, 19, 20, 21, 22, 23, 24, 25, 26, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 60, 66, 67, 73, 75, 77, 78], "hidden_channel": [12, 13, 14, 15, 43, 66, 67, 81], "mlp_num_lay": [12, 13, 14, 15, 66, 67], "mlp_activ": [12, 13, 15, 66], "mlp_dropout": [12, 13, 14, 15, 66, 67], "mlp_norm": [12, 13, 14, 15, 66, 67], "combin": [12, 14, 32, 63, 66, 67], "multipl": [12, 14, 60, 61, 66, 67, 88], "form": [12, 14, 57, 60, 66, 67, 79, 80, 83, 84, 88], "in_dim": [12, 14, 66, 67], "hid_dim": [12, 14, 66, 67], "out_dim": [12, 14, 66, 67, 72, 77], "input_dropout": [12, 14, 66, 67], "mlp": [12, 13, 14, 15, 27, 66, 67, 77], "e21": [12, 13], "eli": [12, 13], "chien": [12, 13, 14, 15, 60, 66, 67], "chao": [12, 13], "pan": [12, 13, 14, 15], "jianhao": [12, 13], "peng": [12, 13, 14, 15], "olgica": [12, 13], "milenkov": [12, 13, 14, 15], "you": [12, 13, 14, 15, 52, 60, 61, 76], "multiset": [12, 13, 14, 15, 66, 67], "framework": [12, 13, 14, 15, 30, 31, 32, 33, 34, 36, 66, 67], "13264": [12, 13, 14, 15], "incidence_1": [12, 13, 14, 15, 18, 19, 20, 21, 22, 23, 28, 29, 30, 31, 32, 33, 34, 35, 36, 38, 39, 40, 41, 49, 51, 52, 55, 58, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 84, 85, 87], "edge_index": [12, 14, 63, 66, 67, 69, 72, 73], "allset": [13, 14, 15, 27, 66, 67], "allsetblock": [13, 27], "block": [13, 15, 60], "bipartit": [13, 15], "incid": [13, 15, 18, 19, 20, 21, 22, 24, 25, 26, 28, 30, 31, 32, 34, 36, 39, 41, 45, 46, 47, 48, 49, 50, 51, 52, 55, 58, 66, 67, 68, 69, 70, 71, 72, 73, 74, 76, 79, 80, 81, 82, 83, 84, 87], "hyperedg": [13, 15, 18, 19, 20, 21, 22, 24, 26, 36, 60, 66, 67, 68, 69, 72, 73, 74], "allsetlay": [13, 27, 66], "vertex": [13, 15, 66, 67, 88], "sigma": [13, 21, 24, 26, 39, 41, 44, 48, 52, 54, 56, 58, 66, 70, 72, 73, 79, 80, 83, 84, 85, 87, 88], "n_hyperedg": [13, 15, 18, 22, 69, 71], "b_1": [13, 15, 19, 20, 21, 22, 24, 26, 28, 30, 32, 34, 36, 39, 41, 50, 51, 52, 58, 66, 67, 68, 69, 70, 71, 72, 73, 74, 79, 80, 81, 85, 87], "norm_lay": [13, 15], "activation_lay": [13, 15], "inplac": [13, 15], "bia": [13, 15, 21, 63, 84, 87], "perceptron": [13, 15, 67], "do": [13, 15, 60, 69, 71, 79, 80, 83, 88], "place": [13, 15, 60, 61, 66, 67], "allsettransform": [14, 15, 27, 60, 67], "eccp22": [14, 15], "j": [14, 15, 72, 73, 79, 83, 84, 85, 88], "o": [14, 15, 61, 67, 70, 81], "intern": [14, 15, 18, 19, 31, 32, 33, 36, 58, 76, 87], "allsettransformerblock": [15, 27], "number_queri": 15, "queri": 15, "allsettransformerlay": [15, 27, 67], "github": [15, 20, 21, 60, 67, 71, 73, 76], "repo": [15, 67, 71, 73], "com": [15, 63, 71, 76, 78], "blob": 15, "main": [15, 23, 24, 61, 76], "md": 15, "ln": [15, 67], "multiheadattent": [15, 27], "qk": 15, "v": [15, 61, 66, 67, 70, 72, 73, 76, 79, 80, 81, 83, 84, 87], "mh": [15, 67], "eq": [15, 79, 80, 81, 84, 87], "7": [15, 18, 19, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88], "factor": 15, "in_featur": [18, 19, 22, 31, 63, 69, 71, 76, 84, 87], "hidden_featur": [18, 69, 71], "adjacency_dropout_r": [18, 69], "regular_dropout_r": [18, 69], "gradual": [18, 69], "reduc": [18, 19, 69, 84], "last": [18, 55, 58, 69, 83, 87, 88], "item": [18, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88], "hmpnnlayer": [18, 19, 27, 69], "regular": [18, 19, 69], "h22": [18, 19], "heydari": [18, 19, 60, 69], "livi": [18, 19, 69], "artifici": [18, 19, 31, 32, 33, 36], "sep": [18, 19], "6": [18, 19, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88], "pp": [18, 19], "583": [18, 19, 70], "592": [18, 19, 70], "cham": [18, 19], "springer": [18, 19], "natur": [18, 19, 23, 24], "switzerland": [18, 19], "2203": [18, 19, 44], "16995": [18, 19], "b1": [18, 49, 50, 69, 70, 79, 80, 83, 84, 85, 87], "y_pred": [18, 69, 71, 76, 79, 80, 81, 83, 84, 87], "logit": [18, 20, 31, 47, 69, 70, 71, 76, 87], "hmpnn": [19, 27, 60], "introduc": [19, 22, 63, 69, 71, 72, 81], "node_to_hyperedge_messaging_func": 19, "hyperedge_to_node_messaging_func": 19, "adjacency_dropout": [19, 69], "updating_dropout": [19, 69], "updating_func": 19, "compris": 19, "make": [19, 22, 34, 60, 63, 64, 65, 66, 67, 68, 69, 71, 74, 76, 82, 88], "new": [19, 22, 60, 61, 64], "reprsent": 19, "them": [19, 50, 63, 64, 65, 66, 67, 68, 69, 71, 72, 74, 76, 81, 82, 83], "also": [19, 56, 61, 63, 64, 65, 66, 67, 68, 70, 71, 72, 73, 74, 75, 77, 78, 79, 80, 81, 82, 86, 87], "reciev": 19, "beforehand": [19, 22], "wai": [19, 61, 81], "could": [19, 63, 81, 83], "explicit": 19, "rightarrow1": [19, 32, 50, 52, 85], "rightarrow0": [19, 24, 30, 32, 34, 36, 50, 72, 85], "m_z": [19, 24, 26, 30, 32, 34, 36, 50, 72, 73], "plu": [19, 81], "accord": [19, 46, 71, 86], "It": [19, 31, 32, 60, 69, 71, 76], "get": [19, 58, 60, 81, 84, 87, 88], "back": 19, "retriev": [19, 63, 64, 65, 66, 67, 68, 70, 72, 73, 74, 75, 77, 78, 79, 80, 81, 82, 83, 84, 86], "apply_regular_dropout": 19, "unmask": 19, "vector": [19, 57, 61, 67, 69, 71, 72, 88], "scale": [19, 88], "d": [19, 44, 56, 61, 72, 79, 85], "mask": [19, 57, 63, 69, 81, 88], "total": [19, 81], "node_in_featur": 19, "hyperedge_in_featur": 19, "channels_nod": [20, 21, 23, 25, 29, 35, 66, 68, 70, 72, 73, 74, 75, 78, 79, 80, 81, 83, 84, 87], "channels_edg": [20, 21, 23, 25, 29, 35, 66, 68, 70, 72, 73, 74, 75, 78], "n_class": [20, 45, 47, 51, 70, 82, 85], "neuron": [20, 21, 22, 60], "multiclass": [20, 70], "dsb20": [20, 21], "dong": [20, 21, 22, 60, 70, 71], "sawin": [20, 21], "icml": [20, 21], "grlplu": [20, 21], "io": [20, 21, 71], "40": [20, 21, 68, 69, 70, 71, 75, 76, 77, 78, 79, 83], "hypernod": [20, 21, 70], "templat": [21, 23, 28, 61, 72], "hnhnlayer": [21, 22, 27, 70, 71], "use_bia": 21, "use_normalized_incid": 21, "beta": [21, 31, 32, 76, 88], "bias_gain": 21, "bias_init": 21, "hnhn": [21, 22, 27, 60], "matric": [21, 45, 46, 47, 48, 49, 52, 60, 63, 64, 65, 66, 67, 68, 70, 72, 73, 74, 79, 80, 81, 83, 84, 86, 87], "usign": 21, "cardin": 21, "hyperparamet": [21, 32, 69, 71, 76], "control": 21, "strenght": 21, "support": [21, 76, 81, 84, 87], "train": [21, 26, 45, 60], "term": [21, 60, 81], "flag": 21, "import": [21, 22, 32, 60, 61, 63, 64, 65, 66, 67, 69, 71, 73, 76, 81, 88], "compute_normalization_matric": 21, "math": [21, 30, 32, 36, 44, 60, 61, 63, 67, 72, 73, 87, 88], "w": [21, 30, 32, 70, 72, 81], "xy": [21, 39, 41, 44, 48, 52, 54, 56, 58, 70, 79, 80, 83, 85, 87, 88], "sum_": [21, 24, 26, 30, 32, 34, 36, 39, 41, 44, 48, 50, 52, 54, 56, 58, 70, 72, 73, 79, 80, 83, 84, 85, 87, 88], "init_bias": 21, "normalize_incidence_matric": 21, "activation_func": 22, "normalization_param_alpha": [22, 71], "normalization_param_beta": [22, 71], "relai": 22, "other": [22, 59, 61, 88], "word": [22, 69, 71], "intermediari": 22, "those": [22, 72, 81], "dure": 22, "multipli": [22, 81], "reflect": 22, "param": 22, "power": [22, 26, 85], "amount": [23, 25, 29, 33, 35, 38, 40, 53, 60, 64, 65, 66, 67, 68, 69, 71, 72, 73, 74, 75, 77, 78, 84], "dwlll20": [23, 24], "kaiz": [23, 24], "ding": [23, 24, 72], "jianl": [23, 24], "wang": [23, 24], "jundong": [23, 24], "li": [23, 24], "dingcheng": [23, 24], "huan": [23, 24], "liu": [23, 24], "Be": [23, 24], "less": [23, 24, 88], "induct": [23, 24, 25, 26], "proceed": [23, 24, 31, 32, 33, 36, 48, 54, 58, 86], "empir": [23, 24], "languag": [23, 24, 61], "emnlp": [23, 24], "aclanthologi": [23, 24], "399": [23, 24, 70, 75], "global": [23, 25, 29, 33, 35, 68, 72, 73, 74, 75, 77, 78], "max": [23, 25, 29, 33, 35, 63, 66, 67, 72, 73, 74, 75, 77, 78, 83], "hypergat": [24, 27, 72], "hypergatlay": [24, 27, 72], "string": [24, 26, 61, 73], "set": [24, 26, 68, 70, 73, 74, 75, 76, 77, 78, 81, 83, 88], "see": [24, 26, 60, 61, 62, 71, 88], "t_1": [24, 32, 72], "odot": [24, 39, 44, 50, 72, 79], "zy": [24, 26, 32, 41, 72, 73, 80], "xz": [24, 26, 41, 72, 73, 80], "agrw20": [25, 26], "devanshu": [25, 26], "arya": [25, 26, 73], "deepak": [25, 26], "gupta": [25, 26], "stevan": [25, 26], "rudinac": [25, 26], "marcel": [25, 26], "wor": [25, 26], "gener": [25, 26, 41, 57, 61, 63, 66, 67, 76, 80, 81], "04558": [25, 26], "features_nod": [25, 73], "hypersag": [26, 27], "generalizedmean": [26, 27], "hypersagelay": [26, 27, 73], "aggr_func_intra": 26, "aggr_func_int": 26, "devic": [26, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 76, 81, 82, 85, 86], "cpu": [26, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 76, 81, 82, 85, 86], "p": [26, 44, 56, 63, 72, 73, 81], "name": [26, 60, 61, 63, 66, 67, 69, 75, 77, 78], "mode": [26, 76], "intra": [26, 46, 73], "either": [26, 60, 61, 81, 88], "w_y": [26, 73], "frac": [26, 32, 50, 72, 73], "vert": [26, 67, 73, 88], "w_z": [26, 73], "lvert": [26, 73], "rvert": [26, 73], "n_target_nod": 26, "allset_lay": [27, 66], "allset_transformer_lay": [27, 67], "allset_transform": 27, "dhgcn_layer": [27, 68], "dhgcn": 27, "hmpnn_layer": [27, 69], "hnhn_layer_bi": [27, 71], "hnhn_layer": [27, 70], "hypergat_lay": [27, 72], "hypersage_lay": [27, 73], "template_lay": [27, 60, 74], "templatelay": [27, 28, 66, 68, 74], "unigcn_lay": [27, 75], "unigcnlay": [27, 30, 75], "unigcn": [27, 30, 60], "unigcnii_lay": [27, 76], "unigcniilay": [27, 32, 76], "unigcnii": [27, 32, 33], "uniginlay": [27, 34, 77], "unigin": [27, 33, 34], "unisagelay": [27, 36, 78], "unisag": [27, 35, 36, 78], "intermediate_channel": [28, 33, 55, 68, 74, 77, 87], "intermedi": [28, 49, 55, 87], "simplici": [28, 37, 38, 39, 40, 41, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 60, 64, 65, 66, 67, 68, 70, 72, 73, 74, 86], "huang": [30, 31, 32, 33, 34, 36, 60], "unignn": [30, 31, 32, 33, 34, 36], "unifi": [30, 31, 32, 33, 34, 36], "use_bn": [30, 36], "jj21": [30, 31, 32, 33, 34, 36], "boolean": [30, 36, 61], "bathnorm": [30, 36], "everi": [30, 31, 32, 34, 36, 60, 76, 77], "hyper": [30, 32, 34, 36, 76, 88], "constitu": [30, 32, 34, 36], "num_lay": [31, 76, 84, 87], "jing": [31, 32, 33, 36], "jie": [31, 32, 33, 36], "yang": [31, 32, 33, 36, 48, 54, 60, 83, 84, 86, 87], "thirtieth": [31, 32, 33, 36], "joint": [31, 32, 33, 36], "intellig": [31, 32, 33, 36], "ijcai": [31, 32, 33, 36], "21": [31, 32, 33, 36, 48, 54, 69, 70, 71, 75, 77, 78, 79, 82, 83, 86], "00956": [31, 32, 33, 36], "expect": [31, 32, 76, 83, 85], "contain": [31, 32, 57, 60, 63, 66, 67, 69, 71, 76, 84, 88], "y_hat": [31, 63, 64, 65, 66, 67, 68, 70, 72, 73, 74, 76, 79, 80, 81, 83, 84, 85, 86, 87], "determin": [32, 88], "theta_2": 32, "theta_1": 32, "x_skip": 32, "degre": 32, "sqrt": 32, "d_x": 32, "d_z": 32, "in_channels_nod": [33, 77], "unigin_lay": [34, 77], "ep": 34, "train_ep": 34, "g": [34, 50, 55, 56, 61, 66, 67, 72, 81, 87, 88], "sequenti": [34, 69, 71, 77], "constant": 34, "gin": 34, "unisage_lay": [36, 78], "e_aggr": 36, "amax": 36, "amin": 36, "v_aggr": 36, "operatornam": [36, 44, 72], "sage": 36, "tdl23": 36, "submodul": 37, "dist2cycl": [38, 39, 42], "binari": [38, 40, 43, 47, 51, 53, 63, 69, 70, 71, 75, 77, 78, 81], "high": [38, 40, 41], "x_1e": [38, 79], "linv": [38, 39, 79], "adjacency_0": [38, 39, 40, 41, 63, 64, 79, 80], "hot": [38, 40, 43, 49, 53, 70, 79, 80, 81, 84], "dist2cycle_lay": 39, "dist2cyclelay": [39, 42], "x_e": 39, "a_0": [39, 41], "hsn": [40, 41, 42, 60], "hsn_layer": [41, 60], "hsnlayer": [41, 42, 60, 80, 83], "hrgz22": 41, "complic": [41, 44], "geometr": [41, 60], "iclr": 41, "openreview": [41, 52], "net": [41, 52, 58, 60], "id": [41, 52, 61], "sc8glb": 41, "k6e9": 41, "sanconv": [42, 44], "sanlay": [42, 44], "san": [42, 43, 44, 60, 63], "compute_projection_matrix": [42, 43], "scacmpslay": [42, 46, 82], "intra_aggr": [42, 46], "weight_func": [42, 46], "scacmp": [42, 45, 82], "sccnlayer": [42, 48, 54, 83], "sccn": [42, 47, 48, 54, 60], "sccnnlayer": [42, 50, 84], "aggr_norm_func": [42, 50, 56], "chebyshev_conv": [42, 50, 56], "sccnn": [42, 49, 50], "sccnncomplex": [42, 49, 84], "scconvlay": [42, 52], "scconv": [42, 51, 52], "scn2layer": [42, 48, 54], "scn2": [42, 48, 53, 86], "scnnlayer": [42, 56, 87], "scnn": [42, 55, 56, 84], "sconelay": [42, 58, 88], "scone": [42, 57, 58, 60], "trajectoriesdataset": [42, 57, 88], "vectorize_path": [42, 57, 88], "generate_complex": [42, 57, 88], "generate_trajectori": [42, 57, 88], "n_filter": [43, 44], "order_harmon": 43, "epsilon_harmon": 43, "simplex_order_k": [43, 81], "simplic": [43, 46, 49, 56, 81, 83, 84, 87, 88], "approxim": [43, 44, 88], "filter": [43, 44, 50, 84], "harmon": 43, "epsilon": 43, "1e": [43, 70, 75, 78, 88], "laplacian": [43, 44, 45, 46, 49, 50, 54, 55, 56, 81, 82, 83, 84, 86], "calcul": [43, 61], "compon": [43, 61, 81], "hodg": [43, 50, 54, 56, 81, 83, 84], "laplacian_up": [43, 44, 55, 56, 81, 87], "laplacian_down": [43, 44, 55, 56, 81, 87], "channels_in": 43, "ld": [43, 79], "san_lay": 44, "lgcb22": 44, "l_": [44, 56, 58, 81, 82, 87, 88], "wh_1": 44, "simplex": [44, 45, 53, 54, 64, 65, 66, 67, 68, 72, 73, 74, 79, 80, 81, 83, 84, 85, 87, 88], "claudio": [44, 60], "paolo": 44, "stefania": 44, "sergio": [44, 60], "07485": 44, "projection_mat": 44, "2p": [44, 81], "q_r": [44, 81], "n_cell": 44, "down_indic": 44, "n_cells_down": 44, "n_neighbor": 44, "up_indic": 44, "n_cells_up": 44, "sca": [45, 46, 60], "cmp": [45, 46], "sca_cmp": [45, 82], "channels_list": [45, 46, 82], "complex_dim": [45, 46, 82], "tetahedron": 45, "respect": [45, 60, 65, 66, 67, 72, 73, 81, 84, 87, 88], "complex_dimens": 45, "highest": [45, 46, 88], "being": [45, 60, 81], "x_list": [45, 46], "laplacian_down_list": 45, "incidence_t_list": 45, "etc": [45, 56, 61, 76], "start": [45, 60, 61, 76, 88], "autoencod": [46, 60], "sca_cmps_lay": 46, "coadjac": 46, "hzpmc22": [46, 82], "maroula": 46, "cai": 46, "2103": 46, "04046": 46, "chain": [46, 58, 88], "down_lap_list": 46, "incidencet_list": 46, "qquad": [46, 50, 82], "_downarrow": [46, 52], "hold": [46, 61], "untouch": 46, "max_rank": [47, 48, 83, 84, 87], "dict": [47, 48], "length": [47, 48, 61, 69, 71, 76], "n_rank_r_cel": [47, 48], "n_rank_r_minus_1_cel": [47, 48], "b_r": [47, 48, 65, 83], "h_r": [47, 48, 54, 83], "log": [48, 54, 61, 83, 88], "sccn_layer": [48, 54], "ysb22": [48, 54, 86], "ani": [48, 49, 61, 63, 64, 73, 88], "leftmost": 48, "diagram": [48, 54, 60], "yang22c": [48, 54, 86], "figur": [48, 54, 70], "11": [48, 54, 60, 63, 64, 65, 69, 70, 71, 72, 74, 75, 76, 77, 78, 79, 80, 81, 83, 84, 85, 87, 88], "scn2_layer": [48, 54], "scn": [48, 54], "abov": [48, 54, 60, 61, 87, 88], "below": [48, 54, 69, 77, 88], "ruochen": [48, 54, 86], "freder": [48, 54, 86], "sala": [48, 54, 86], "paul": [48, 54, 60, 62, 86], "bogdan": [48, 54, 86], "effici": [48, 54, 79, 83, 84, 86], "bastian": [48, 54, 60, 86], "rieck": [48, 54, 60, 86], "razvan": [48, 54, 86], "pascanu": [48, 54, 86], "editor": [48, 54, 86], "volum": [48, 54, 86], "198": [48, 54, 70, 75, 83, 86], "machin": [48, 54, 58, 60, 86, 87], "research": [48, 54, 60, 86], "page": [48, 54, 61, 86], "13": [48, 54, 60, 64, 69, 70, 71, 72, 74, 75, 76, 77, 78, 79, 80, 82, 83, 84, 85, 86, 87, 88], "pmlr": [48, 54, 86], "09": [48, 54, 79, 86, 88], "12": [48, 54, 64, 69, 70, 71, 72, 75, 76, 77, 78, 79, 80, 81, 83, 84, 86, 87, 88], "dec": [48, 54, 86], "2022a": [48, 54, 86], "mlr": [48, 54, 58, 86], "press": [48, 54, 58, 86], "v198": [48, 54, 86], "yang22a": [48, 54, 86], "html": [48, 54, 58, 71, 86], "describ": [48, 61, 67], "unnorm": 48, "bigcup": [48, 83], "out_featur": [48, 63, 84, 87], "in_channels_al": [49, 84], "intermediate_channels_al": [49, 84], "out_channels_al": [49, 84], "conv_ord": [49, 50, 56, 84], "sc_order": [49, 50, 84], "task": [49, 55, 60, 66, 67, 69, 70, 71, 76, 79, 80, 83, 84, 87, 88], "we": [49, 50, 55, 56, 58, 60, 61, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 85, 86, 88], "cours": [49, 61], "amend": 49, "sc": [49, 50, 57, 82, 84, 87, 88], "numer": [49, 55, 87], "x_all": [49, 50, 84], "laplacian_al": [49, 50, 84], "incidence_al": [49, 50, 84], "entri": [49, 81], "n_simplic": [49, 50, 55, 56, 87], "l0": 49, "l1_d": 49, "l1_u": 49, "l2": 49, "pf": 49, "b2": [49, 50, 83, 84, 85, 87], "sccnn_layer": 50, "triangl": [50, 56, 57, 58, 81, 84, 88], "ndoe": 50, "too": 50, "mani": [50, 61, 63], "exampl": [50, 56, 60, 81, 84, 87, 88], "here": [50, 52, 56, 60, 61, 63, 68, 69, 71, 76, 79, 80, 83, 84, 88], "pseudocod": [50, 56], "l_0": 50, "lap_down": [50, 56], "l_1_down": 50, "lap_up": [50, 56], "l_1_up": 50, "lap": 50, "l_2": 50, "y_0": 50, "y_1": 50, "y_2": 50, "look": [50, 56, 61, 88], "einsum": [50, 56], "weight_0": 50, "weight_1": 50, "weight_2": 50, "total_order_0": 50, "total_order_1": 50, "total_order_2": 50, "chebyshev": [50, 56], "conv_oper": [50, 56], "perform": [50, 55, 56, 60, 61, 63, 64, 65, 66, 67, 68, 70, 72, 73, 74, 75, 76, 78, 79, 80, 81, 83, 85, 86, 88], "num_channel": [50, 56], "repres": [50, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 76, 79, 80, 81, 86, 88], "n_triangl": [50, 58], "laplacian_0": [50, 53, 54, 84, 86, 87], "laplacian_down_1": [50, 82, 84, 87], "laplacian_up_1": [50, 84, 87], "laplacian_2": [50, 53, 54, 84, 86, 87], "part": [50, 56, 63, 79, 80, 81, 83, 84], "node_channel": [51, 52, 68, 85], "edge_channel": [51, 52, 85], "face_channel": [51, 52, 85], "incidence_1_norm": [51, 52, 85], "incidence_2": [51, 52, 58, 65, 84, 85, 87], "incidence_2_norm": [51, 52, 85], "adjacency_up_0_norm": [51, 52, 85], "adjacency_up_1_norm": [51, 52, 85], "adjacency_down_1_norm": [51, 52, 85], "adjacency_down_2_norm": [51, 52, 85], "_1": [51, 52, 84, 85, 87], "_2": [51, 52, 84], "scconv_lay": 52, "bunch20": 52, "bunch": [52, 85], "eric": 52, "qian": 52, "glenn": 52, "fung": 52, "vika": 52, "singh": [52, 60], "tda": 52, "homepag": 52, "forum": 52, "tlbnskrt6j": 52, "yrightarrow": 52, "0rightarrow": 52, "tild": [52, 85], "0rightarrow0": 52, "1rightarrow0": 52, "1rightarrow": 52, "1rightarrow1": 52, "2rightarrow1": 52, "_uparrow": 52, "For": [52, 55, 58, 61, 62, 70, 73, 79, 80, 84, 87, 88], "mai": [52, 60], "helper": 52, "pyt": 52, "team": [52, 60], "laplacian_1": [53, 54, 86], "rightmost": 54, "2i": [54, 83], "node_featur": 54, "edge_featur": 54, "face_featur": 54, "l_upper": 54, "l_lower": 54, "conv_order_down": [55, 56, 87], "conv_order_up": [55, 56, 87], "At": [55, 84, 87], "simplci": [55, 87], "To": [55, 62, 66, 70, 78, 79, 80, 87, 88], "challeng": [55, 87], "scnn_layer": [56, 87], "total_ord": 56, "2110": 56, "02585": 56, "n_simplex": 56, "simplicialcomplex": [57, 75, 77, 78, 88], "hidden_dim": [57, 88], "trajectori": [57, 58, 87], "dataset": [57, 60, 63, 64, 65, 66, 67, 68, 69, 71, 72, 73, 74, 75, 76, 77, 78, 81, 82], "path": [57, 88], "100": [57, 68, 69, 70, 71, 75, 76, 77, 78, 82, 83, 85, 86, 88], "ndarrai": [57, 61, 88], "uniformli": [57, 88], "sampl": [57, 61, 63, 70, 88], "random": [57, 63, 71, 88], "point": [57, 88], "unit": [57, 60, 61, 88], "squar": [57, 61, 88], "delaunai": [57, 88], "triangul": [57, 88], "delet": [57, 88], "some": [57, 61, 83, 88], "pre": [57, 60, 88], "disk": [57, 88], "coord": [57, 88], "n_max": [57, 88], "1000": [57, 70, 88], "corner": [57, 88], "middl": [57, 88], "scone_lay": 58, "rgs21": [58, 88], "stack": [58, 63, 64, 65, 66, 68, 69, 70, 71, 74, 75, 77, 78, 79, 80, 81, 83, 84, 87, 88], "befor": [58, 60, 61, 76, 88], "neighbour": [58, 88], "next": [58, 61, 70, 72, 76, 88], "when": [58, 60, 61, 63, 81, 88], "roddenberri": [58, 60, 87, 88], "mitchel": [58, 87], "glaze": [58, 87], "principl": [58, 87, 88], "38th": 58, "v139": 58, "roddenberry21a": 58, "variou": 59, "librari": 59, "torch_scatt": 59, "py": [59, 60, 61, 64, 71, 76, 79, 83, 84, 85, 87], "rusty1": 59, "pytorch_scatt": 59, "dim": [59, 61, 63, 64, 65, 66, 67, 68, 69, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 84, 87, 88], "dim_siz": 59, "welcom": [60, 61], "host": 60, "annual": 60, "topologi": [60, 68, 81], "geometri": 60, "tag": 60, "review": [60, 61, 62], "contributor": [60, 61], "mathild": [60, 62], "mustafa": [60, 62], "nina": [60, 62], "florian": 60, "frantzen": 60, "ghada": [60, 62], "alzamzmi": 60, "theodor": [60, 62], "michael": [60, 62], "scholkemp": 60, "josef": 60, "hopp": 60, "karthikeyan": [60, 62], "natesan": [60, 62], "johan": 60, "audun": 60, "myer": 60, "helen": 60, "jenn": 60, "tim": 60, "doster": 60, "tegan": 60, "emerson": 60, "henri": 60, "kving": 60, "sophia": [60, 62], "jan": 60, "meissner": 60, "tolga": [60, 62], "vincent": 60, "grand": 60, "aldo": [60, 62], "tamal": [60, 62], "soham": [60, 62], "shreya": [60, 62], "neal": [60, 62], "robin": [60, 62], "edit": [60, 61], "now": [60, 63, 64, 66, 67, 68, 69, 70, 71, 72, 73, 74, 76, 79, 80, 81, 86, 88], "over": [60, 63, 66, 67, 68, 69, 71, 73, 74, 75, 78, 79, 80, 81, 83, 85, 86, 88], "thank": 60, "stellar": 60, "contirbut": 60, "foster": 60, "reproduc": [60, 62], "open": [60, 76], "winner": 60, "announc": 60, "luca": 60, "scofano": 60, "guillermo": 60, "bernardez": 60, "simon": 60, "fiorellino": 60, "indro": 60, "spinelli": 60, "scardapan": 60, "lev": 60, "telyatninkov": 60, "olga": 60, "zaghen": 60, "sadrodin": 60, "barikbin": [60, 71], "odin": 60, "hoff": 60, "gardaa": 60, "dmitrii": 60, "gavrilev": 60, "gleb": 60, "bazhenov": 60, "suraj": 60, "combinatori": 60, "rub\u00e9n": 60, "ballest": 60, "manuel": 60, "lecha": 60, "escalera": 60, "hoan": 60, "aiden": 60, "brent": 60, "honor": 60, "mention": 60, "jen": 60, "agerberg": 60, "georg": 60, "b\u00f6kman": 60, "pavlo": 60, "melnyk": 60, "alessandro": 60, "salatiello": 60, "alexand": 60, "nikitin": 60, "purpos": [60, 61, 64, 65, 66, 67, 68, 69, 71, 73, 74, 75, 78, 79, 80, 81, 83, 84], "crowdsourc": 60, "ask": 60, "contribut": [60, 72, 84], "code": [60, 61, 88], "previous": 60, "exist": 60, "benchmark": [60, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 82, 83, 84, 85, 86, 87], "python": [60, 61, 62], "packag": [60, 62, 64, 71, 76, 79, 83, 84, 85, 87], "take": [60, 63, 64, 65, 81, 88], "pull": [60, 61], "request": [60, 61, 76, 88], "literatur": [60, 62, 76], "leverag": [60, 81], "infrastructur": 60, "invit": 60, "regularli": 60, "white": 60, "summar": 60, "find": [60, 63, 88], "publish": 60, "qualifi": 60, "opportun": 60, "author": [60, 62, 71, 88], "top": [60, 63, 84], "8": [60, 63, 64, 65, 67, 69, 70, 71, 73, 74, 75, 76, 77, 78, 79, 80, 81, 83, 84, 86, 87, 88], "best": [60, 67], "addit": 60, "softwar": [60, 61], "journal": 60, "special": 60, "recognit": 60, "date": 60, "time": [60, 63, 66, 67, 68, 69, 70, 71, 72, 73, 74, 79, 80, 81, 83, 84, 87, 88], "must": [60, 66, 67, 68, 70, 72, 73, 74, 79, 80, 81, 83, 84, 87], "juli": 60, "16": [60, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 77, 78, 79, 81, 82, 83, 84, 85, 86, 87, 88], "59": [60, 69, 70, 75, 79, 83], "pacif": 60, "standard": [60, 61, 76, 81], "modifi": [60, 61, 82], "until": 60, "everyon": [60, 61], "free": [60, 88], "suffici": 60, "accept": 60, "automat": [60, 81], "subscrib": 60, "encourag": 60, "earli": 60, "help": [60, 61], "debug": 60, "fail": 60, "test": [60, 63, 64, 65, 66, 67, 68, 69, 70, 71, 73, 74, 75, 76, 77, 78, 79, 80, 81, 83, 84], "address": 60, "potenti": 60, "issu": [60, 76], "similar": [60, 83], "qualiti": 60, "earlier": [60, 83], "prioriti": 60, "consider": 60, "restrict": 60, "member": 60, "than": [60, 87, 88], "princip": 60, "develop": [60, 61], "allow": [60, 83], "fig": [60, 88], "compli": 60, "action": 60, "workflow": 60, "successfulli": 60, "lint": 60, "format": [60, 61, 76, 84], "black": [60, 88], "isort": 60, "flake8": 60, "_layer": 60, "ex": 60, "store": [60, 70], "directori": [60, 61], "primit": 60, "equival": [60, 61], "depict": 60, "_train": 60, "ipynb": 60, "hsn_train": 60, "tutori": [60, 62, 76], "well": [60, 61], "load": [60, 64, 65, 66, 67, 68, 71, 72, 73, 74, 78, 79, 80, 82, 83, 84, 85, 86, 87], "toponetx": [60, 63, 64, 65, 66, 67, 68, 70, 72, 73, 74, 75, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88], "shrec16": [60, 64, 65, 66, 67, 68, 72, 73, 74, 82, 84, 87], "suitabl": [60, 70], "karat": [60, 70, 79, 80, 81, 83, 84], "club": [60, 70, 79, 80, 81, 83, 84], "choic": [60, 63, 64, 65, 66, 67, 68, 72, 73, 74, 75, 77, 78, 82], "along": [60, 63, 81], "simpl": [60, 87], "depend": 60, "accuraci": [60, 70, 71, 75, 76, 77, 78, 79, 80, 81, 83, 84, 87, 88], "test_": [60, 61], "name_of_model": 60, "test_hsn_lay": 60, "testhsnlay": 60, "pleas": [60, 61, 63, 64, 71, 76, 81, 84, 87], "pytest": [60, 61], "unittest": 60, "further": [60, 63, 81], "manipul": 60, "modif": 60, "accompani": 60, "appropri": [60, 61], "locat": [60, 61, 76], "With": [60, 72], "said": 60, "highli": 60, "most": [60, 61, 81, 83], "resort": 60, "absolut": 60, "condorcet": 60, "decid": [60, 81], "criteria": 60, "chosen": [60, 83], "correctli": 60, "need": [60, 63, 64, 65, 66, 67, 68, 70, 72, 73, 74, 79, 80, 81, 86, 88], "match": 60, "readabl": [60, 61], "clean": 60, "api": [60, 61], "written": 60, "clearli": 60, "explain": 60, "robust": 60, "reward": 60, "nor": 60, "goal": 60, "accur": 60, "our": [60, 61, 62, 63, 64, 65, 66, 67, 68, 70, 72, 73, 74, 75, 77, 78, 79, 80, 81, 82, 83, 84, 86, 87, 88], "field": 60, "select": [60, 83, 87], "maintain": 60, "collabor": 60, "whose": [60, 61], "vote": 60, "onc": [60, 66, 67, 68, 72, 73, 74], "googl": [60, 61], "express": [60, 72], "even": 60, "link": [60, 61], "record": [60, 61, 66, 67, 68, 72, 73, 74], "email": 60, "identifi": 60, "voter": 60, "ident": [60, 67, 79, 80, 84], "remain": [60, 88], "secret": 60, "feel": [60, 88], "contact": 60, "slack": 60, "ucsb": 60, "edu": 60, "guid": 61, "aim": [61, 63, 81], "eas": 61, "both": [61, 63, 81, 88], "novic": 61, "experienc": 61, "commun": 61, "effort": 61, "fork": 61, "upstream": 61, "submit": [61, 76], "pr": 61, "synchron": 61, "your": 61, "branch": 61, "git": 61, "checkout": 61, "sure": 61, "section": [61, 88], "re": [61, 79, 88], "done": [61, 64, 65, 66, 67, 68, 72, 73, 74, 78, 82, 84, 85, 86, 87, 88], "commit": 61, "modified_fil": 61, "my": [61, 82], "push": 61, "toponextx": 61, "instruct": 61, "repeat": 61, "folder": 61, "filenam": 61, "test_add": 61, "def": [61, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 83, 84, 85, 87, 88], "test_capital_cas": 61, "assert": [61, 88], "9": [61, 63, 64, 65, 67, 69, 70, 71, 73, 74, 75, 76, 77, 78, 79, 80, 81, 83, 84, 86, 87, 88], "statement": 61, "under": 61, "correct": [61, 63, 75, 77, 78, 88], "instal": 61, "tool": 61, "pip": 61, "dev": 61, "verifi": 61, "break": 61, "doc": 61, "descript": [61, 88], "usag": 61, "markdown": 61, "common": [61, 63], "restructuredtext": 61, "numpi": [61, 63, 64, 65, 66, 67, 68, 70, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88], "style": 61, "understand": 61, "role": 61, "syntax": 61, "autom": 61, "pars": 61, "inclus": 61, "print": [61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88], "__doc__": 61, "attribut": 61, "try": [61, 63, 64, 81, 88], "np": [61, 63, 64, 65, 66, 67, 68, 70, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88], "good": [61, 86], "These": 61, "ones": [61, 69, 71], "summari": 61, "line": 61, "79": [61, 68, 69, 70, 75, 83], "char": 61, "immedi": 61, "capit": 61, "letter": 61, "period": 61, "verb": 61, "imper": 61, "mood": 61, "possibl": [61, 76], "uncertain": 61, "oppos": 61, "evalu": [61, 63, 69, 70, 71, 76, 77], "separ": 61, "blank": 61, "argument": [61, 66, 67, 81], "On": 61, "state": [61, 73, 76], "rest": 61, "space": [61, 84, 88], "side": 61, "default_valu": 61, "indent": 61, "esp": 61, "would": [61, 79, 80], "want": [61, 76, 88], "veri": [61, 76], "rais": [61, 84, 87], "latex": 61, "cite": [61, 76], "my_method": 61, "my_param_1": 61, "my_param_2": 61, "big": 61, "short": 61, "my_result": 61, "relev": 61, "snippet": 61, "show": [61, 70, 79, 80, 88], "script": 61, "wikipedia": 61, "And": 61, "fill": 61, "scikit": 61, "fit_predict": 61, "sample_weight": 61, "cluster": [61, 82], "center": [61, 88], "conveni": 61, "fit": 61, "sparse_matrix": 61, "n_featur": 61, "ignor": [61, 75, 78], "Not": 61, "present": [61, 63], "convent": [61, 63], "observ": 61, "labels_": 61, "mind": 61, "instead": [61, 76, 84], "vari": 61, "notat": [61, 63, 64, 65, 70, 79, 80, 81, 82, 83, 85, 87, 88], "axi": [61, 88], "bracket": 61, "multinomi": 61, "1d": [61, 64, 65], "2d": [61, 64, 65], "subset": [61, 63, 69, 71], "datafram": 61, "explicitli": 61, "relat": [61, 69], "colon": 61, "explan": 61, "_weight_boost": 61, "adaboost": 61, "great": 61, "ve": 61, "discuss": 61, "Of": 61, "verbos": 61, "thei": [61, 63, 64, 65, 66, 67, 79, 80, 81, 83, 84, 87], "rst": 61, "80": [61, 68, 69, 70, 75, 82, 83, 87], "charact": 61, "except": [61, 63, 81], "tabl": 61, "tdl": 62, "blue": 62, "laid": 62, "extend": [62, 84], "avail": [62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 74, 76, 81, 82], "about": 62, "blueprint": 62, "misc": 62, "hajij2023topolog": 62, "titl": [62, 70], "year": 62, "eprint": 62, "archiveprefix": 62, "primaryclass": 62, "lg": 62, "papillon2023architectur": 62, "notebook": [63, 66, 67, 68, 69, 70, 71, 72, 73, 74, 79, 80, 81, 84, 85, 86, 87, 88], "didact": [63, 81], "clear": [63, 81], "technic": [63, 81], "document": [63, 69, 71, 76, 81], "sinc": [63, 79, 80, 83, 84, 88], "introduct": 63, "achiev": [63, 67, 83, 88], "outstand": 63, "howev": [63, 76, 84, 87], "pairwis": [63, 66, 67, 68, 72, 73, 74, 88], "relationship": 63, "among": 63, "abl": [63, 81], "fulli": 63, "interact": 63, "real": [63, 88], "world": [63, 88], "vertic": [63, 88], "captur": 63, "particular": 63, "encod": [63, 70, 79, 80, 81, 84, 88], "design": 63, "independ": [63, 67], "thu": [63, 81, 86], "strategi": [63, 73], "approach": 63, "hierarch": 63, "incorpor": 63, "algorithm": 63, "ii": 63, "optim": [63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88], "iii": 63, "extract": [63, 66, 67, 72, 73, 78], "compact": 63, "meaning": 63, "remark": [63, 81], "custom": [63, 81], "symbol": [63, 81], "involv": [63, 81], "made": [63, 70, 79, 80, 81, 83, 84, 86, 87], "stage": 63, "nbsphinx": [63, 67, 72, 73, 87], "textrm": [63, 81], "parameter": 63, "mathbb": [63, 66, 67, 72, 88], "2f_0": 63, "f_0": 63, "textbf": [63, 66, 67, 73, 81], "bigg": [63, 81, 84, 87], "f": [63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88], "respons": [63, 76], "reciproc": 63, "round": [63, 85], "tcdot": 63, "xin": 63, "score": [63, 88], "_r": 63, "coars": 63, "mutag": [63, 75, 77, 78], "tudataset": [63, 75, 77, 78], "paperswithcod": 63, "__": 63, "188": [63, 70, 75, 83], "chemic": 63, "compound": 63, "discret": 63, "mutagen": 63, "salmonella": 63, "typhimurium": 63, "sklearn": [63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 77, 78, 82, 84, 86, 87], "model_select": [63, 64, 65, 66, 67, 68, 70, 72, 73, 74, 75, 77, 78, 82, 84, 86, 87], "train_test_split": [63, 64, 65, 66, 67, 68, 70, 72, 73, 74, 75, 77, 78, 82, 84, 86, 87], "cell_complex": [63, 64, 65], "cellcomplex": 63, "torch_geometr": [63, 69, 71, 75, 77, 78, 81], "convert": [63, 64, 65, 70, 75, 77, 78, 79, 80, 81, 84, 86], "to_networkx": [63, 75, 77, 78, 81], "gpu": [63, 64, 65, 66, 67, 68, 69, 70, 71, 74, 76, 82], "run": [63, 64, 65, 66, 67, 68, 69, 70, 71, 74, 76, 83, 88], "cuda": [63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 76, 81, 82, 85, 86], "is_avail": [63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 76, 81, 82, 85, 86], "els": [63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 76, 81, 82, 84, 85, 86, 87], "root": [63, 75, 77, 78], "tmp": [63, 75, 76, 77, 78], "use_edge_attr": [63, 75, 77, 78], "use_node_attr": 63, "cc_list": [63, 64, 65], "x_0_list": 63, "x_1_list": [63, 75, 77, 78], "y_list": [63, 75, 77, 78], "append": [63, 64, 65, 66, 67, 68, 70, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88], "edge_attr": 63, "i_cc": 63, "th": [63, 64, 65, 66, 67, 68, 72, 73, 74, 81, 82, 85, 86], "36": [63, 64, 69, 70, 71, 75, 77, 78, 79, 83], "0th": [63, 85], "17": [63, 69, 70, 71, 75, 77, 78, 79, 82, 83, 87], "38": [63, 69, 70, 71, 75, 77, 78, 79, 83], "lower_neighborhood_list": 63, "upper_neighborhood_list": 63, "adjacency_0_list": [63, 64], "adjacency_matrix": [63, 64, 65, 79, 80, 83, 84, 85, 88], "from_numpi": [63, 64, 65, 66, 67, 68, 70, 71, 72, 73, 74, 75, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87], "todens": [63, 64, 65, 66, 67, 68, 70, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87], "to_spars": [63, 64, 65, 66, 67, 68, 70, 72, 73, 74, 77, 79, 80, 81, 82, 83, 84, 85, 86, 87], "lower_neighborhood_t": 63, "down_laplacian_matrix": [63, 79, 81, 82, 84, 85, 87], "upper_neighborhood_t": 63, "up_laplacian_matrix": [63, 81, 84, 85, 87], "__init__": [63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 85, 87, 88], "arg": [63, 84, 85], "super": [63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 87], "lift_lay": 63, "rang": [63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 83, 84, 85, 86, 87, 88], "modulelist": [63, 64, 65, 66, 67, 68, 69, 70, 72, 73, 74, 75, 76, 77, 78, 84, 87], "lin_0": [63, 64, 65], "128": [63, 70, 75, 83], "lin_1": [63, 64, 65], "hasattr": 63, "isinst": 63, "feed": [63, 66, 67, 68, 72, 73, 74, 75, 77, 78, 88], "foward": 63, "specifi": [63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 83, 84, 86, 87], "loss": [63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 83, 84, 85, 86, 87, 88], "without": [63, 64], "32": [63, 66, 69, 70, 71, 72, 75, 77, 78, 79, 83, 84, 88], "crit": [63, 64, 75, 77, 78], "crossentropyloss": [63, 64, 69, 70, 71, 76, 77], "opt": [63, 64, 66, 67, 68, 72, 73, 74, 82, 83, 84, 85, 87], "adam": [63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88], "lr": [63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88], "001": [63, 77, 86], "lower_att": 63, "lin": 63, "64": [63, 66, 67, 69, 70, 75, 79, 83], "upper_att": 63, "split": [63, 64, 65, 66, 67, 68, 70, 71, 73, 74, 75, 76, 77, 78, 85, 88], "test_siz": [63, 64, 65, 66, 67, 68, 70, 72, 73, 74, 75, 77, 78, 82, 84, 86, 87, 88], "x_1_train": [63, 64, 65, 74, 75, 77, 78, 82, 84, 85], "x_1_test": [63, 64, 65, 74, 75, 77, 78, 82, 84], "shuffl": [63, 64, 65, 66, 67, 68, 72, 73, 74, 75, 77, 78, 82, 84, 86, 87, 88], "x_0_train": [63, 64, 65, 66, 67, 68, 71, 72, 73, 82, 84, 85], "x_0_test": [63, 64, 65, 66, 67, 68, 71, 72, 73, 82, 84], "lower_neighborhood_train": 63, "lower_neighborhood_test": 63, "upper_neighborhood_train": 63, "upper_neighborhood_test": 63, "adjacency_0_train": [63, 64], "adjacency_0_test": [63, 64], "y_train": [63, 64, 65, 66, 67, 68, 70, 72, 73, 74, 75, 77, 78, 79, 80, 81, 82, 83, 84, 86, 87], "y_test": [63, 64, 65, 66, 67, 68, 70, 72, 73, 74, 75, 77, 78, 79, 80, 81, 82, 83, 84, 86, 87], "test_interv": [63, 64, 65, 66, 67, 68, 70, 72, 73, 74, 76, 79, 80, 81, 83, 84, 85, 86, 87], "num_epoch": [63, 64, 65, 66, 67, 68, 70, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 83, 84, 85, 86, 87], "epoch_i": [63, 64, 65, 66, 67, 68, 70, 72, 73, 74, 79, 80, 81, 83, 84, 85, 86, 87], "epoch_loss": [63, 64, 65, 66, 67, 68, 72, 73, 74, 79, 80, 81, 83, 84, 85, 86, 87], "num_sampl": 63, "zip": [63, 64, 65, 66, 67, 68, 72, 73, 74, 75, 77, 78, 84, 85, 86, 87], "dtype": [63, 66, 67, 68, 69, 70, 71, 72, 73, 74, 76, 84, 87], "long": [63, 69, 71, 83], "zero_grad": [63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 83, 84, 85, 86, 87, 88], "argmax": [63, 69, 70, 71, 76, 77, 88], "backward": [63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 83, 84, 85, 86, 87, 88], "train_acc": [63, 69, 70, 76, 79, 80, 81, 83, 84, 87, 88], "epoch": [63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 83, 84, 85, 86, 87, 88], "4f": [63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 79, 80, 81, 83, 84, 86, 87], "flush": [63, 64, 65, 66, 67, 68, 70, 72, 73, 74, 79, 80, 81, 83, 84, 85, 86, 87], "no_grad": [63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 83, 84, 85, 86, 87], "test_acc": [63, 69, 70, 76, 79, 80, 81, 83, 84, 87], "6234": 63, "6794": 63, "5965": 63, "6047": 63, "6947": 63, "5953": 63, "5879": 63, "7099": 63, "5614": 63, "5801": 63, "6316": 63, "5755": [63, 70], "7405": 63, "7544": 63, "5624": [63, 79, 86], "5565": [63, 70], "7719": 63, "small": [64, 65, 66, 67, 68, 72, 73, 74, 82, 83, 84, 85, 86, 87], "3d": [64, 65, 66, 67, 68, 72, 73, 74, 82, 85, 86], "mesh": [64, 65, 66, 67, 68, 72, 73, 74, 82, 84, 85, 86, 87], "shrec": [64, 65, 66, 67, 68, 72, 73, 74, 82, 85, 86, 87], "shrec_16": [64, 65, 66, 67, 68, 72, 73, 74, 82, 84, 85, 86, 87], "kei": [64, 65, 66, 67, 68, 72, 73, 74, 82, 84, 85, 86, 87], "node_feat": [64, 65, 66, 67, 68, 70, 72, 73, 74, 79, 80, 81, 82, 83, 84, 85, 86, 87], "edge_feat": [64, 65, 66, 67, 68, 70, 72, 73, 74, 79, 80, 81, 82, 83, 84, 85, 86, 87], "face_feat": [64, 65, 66, 67, 68, 72, 73, 74, 79, 80, 81, 82, 83, 84, 85, 86, 87], "i_complex": [64, 65, 66, 67, 68, 72, 73, 74, 82, 85, 86], "6th": [64, 65, 66, 67, 68, 72, 73, 74, 82, 86], "252": [64, 65, 66, 67, 68, 70, 72, 73, 74, 75, 82, 85, 86], "750": [64, 65, 66, 67, 68, 70, 72, 73, 74, 82, 85, 86], "10": [64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88], "500": [64, 65, 66, 67, 68, 70, 72, 73, 74, 75, 82, 85, 86], "messg": [64, 66, 67, 72, 73, 74, 79, 80, 86], "incidence_2_t_list": 64, "to_cell_complex": [64, 65], "incidence_2_t": [64, 82], "incidence_matrix": [64, 65, 66, 67, 68, 70, 72, 73, 74, 75, 77, 78, 79, 80, 81, 82, 83, 84, 85, 87], "lin_2": [64, 65], "0d": [64, 65], "nan": [64, 65, 70, 85], "two_dimensional_cells_mean": [64, 65], "nanmean": [64, 65], "isnan": [64, 65], "one_dimensional_cells_mean": [64, 65], "zero_dimensional_cells_mean": [64, 65], "loss_fn": [64, 66, 67, 68, 69, 71, 72, 73, 74, 76, 82, 84, 85, 86, 87], "mseloss": [64, 65, 66, 67, 68, 72, 73, 74, 82, 84, 85, 86, 87], "incidence_2_t_train": 64, "incidence_2_t_test": 64, "low": [64, 65, 66, 67, 68, 69, 71, 73, 74, 75, 78, 79, 80, 81, 83, 84, 85, 86], "minim": [64, 65, 66, 67, 68, 69, 71, 73, 74, 75, 78, 84], "rapid": [64, 65, 66, 67, 68, 69, 71, 73, 74, 75, 78, 84], "test_loss": [64, 65, 66, 67, 68, 69, 72, 73, 74, 84, 85, 86, 87], "96": [64, 69, 70, 71, 75, 83], "4544": 64, "ninamiolan": [64, 83, 84, 87], "anaconda3": [64, 79, 83, 84, 85, 87], "env": [64, 79, 83, 84, 85, 87], "tmxtest": 64, "lib": [64, 71, 79, 83, 84, 85, 87], "python3": [64, 71, 83, 84, 85, 87], "site": [64, 71, 79, 83, 84, 85, 87], "536": [64, 70, 84, 87], "userwarn": [64, 76, 84, 85, 87], "lead": [64, 84, 87], "incorrect": [64, 84, 87], "due": [64, 83, 84, 87], "ensur": [64, 84, 87], "mse_loss": [64, 84, 87], "reduct": [64, 84, 87], "82": [64, 68, 69, 70, 75, 76, 83], "0496": [64, 70], "4422": [64, 70], "83": [64, 68, 69, 70, 71, 75, 83], "8916": 64, "9388": 64, "49": [64, 69, 70, 71, 75, 77, 78, 79, 83], "7630": 64, "99": [64, 69, 70, 71, 75, 76, 83, 85, 88], "6948": 64, "84": [64, 65, 68, 69, 70, 75, 83, 87], "4177": 64, "39": [64, 69, 70, 71, 75, 76, 77, 78, 79, 83, 84, 85], "5379": [64, 79], "85": [64, 65, 68, 69, 70, 71, 75, 83], "5503": 64, "8946": 64, "6596": 64, "interc": 65, "incidence_2_list": [65, 84, 85, 87], "adjacency_1_list": 65, "incidence_1_t_list": 65, "adjacency_1": [65, 85], "incidence_1_t": [65, 82], "proj_0": 65, "proj_1": 65, "proj_2": 65, "elu": 65, "05": [65, 88], "criterion": [65, 70], "x_2_train": [65, 82, 84, 85], "x_2_test": [65, 82, 84], "adjacency_1_train": 65, "adjacency_1_test": 65, "incidence_2_train": [65, 84], "incidence_2_test": [65, 84], "incidence_1_t_train": 65, "incidence_1_t_test": 65, "106": [65, 70, 75, 83], "5665": 65, "4893": [65, 69], "54": [65, 69, 70, 75, 76, 79, 83], "3770": 65, "0177": 65, "6247": 65, "51": [65, 69, 70, 71, 75, 76, 79, 83], "4964": 65, "collect": [66, 67, 68, 69, 71, 72, 73, 74, 87], "let": [66, 67, 72, 73, 88], "v_": [66, 67, 72], "e_": [66, 67], "rule": [66, 67], "put": [66, 67, 83], "f_": [66, 67], "permut": [66, 67], "invari": [66, 67], "parametr": [66, 67], "learnt": [66, 67], "25": [66, 69, 70, 71, 75, 76, 77, 78, 79, 83, 88], "load_ext": [66, 67, 75, 78, 81], "autoreload": [66, 67, 75, 78, 81], "extens": [66, 78], "alreadi": [66, 78], "reload": [66, 78], "reload_ext": [66, 78], "26": [66, 69, 70, 71, 75, 77, 78, 79, 83], "what": [66, 67, 68, 72, 73, 74, 75, 77, 78, 83], "27": [66, 69, 70, 71, 75, 77, 78, 79, 83, 85], "28": [66, 69, 70, 71, 75, 77, 78, 79, 83], "amtric": [66, 67, 68, 72, 73, 74], "unsign": [66, 67, 68, 72, 73, 74], "becom": [66, 67, 68, 72, 73, 74, 81], "simplciial": [66, 67, 68, 72, 73, 74], "wise": [66, 67, 68, 72, 73, 74], "29": [66, 69, 70, 71, 75, 77, 78, 79, 83], "hg_list": [66, 67, 68, 72, 73, 74, 75, 77, 78], "incidence_1_list": [66, 67, 68, 72, 73, 74, 75, 77, 78, 84, 85, 87], "sign": [66, 67, 68, 70, 72, 73, 74, 85, 88], "hg": [66, 67, 68, 72, 73, 74, 75, 77, 78], "to_hypergraph": [66, 67, 68, 70, 72, 73, 74, 75, 77, 78], "30": [66, 69, 70, 71, 75, 76, 77, 78, 79, 80, 81, 83, 84, 85, 87, 88], "1250": [66, 67, 70, 72, 73], "31": [66, 69, 70, 71, 75, 77, 78, 79, 83, 84], "allsetnn": 66, "cidx": 66, "min": 66, "reversed_edge_index": 66, "pooled_x": [66, 67, 68, 72, 73, 74, 75, 78], "33": [66, 69, 70, 71, 75, 77, 78, 79, 83], "34": [66, 69, 70, 71, 75, 77, 78, 79, 80, 81, 83, 84, 85, 87], "incidence_1_train": [66, 67, 72, 73, 74, 75, 77, 78, 84], "incidence_1_test": [66, 67, 72, 73, 74, 75, 77, 78, 84], "35": [66, 69, 70, 71, 75, 76, 77, 78, 79, 83], "to_edge_index": [66, 67, 72, 73], "274": [66, 67, 70, 73, 74, 75], "8233": 66, "529": [66, 67, 70, 72, 73, 74], "0000": [66, 67, 70, 72, 73, 79, 80, 81, 83, 84, 87], "6125": [66, 67, 73], "rise": 67, "so": [67, 79, 80, 81, 83, 84, 87, 88], "dive": 67, "iter": 67, "Their": 67, "dimension": [67, 88], "omega": 67, "overset": [67, 81], "delta": 67, "mathbin": 67, "ba": 67, "2016": 67, "hf_": 67, "multihead": 67, "vaswani": 67, "row": 67, "q_n": 67, "9018": 67, "75": [68, 69, 70, 75, 76, 77, 78, 79, 83, 84], "dhgcnlayer": 68, "76": [68, 69, 70, 75, 79, 83], "77": [68, 69, 70, 75, 79, 83, 86], "78": [68, 69, 70, 75, 79, 80, 81, 83, 84, 87], "dir": 68, "81": [68, 69, 70, 75, 83], "dynam": 68, "86": [68, 69, 70, 71, 75, 83], "87": [68, 69, 70, 75, 83, 86], "88": [68, 69, 70, 75, 83], "8821578": 68, "6946": 68, "36521": 68, "6562": 68, "17477": 68, "7685": 68, "212": [68, 70, 75, 84], "8590": 68, "152": [68, 70, 75, 83], "9723": 68, "0999": 68, "177": [68, 70, 75, 83], "0583": [68, 70], "163": [68, 70, 75, 83], "9508": 68, "202": [68, 70, 75, 84], "9814": 68, "22": [68, 69, 70, 71, 75, 77, 78, 79, 83], "2991": 68, "cora": [69, 76], "2708": 69, "academ": [69, 71], "5429": 69, "citat": [69, 76], "categori": [69, 71], "case_bas": 69, "genetic_algorithm": 69, "neural_network": 69, "probabilistic_method": 69, "reinforcement_learn": 69, "rule_learn": 69, "theori": 69, "1433": [69, 70], "stand": [69, 71], "uniqu": [69, 70, 71, 85], "presenc": [69, 71], "planetoid": 69, "metric": [69, 70, 71], "accuracy_scor": [69, 71], "24": [69, 70, 71, 75, 77, 78, 79, 83], "download": [69, 71, 76, 78, 84], "val": [69, 75, 77, 78, 88], "to_hidden_linear": [69, 71], "to_categories_linear": [69, 71], "manual_se": [69, 88], "41": [69, 70, 71, 75, 77, 78, 79, 83], "256": [69, 70, 75], "train_y_tru": [69, 71], "train_mask": [69, 71], "val_y_tru": 69, "val_mask": 69, "initial_x_1": 69, "zeros_lik": 69, "y_pred_logit": [69, 71], "train_loss": [69, 88], "eval": [69, 71, 75, 76, 77, 78, 88], "val_loss": [69, 88], "val_acc": [69, 88], "acc": [69, 71], "2f": [69, 71], "1079": [69, 70], "14": [69, 70, 71, 72, 75, 76, 77, 78, 79, 82, 83, 84, 87, 88], "1436": [69, 70], "0234": 69, "15": [69, 70, 71, 74, 75, 76, 77, 78, 79, 82, 83, 84, 87, 88], "1016": [69, 70], "9800": 69, "0681": [69, 70], "9504": 69, "18": [69, 70, 71, 75, 76, 77, 78, 79, 82, 83, 87], "0389": [69, 70], "9194": 69, "0137": 69, "9241": 69, "19": [69, 70, 71, 75, 77, 78, 79, 82, 83, 85, 88], "9917": 69, "8917": 69, "9729": 69, "8710": 69, "23": [69, 70, 71, 75, 77, 78, 79, 83], "9556": 69, "8574": 69, "9402": 69, "8646": 69, "9265": 69, "8540": 69, "9136": 69, "8430": 69, "9012": 69, "8336": 69, "8886": 69, "8405": 69, "8775": 69, "8264": 69, "8668": 69, "8065": 69, "8562": 69, "37": [69, 70, 71, 75, 76, 77, 78, 79, 83], "8158": 69, "8456": 69, "7957": 69, "44": [69, 70, 71, 75, 77, 78, 79, 83, 87], "8346": 69, "8028": 69, "8249": 69, "20": [69, 70, 71, 75, 76, 77, 78, 79, 82, 83, 88], "7882": 69, "8156": 69, "42": [69, 70, 71, 75, 77, 78, 79, 83, 84], "7912": 69, "8070": 69, "7610": 69, "46": [69, 70, 71, 75, 77, 78, 79, 83], "7987": 69, "7617": 69, "47": [69, 70, 71, 75, 77, 78, 79, 83, 87], "7905": 69, "7596": 69, "7830": 69, "7391": 69, "7740": 69, "7315": 69, "7655": 69, "7365": 69, "7565": 69, "43": [69, 70, 71, 75, 77, 78, 79, 83, 85], "7184": 69, "48": [69, 70, 71, 75, 77, 78, 79, 83], "7459": 69, "7085": 69, "7367": 69, "45": [69, 70, 71, 75, 76, 77, 78, 79, 80, 81, 83, 84, 85, 87], "6815": [69, 70], "7279": 69, "6673": 69, "50": [69, 70, 71, 75, 76, 77, 78, 79, 83], "7178": 69, "6846": 69, "7077": 69, "6483": 69, "7000": [69, 83], "6436": 69, "6971": 69, "6353": 69, "6991": 69, "6336": 69, "6982": 69, "5938": 69, "60": [69, 70, 75, 79, 83, 88], "6980": 69, "5886": 69, "56": [69, 70, 75, 79, 83], "6979": 69, "5974": 69, "55": [69, 70, 75, 79, 83], "6881": [69, 70], "5600": 69, "52": [69, 70, 75, 76, 79, 83], "6694": 69, "5445": 69, "6513": 69, "5501": 69, "6308": 69, "5397": [69, 79], "6141": 69, "5096": 69, "6020": 69, "4992": 69, "5915": [69, 70], "5020": 69, "58": [69, 70, 75, 79, 83], "5829": 69, "4710": 69, "5747": 69, "4608": 69, "67": [69, 70, 71, 75, 79, 83, 84], "5703": 69, "4341": 69, "62": [69, 70, 75, 79, 83, 84], "5632": 69, "4428": 69, "66": [69, 70, 71, 75, 79, 83], "5630": 69, "4209": 69, "5502": 69, "4151": 69, "63": [69, 70, 75, 83], "5303": 69, "53": [69, 70, 75, 76, 79, 83], "4090": 69, "5051": 69, "4021": 69, "3847": 69, "65": [69, 70, 75, 79, 83], "4842": 69, "3907": 69, "61": [69, 70, 75, 79, 83], "4849": 69, "57": [69, 70, 71, 75, 79, 83], "3434": 69, "70": [69, 70, 75, 79, 83], "4866": 69, "3253": 69, "69": [69, 70, 75, 79, 83], "4864": 69, "3380": 69, "4896": 69, "2933": 69, "4921": 69, "3124": 69, "68": [69, 70, 75, 79, 83], "4948": 69, "3091": 69, "4931": 69, "2768": 69, "4881": 69, "2749": 69, "4827": [69, 70], "2740": 69, "4833": 69, "2773": 69, "4744": 69, "2430": 69, "4646": 69, "4648": 69, "1958": [69, 70], "4734": 69, "1895": [69, 70], "71": [69, 70, 75, 79, 83], "4748": 69, "2008": 69, "4760": 69, "72": [69, 70, 75, 79, 83], "1573": [69, 70], "74": [69, 70, 75, 79, 83, 84], "4385": 69, "73": [69, 70, 75, 79, 83, 84], "1751": [69, 70], "4242": 69, "1889": [69, 70], "4183": 69, "1762": [69, 70], "4250": 69, "1737": [69, 70], "4471": 69, "1242": [69, 70], "4559": 69, "0648": [69, 70], "4498": 69, "0717": [69, 70, 87], "4506": 69, "0568": [69, 70, 71], "4410": 69, "0650": [69, 70, 79], "4375": 69, "0475": [69, 70], "4441": 69, "0293": [69, 83], "4450": 69, "0494": [69, 70], "4622": 69, "0200": 69, "4609": 69, "0358": [69, 70], "4645": 69, "9877": 69, "0173": 69, "4788": 69, "89": [69, 70, 71, 75, 83], "9744": 69, "4970": 69, "90": [69, 70, 75, 83], "9497": 69, "4981": 69, "91": [69, 70, 71, 75, 83], "9345": 69, "4736": 69, "92": [69, 70, 75, 83], "9636": 69, "4339": 69, "93": [69, 70, 71, 75, 76, 83, 85], "9197": 69, "4162": 69, "94": [69, 70, 71, 75, 83], "8984": 69, "3813": 69, "95": [69, 70, 71, 75, 83], "8895": 69, "3547": 69, "9048": 69, "3499": 69, "97": [69, 70, 71, 75, 83, 85, 86], "8737": 69, "3502": [69, 71], "98": [69, 70, 71, 75, 83], "9479": 69, "3518": 69, "8906": 69, "3639": 69, "8589": 69, "3789": 69, "against": [69, 71], "test_y_tru": [69, 71], "test_mask": [69, 71], "2982": 69, "karateclub": [70, 79, 80, 81, 83, 84], "matplotlib": [70, 71, 88], "pyplot": [70, 71, 88], "plt": [70, 71, 88], "www": [70, 78, 79, 80, 81, 83, 84], "jstor": [70, 79, 80, 81, 83, 84], "stabl": [70, 71, 79, 80, 81, 83, 84], "3629752": [70, 79, 80, 81, 83, 84], "singular": [70, 79, 80, 81, 83, 84], "social": [70, 79, 80, 81, 83, 84], "group": [70, 79, 80, 81, 83, 84], "dataset_sim": 70, "karate_club": [70, 79, 80, 81, 83, 84, 87], "complex_typ": [70, 79, 80, 81, 83, 84, 87], "dataset_hyp": 70, "santii": [70, 79, 80], "classifi": [70, 71, 76], "channels_": 70, "get_simplex_attribut": [70, 79, 80, 81, 83, 84, 87], "y_1h": 70, "ey": [70, 76, 79, 83], "astyp": 70, "stratifi": 70, "ind_train": 70, "ind_test": 70, "arang": 70, "random_st": 70, "float32": [70, 74, 76], "int32": 70, "hnhnnetwork": 70, "2000": [70, 84], "get_accuraci": 70, "lambda": 70, "yhat": 70, "ytrue": 70, "full": 70, "y_hat_cl": 70, "nloss": 70, "ntrain_acc": 70, "7157": 70, "5000": [70, 83, 84], "7135": 70, "7113": 70, "7093": 70, "7074": 70, "7056": 70, "7039": [70, 80], "7024": 70, "7010": 70, "6997": 70, "6985": 70, "6975": 70, "6966": 70, "6958": 70, "6951": 70, "6945": 70, "6940": 70, "6936": 70, "6933": 70, "6931": 70, "6930": 70, "6929": 70, "6928": 70, "6932": 70, "6927": 70, "6926": 70, "6925": 70, "6924": 70, "6429": 70, "6923": 70, "6922": 70, "6921": 70, "6920": 70, "6919": 70, "8571": 70, "9643": 70, "6918": [70, 81], "7500": [70, 79, 83, 84], "6917": 70, "5714": 70, "5357": 70, "6916": 70, "6915": 70, "6914": 70, "6913": 70, "6912": 70, "6911": 70, "6910": 70, "6909": 70, "6908": 70, "6667": [70, 83], "101": [70, 75, 83], "6907": 70, "102": [70, 75, 83], "103": [70, 75, 83, 85], "6906": 70, "104": [70, 75, 83, 86], "105": [70, 75, 83, 85, 87], "6905": 70, "107": [70, 75, 83], "6904": 70, "108": [70, 75, 76, 83], "109": [70, 75, 76, 83], "6903": 70, "110": [70, 75, 76, 83], "6902": 70, "111": [70, 75, 76, 83], "112": [70, 75, 83], "6901": 70, "113": [70, 75, 83, 85], "114": [70, 75, 83], "6900": 70, "115": [70, 75, 83], "6899": 70, "116": [70, 75, 83], "117": [70, 75, 83], "6898": 70, "118": [70, 75, 83], "6897": 70, "119": [70, 75, 83], "120": [70, 75, 83], "6896": 70, "121": [70, 75, 76, 83], "6895": 70, "122": [70, 75, 83], "6894": 70, "123": [70, 75, 83], "124": [70, 75, 83], "6893": 70, "125": [70, 75, 83], "6892": 70, "126": [70, 75, 83], "6891": [70, 83], "127": [70, 75, 83, 87], "6890": 70, "129": [70, 75, 83], "6889": 70, "130": [70, 75, 83], "6888": 70, "131": [70, 75, 83], "6887": 70, "132": [70, 75, 83], "6886": 70, "133": [70, 75, 76, 83], "134": [70, 75, 83], "6885": 70, "135": [70, 75, 83], "6884": 70, "136": [70, 75, 83], "6883": 70, "137": [70, 75, 83], "6882": 70, "138": [70, 75, 83], "139": [70, 75, 83, 87], "6880": 70, "140": [70, 75, 76, 83], "6879": [70, 79], "141": [70, 75, 83], "6878": 70, "142": [70, 75, 83], "6877": 70, "143": [70, 75, 79, 83, 84], "6876": 70, "144": [70, 75, 83], "6875": [70, 75], "145": [70, 75, 83], "6874": 70, "146": [70, 75, 83], "6873": 70, "147": [70, 75, 83, 86], "6871": 70, "148": [70, 75, 83], "6870": 70, "149": [70, 75, 83], "6869": 70, "150": [70, 75, 83, 88], "6868": 70, "151": [70, 75, 83], "6867": 70, "6865": 70, "153": [70, 75, 83], "6864": 70, "154": [70, 75, 83], "6863": 70, "155": [70, 75, 83], "6861": 70, "156": [70, 75, 83], "6860": 70, "157": [70, 75, 83], "6859": 70, "158": [70, 75, 83], "6857": 70, "159": [70, 75, 83], "6856": 70, "160": [70, 75, 83], "6855": 70, "161": [70, 75, 83], "6853": 70, "162": [70, 75, 83], "6852": 70, "6850": 70, "164": [70, 75, 83], "6849": 70, "165": [70, 75, 83], "6847": 70, "166": [70, 75, 83], "6845": 70, "167": [70, 75, 83], "6844": 70, "168": [70, 75, 83], "6842": 70, "169": [70, 75, 83], "6840": 70, "170": [70, 75, 83], "6839": 70, "171": [70, 75, 83], "6837": 70, "172": [70, 75, 83], "6835": 70, "173": [70, 75, 83], "6833": 70, "174": [70, 75, 83], "6831": 70, "175": [70, 75, 83], "6829": 70, "176": [70, 75, 83, 87], "6827": 70, "6825": 70, "178": [70, 75, 83], "6823": 70, "179": [70, 75, 83], "6821": 70, "180": [70, 75, 83], "6819": 70, "181": [70, 75, 83], "6817": 70, "182": [70, 75, 83], "183": [70, 75, 83], "6813": 70, "184": [70, 75, 83], "6810": 70, "185": [70, 75, 76, 83], "6808": 70, "186": [70, 75, 83], "6806": 70, "187": [70, 75, 83], "6803": 70, "6801": 70, "189": [70, 75, 83], "6799": 70, "190": [70, 75, 83], "6796": 70, "191": [70, 75, 83], "6793": 70, "192": [70, 75, 83], "6791": 70, "193": [70, 75, 83], "6788": 70, "194": [70, 75, 83], "6785": 70, "195": [70, 75, 83], "6783": 70, "196": [70, 75, 83], "6780": 70, "197": [70, 75, 83], "6777": 70, "6774": 70, "199": [70, 75, 76, 83], "6771": 70, "200": [70, 71, 75, 76, 83], "6768": 70, "201": [70, 75], "6765": 70, "6762": 70, "203": [70, 75], "6758": 70, "204": [70, 75], "6755": 70, "205": [70, 75], "6752": 70, "206": [70, 75], "6748": 70, "207": [70, 75], "6745": 70, "208": [70, 75], "6741": 70, "209": [70, 75], "6738": 70, "210": [70, 75], "6734": 70, "211": [70, 75], "6730": 70, "6726": 70, "213": [70, 75], "6723": 70, "214": [70, 75], "6719": 70, "215": [70, 75], "6715": 70, "216": [70, 75], "6710": 70, "217": [70, 75, 84], "6706": 70, "218": [70, 75], "6702": 70, "6786": 70, "219": [70, 75], "6698": 70, "220": [70, 75], "6693": 70, "221": [70, 75], "6689": 70, "222": [70, 75], "6684": 70, "223": [70, 75], "6679": 70, "224": [70, 75], "6675": 70, "225": [70, 75], "6670": 70, "226": [70, 75], "6665": 70, "227": [70, 75, 86], "6660": 70, "228": [70, 75], "6655": 70, "229": [70, 75], "6649": 70, "230": [70, 75], "6644": 70, "231": [70, 75], "6639": 70, "232": [70, 75, 86], "6633": 70, "233": [70, 75], "6628": 70, "234": [70, 75], "6622": 70, "235": [70, 75], "6616": 70, "236": [70, 75], "6610": 70, "237": [70, 75], "6604": 70, "238": [70, 75], "6598": 70, "239": [70, 75], "6592": 70, "240": [70, 75], "6585": 70, "7143": 70, "241": [70, 75], "6579": 70, "242": [70, 75], "6572": 70, "243": [70, 75], "6566": 70, "244": [70, 75], "6559": 70, "245": [70, 75], "6552": 70, "246": [70, 75], "6545": 70, "247": [70, 75], "6538": 70, "248": [70, 75], "6531": 70, "249": [70, 75], "6523": 70, "250": [70, 75], "6516": 70, "8214": 70, "251": [70, 75], "6508": 70, "6500": 70, "253": [70, 75], "6492": 70, "254": [70, 75], "6484": 70, "255": [70, 75], "6476": 70, "6468": 70, "257": [70, 75], "6459": 70, "258": [70, 75], "6451": 70, "259": [70, 75], "6442": 70, "260": [70, 75], "6433": 70, "261": [70, 75], "6424": 70, "262": [70, 75], "6415": 70, "263": [70, 75], "6406": 70, "264": [70, 75], "6397": 70, "265": [70, 75], "6387": 70, "266": [70, 75], "6378": 70, "267": [70, 75], "6368": 70, "268": [70, 75], "6358": 70, "269": [70, 75], "6348": 70, "270": [70, 75, 79], "6337": 70, "271": [70, 75], "6327": 70, "272": [70, 75], "6317": 70, "273": [70, 75], "6306": 70, "6295": 70, "275": [70, 73, 75], "6284": [70, 83], "276": [70, 75], "6273": 70, "277": [70, 75], "6262": 70, "278": [70, 72, 75], "6250": 70, "279": [70, 75], "6239": 70, "280": [70, 75], "6227": 70, "281": [70, 72, 75], "6215": 70, "282": [70, 72, 75, 86], "6203": 70, "283": [70, 75], "6191": 70, "284": [70, 75], "6178": 70, "285": [70, 75], "6166": 70, "286": [70, 75], "6153": 70, "287": [70, 75], "6140": 70, "288": [70, 75], "6127": 70, "289": [70, 75], "6114": 70, "290": [70, 75], "6101": 70, "291": [70, 75], "6088": 70, "292": [70, 75], "6074": 70, "293": [70, 75], "6060": 70, "294": [70, 75], "6046": [70, 71], "295": [70, 75], "6032": 70, "296": [70, 75], "6018": 70, "297": [70, 75], "6004": 70, "298": [70, 75], "5989": 70, "299": [70, 75], "5975": 70, "300": [70, 75], "5960": 70, "301": [70, 75], "5945": 70, "302": [70, 75, 76], "5930": 70, "303": [70, 75], "304": [70, 75, 86], "5900": 70, "305": [70, 75], "5884": 70, "306": [70, 75], "5868": 70, "307": [70, 75], "5853": 70, "308": [70, 75], "5837": 70, "309": [70, 75], "5821": 70, "310": [70, 75], "5805": [70, 79], "311": [70, 75], "5788": 70, "312": [70, 75], "5772": 70, "313": [70, 75], "314": [70, 75], "5739": 70, "315": [70, 75], "5722": 70, "316": [70, 75], "5705": 70, "317": [70, 75], "5688": 70, "318": [70, 75], "5671": 70, "319": [70, 75], "5653": 70, "320": [70, 75], "5636": 70, "321": [70, 75], "5618": 70, "322": [70, 75], "5601": 70, "323": [70, 75], "5583": 70, "324": [70, 75], "325": [70, 75], "5547": 70, "326": [70, 75], "5529": 70, "327": [70, 75], "5511": 70, "328": [70, 75], "5492": 70, "329": [70, 75], "5474": 70, "330": [70, 75], "5455": 70, "331": [70, 75], "5437": 70, "332": [70, 75], "5418": 70, "333": [70, 75], "5399": 70, "334": [70, 75], "5381": 70, "335": [70, 75], "5362": [70, 79], "336": [70, 75], "5343": 70, "337": [70, 75], "5323": 70, "338": [70, 75], "5304": 70, "339": [70, 75], "5285": 70, "340": [70, 75], "5266": 70, "8929": 70, "341": [70, 75], "5246": [70, 79], "342": [70, 75], "5227": 70, "343": [70, 75], "5207": 70, "344": [70, 75], "5188": [70, 79], "345": [70, 75], "5168": 70, "346": [70, 75], "5148": 70, "347": [70, 75], "5129": 70, "348": [70, 75], "5109": 70, "349": [70, 75], "5089": 70, "350": [70, 75], "5069": 70, "9286": 70, "351": [70, 75], "5049": 70, "352": [70, 75], "5029": 70, "353": [70, 75], "5009": 70, "354": [70, 75], "4989": 70, "355": [70, 75], "4969": 70, "356": [70, 75], "4949": 70, "357": [70, 75], "4928": 70, "358": [70, 75], "4908": 70, "359": [70, 75], "4888": 70, "360": [70, 75], "4868": 70, "361": [70, 75], "4847": 70, "362": [70, 75], "363": [70, 75], "4807": 70, "364": [70, 75], "4787": 70, "365": [70, 75], "4766": 70, "366": [70, 75], "4746": 70, "367": [70, 75], "4726": 70, "368": [70, 75], "4705": 70, "369": [70, 75], "4685": 70, "370": [70, 75], "4665": 70, "371": [70, 75], "4644": 70, "372": [70, 75], "4624": 70, "373": [70, 75], "4604": 70, "374": [70, 75], "4583": 70, "375": [70, 75], "4563": 70, "376": [70, 75], "4543": 70, "377": [70, 75], "4523": 70, "378": [70, 75], "4503": 70, "379": [70, 75], "4482": 70, "380": [70, 75], "4462": 70, "381": [70, 75], "4442": 70, "382": [70, 75], "383": [70, 75], "4402": 70, "384": [70, 75], "4382": 70, "385": [70, 75], "4362": 70, "386": [70, 75], "4342": 70, "387": [70, 75], "4322": 70, "388": [70, 75], "4302": 70, "389": [70, 75], "4282": 70, "390": [70, 75], "4262": 70, "391": [70, 75], "4243": 70, "392": [70, 75], "4223": 70, "393": [70, 75], "4203": 70, "394": [70, 75], "4184": 70, "395": [70, 75, 76], "4164": 70, "396": [70, 75], "4145": 70, "397": [70, 75], "4125": 70, "398": [70, 75], "4106": 70, "4086": 70, "400": [70, 71, 75], "4067": 70, "401": [70, 75], "4048": 70, "402": [70, 75], "4029": 70, "403": [70, 75], "4010": 70, "404": [70, 75], "3991": 70, "405": [70, 75], "3972": 70, "406": [70, 75, 84], "3953": 70, "407": [70, 75], "3934": 70, "408": [70, 75], "3915": 70, "409": [70, 75], "3897": 70, "410": [70, 75], "3878": 70, "411": [70, 75], "3859": 70, "412": [70, 75], "3841": 70, "413": [70, 75], "3823": 70, "3804": 70, "415": [70, 75], "3786": 70, "416": [70, 75], "3768": 70, "417": [70, 75], "3750": [70, 72], "418": [70, 75], "3732": 70, "419": [70, 75], "3714": 70, "420": [70, 75], "3696": 70, "421": [70, 75], "3678": 70, "422": [70, 75], "3661": 70, "423": [70, 75], "3643": 70, "424": [70, 75], "3626": 70, "425": [70, 75], "3608": 70, "426": [70, 75], "3591": 70, "427": [70, 75], "3573": 70, "428": [70, 75], "3556": 70, "429": [70, 75], "3539": 70, "430": [70, 75], "3522": 70, "431": [70, 75], "3505": 70, "432": [70, 75], "3488": 70, "433": [70, 75], "3472": 70, "434": [70, 75], "3455": 70, "435": [70, 75], "3438": 70, "436": [70, 75], "3422": 70, "437": [70, 75], "3406": 70, "438": [70, 75], "3389": 70, "439": [70, 75], "3373": 70, "440": [70, 75], "3357": 70, "441": [70, 75], "3341": 70, "442": [70, 75], "3325": 70, "443": [70, 75, 76], "3309": 70, "444": [70, 75], "3293": 70, "445": [70, 75], "3278": 70, "446": [70, 75], "3262": 70, "447": [70, 75], "3247": 70, "448": [70, 75], "3231": 70, "449": [70, 75], "3216": 70, "450": [70, 75], "3201": 70, "451": [70, 75], "3185": 70, "452": [70, 75], "3170": 70, "453": [70, 75], "3155": 70, "454": [70, 75], "3140": 70, "455": [70, 75], "3126": 70, "456": [70, 75], "3111": 70, "457": [70, 75], "3096": 70, "458": [70, 75], "3082": 70, "459": [70, 75], "3067": 70, "460": [70, 75], "3053": 70, "461": [70, 75], "3039": 70, "462": [70, 75], "3025": 70, "463": [70, 75], "3011": 70, "464": [70, 75], "2997": 70, "465": [70, 75], "2983": 70, "466": [70, 75], "2969": 70, "467": [70, 75], "2955": 70, "468": [70, 75], "2941": 70, "469": [70, 75], "2928": 70, "470": [70, 75], "2914": [70, 71], "471": [70, 75], "2901": 70, "472": [70, 75], "2888": 70, "473": [70, 75], "2874": 70, "474": [70, 75], "2861": 70, "475": [70, 75], "2848": 70, "476": [70, 75], "2835": 70, "477": [70, 75], "2822": 70, "478": [70, 75], "2810": 70, "479": [70, 75], "2797": 70, "480": [70, 75], "2784": 70, "481": [70, 75], "2772": 70, "482": [70, 75], "2759": 70, "483": [70, 75], "2747": 70, "484": [70, 75], "2735": 70, "485": [70, 75], "2722": 70, "486": [70, 75], "2710": 70, "487": [70, 75], "2698": 70, "488": [70, 75], "2686": 70, "489": [70, 75], "2674": 70, "490": [70, 75], "2662": 70, "491": [70, 75], "2651": 70, "492": [70, 75], "2639": 70, "493": [70, 75], "2627": 70, "494": [70, 75], "2616": 70, "495": [70, 75], "2604": 70, "496": [70, 75], "2593": 70, "497": [70, 75], "2582": 70, "498": [70, 75], "2571": 70, "499": [70, 75], "2559": 70, "2548": 70, "501": 70, "2537": 70, "502": 70, "2526": 70, "503": 70, "2516": 70, "504": 70, "2505": 70, "505": 70, "2494": 70, "506": 70, "2484": 70, "507": 70, "2473": 70, "508": 70, "2463": 70, "509": 70, "2452": [70, 71], "510": 70, "2442": 70, "511": 70, "2432": 70, "512": 70, "2421": 70, "513": 70, "2411": 70, "514": [70, 86], "2401": 70, "515": 70, "2391": 70, "516": 70, "2381": 70, "517": 70, "2371": 70, "518": 70, "2362": 70, "519": 70, "2352": 70, "520": 70, "2342": 70, "521": 70, "2333": [70, 84], "522": 70, "2323": 70, "523": 70, "2314": 70, "524": 70, "525": 70, "2295": 70, "526": 70, "2286": 70, "527": 70, "2276": 70, "528": 70, "2267": 70, "2258": 70, "530": 70, "2249": 70, "531": 70, "2240": 70, "532": 70, "2231": [70, 71], "533": 70, "2223": 70, "534": 70, "2214": 70, "535": 70, "2205": 70, "2196": 70, "537": 70, "2188": 70, "538": 70, "2179": 70, "539": 70, "2171": 70, "540": 70, "2162": 70, "541": 70, "2154": 70, "542": 70, "2146": 70, "543": 70, "2137": 70, "544": 70, "2129": 70, "545": 70, "2121": 70, "546": [70, 85], "2113": 70, "547": 70, "548": 70, "2097": 70, "549": 70, "2089": 70, "550": 70, "2081": 70, "551": 70, "2073": 70, "552": 70, "2066": 70, "553": 70, "2058": 70, "554": 70, "2050": 70, "555": 70, "2043": 70, "556": 70, "2035": 70, "557": 70, "2028": 70, "558": 70, "559": 70, "2013": 70, "560": 70, "2005": 70, "561": 70, "1998": 70, "562": 70, "1991": 70, "563": 70, "1983": 70, "564": 70, "1976": 70, "565": 70, "1969": 70, "566": 70, "1962": [70, 79], "567": 70, "1955": 70, "568": 70, "1948": 70, "569": 70, "1941": 70, "570": 70, "1934": 70, "571": 70, "1927": 70, "572": 70, "1921": 70, "573": 70, "1914": 70, "574": 70, "1907": 70, "575": 70, "1900": 70, "576": 70, "1894": 70, "577": 70, "1887": 70, "578": 70, "1881": 70, "579": 70, "1874": 70, "580": 70, "1868": 70, "581": 70, "1861": 70, "582": 70, "1855": 70, "1849": 70, "584": 70, "1842": 70, "585": 70, "1836": [70, 79], "586": 70, "1830": 70, "587": 70, "1824": 70, "588": 70, "1818": 70, "589": 70, "1811": 70, "590": 70, "1805": 70, "591": 70, "1799": 70, "1793": 70, "593": 70, "1788": 70, "594": 70, "1782": 70, "595": 70, "1776": 70, "596": 70, "1770": 70, "597": 70, "1764": 70, "598": 70, "1758": 70, "599": 70, "1753": 70, "600": 70, "1747": 70, "601": 70, "1741": 70, "602": 70, "1736": 70, "603": 70, "1730": [70, 79], "604": 70, "1725": [70, 79], "605": 70, "1719": 70, "606": 70, "1714": 70, "607": 70, "1708": 70, "608": 70, "1703": 70, "609": 70, "1697": 70, "610": 70, "1692": 70, "611": 70, "1687": 70, "612": 70, "1682": 70, "613": [70, 88], "1676": 70, "614": 70, "1671": 70, "615": 70, "1666": 70, "616": 70, "1661": 70, "617": 70, "1656": 70, "618": 70, "1651": 70, "619": 70, "1646": 70, "620": 70, "1640": 70, "621": 70, "1636": 70, "622": 70, "1631": 70, "623": 70, "1626": 70, "624": 70, "1621": 70, "625": [70, 75], "1616": 70, "626": 70, "1611": 70, "627": 70, "1606": 70, "628": 70, "1601": 70, "629": 70, "1597": 70, "630": 70, "1592": 70, "631": 70, "1587": 70, "632": 70, "1583": 70, "633": 70, "1578": 70, "634": 70, "635": 70, "1569": 70, "636": 70, "1564": 70, "637": 70, "1560": [70, 79], "638": 70, "1555": 70, "639": 70, "1551": 70, "640": 70, "1546": 70, "641": 70, "1542": 70, "642": 70, "1537": 70, "643": 70, "1533": 70, "644": 70, "1529": 70, "645": 70, "1524": 70, "646": 70, "1520": 70, "647": 70, "1516": [70, 83], "648": 70, "1511": [70, 79], "649": 70, "1507": 70, "650": 70, "1503": 70, "651": 70, "1499": 70, "652": 70, "1495": [70, 79], "653": 70, "1491": 70, "654": 70, "1487": [70, 79], "655": 70, "1482": 70, "656": 70, "1478": 70, "657": 70, "1474": [70, 79], "658": 70, "1470": 70, "659": 70, "1466": 70, "660": 70, "1462": 70, "661": 70, "1458": 70, "662": 70, "1454": 70, "663": 70, "1451": 70, "664": 70, "1447": 70, "665": 70, "1443": 70, "666": 70, "1439": 70, "667": 70, "1435": 70, "668": 70, "1431": 70, "669": 70, "1428": 70, "670": 70, "1424": 70, "671": 70, "1420": 70, "672": 70, "1416": 70, "673": 70, "1413": 70, "674": 70, "1409": 70, "675": 70, "1405": 70, "676": 70, "1402": 70, "677": 70, "1398": 70, "678": 70, "1395": 70, "679": 70, "1391": 70, "680": 70, "1387": 70, "681": 70, "1384": 70, "682": 70, "1380": [70, 79], "683": 70, "1377": 70, "684": 70, "1373": 70, "685": 70, "1370": 70, "686": 70, "1367": 70, "687": 70, "1363": 70, "688": 70, "1360": 70, "689": 70, "1356": [70, 79], "690": 70, "1353": 70, "691": 70, "1350": 70, "692": 70, "1346": 70, "693": 70, "1343": [70, 79], "694": 70, "1340": 70, "695": 70, "1336": 70, "696": 70, "1333": [70, 84], "697": 70, "1330": 70, "698": 70, "1327": 70, "699": 70, "1323": 70, "700": 70, "1320": 70, "701": 70, "1317": 70, "702": 70, "1314": 70, "703": 70, "1311": 70, "704": 70, "1308": 70, "705": 70, "1304": 70, "706": 70, "1301": 70, "707": 70, "1298": 70, "708": 70, "1295": 70, "709": 70, "1292": 70, "710": 70, "1289": 70, "711": 70, "1286": 70, "712": 70, "1283": [70, 83], "713": 70, "1280": 70, "714": 70, "1277": 70, "715": 70, "1274": 70, "716": 70, "1271": 70, "717": 70, "1268": 70, "718": 70, "1265": 70, "719": 70, "1263": 70, "720": 70, "1260": 70, "721": 70, "1257": [70, 83], "722": 70, "1254": 70, "723": 70, "1251": 70, "724": 70, "1248": 70, "725": 70, "1245": 70, "726": 70, "1243": 70, "727": 70, "1240": 70, "728": 70, "1237": 70, "729": 70, "1234": 70, "730": 70, "1232": 70, "731": 70, "1229": 70, "732": 70, "1226": 70, "733": 70, "1223": 70, "734": 70, "1221": 70, "735": 70, "1218": 70, "736": 70, "1215": 70, "737": 70, "1213": 70, "738": 70, "1210": 70, "739": 70, "1207": 70, "740": 70, "1205": 70, "741": 70, "1202": 70, "742": 70, "1200": [70, 88], "743": 70, "1197": 70, "744": 70, "1194": 70, "745": 70, "1192": 70, "746": 70, "1189": 70, "747": 70, "1187": 70, "748": 70, "1184": 70, "749": 70, "1182": 70, "1179": [70, 79], "751": 70, "1177": 70, "752": 70, "1174": 70, "753": 70, "1172": 70, "754": 70, "1169": 70, "755": 70, "1167": 70, "756": 70, "1165": 70, "757": 70, "1162": 70, "758": 70, "1160": 70, "759": 70, "1157": 70, "760": 70, "1155": 70, "761": 70, "1153": 70, "762": 70, "1150": 70, "763": 70, "1148": 70, "764": 70, "1146": 70, "765": 70, "1143": 70, "766": 70, "1141": 70, "767": 70, "1139": [70, 71], "768": 70, "1136": [70, 83], "769": 70, "1134": 70, "770": 70, "1132": 70, "771": 70, "1130": 70, "772": 70, "1127": 70, "773": 70, "1125": 70, "774": 70, "1123": 70, "775": 70, "1121": 70, "776": 70, "1118": 70, "777": 70, "1116": 70, "778": 70, "1114": 70, "779": 70, "1112": 70, "780": 70, "1110": 70, "781": 70, "1107": [70, 71], "782": 70, "1105": 70, "783": 70, "1103": 70, "784": 70, "1101": [70, 83], "785": 70, "1099": 70, "786": 70, "1097": 70, "787": 70, "1095": 70, "788": 70, "1093": 70, "789": 70, "1090": 70, "790": 70, "1088": 70, "791": 70, "1086": 70, "792": 70, "1084": 70, "793": 70, "1082": 70, "794": 70, "1080": [70, 79], "795": [70, 71], "1078": 70, "796": 70, "1076": 70, "797": 70, "1074": 70, "798": 70, "1072": 70, "799": 70, "1070": 70, "800": 70, "1068": 70, "801": 70, "1066": 70, "802": 70, "1064": 70, "803": 70, "1062": 70, "804": 70, "1060": 70, "805": [70, 71], "1058": 70, "806": 70, "1056": 70, "807": 70, "1054": 70, "808": 70, "1052": 70, "809": 70, "1050": 70, "810": 70, "1049": 70, "811": 70, "1047": 70, "812": 70, "1045": 70, "813": 70, "1043": 70, "814": 70, "1041": 70, "815": 70, "1039": 70, "816": 70, "1037": 70, "817": 70, "1035": 70, "818": 70, "1033": 70, "819": 70, "1032": 70, "820": 70, "1030": 70, "821": 70, "1028": 70, "822": 70, "1026": 70, "823": 70, "1024": 70, "824": 70, "1023": [70, 79], "825": 70, "1021": 70, "826": 70, "1019": [70, 79], "827": 70, "1017": 70, "828": 70, "1015": 70, "829": 70, "1014": 70, "830": 70, "1012": [70, 79], "831": 70, "1010": 70, "832": 70, "1008": 70, "833": 70, "1007": 70, "834": 70, "1005": 70, "835": 70, "1003": 70, "836": 70, "1001": 70, "837": 70, "838": 70, "0998": [70, 79], "839": 70, "0996": [70, 79], "840": 70, "0995": 70, "841": 70, "0993": 70, "842": 70, "0991": 70, "843": 70, "0990": 70, "844": 70, "0988": 70, "845": 70, "0986": 70, "846": 70, "0985": 70, "847": 70, "0983": 70, "848": 70, "0981": 70, "849": 70, "0980": 70, "850": 70, "0978": 70, "851": 70, "0976": 70, "852": 70, "0975": 70, "853": 70, "0973": 70, "854": 70, "0972": 70, "855": 70, "0970": 70, "856": 70, "0968": [70, 79], "857": 70, "0967": 70, "858": 70, "0965": 70, "859": 70, "0964": 70, "860": 70, "0962": 70, "861": 70, "0961": 70, "862": 70, "0959": [70, 79], "863": 70, "0957": 70, "864": 70, "0956": 70, "865": 70, "0954": [70, 79], "866": 70, "0953": 70, "867": 70, "0951": 70, "868": 70, "0950": 70, "869": 70, "0948": [70, 79], "870": 70, "0947": 70, "871": 70, "0945": 70, "872": 70, "0944": 70, "873": 70, "0942": 70, "874": 70, "0941": 70, "875": 70, "0939": 70, "876": 70, "0938": 70, "877": 70, "0936": 70, "878": 70, "0935": 70, "879": 70, "0933": 70, "880": 70, "0932": 70, "881": 70, "0930": [70, 79], "882": 70, "0929": 70, "883": 70, "0928": 70, "884": 70, "0926": 70, "885": 70, "0925": 70, "886": 70, "0923": [70, 79], "887": 70, "0922": 70, "888": 70, "0920": [70, 79], "889": 70, "0919": 70, "890": 70, "0918": 70, "891": 70, "0916": 70, "892": 70, "0915": 70, "893": 70, "0913": 70, "894": 70, "0912": 70, "895": 70, "0911": 70, "896": 70, "0909": 70, "897": 70, "0908": 70, "898": 70, "0907": 70, "899": 70, "0905": 70, "900": 70, "0904": 70, "901": 70, "0902": 70, "902": 70, "0901": 70, "903": 70, "0900": 70, "904": 70, "0898": 70, "905": 70, "0897": [70, 79], "906": 70, "0896": 70, "907": 70, "0894": 70, "908": 70, "0893": 70, "909": 70, "0892": [70, 79], "910": 70, "0891": [70, 79], "911": 70, "0889": 70, "912": 70, "0888": 70, "913": 70, "0887": 70, "914": 70, "0885": 70, "915": 70, "0884": 70, "916": 70, "0883": 70, "917": 70, "0881": 70, "918": 70, "0880": 70, "919": 70, "0879": 70, "920": 70, "0878": 70, "921": 70, "0876": 70, "922": 70, "0875": 70, "923": 70, "0874": 70, "924": 70, "0873": [70, 83], "925": 70, "0871": [70, 86], "926": 70, "0870": 70, "927": 70, "0869": 70, "928": 70, "0868": 70, "929": 70, "0866": 70, "930": 70, "0865": 70, "931": 70, "0864": [70, 79], "932": 70, "0863": 70, "933": 70, "0862": 70, "934": 70, "0860": 70, "935": 70, "0859": 70, "936": 70, "0858": 70, "937": 70, "0857": 70, "938": 70, "0856": 70, "939": 70, "0854": 70, "940": 70, "0853": 70, "941": 70, "0852": 70, "942": 70, "0851": [70, 83], "943": 70, "0850": 70, "944": 70, "0848": 70, "945": 70, "0847": 70, "946": 70, "0846": 70, "947": 70, "0845": 70, "948": 70, "0844": 70, "949": 70, "0843": 70, "950": 70, "0842": 70, "951": 70, "0840": 70, "952": 70, "0839": 70, "953": 70, "0838": 70, "954": 70, "0837": 70, "955": 70, "0836": 70, "956": 70, "0835": 70, "957": 70, "0834": 70, "958": 70, "0833": 70, "959": 70, "0831": 70, "960": 70, "0830": 70, "961": 70, "0829": 70, "962": 70, "0828": 70, "963": 70, "0827": 70, "964": 70, "0826": 70, "965": 70, "0825": 70, "966": 70, "0824": 70, "967": 70, "0823": 70, "968": 70, "0822": 70, "969": 70, "0820": 70, "970": 70, "0819": [70, 79], "971": 70, "0818": 70, "972": 70, "0817": 70, "973": 70, "0816": 70, "974": 70, "0815": 70, "975": 70, "0814": 70, "976": 70, "0813": 70, "977": [70, 84], "0812": 70, "978": 70, "0811": 70, "979": 70, "0810": 70, "980": 70, "0809": 70, "981": 70, "0808": 70, "982": 70, "0807": 70, "983": 70, "0806": 70, "984": 70, "0805": 70, "985": 70, "0804": [70, 79], "986": 70, "0803": [70, 79], "987": 70, "0802": 70, "988": 70, "0800": 70, "989": 70, "0799": 70, "990": 70, "0798": 70, "991": 70, "0797": 70, "992": 70, "0796": 70, "993": 70, "0795": 70, "994": 70, "0794": 70, "995": 70, "0793": 70, "996": 70, "0792": 70, "997": 70, "0791": 70, "998": 70, "0790": 70, "999": 70, "0789": [70, 79], "0788": 70, "0787": 70, "1002": 70, "0786": [70, 71], "1004": 70, "0785": 70, "0784": 70, "1006": [70, 79], "0783": 70, "0782": 70, "0781": 70, "1009": [70, 79], "0780": 70, "0779": 70, "1011": 70, "0778": 70, "0777": 70, "1013": 70, "0776": 70, "0775": 70, "0774": 70, "0773": 70, "0772": 70, "1018": 70, "0771": 70, "0770": 70, "1020": 70, "0769": 70, "0768": 70, "1022": 70, "0767": 70, "0766": 70, "1025": 70, "0765": 70, "0764": 70, "1027": 70, "0763": 70, "0762": [70, 79], "1029": 70, "0761": 70, "0760": 70, "1031": 70, "0759": [70, 79], "0758": 70, "1034": 70, "0757": 70, "0756": 70, "1036": 70, "0755": 70, "0754": [70, 79], "1038": 70, "0753": 70, "0752": 70, "1040": 70, "0751": 70, "0750": 70, "1042": 70, "0749": [70, 79], "1044": 70, "0748": 70, "0747": 70, "1046": [70, 79], "0746": [70, 79], "0745": 70, "1048": 70, "0744": 70, "0743": 70, "1051": 70, "0742": 70, "0741": 70, "1053": 70, "0740": 70, "0739": 70, "1055": 70, "0738": 70, "1057": 70, "0737": 70, "0736": 70, "1059": 70, "0735": 70, "0734": 70, "1061": 70, "0733": 70, "1063": 70, "0732": 70, "0731": [70, 79], "1065": 70, "0730": 70, "0729": 70, "1067": 70, "0728": 70, "1069": 70, "0727": 70, "0726": 70, "1071": 70, "0725": [70, 79], "0724": [70, 79], "1073": 70, "0723": 70, "1075": 70, "0722": 70, "0721": [70, 79], "1077": 70, "0720": [70, 79], "0719": 70, "0718": 70, "1081": 70, "0716": 70, "1083": 70, "0715": 70, "1085": 70, "0714": 70, "0713": 70, "1087": 70, "0712": 70, "1089": 70, "0711": 70, "0710": 70, "1091": 70, "0709": 70, "1092": 70, "0708": 70, "1094": [70, 87], "0707": 70, "0706": 70, "1096": 70, "0705": 70, "1098": 70, "0704": 70, "0703": 70, "1100": 70, "0702": 70, "1102": 70, "0701": [70, 79], "0700": 70, "1104": 70, "0699": [70, 79], "1106": [70, 79], "0698": 70, "0697": 70, "1108": 70, "1109": 70, "0696": 70, "0695": 70, "1111": 70, "0694": 70, "1113": 70, "0693": [70, 79], "0692": 70, "1115": 70, "0691": 70, "1117": 70, "0690": 70, "0689": 70, "1119": 70, "1120": 70, "0688": 70, "0687": [70, 79], "1122": 70, "0686": 70, "1124": 70, "0685": [70, 79], "0684": 70, "1126": 70, "0683": 70, "1128": 70, "0682": 70, "1129": 70, "1131": 70, "0680": 70, "0679": 70, "1133": 70, "0678": 70, "1135": 70, "0677": 70, "1137": 70, "0676": 70, "1138": 70, "0675": 70, "0674": [70, 83], "1140": 70, "0673": 70, "1142": 70, "0672": 70, "1144": 70, "0671": 70, "1145": 70, "0670": 70, "1147": 70, "0669": 70, "0668": 70, "1149": 70, "0667": 70, "1151": 70, "0666": 70, "1152": 70, "0665": 70, "1154": 70, "0664": 70, "1156": 70, "0663": [70, 79], "0662": 70, "1158": 70, "1159": 70, "0661": 70, "0660": 70, "1161": 70, "0659": 70, "1163": 70, "0658": 70, "1164": 70, "0657": 70, "1166": 70, "0656": 70, "1168": 70, "0655": 70, "1170": 70, "0654": 70, "1171": 70, "0653": 70, "1173": 70, "0652": 70, "0651": 70, "1175": 70, "1176": 70, "1178": 70, "0649": 70, "1180": 70, "1181": 70, "0647": 70, "0646": 70, "1183": 70, "0645": 70, "1185": 70, "1186": 70, "0644": 70, "0643": 70, "1188": 70, "0642": 70, "1190": 70, "0641": 70, "1191": 70, "0640": 70, "1193": 70, "0639": 70, "1195": 70, "0638": 70, "1196": 70, "0637": 70, "1198": 70, "1199": 70, "0636": 70, "0635": 70, "1201": 70, "0634": 70, "1203": 70, "1204": 70, "0633": 70, "0632": 70, "1206": 70, "0631": 70, "1208": 70, "1209": 70, "0630": 70, "0629": 70, "1211": 70, "1212": 70, "0628": 70, "1214": 70, "0627": 70, "1216": 70, "0626": 70, "1217": 70, "0625": 70, "1219": 70, "0624": 70, "1220": 70, "0623": 70, "1222": 70, "0622": 70, "1224": 70, "0621": 70, "1225": 70, "0620": 70, "1227": 70, "1228": 70, "0619": 70, "1230": 70, "0618": 70, "1231": 70, "0617": 70, "1233": 70, "0616": 70, "1235": 70, "0615": 70, "1236": 70, "0614": 70, "1238": 70, "0613": 70, "1239": 70, "0612": 70, "1241": 70, "0611": 70, "1244": 70, "0610": 70, "1246": 70, "0609": 70, "1247": 70, "0608": 70, "1249": 70, "0607": [70, 79], "0606": 70, "1252": 70, "1253": 70, "0605": 70, "1255": [70, 79], "0604": 70, "1256": 70, "0603": 70, "1258": 70, "1259": 70, "0602": 70, "1261": 70, "0601": 70, "1262": 70, "0600": 70, "1264": 70, "0599": 70, "1266": 70, "0598": 70, "1267": 70, "0597": 70, "1269": 70, "1270": 70, "0596": 70, "1272": 70, "0595": 70, "1273": 70, "0594": 70, "1275": 70, "1276": 70, "0593": 70, "1278": 70, "0592": 70, "1279": 70, "0591": 70, "1281": 70, "1282": 70, "0590": 70, "1284": 70, "0589": 70, "1285": 70, "0588": 70, "1287": 70, "0587": 70, "1288": [70, 79], "0586": 70, "1290": 70, "1291": 70, "0585": 70, "1293": [70, 79], "0584": 70, "1294": [70, 79], "1296": 70, "1297": 70, "0582": 70, "1299": 70, "0581": 70, "1300": 70, "0580": [70, 79], "1302": 70, "1303": 70, "0579": 70, "1305": 70, "1306": 70, "0578": 70, "1307": 70, "0577": 70, "1309": 70, "1310": 70, "0576": [70, 79], "1312": 70, "0575": 70, "1313": 70, "0574": 70, "1315": 70, "1316": [70, 79], "0573": 70, "1318": 70, "0572": 70, "1319": 70, "0571": 70, "1321": 70, "1322": 70, "0570": 70, "1324": 70, "0569": 70, "1325": 70, "1326": 70, "1328": 70, "0567": 70, "1329": 70, "1331": 70, "0566": 70, "1332": [70, 79], "0565": 70, "1334": 70, "1335": 70, "0564": 70, "1337": 70, "0563": 70, "1338": 70, "1339": 70, "0562": [70, 83], "1341": 70, "0561": [70, 71], "1342": 70, "1344": 70, "0560": [70, 71], "1345": 70, "0559": [70, 71], "1347": 70, "1348": [70, 83], "0558": 70, "1349": 70, "0557": 70, "1351": 70, "1352": 70, "0556": 70, "1354": 70, "1355": 70, "0555": 70, "1357": 70, "0554": 70, "1358": 70, "1359": 70, "0553": [70, 79], "1361": 70, "0552": 70, "1362": 70, "0551": 70, "1364": 70, "1365": 70, "1366": 70, "0550": 70, "1368": 70, "0549": 70, "1369": 70, "0548": 70, "1371": 70, "1372": 70, "0547": 70, "1374": 70, "1375": 70, "0546": [70, 79], "1376": 70, "0545": 70, "1378": 70, "1379": 70, "0544": 70, "1381": 70, "1382": 70, "0543": 70, "1383": 70, "0542": 70, "1385": 70, "1386": 70, "0541": 70, "1388": 70, "1389": 70, "0540": 70, "1390": 70, "0539": 70, "1392": 70, "1393": 70, "1394": 70, "0538": 70, "1396": 70, "0537": [70, 79], "1397": 70, "0536": 70, "1399": 70, "1400": 70, "1401": 70, "0535": 70, "1403": 70, "0534": 70, "1404": 70, "0533": 70, "1406": 70, "1407": 70, "1408": 70, "0532": 70, "1410": 70, "0531": 70, "1411": 70, "1412": 70, "0530": 70, "1414": 70, "1415": 70, "0529": [70, 83], "1417": 70, "1418": 70, "0528": 70, "1419": 70, "0527": 70, "1421": 70, "1422": 70, "1423": 70, "0526": 70, "1425": 70, "0525": 70, "1426": [70, 79], "1427": 70, "0524": 70, "1429": [70, 79], "1430": 70, "0523": 70, "1432": 70, "0522": 70, "1434": 70, "0521": 70, "1437": 70, "1438": 70, "0520": 70, "1440": 70, "0519": 70, "1441": 70, "1442": 70, "0518": 70, "1444": 70, "1445": 70, "0517": 70, "1446": 70, "1448": 70, "0516": 70, "1449": 70, "1450": 70, "0515": 70, "1452": 70, "1453": 70, "0514": 70, "1455": 70, "0513": 70, "1456": 70, "1457": [70, 79], "0512": [70, 83], "1459": 70, "1460": 70, "1461": 70, "0511": 70, "1463": 70, "0510": 70, "1464": 70, "1465": 70, "0509": 70, "1467": 70, "1468": 70, "1469": 70, "0508": 70, "1471": 70, "0507": 70, "1472": 70, "1473": [70, 79], "0506": 70, "1475": [70, 79], "1476": 70, "0505": 70, "1477": 70, "1479": 70, "0504": 70, "1480": 70, "1481": 70, "0503": 70, "1483": 70, "1484": [70, 79], "1485": 70, "0502": 70, "1486": 70, "0501": 70, "1488": 70, "1489": 70, "1490": 70, "0500": 70, "1492": 70, "1493": 70, "0499": 70, "1494": 70, "0498": 70, "1496": 70, "1497": 70, "1498": [70, 71], "0497": 70, "1500": 70, "1501": 70, "1502": 70, "1504": 70, "0495": 70, "1505": 70, "1506": 70, "1508": 70, "1509": 70, "0493": 70, "1510": 70, "1512": 70, "0492": 70, "1513": 70, "1514": 70, "1515": 70, "0491": 70, "1517": 70, "1518": [70, 79], "0490": 70, "1519": [70, 79], "0489": 70, "1521": 70, "1522": 70, "1523": 70, "0488": 70, "1525": 70, "1526": 70, "0487": 70, "1527": 70, "1528": 70, "0486": 70, "1530": 70, "1531": 70, "1532": 70, "0485": 70, "1534": 70, "1535": 70, "0484": 70, "1536": 70, "1538": 70, "0483": 70, "1539": 70, "1540": 70, "1541": 70, "0482": 70, "1543": 70, "1544": 70, "0481": 70, "1545": [70, 79], "0480": 70, "1547": 70, "1548": 70, "1549": 70, "0479": 70, "1550": 70, "1552": 70, "0478": 70, "1553": 70, "1554": 70, "0477": 70, "1556": 70, "1557": 70, "1558": 70, "0476": 70, "1559": 70, "1561": 70, "1562": 70, "1563": 70, "0474": 70, "1565": 70, "1566": 70, "1567": 70, "0473": 70, "1568": 70, "1570": 70, "0472": 70, "1571": 70, "1572": 70, "0471": 70, "1574": 70, "1575": [70, 79], "1576": 70, "0470": 70, "1577": 70, "1579": 70, "0469": [70, 83], "1580": 70, "1581": 70, "1582": 70, "0468": 70, "1584": 70, "1585": 70, "1586": 70, "0467": 70, "1588": 70, "1589": 70, "0466": 70, "1590": 70, "1591": [70, 84], "0465": 70, "1593": 70, "1594": 70, "1595": 70, "0464": 70, "1596": 70, "1598": 70, "0463": 70, "1599": 70, "1600": 70, "0462": 70, "1602": 70, "1603": 70, "1604": 70, "0461": 70, "1605": 70, "1607": 70, "0460": 70, "1608": 70, "1609": 70, "1610": 70, "0459": 70, "1612": 70, "1613": 70, "1614": 70, "0458": 70, "1615": 70, "1617": 70, "0457": 70, "1618": 70, "1619": 70, "1620": 70, "0456": 70, "1622": 70, "1623": 70, "0455": 70, "1624": 70, "1625": 70, "0454": 70, "1627": 70, "1628": [70, 79], "1629": 70, "1630": 70, "0453": 70, "1632": 70, "1633": 70, "0452": 70, "1634": 70, "1635": 70, "0451": 70, "1637": 70, "1638": 70, "1639": 70, "0450": 70, "1641": 70, "1642": 70, "1643": 70, "0449": 70, "1644": 70, "1645": 70, "0448": 70, "1647": 70, "1648": 70, "1649": 70, "0447": [70, 71], "1650": 70, "1652": 70, "0446": 70, "1653": 70, "1654": [70, 79], "1655": 70, "0445": 70, "1657": 70, "1658": 70, "1659": 70, "0444": 70, "1660": 70, "1662": 70, "1663": 70, "0443": 70, "1664": 70, "1665": 70, "0442": 70, "1667": 70, "1668": 70, "1669": 70, "0441": 70, "1670": 70, "1672": 70, "1673": 70, "0440": 70, "1674": 70, "1675": 70, "0439": 70, "1677": 70, "1678": 70, "1679": 70, "0438": 70, "1680": 70, "1681": 70, "1683": 70, "0437": 70, "1684": 70, "1685": 70, "1686": 70, "0436": 70, "1688": 70, "1689": 70, "1690": 70, "0435": 70, "1691": 70, "1693": 70, "0434": 70, "1694": 70, "1695": 70, "1696": 70, "0433": 70, "1698": 70, "1699": 70, "1700": 70, "0432": 70, "1701": 70, "1702": 70, "1704": 70, "0431": 70, "1705": 70, "1706": 70, "1707": 70, "0430": 70, "1709": 70, "1711": 70, "0429": 70, "1712": 70, "1713": 70, "0428": 70, "1715": 70, "1716": 70, "1717": 70, "1718": 70, "0427": 70, "1720": 70, "1721": 70, "0426": 70, "1722": 70, "1723": 70, "1724": [70, 79], "0425": 70, "1726": 70, "1727": 70, "1728": 70, "0424": 70, "1729": 70, "1731": 70, "1732": 70, "0423": 70, "1733": 70, "1734": [70, 79], "1735": 70, "0422": 70, "1738": [70, 79], "1739": 70, "0421": 70, "1740": 70, "1742": 70, "1743": 70, "0420": 70, "1744": 70, "1745": 70, "1746": 70, "0419": 70, "1748": 70, "1749": 70, "1750": 70, "0418": 70, "1752": 70, "1754": 70, "0417": 70, "1755": 70, "1756": 70, "1757": 70, "0416": 70, "1759": 70, "1760": 70, "1761": 70, "0415": 70, "1763": [70, 79], "1765": 70, "0414": 70, "1766": 70, "1767": 70, "1768": 70, "1769": 70, "0413": 70, "1771": 70, "1772": 70, "0412": 70, "1773": 70, "1774": 70, "1775": 70, "0411": 70, "1777": 70, "1778": 70, "1779": 70, "1780": [70, 79], "0410": 70, "1781": 70, "1783": 70, "1784": 70, "0409": [70, 87], "1785": 70, "1786": 70, "1787": 70, "0408": 70, "1789": 70, "1790": 70, "1791": 70, "0407": 70, "1792": 70, "1794": 70, "1795": 70, "0406": [70, 71], "1796": 70, "1797": 70, "1798": 70, "0405": 70, "1800": 70, "1801": 70, "1802": 70, "1803": 70, "0404": 70, "1804": [70, 79], "1806": 70, "1807": 70, "0403": 70, "1808": 70, "1809": 70, "1810": 70, "0402": 70, "1812": [70, 79], "1813": 70, "1814": 70, "0401": 70, "1815": 70, "1816": 70, "1817": 70, "0400": 70, "1819": 70, "1820": 70, "1821": [70, 79], "1822": 70, "0399": 70, "1823": 70, "1825": 70, "1826": 70, "0398": 70, "1827": 70, "1828": 70, "1829": 70, "0397": 70, "1831": [70, 79], "1832": 70, "1833": 70, "1834": 70, "0396": [70, 71], "1835": 70, "1837": 70, "1838": 70, "0395": 70, "1839": 70, "1840": 70, "1841": 70, "0394": 70, "1843": 70, "1844": 70, "1845": 70, "1846": 70, "0393": 70, "1847": [70, 84], "1848": 70, "1850": 70, "0392": 70, "1851": [70, 79], "1852": 70, "1853": 70, "1854": 70, "0391": [70, 86], "1856": 70, "1857": 70, "1858": 70, "0390": 70, "1859": 70, "1860": 70, "1862": 70, "1863": 70, "1864": 70, "1865": 70, "1866": 70, "0388": 70, "1867": 70, "1869": 70, "1870": 70, "0387": 70, "1871": 70, "1872": 70, "1873": 70, "0386": 70, "1875": 70, "1876": 70, "1877": 70, "1878": 70, "0385": 70, "1879": 70, "1880": 70, "1882": 70, "0384": 70, "1883": 70, "1884": 70, "1885": 70, "1886": 70, "0383": 70, "1888": 70, "1890": 70, "1891": 70, "0382": 70, "1892": 70, "1893": 70, "0381": 70, "1896": 70, "1897": 70, "1898": 70, "1899": 70, "0380": 70, "1901": 70, "1902": 70, "1903": 70, "0379": 70, "1904": 70, "1905": 70, "1906": 70, "0378": 70, "1908": 70, "1909": 70, "1910": 70, "1911": 70, "1912": 70, "0377": 70, "1913": [70, 83], "1915": 70, "1916": 70, "0376": 70, "1917": 70, "1918": 70, "1919": 70, "1920": 70, "0375": 70, "1922": 70, "1923": 70, "1924": 70, "0374": 70, "1925": 70, "1926": 70, "1928": 70, "1929": 70, "0373": 70, "1930": 70, "1931": 70, "1932": 70, "1933": 70, "0372": 70, "1935": 70, "1936": 70, "1937": 70, "0371": 70, "1938": 70, "1939": 70, "1940": 70, "1942": 70, "0370": 70, "1943": 70, "1944": 70, "1945": 70, "1946": 70, "0369": 70, "1947": 70, "1949": 70, "1950": 70, "0368": 70, "1951": 70, "1952": 70, "1953": 70, "1954": 70, "0367": 70, "1956": 70, "1957": 70, "1959": 70, "0366": 70, "1960": 70, "1961": 70, "1963": 70, "0365": 70, "1964": 70, "1965": 70, "1966": 70, "1967": 70, "1968": 70, "0364": 70, "1970": 70, "1971": 70, "1972": 70, "0363": [70, 83], "1973": 70, "1974": 70, "1975": [70, 84], "1977": 70, "0362": 70, "1978": 70, "1979": 70, "1980": 70, "1981": 70, "0361": 70, "1982": 70, "1984": 70, "1985": 70, "0360": 70, "1986": 70, "1987": 70, "1988": 70, "1989": 70, "1990": 70, "0359": 70, "1992": 70, "1993": 70, "1994": 70, "1995": 70, "1996": 70, "1997": 70, "1999": 70, "0357": 70, "plot": [70, 88], "dpi": 70, "test_epoch": 70, "isfinit": 70, "linestyl": 70, "marker": 70, "legend": [70, 88], "xlabel": 70, "cites": 71, "3703": 71, "manifold": 71, "tsne": 71, "randomnodesplit": 71, "home": [71, 85], "sadra": 71, "local": 71, "tqdm": [71, 88], "auto": 71, "tqdmwarn": 71, "iprogress": 71, "found": [71, 76], "jupyt": 71, "ipywidget": 71, "readthedoc": 71, "en": 71, "user_instal": 71, "autonotebook": 71, "notebook_tqdm": 71, "wget": [71, 76], "twistedcub": 71, "raw": [71, 76], "master": [71, 76], "citeseer6cls3703": 71, "pt": [71, 88], "paper_x": 71, "longtensor": [71, 76], "paper_author": 71, "train_test_splitt": 71, "num_test": 71, "num_val": 71, "dropout_r": 71, "enumer": [71, 76, 88], "schedul": 71, "initial_lr": 71, "04": [71, 79, 88], "lr_schedul": 71, "steplr": 71, "7889": 71, "6347": 71, "6458": 71, "9118": 71, "5491": 71, "7203": 71, "5956": 71, "4068": 71, "4721": 71, "7964": 71, "3431": 71, "5075": 71, "0124": 71, "9682": 71, "2804": 71, "2483": 71, "2004": 71, "2019": [71, 83], "0248": 71, "0219": 71, "0217": 71, "0328": 71, "0180": 71, "0220": 71, "0232": 71, "0241": 71, "0130": 71, "0239": 71, "0240": 71, "0284": 71, "0295": 71, "0163": 71, "0195": 71, "0323": 71, "8441": 71, "worth": 71, "visual": [71, 88], "n_compon": 71, "fit_transform": 71, "ax1": 71, "ax2": 71, "subplot": [71, 88], "suptitl": 71, "set_titl": [71, 88], "_t_sne": 71, "futurewarn": 71, "chang": [71, 79, 83, 84, 88], "pca": 71, "warn": [71, 75, 78, 84, 85], "As": 72, "highlight": 72, "formal": [72, 81], "alpha_": 72, "jk": 72, "nonlinear": [72, 87], "exp": 72, "u_": 72, "limits_": 72, "context": 72, "again": [72, 81, 82], "vi": [72, 73], "beta_": 72, "ij": 72, "anoth": 72, "measur": 72, "7875": 72, "9625": 72, "interpret": 73, "propag": 73, "problem": [73, 76], "divid": 73, "hypersagemodel": 73, "2431": 73, "templatenn": 74, "6126": 74, "0002": 74, "0001": 74, "simplicial_complex": [75, 77, 78, 88], "filterwarn": [75, 78], "to_sparse_csr": [75, 76, 78], "bceloss": [75, 78], "x_1_val": [75, 77, 78], "incidence_1_v": [75, 77, 78], "y_val": [75, 77, 78], "pred": [75, 77, 78, 88], "0971908569336": 75, "94380187988281": 75, "2369270324707": 75, "26074981689453": 75, "98492431640625": 75, "20859909057617": 75, "80668640136719": 75, "686824798583984": 75, "83675765991211": 75, "03213882446289": 75, "238582611083984": 75, "33390426635742": 75, "644731521606445": 75, "585487365722656": 75, "487186431884766": 75, "787471771240234": 75, "63153076171875": 75, "06392478942871": 75, "550783157348633": 75, "08787727355957": 75, "46900749206543": 75, "992172241210938": 75, "090089797973633": 75, "6256160736084": 75, "858888626098633": 75, "25303077697754": 75, "14973258972168": 75, "443086624145508": 75, "693538665771484": 75, "53269386291504": 75, "046640396118164": 75, "61376190185547": 75, "491796493530273": 75, "60036277770996": 75, "690528869628906": 75, "603723526000977": 75, "351343154907227": 75, "073862075805664": 75, "908754348754883": 75, "88480567932129": 75, "900497436523438": 75, "822755813598633": 75, "63096809387207": 75, "415300369262695": 75, "28574562072754": 75, "239614486694336": 75, "227548599243164": 75, "16373634338379": 75, "053255081176758": 75, "935653686523438": 75, "852752685546875": 75, "81441307067871": 75, "792282104492188": 75, "750215530395508": 75, "678442001342773": 75, "601423263549805": 75, "548017501831055": 75, "518722534179688": 75, "489871978759766": 75, "43247413635254": 75, "345685958862305": 75, "245248794555664": 75, "150096893310547": 75, "066505432128906": 75, "986852645874023": 75, "90121841430664": 75, "803401947021484": 75, "703289031982422": 75, "632183074951172": 75, "587385177612305": 75, "54228401184082": 75, "49706268310547": 75, "451065063476562": 75, "407331466674805": 75, "371335983276367": 75, "337526321411133": 75, "306472778320312": 75, "273452758789062": 75, "237478256225586": 75, "20127296447754": 75, "167097091674805": 75, "132389068603516": 75, "098857879638672": 75, "060476303100586": 75, "02267837524414": 75, "9860782623291": 75, "950490951538086": 75, "914758682250977": 75, "878108978271484": 75, "842838287353516": 75, "811080932617188": 75, "780370712280273": 75, "74917221069336": 75, "71664810180664": 75, "699419021606445": 75, "676870346069336": 75, "647939682006836": 75, "61389923095703": 75, "5761775970459": 75, "549442291259766": 75, "5289249420166": 75, "49772071838379": 75, "474157333374023": 75, "45038414001465": 75, "424612045288086": 75, "40214729309082": 75, "378002166748047": 75, "351882934570312": 75, "324098587036133": 75, "298376083374023": 75, "274303436279297": 75, "24971580505371": 75, "228342056274414": 75, "197002410888672": 75, "17634391784668": 75, "151491165161133": 75, "128210067749023": 75, "113391876220703": 75, "088733673095703": 75, "06772232055664": 75, "04320526123047": 75, "0169734954834": 75, "99879264831543": 75, "976442337036133": 75, "94951820373535": 75, "926555633544922": 75, "90591812133789": 75, "884706497192383": 75, "859689712524414": 75, "834184646606445": 75, "812297821044922": 75, "79071807861328": 75, "770198822021484": 75, "74526596069336": 75, "72126579284668": 75, "695659637451172": 75, "671920776367188": 75, "650121688842773": 75, "63254737854004": 75, "603469848632812": 75, "58234405517578": 75, "55916976928711": 75, "540271759033203": 75, "513303756713867": 75, "49028968811035": 75, "466527938842773": 75, "449338912963867": 75, "419330596923828": 75, "39695167541504": 75, "375646591186523": 75, "3538875579834": 75, "334510803222656": 75, "312015533447266": 75, "285579681396484": 75, "269054412841797": 75, "242639541625977": 75, "22045135498047": 75, "203929901123047": 75, "183347702026367": 75, "160404205322266": 75, "133930206298828": 75, "104745864868164": 75, "098697662353516": 75, "078676223754883": 75, "040504455566406": 75, "02244758605957": 75, "011486053466797": 75, "993385314941406": 75, "96799087524414": 75, "939449310302734": 75, "912992477416992": 75, "884111404418945": 75, "86220359802246": 75, "8441219329834": 75, "813901901245117": 75, "801708221435547": 75, "786155700683594": 75, "761951446533203": 75, "740060806274414": 75, "71567726135254": 75, "68885040283203": 75, "670719146728516": 75, "652807235717773": 75, "618999481201172": 75, "5964298248291": 75, "57932472229004": 75, "557270050048828": 75, "530651092529297": 75, "504606246948242": 75, "476253509521484": 75, "45449447631836": 75, "434545516967773": 75, "411243438720703": 75, "382360458374023": 75, "362293243408203": 75, "335590362548828": 75, "30755043029785": 75, "284061431884766": 75, "25484275817871": 75, "22318458557129": 75, "19426155090332": 75, "173973083496094": 75, "14513397216797": 75, "10979652404785": 75, "082422256469727": 75, "0560359954834": 75, "033998489379883": 75, "013517379760742": 75, "987682342529297": 75, "960020065307617": 75, "938085556030273": 75, "911584854125977": 75, "887819290161133": 75, "859230041503906": 75, "83808708190918": 75, "825475692749023": 75, "791316986083984": 75, "769617080688477": 75, "746116638183594": 75, "724519729614258": 75, "69559097290039": 75, "676664352416992": 75, "659330368041992": 75, "623565673828125": 75, "596616744995117": 75, "573455810546875": 75, "547086715698242": 75, "527324676513672": 75, "50086784362793": 75, "473081588745117": 75, "447193145751953": 75, "41757583618164": 75, "402799606323242": 75, "38097381591797": 75, "355859756469727": 75, "32827377319336": 75, "316627502441406": 75, "297752380371094": 75, "263805389404297": 75, "23656463623047": 75, "23184585571289": 75, "2138671875": 75, "183242797851562": 75, "142995834350586": 75, "137985229492188": 75, "11527442932129": 75, "083709716796875": 75, "5625": [75, 77, 78], "04819107055664": 75, "046104431152344": 75, "027341842651367": 75, "99266242980957": 75, "961795806884766": 75, "930912017822266": 75, "911731719970703": 75, "880786895751953": 75, "845874786376953": 75, "83226203918457": 75, "812429428100586": 75, "780649185180664": 75, "75445556640625": 75, "74269676208496": 75, "706310272216797": 75, "682571411132812": 75, "67528533935547": 75, "641508102416992": 75, "615623474121094": 75, "612245559692383": 75, "574350357055664": 75, "543176651000977": 75, "523324966430664": 75, "500341415405273": 75, "460468292236328": 75, "45850944519043": 75, "433353424072266": 75, "38702392578125": 75, "373823165893555": 75, "356149673461914": 75, "33302116394043": 75, "2984619140625": 75, "27631950378418": 75, "242095947265625": 75, "220375061035156": 75, "19377326965332": 75, "166545867919922": 75, "145954132080078": 75, "118263244628906": 75, "103313446044922": 75, "090181350708008": 75, "05240821838379": 75, "038013458251953": 75, "01729965209961": 75, "008516311645508": 75, "97707748413086": 75, "94068717956543": 75, "90737533569336": 75, "87880516052246": 75, "850223541259766": 75, "817041397094727": 75, "79216194152832": 75, "768423080444336": 75, "73453712463379": 75, "714162826538086": 75, "683536529541016": 75, "661930084228516": 75, "627168655395508": 75, "591276168823242": 75, "575401306152344": 75, "538806915283203": 75, "532752990722656": 75, "491992950439453": 75, "470623016357422": 75, "447834014892578": 75, "417621612548828": 75, "395700454711914": 75, "371519088745117": 75, "34141731262207": 75, "317161560058594": 75, "290822982788086": 75, "27621078491211": 75, "252424240112305": 75, "223106384277344": 75, "191679000854492": 75, "167285919189453": 75, "146617889404297": 75, "1218204498291": 75, "092159271240234": 75, "068817138671875": 75, "043107986450195": 75, "020431518554688": 75, "99953842163086": 75, "965152740478516": 75, "948270797729492": 75, "9256591796875": 75, "89617347717285": 75, "867271423339844": 75, "838720321655273": 75, "820175170898438": 75, "80241584777832": 75, "77659797668457": 75, "735746383666992": 75, "71509552001953": 75, "694164276123047": 75, "66806411743164": 75, "638553619384766": 75, "61126136779785": 75, "588682174682617": 75, "57335090637207": 75, "55010223388672": 75, "518211364746094": 75, "497529983520508": 75, "474842071533203": 75, "450790405273438": 75, "426084518432617": 75, "395307540893555": 75, "374881744384766": 75, "35700798034668": 75, "3305721282959": 75, "315614700317383": 75, "27733039855957": 75, "254539489746094": 75, "226396560668945": 75, "21523666381836": 75, "177637100219727": 75, "150047302246094": 75, "121498107910156": 75, "07767677307129": 75, "025562286376953": 75, "97791290283203": 75, "93244743347168": 75, "88832664489746": 75, "836246490478516": 75, "796043395996094": 75, "74243927001953": 75, "702007293701172": 75, "6556339263916": 75, "615976333618164": 75, "584707260131836": 75, "56226348876953": 75, "522308349609375": 75, "483394622802734": 75, "425535202026367": 75, "371274948120117": 75, "33834457397461": 75, "30695152282715": 75, "257896423339844": 75, "215682983398438": 75, "17093849182129": 75, "115076065063477": 75, "079204559326172": 75, "051490783691406": 75, "033987045288086": 75, "01789093017578": 75, "02878189086914": 75, "052005767822266": 75, "115055084228516": 75, "044801712036133": 75, "891642570495605": 75, "728387832641602": 75, "788483619689941": 75, "005611419677734": 75, "045494079589844": 75, "028085708618164": 75, "70627212524414": 75, "557747840881348": 75, "639880180358887": 75, "732135772705078": 75, "785037994384766": 75, "548952102661133": 75, "409181594848633": 75, "439441680908203": 75, "51244068145752": 75, "506211280822754": 75, "33906364440918": 75, "261138916015625": 75, "3150053024292": 75, "368926048278809": 75, "32858943939209": 75, "185369491577148": 75, "1485595703125": 75, "187846183776855": 75, "209299087524414": 75, "182910919189453": 75, "098752975463867": 75, "044387817382812": 75, "052579879760742": 75, "05774211883545": 75, "031035423278809": 75, "977952003479004": 75, "939553260803223": 75, "92505931854248": 75, "922098159790039": 75, "910082817077637": 75, "877153396606445": 75, "838932991027832": 75, "809442520141602": 75, "788487434387207": 75, "78007698059082": 75, "799392700195312": 75, "814967155456543": 75, "780838966369629": 75, "738446235656738": 75, "707448959350586": 75, "675198554992676": 75, "641422271728516": 75, "6210355758667": 75, "609297752380371": 75, "609857559204102": 75, "6266508102417": 75, "633983612060547": 75, "6951904296875": 75, "658025741577148": 75, "600869178771973": 75, "535354614257812": 75, "497736930847168": 75, "458619117736816": 75, "456388473510742": 75, "473320007324219": 75, "49333667755127": 75, "535053253173828": 75, "544401168823242": 75, "582549095153809": 75, "526555061340332": 75, "50399112701416": 75, "38758373260498": 75, "308337211608887": 75, "299043655395508": 75, "327571868896484": 75, "388229370117188": 75, "417525291442871": 75, "455561637878418": 75, "416516304016113": 75, "392356872558594": 75, "28682804107666": 75, "192612648010254": 75, "12002182006836": 75, "091236114501953": 75, "14907455444336": 75, "283432006835938": 75, "588083267211914": 75, "7717866897583": 75, "171638488769531": 75, "575143814086914": 75, "168379783630371": 75, "044833183288574": 75, "277763366699219": 75, "634512901306152": 75, "24776554107666": 75, "931353569030762": 75, "041153907775879": 75, "249833106994629": 75, "302783966064453": 75, "985976219177246": 75, "880131721496582": 75, "02563762664795": 75, "059005737304688": 75, "942398071289062": 75, "796463012695312": 75, "87881088256836": 75, "039618492126465": 75, "918492317199707": 75, "420305252075195": 75, "406696319580078": 75, "394020080566406": 75, "37169075012207": 75, "35594940185547": 75, "34318733215332": 75, "33356475830078": 75, "31667137145996": 75, "302318572998047": 75, "28500747680664": 75, "26559066772461": 75, "25322914123535": 75, "238706588745117": 75, "220836639404297": 75, "208791732788086": 75, "1856746673584": 75, "171249389648438": 75, "15633201599121": 75, "145591735839844": 75, "12535285949707": 75, "112903594970703": 75, "096086502075195": 75, "082990646362305": 75, "068315505981445": 75, "050025939941406": 75, "046539306640625": 75, "030738830566406": 75, "007352828979492": 75, "004419326782227": 75, "990276336669922": 75, "96859359741211": 75, "95843505859375": 75, "941267013549805": 75, "93852424621582": 75, "93297004699707": 75, "905284881591797": 75, "90688705444336": 75, "88941764831543": 75, "867488861083984": 75, "852458953857422": 75, "82878303527832": 75, "848604202270508": 75, "816743850708008": 75, "798707962036133": 75, "782867431640625": 75, "770641326904297": 75, "765010833740234": 75, "74397087097168": 75, "730905532836914": 75, "71582794189453": 75, "692882537841797": 75, "678205490112305": 75, "659757614135742": 75, "66231918334961": 75, "63972282409668": 75, "637910842895508": 75, "620912551879883": 75, "613643646240234": 75, "592449188232422": 75, "578832626342773": 75, "5690860748291": 75, "5478572845459": 75, "536046981811523": 75, "518606185913086": 75, "507850646972656": 75, "49575424194336": 75, "47629737854004": 75, "466197967529297": 75, "446958541870117": 75, "431520462036133": 75, "42148780822754": 75, "418920516967773": 75, "382413864135742": 75, "38419532775879": 75, "382558822631836": 75, "373292922973633": 75, "359771728515625": 75, "339229583740234": 75, "32771873474121": 75, "31146240234375": 75, "291799545288086": 75, "266311645507812": 75, "25176429748535": 75, "275617599487305": 75, "243261337280273": 75, "198863983154297": 75, "190418243408203": 75, "180404663085938": 75, "159381866455078": 75, "132848739624023": 75, "08831024169922": 75, "07340431213379": 75, "040719985961914": 75, "018062591552734": 75, "98824119567871": 75, "994766235351562": 75, "97658348083496": 75, "954269409179688": 75, "931133270263672": 75, "923803329467773": 75, "900142669677734": 75, "883092880249023": 75, "874980926513672": 75, "859506607055664": 75, "83385467529297": 75, "821224212646484": 75, "808631896972656": 75, "790834426879883": 75, "770950317382812": 75, "753965377807617": 75, "74176597595215": 75, "726638793945312": 75, "71209144592285": 75, "717884063720703": 75, "69021987915039": 75, "684720993041992": 75, "65319061279297": 75, "64687728881836": 75, "641027450561523": 75, "62809181213379": 75, "599239349365234": 75, "57975196838379": 75, "56479835510254": 75, "56086540222168": 75, "538368225097656": 75, "528959274291992": 75, "50667953491211": 75, "503355026245117": 75, "473203659057617": 75, "480627059936523": 75, "455434799194336": 75, "448516845703125": 75, "433578491210938": 75, "42472267150879": 75, "41015625": 75, "397563934326172": 75, "366233825683594": 75, "36186981201172": 75, "347782135009766": 75, "334121704101562": 75, "319826126098633": 75, "311002731323242": 75, "29153060913086": 75, "269283294677734": 75, "26325225830078": 75, "24094581604004": 75, "23046112060547": 75, "2217960357666": 75, "21772575378418": 75, "204160690307617": 75, "186527252197266": 75, "18400764465332": 75, "16082763671875": 75, "150056838989258": 75, "157442092895508": 75, "121925354003906": 75, "12659454345703": 75, "10972023010254": 75, "108051300048828": 75, "09576988220215": 75, "07231903076172": 75, "065282821655273": 75, "032222747802734": 75, "040668487548828": 75, "02615737915039": 75, "99028778076172": 75, "992895126342773": 75, "986125946044922": 75, "971839904785156": 75, "947101593017578": 75, "93064308166504": 75, "940488815307617": 75, "926761627197266": 75, "89728355407715": 75, "92677116394043": 75, "90224266052246": 75, "888980865478516": 75, "871950149536133": 75, "850805282592773": 75, "85380744934082": 75, "83152961730957": 75, "820480346679688": 75, "813871383666992": 75, "811283111572266": 75, "79723358154297": 75, "773174285888672": 75, "76585578918457": 75, "763370513916016": 75, "74306297302246": 75, "733776092529297": 75, "727794647216797": 75, "726064682006836": 75, "721574783325195": 75, "69578742980957": 75, "671518325805664": 75, "67192268371582": 75, "675018310546875": 75, "652587890625": 75, "63589859008789": 75, "632600784301758": 75, "618101119995117": 75, "61069679260254": 75, "603355407714844": 75, "587169647216797": 75, "568763732910156": 75, "55999183654785": 75, "554792404174805": 75, "530359268188477": 75, "52699851989746": 75, "524051666259766": 75, "511272430419922": 75, "492382049560547": 75, "48703384399414": 75, "472692489624023": 75, "478933334350586": 75, "467594146728516": 75, "44580841064453": 75, "4395809173584": 75, "439208984375": 75, "427417755126953": 75, "416553497314453": 75, "407917022705078": 75, "396162033081055": 75, "377273559570312": 75, "370101928710938": 75, "381824493408203": 75, "365995407104492": 75, "33931541442871": 75, "341108322143555": 75, "34080696105957": 75, "326269149780273": 75, "295074462890625": 75, "290189743041992": 75, "294099807739258": 75, "279863357543945": 75, "259613037109375": 75, "255352020263672": 75, "243770599365234": 75, "24730682373047": 75, "235071182250977": 75, "221952438354492": 75, "20149803161621": 75, "19734001159668": 75, "199737548828125": 75, "179275512695312": 75, "176069259643555": 75, "170211791992188": 75, "16652488708496": 75, "136791229248047": 75, "125789642333984": 75, "123292922973633": 75, "109601974487305": 75, "100919723510742": 75, "0986270904541": 75, "09308624267578": 75, "078474044799805": 75, "067218780517578": 75, "05537223815918": 75, "03988265991211": 75, "03138542175293": 75, "026723861694336": 75, "008121490478516": 75, "018474578857422": 75, "014162063598633": 75, "001453399658203": 75, "981800079345703": 75, "987455368041992": 75, "983600616455078": 75, "cicitationcora": 76, "hgnn": 76, "utlil": 76, "neccessari": 76, "pickl": 76, "scipi": [76, 79, 83, 84, 85, 88], "sp": 76, "computation": 76, "expens": [76, 79, 83, 84], "malllabiisc": 76, "hypergcn": 76, "cocit": 76, "07": [76, 79, 88], "ca": 76, "certif": 76, "ssl": 76, "cert": 76, "crt": 76, "resolv": 76, "await": 76, "githubusercont": 76, "ok": 76, "404937": 76, "395k": 76, "applic": 76, "octet": 76, "stream": 76, "save": 76, "gt": [76, 88], "45k": 76, "kb": 76, "mb": 76, "101905": 76, "100k": 76, "52k": 76, "02": [76, 79, 88], "5436": 76, "3k": 76, "31k": 76, "51582": 76, "50k": 76, "37k": 76, "rb": 76, "handl": 76, "ipykernel_14655": 76, "121206761": 76, "deprecationwarn": 76, "csr_matrix": [76, 79, 83, 84], "namespac": 76, "csr": 76, "deprec": [76, 84], "pytorch": 76, "floattensor": 76, "num": 76, "gcnii": 76, "h2": [76, 83], "hstack": 76, "2226475299": 76, "miss": 76, "trigger": 76, "aten": 76, "sparsecsrtensorimpl": 76, "cpp": 76, "predefin": 76, "train_idx": 76, "test_idx": 76, "unigcniimodel": 76, "num_featur": 76, "copi": 76, "x_0_skip": 76, "clone": 76, "ommit": 76, "cross": 76, "entropi": 76, "current": [76, 81, 84, 87], "readi": [76, 88], "7071428298950195": 76, "39291277527809143": 76, "4376946985721588": 76, "9357143044471741": 76, "6137071847915649": 76, "8714285492897034": 76, "45210281014442444": 76, "9071428775787354": 76, "5502336621284485": 76, "9785714149475098": 76, "5463395714759827": 76, "5747663378715515": 76, "9857142567634583": 76, "5673676133155823": 76, "9428571462631226": 76, "552570104598999": 76, "9571428298950195": 76, "5607476830482483": 76, "unigin_nn": 77, "inp_emb": 77, "out_decod": 77, "pooled_x_0": 77, "node_dim": 77, "seper": 77, "unsqueez": [77, 88], "3916015625": 77, "91788864135742": 77, "573387145996094": 77, "32501220703125": 77, "29070281982422": 77, "48554229736328": 77, "905006408691406": 77, "502838134765625": 77, "23980712890625": 77, "084415435791016": 77, "003536224365234": 77, "96804428100586": 77, "95846939086914": 77, "9611930847168": 77, "96879196166992": 77, "977806091308594": 77, "98638916015625": 77, "99394989013672": 77, "00029754638672": 77, "00556945800781": 77, "00981521606445": 77, "01326370239258": 77, "01602554321289": 77, "018226623535156": 77, "01999282836914": 77, "0213737487793": 77, "02241897583008": 77, "02317810058594": 77, "023704528808594": 77, "02401351928711": 77, "02414321899414": 77, "024078369140625": 77, "023826599121094": 77, "02338790893555": 77, "02278137207031": 77, "021995544433594": 77, "02100372314453": 77, "019805908203125": 77, "018394470214844": 77, "01671600341797": 77, "0147590637207": 77, "01249313354492": 77, "00986862182617": 77, "0068359375": 77, "003334045410156": 77, "999298095703125": 77, "99464416503906": 77, "98926544189453": 77, "98302459716797": 77, "975791931152344": 77, "chrsmrr": 78, "graphkerneldataset": 78, "unisagenn": 78, "38711929321289": 78, "50642013549805": 78, "88081741333008": 78, "313419342041016": 78, "208885192871094": 78, "00963592529297": 78, "59610366821289": 78, "292537689208984": 78, "19595718383789": 78, "106815338134766": 78, "913330078125": 78, "710723876953125": 78, "6346549987793": 78, "63870620727539": 78, "57096481323242": 78, "44948196411133": 78, "391658782958984": 78, "39373779296875": 78, "34821319580078": 78, "241302490234375": 78, "159812927246094": 78, "131492614746094": 78, "08507537841797": 78, "99526023864746": 78, "924638748168945": 78, "894010543823242": 78, "868173599243164": 78, "829978942871094": 78, "808109283447266": 78, "807811737060547": 78, "785655975341797": 78, "746475219726562": 78, "724843978881836": 78, "700727462768555": 78, "660417556762695": 78, "624387741088867": 78, "606855392456055": 78, "585779190063477": 78, "568805694580078": 78, "556062698364258": 78, "53620147705078": 78, "51841163635254": 78, "507102966308594": 78, "491756439208984": 78, "478620529174805": 78, "475711822509766": 78, "467418670654297": 78, "449710845947266": 78, "429576873779297": 78, "42317771911621": 78, "alexandro": 79, "kero": 79, "linalg": 79, "npla": 79, "a0": [79, 80], "becaus": [79, 80, 82, 84, 87], "serv": [79, 80], "simpli": [79, 80, 88], "demonstr": [79, 80], "similarli": [79, 80, 83], "emerg": [79, 80, 81, 83, 84], "four": [79, 80, 81, 83, 84], "y_true": [79, 80, 81, 84, 87], "l_tilde_pinv": 79, "pinv": 79, "invers": 79, "0971": 79, "0937": 79, "2140": 79, "2069": 79, "2927": 79, "3018": 79, "2309": 79, "0992": 79, "0943": 79, "0927": 79, "2678": 79, "3090": 79, "0960": 79, "2077": 79, "2056": 79, "2813": 79, "nnz": 79, "layout": 79, "sparse_coo": 79, "56771909e": 79, "49643084e": 79, "13434650e": 79, "60154799e": 79, "03": [79, 88], "73820292e": 79, "65885226e": 79, "04038181e": 79, "08": [79, 88], "51925802e": 79, "73643677e": 79, "95577741e": 79, "09312067e": 79, "39698386e": 79, "11006736e": 79, "25540316e": 79, "87149896e": 79, "65674657e": 79, "43987098e": 79, "79396772e": 79, "00662204e": 79, "45058060e": 79, "36910174e": 79, "82942520e": 79, "24798042e": 79, "85055751e": 79, "78386103e": 79, "24821486e": 79, "81510593e": 79, "07917011e": 79, "30485535e": 79, "19925834e": 79, "56662779e": 79, "25658545e": 79, "29514395e": 79, "73054542e": 79, "57650283e": 79, "87089108e": 79, "31973699e": 79, "45874534e": 79, "78385898e": 79, "24821523e": 79, "38282800e": 79, "29527006e": 79, "24821542e": 79, "45585343e": 79, "20149602e": 79, "39614227e": 79, "52603984e": 79, "02427802e": 79, "38569428e": 79, "20058507e": 79, "89658767e": 79, "67997003e": 79, "90682733e": 79, "88636552e": 79, "61071175e": 79, "75768661e": 79, "22418800e": 79, "07488209e": 79, "26928225e": 79, "52925774e": 79, "50903371e": 79, "71863856e": 79, "40345353e": 79, "36909867e": 79, "82943824e": 79, "90223058e": 79, "08467136e": 79, "43380561e": 79, "27135092e": 79, "31898531e": 79, "01219751e": 79, "78963115e": 79, "97890193e": 79, "49229891e": 79, "67953214e": 79, "75078206e": 79, "75904313e": 79, "03583546e": 79, "12457962e": 79, "10897127e": 79, "18870673e": 79, "28672193e": 79, "61245163e": 79, "48166016e": 79, "75217551e": 79, "67996958e": 79, "90682673e": 79, "44834775e": 79, "90006804e": 79, "59747154e": 79, "69860917e": 79, "59747209e": 79, "69862127e": 79, "59747284e": 79, "69861429e": 79, "59747191e": 79, "59747247e": 79, "69860823e": 79, "59747135e": 79, "11979373e": 79, "90869734e": 79, "59747228e": 79, "69860637e": 79, "59747303e": 79, "69861010e": 79, "59747116e": 79, "17587730e": 79, "43268425e": 79, "43105909e": 79, "32787512e": 79, "03376685e": 79, "44168448e": 79, "62169540e": 79, "41996737e": 79, "73246880e": 79, "97727704e": 79, "03496753e": 79, "71378374e": 79, "92902595e": 79, "15740368e": 79, "94057676e": 79, "48602486e": 79, "40909785e": 79, "14646482e": 79, "38315065e": 79, "76777497e": 79, "38311899e": 79, "76780128e": 79, "37373477e": 79, "49392605e": 79, "30545244e": 79, "10224779e": 79, "69429579e": 79, "59057510e": 79, "11831834e": 79, "86165255e": 79, "07662510e": 79, "53556532e": 79, "82225195e": 79, "76254632e": 79, "62731223e": 79, "63466549e": 79, "16528196e": 79, "62805045e": 79, "36022410e": 79, "48832843e": 79, "19494419e": 79, "13972221e": 79, "zia003": 79, "topox2": 79, "_index": [79, 83, 84], "sparseefficiencywarn": [79, 83, 84], "sparsiti": [79, 83, 84], "lil_matrix": [79, 83, 84], "_set_arrayxarrai": [79, 83, 84], "produc": [79, 80, 81, 83, 87], "compar": [79, 80, 81, 83, 87], "binary_cross_entropy_with_logit": [79, 80, 81, 83, 87], "y_hat_test": [79, 80, 81, 83, 84, 87], "y_pred_test": [79, 80, 81, 83, 84, 87], "test_accuraci": [79, 80, 81, 83, 84, 87], "7231": 79, "6000": 79, "6989": 79, "5667": [79, 80, 83], "2500": [79, 84], "6737": 79, "6564": 79, "6434": 79, "6362": 79, "6290": 79, "4333": [79, 87], "6199": 79, "6117": 79, "6057": 79, "6011": 79, "5964": 79, "5911": 79, "5855": 79, "5764": 79, "4000": 79, "5730": 79, "5696": 79, "5660": 79, "5593": 79, "5567": 79, "5544": 79, "5520": 79, "5496": 79, "5472": 79, "5451": 79, "5432": 79, "5414": 79, "5346": 79, "5333": 79, "5320": 79, "5308": 79, "5295": 79, "5284": 79, "5273": 79, "5264": 79, "5255": 79, "5238": 79, "5230": 79, "5223": 79, "5216": 79, "5210": 79, "5204": 79, "5198": 79, "5193": 79, "5183": 79, "5178": 79, "5174": 79, "5170": 79, "5166": 79, "5163": 79, "5159": 79, "5156": 79, "7216": 80, "7169": 80, "7151": 80, "7109": 80, "work": 81, "novel": 81, "hing": 81, "proper": 81, "orient": 81, "fashion": 81, "kernel": 81, "l_r": 81, "widetild": 81, "hy": 81, "neq": [81, 88], "affin": 81, "therefor": 81, "hop": [81, 87], "suppos": 81, "_j": 81, "underset": 81, "w_": 81, "q_": 81, "tb_1": 81, "b_2b_2": 81, "notic": 81, "pattern": 81, "just": [81, 83, 88], "maxium": 81, "valueerror": [81, 84, 87], "gradient": 81, "tx_0": 81, "estim": 81, "diverg": 81, "deriv": 81, "seen": 81, "incidence_0_1": 81, "accordingli": 81, "mm": 81, "henc": 81, "y_hat_edg": 81, "fn": 81, "y_hat_edge_test": 81, "_pred_test": 81, "ge": 81, "7322": 81, "3667": 81, "7208": 81, "7333": [81, 83], "7070": 81, "6814": 81, "6753": 81, "6717": 81, "6695": 81, "6682": 81, "6674": 81, "laplacian_down_1_list": [82, 84, 87], "laplacian_down_2_list": 82, "incidence1_t_list": 82, "incidence2_t_list": 82, "laplacian_down_2": [82, 84, 87], "laplacian_down_1_train": [82, 84], "laplacian_down_1_test": [82, 84], "laplacian_down_2_train": 82, "laplacian_down_2_test": 82, "incidence1_t_train": 82, "incidence1_t_test": 82, "incidence2_t_train": 82, "incidence2_t_test": 82, "did": 82, "la": 83, "r_": 83, "mathrm": 83, "leq": [83, 88], "feat_dim": 83, "arbitrari": 83, "choos": [83, 88], "dictionari": 83, "tha": 83, "arbitrarili": 83, "formul": 83, "quit": 83, "close": 83, "usual": 83, "suggest": 83, "refrain": 83, "tetrahedron": 83, "sparse_to_torch": 83, "rank_": 83, "rank_0": 83, "coadjacency_matrix": [83, 84], "h0": 83, "h1": 83, "h3": 83, "b3": 83, "tmx": [83, 84, 87], "x_3": 83, "tetrahedron_feat": 83, "track": 83, "rank_1": 83, "rank_2": 83, "rank_3": 83, "typic": 83, "6721": 83, "6333": 83, "6173": 83, "6110": 83, "5831": 83, "5695": 83, "5638": 83, "5493": 83, "5384": 83, "7667": 83, "5141": 83, "5201": 83, "5038": 83, "5016": 83, "4906": 83, "4763": 83, "4545": 83, "4483": 83, "4153": 83, "8000": 83, "4062": 83, "3790": 83, "8333": 83, "3916": 83, "3529": 83, "8667": 83, "2900": 83, "2359": 83, "9333": 83, "2002": 83, "9667": 83, "2970": 83, "9000": 83, "2032": 83, "2329": 83, "0272": 83, "0264": 83, "0245": 83, "0207": 83, "0165": 83, "0132": 83, "0114": 83, "0113": 83, "0117": 83, "0101": 83, "0081": 83, "0071": 83, "0065": 83, "0061": 83, "0057": 83, "0054": 83, "0050": 83, "0046": 83, "0043": 83, "0040": 83, "0038": 83, "0036": 83, "0034": 83, "0033": 83, "0032": 83, "0031": 83, "0030": 83, "0029": 83, "0028": 83, "0027": 83, "0026": 83, "0025": 83, "0024": 83, "0023": 83, "0022": 83, "0021": 83, "0020": 83, "0019": 83, "0018": 83, "0017": 83, "0016": 83, "0015": 83, "0014": 83, "0013": 83, "0012": 83, "0011": 83, "0010": 83, "0009": 83, "0008": 83, "account": 84, "_t": [84, 87], "p_d": [84, 87], "p_u": [84, 87], "likewis": 84, "essenti": 84, "_0": 84, "yet": [84, 87], "laplacian_0_list": [84, 87], "laplacian_up_1_list": [84, 87], "laplacian_2_list": [84, 87], "hodge_laplacian_matrix": [84, 87], "size_averag": 84, "in_linear_0": 84, "in_linear_1": 84, "in_linear_2": 84, "out_linear_0": 84, "out_linear_1": 84, "out_linear_2": 84, "_reduct": 84, "ret": 84, "laplacian_0_train": 84, "laplacian_0_test": 84, "laplacian_up_1_train": 84, "laplacian_up_1_test": 84, "laplacian_2_train": 84, "laplacian_2_test": 84, "117979": 84, "8366": 84, "8377": 84, "5059": 84, "4212": 84, "5435": 84, "9460": 84, "7320": 84, "8642": 84, "laplacian_up_2": [84, 87], "get_simplicial_featur": [84, 87], "which_feat": [84, 87], "elif": [84, 87], "binary_cross_entropi": 84, "6678": 84, "3241": 84, "2667": 84, "3333": 84, "coo_matrix": 85, "diag": 85, "return_count": 85, "normalize_higher_order_adj": 85, "a_opt": 85, "cochain": 85, "num_of_k_simplic": 85, "num_of_j_simplic": 85, "rowsum": 85, "r_inv_sqrt": 85, "flatten": 85, "isinf": 85, "r_mat_inv_sqrt": 85, "a_opt_to": 85, "dot": 85, "neigborood": 85, "ssconv": 85, "get_neighborhood": 85, "incidence_1_norm_list": 85, "incidence_2_norm_list": 85, "adjacency_up_0_norm_list": 85, "adjacency_up_1_norm_list": 85, "adjacency_down_1_norm_list": 85, "adjacency_down_2_norm_list": 85, "up_laplacian_1_list": 85, "up_laplacian_2_list": 85, "down_laplacian_1_list": 85, "down_laplacian_2_list": 85, "up_laplacian_1": 85, "up_laplacian_2": 85, "down_laplacian_1": 85, "down_laplacian_2": 85, "todo": 85, "kha053": 85, "nvml": 85, "incid1": 85, "incid1_norm": 85, "incid2": 85, "incid2_norm": 85, "adj0_up_norm": 85, "adj1_up_norm": 85, "adj1_down_norm": 85, "adj2_down_norm": 85, "correct_count": 85, "x_0t": 85, "x_1t": 85, "x_2t": 85, "incid1t": 85, "incid1_normt": 85, "incid2t": 85, "incid2_normt": 85, "adj0_up_normt": 85, "adj1_up_normt": 85, "adj1_down_normt": 85, "adj2_down_normt": 85, "yt": 85, "chose": 86, "reshap": 86, "normalized_laplacian_matrix": 86, "x_0s_train": 86, "x_0s_test": 86, "x_1s_train": 86, "x_1s_test": 86, "x_2s_train": 86, "x_2s_test": 86, "laplacian_0s_train": 86, "laplacian_0s_test": 86, "laplacian_1s_train": 86, "laplacian_1s_test": 86, "laplacian_2s_train": 86, "laplacian_2s_test": 86, "6056": 86, "2707": 86, "9831": 86, "8605": 86, "0164": 86, "0106": 86, "9957": 86, "0872": 86, "5802": 86, "itself": 87, "larger": 87, "x_train": 87, "x_test": 87, "laplacian_down_train": 87, "laplacian_down_test": 87, "laplacian_up_train": 87, "laplacian_up_test": 87, "simplex_order_select": 87, "0324": 87, "8604": 87, "5140": 87, "2026": 87, "5221": 87, "9650": 87, "8648": 87, "5553": 87, "maxim": 87, "chennel_edg": 87, "channel_fac": 87, "certain": 87, "classm": 87, "rm": 87, "nichola": 87, "santiago": 87, "segarra": 87, "inter_channel": 87, "channels_x": 87, "7314": 87, "7310": 87, "7306": 87, "7302": 87, "7297": 87, "spend": 88, "synthet": 88, "ahead": 88, "itertool": 88, "product": 88, "tnx": 88, "networkx": 88, "nx": 88, "random_split": 88, "spatial": 88, "distanc": 88, "seed": 88, "lt": 88, "_c": 88, "0x1664f0f50": 88, "cloud": 88, "insid": 88, "remov": 88, "centroid": 88, "sort": 88, "coordin": 88, "argsort": 88, "tri": 88, "disk_cent": 88, "disk_radiu": 88, "indices_includ": 88, "cdist": 88, "idx_dict": 88, "instanc": 88, "euclidean": 88, "shortest": 88, "plot_complex": 88, "plane": 88, "idx": 88, "poli": 88, "polygon": 88, "color": 88, "green": 88, "gca": 88, "add_patch": 88, "vstack": 88, "i_1": 88, "i_2": 88, "ldot": 88, "i_m": 88, "i_j": 88, "i_": 88, "ground": 88, "truth": 88, "supervis": 88, "setup": 88, "subsect": 88, "randomli": 88, "pick": 88, "triplet": 88, "around": 88, "anti": 88, "diagon": 88, "mid": 88, "region": 88, "start_nod": 88, "mid_nod": 88, "end_nod": 88, "all_triplet": 88, "increas": 88, "underli": 88, "distance_matrix": 88, "squareform": 88, "pdist": 88, "toarrai": 88, "from_numpy_arrai": 88, "path_1": 88, "shortest_path": 88, "path_2": 88, "plot_path": 88, "red": 88, "arrow": 88, "quiver": 88, "scale_unit": 88, "angl": 88, "yield": 88, "vectorized_trajectori": 88, "neigbors_mask": 88, "last_nod": 88, "turn": 88, "a_1": 88, "a_2": 88, "a_j": 88, "i_n": 88, "later": 88, "lookup": 88, "speed": 88, "edge_lookup_t": 88, "__getitem__": 88, "discard": 88, "neighbors_mask": 88, "__len__": 88, "c0": 88, "loader": 88, "batch_siz": 88, "val_siz": 88, "train_siz": 88, "train_d": 88, "val_d": 88, "test_d": 88, "train_dl": 88, "val_dl": 88, "test_dl": 88, "c_1": 88, "partial_1": 88, "c_0": 88, "That": 88, "hat": 88, "_m": 88, "neg": 88, "likelihood": 88, "penal": 88, "weight_decai": 88, "5e": 88, "loss_funct": 88, "nllloss": 88, "nll": 88, "training_histori": 88, "training_loss": 88, "traj": 88, "squeez": 88, "06": 88, "quick": 88, "confirm": 88, "everyth": 88, "reason": 88, "ax": 88, "ncol": 88, "figsiz": 88, "better": 88, "guess": 88, "3f": 88, "constructor": 88, "affect": 88, "capabl": 88, "revers": 88, "Or": 88, "ocean": 88, "drifter": 88}, "objects": {"topomodelx.base": [[0, 0, 0, "-", "aggregation"], [1, 0, 0, "-", "conv"], [3, 0, 0, "-", "message_passing"]], "topomodelx.base.aggregation": [[0, 1, 1, "", "Aggregation"]], "topomodelx.base.aggregation.Aggregation": [[0, 2, 1, "", "forward"], [0, 2, 1, "", "update"]], "topomodelx.base.conv": [[1, 1, 1, "", "Conv"]], "topomodelx.base.conv.Conv": [[1, 2, 1, "", "forward"], [1, 2, 1, "", "update"]], "topomodelx.base.message_passing": [[3, 1, 1, "", "MessagePassing"]], "topomodelx.base.message_passing.MessagePassing": [[3, 2, 1, "", "aggregate"], [3, 2, 1, "", "attention"], [3, 2, 1, "", "forward"], [3, 2, 1, "", "message"], [3, 2, 1, "", "reset_parameters"]], "topomodelx.nn.cell": [[5, 0, 0, "-", "can"], [6, 0, 0, "-", "can_layer"], [7, 0, 0, "-", "ccxn"], [8, 0, 0, "-", "ccxn_layer"], [9, 0, 0, "-", "cwn"], [10, 0, 0, "-", "cwn_layer"]], "topomodelx.nn.cell.can": [[5, 1, 1, "", "CAN"]], "topomodelx.nn.cell.can.CAN": [[5, 2, 1, "", "forward"]], "topomodelx.nn.cell.can_layer": [[6, 1, 1, "", "CANLayer"], [6, 1, 1, "", "LiftLayer"], [6, 1, 1, "", "MultiHeadCellAttention"], [6, 1, 1, "", "MultiHeadCellAttention_v2"], [6, 1, 1, "", "MultiHeadLiftLayer"], [6, 1, 1, "", "PoolLayer"], [6, 3, 1, "", "add_self_loops"], [6, 3, 1, "", "softmax"]], "topomodelx.nn.cell.can_layer.CANLayer": [[6, 2, 1, "", "forward"], [6, 2, 1, "", "reset_parameters"]], "topomodelx.nn.cell.can_layer.LiftLayer": [[6, 2, 1, "", "forward"], [6, 2, 1, "", "message"], [6, 2, 1, "", "reset_parameters"]], "topomodelx.nn.cell.can_layer.MultiHeadCellAttention": [[6, 2, 1, "", "attention"], [6, 2, 1, "", "forward"], [6, 2, 1, "", "message"], [6, 2, 1, "", "reset_parameters"]], "topomodelx.nn.cell.can_layer.MultiHeadCellAttention_v2": [[6, 2, 1, "", "attention"], [6, 2, 1, "", "forward"], [6, 2, 1, "", "message"], [6, 2, 1, "", "reset_parameters"]], "topomodelx.nn.cell.can_layer.MultiHeadLiftLayer": [[6, 2, 1, "", "forward"], [6, 2, 1, "", "reset_parameters"]], "topomodelx.nn.cell.can_layer.PoolLayer": [[6, 2, 1, "", "forward"], [6, 2, 1, "", "reset_parameters"]], "topomodelx.nn.cell.ccxn": [[7, 1, 1, "", "CCXN"]], "topomodelx.nn.cell.ccxn.CCXN": [[7, 2, 1, "", "forward"]], "topomodelx.nn.cell.ccxn_layer": [[8, 1, 1, "", "CCXNLayer"]], "topomodelx.nn.cell.ccxn_layer.CCXNLayer": [[8, 2, 1, "", "forward"]], "topomodelx.nn.cell.cwn": [[9, 1, 1, "", "CWN"]], "topomodelx.nn.cell.cwn.CWN": [[9, 2, 1, "", "forward"]], "topomodelx.nn.cell.cwn_layer": [[10, 1, 1, "", "CWNLayer"]], "topomodelx.nn.cell.cwn_layer.CWNLayer": [[10, 2, 1, "", "forward"]], "topomodelx.nn.hypergraph": [[12, 0, 0, "-", "allset"], [13, 0, 0, "-", "allset_layer"], [14, 0, 0, "-", "allset_transformer"], [15, 0, 0, "-", "allset_transformer_layer"], [18, 0, 0, "-", "hmpnn"], [19, 0, 0, "-", "hmpnn_layer"], [20, 0, 0, "-", "hnhn"], [21, 0, 0, "-", "hnhn_layer"], [22, 0, 0, "-", "hnhn_layer_bis"], [23, 0, 0, "-", "hypergat"], [24, 0, 0, "-", "hypergat_layer"], [25, 0, 0, "-", "hypersage"], [26, 0, 0, "-", "hypersage_layer"], [28, 0, 0, "-", "template_layer"], [29, 0, 0, "-", "unigcn"], [30, 0, 0, "-", "unigcn_layer"], [31, 0, 0, "-", "unigcnii"], [32, 0, 0, "-", "unigcnii_layer"], [33, 0, 0, "-", "unigin"], [34, 0, 0, "-", "unigin_layer"], [35, 0, 0, "-", "unisage"], [36, 0, 0, "-", "unisage_layer"]], "topomodelx.nn.hypergraph.allset": [[12, 1, 1, "", "AllSet"]], "topomodelx.nn.hypergraph.allset.AllSet": [[12, 2, 1, "", "forward"]], "topomodelx.nn.hypergraph.allset_layer": [[13, 1, 1, "", "AllSetBlock"], [13, 1, 1, "", "AllSetLayer"], [13, 1, 1, "", "MLP"]], "topomodelx.nn.hypergraph.allset_layer.AllSetBlock": [[13, 2, 1, "", "forward"], [13, 2, 1, "", "reset_parameters"]], "topomodelx.nn.hypergraph.allset_layer.AllSetLayer": [[13, 2, 1, "", "forward"], [13, 2, 1, "", "reset_parameters"]], "topomodelx.nn.hypergraph.allset_transformer": [[14, 1, 1, "", "AllSetTransformer"]], "topomodelx.nn.hypergraph.allset_transformer.AllSetTransformer": [[14, 2, 1, "", "forward"]], "topomodelx.nn.hypergraph.allset_transformer_layer": [[15, 1, 1, "", "AllSetTransformerBlock"], [15, 1, 1, "", "AllSetTransformerLayer"], [15, 1, 1, "", "MLP"], [15, 1, 1, "", "MultiHeadAttention"]], "topomodelx.nn.hypergraph.allset_transformer_layer.AllSetTransformerBlock": [[15, 2, 1, "", "forward"], [15, 2, 1, "", "reset_parameters"]], "topomodelx.nn.hypergraph.allset_transformer_layer.AllSetTransformerLayer": [[15, 2, 1, "", "forward"], [15, 2, 1, "", "reset_parameters"]], "topomodelx.nn.hypergraph.allset_transformer_layer.MultiHeadAttention": [[15, 2, 1, "", "attention"], [15, 2, 1, "", "forward"], [15, 2, 1, "", "reset_parameters"]], "topomodelx.nn.hypergraph.hmpnn": [[18, 1, 1, "", "HMPNN"]], "topomodelx.nn.hypergraph.hmpnn.HMPNN": [[18, 2, 1, "", "forward"]], "topomodelx.nn.hypergraph.hmpnn_layer": [[19, 1, 1, "", "HMPNNLayer"]], "topomodelx.nn.hypergraph.hmpnn_layer.HMPNNLayer": [[19, 2, 1, "", "apply_regular_dropout"], [19, 2, 1, "", "forward"]], "topomodelx.nn.hypergraph.hnhn": [[20, 1, 1, "", "HNHN"]], "topomodelx.nn.hypergraph.hnhn.HNHN": [[20, 2, 1, "", "forward"]], "topomodelx.nn.hypergraph.hnhn_layer": [[21, 1, 1, "", "HNHNLayer"]], "topomodelx.nn.hypergraph.hnhn_layer.HNHNLayer": [[21, 2, 1, "", "compute_normalization_matrices"], [21, 2, 1, "", "forward"], [21, 2, 1, "", "init_biases"], [21, 2, 1, "", "normalize_incidence_matrices"], [21, 2, 1, "", "reset_parameters"]], "topomodelx.nn.hypergraph.hnhn_layer_bis": [[22, 1, 1, "", "HNHNLayer"]], "topomodelx.nn.hypergraph.hnhn_layer_bis.HNHNLayer": [[22, 2, 1, "", "forward"]], "topomodelx.nn.hypergraph.hypergat": [[23, 1, 1, "", "HyperGAT"]], "topomodelx.nn.hypergraph.hypergat.HyperGAT": [[23, 2, 1, "", "forward"]], "topomodelx.nn.hypergraph.hypergat_layer": [[24, 1, 1, "", "HyperGATLayer"]], "topomodelx.nn.hypergraph.hypergat_layer.HyperGATLayer": [[24, 2, 1, "", "attention"], [24, 2, 1, "", "forward"], [24, 2, 1, "", "reset_parameters"], [24, 2, 1, "", "update"]], "topomodelx.nn.hypergraph.hypersage": [[25, 1, 1, "", "HyperSAGE"]], "topomodelx.nn.hypergraph.hypersage.HyperSAGE": [[25, 2, 1, "", "forward"]], "topomodelx.nn.hypergraph.hypersage_layer": [[26, 1, 1, "", "GeneralizedMean"], [26, 1, 1, "", "HyperSAGELayer"]], "topomodelx.nn.hypergraph.hypersage_layer.GeneralizedMean": [[26, 2, 1, "", "forward"]], "topomodelx.nn.hypergraph.hypersage_layer.HyperSAGELayer": [[26, 2, 1, "", "aggregate"], [26, 2, 1, "", "forward"], [26, 2, 1, "", "update"]], "topomodelx.nn.hypergraph.template_layer": [[28, 1, 1, "", "TemplateLayer"]], "topomodelx.nn.hypergraph.template_layer.TemplateLayer": [[28, 2, 1, "", "forward"], [28, 2, 1, "", "reset_parameters"]], "topomodelx.nn.hypergraph.unigcn": [[29, 1, 1, "", "UniGCN"]], "topomodelx.nn.hypergraph.unigcn.UniGCN": [[29, 2, 1, "", "forward"]], "topomodelx.nn.hypergraph.unigcn_layer": [[30, 1, 1, "", "UniGCNLayer"]], "topomodelx.nn.hypergraph.unigcn_layer.UniGCNLayer": [[30, 2, 1, "", "forward"], [30, 2, 1, "", "reset_parameters"]], "topomodelx.nn.hypergraph.unigcnii": [[31, 1, 1, "", "UniGCNII"]], "topomodelx.nn.hypergraph.unigcnii.UniGCNII": [[31, 2, 1, "", "forward"]], "topomodelx.nn.hypergraph.unigcnii_layer": [[32, 1, 1, "", "UniGCNIILayer"]], "topomodelx.nn.hypergraph.unigcnii_layer.UniGCNIILayer": [[32, 2, 1, "", "forward"], [32, 2, 1, "", "reset_parameters"]], "topomodelx.nn.hypergraph.unigin": [[33, 1, 1, "", "UniGIN"]], "topomodelx.nn.hypergraph.unigin.UniGIN": [[33, 2, 1, "", "forward"]], "topomodelx.nn.hypergraph.unigin_layer": [[34, 1, 1, "", "UniGINLayer"]], "topomodelx.nn.hypergraph.unigin_layer.UniGINLayer": [[34, 2, 1, "", "forward"]], "topomodelx.nn.hypergraph.unisage": [[35, 1, 1, "", "UniSAGE"]], "topomodelx.nn.hypergraph.unisage.UniSAGE": [[35, 2, 1, "", "forward"]], "topomodelx.nn.hypergraph.unisage_layer": [[36, 1, 1, "", "UniSAGELayer"]], "topomodelx.nn.hypergraph.unisage_layer.UniSAGELayer": [[36, 2, 1, "", "forward"], [36, 2, 1, "", "reset_parameters"]], "topomodelx.nn.simplicial": [[38, 0, 0, "-", "dist2cycle"], [39, 0, 0, "-", "dist2cycle_layer"], [40, 0, 0, "-", "hsn"], [41, 0, 0, "-", "hsn_layer"], [43, 0, 0, "-", "san"], [44, 0, 0, "-", "san_layer"], [45, 0, 0, "-", "sca_cmps"], [46, 0, 0, "-", "sca_cmps_layer"], [47, 0, 0, "-", "sccn"], [48, 0, 0, "-", "sccn_layer"], [49, 0, 0, "-", "sccnn"], [50, 0, 0, "-", "sccnn_layer"], [51, 0, 0, "-", "scconv"], [52, 0, 0, "-", "scconv_layer"], [53, 0, 0, "-", "scn2"], [54, 0, 0, "-", "scn2_layer"], [55, 0, 0, "-", "scnn"], [56, 0, 0, "-", "scnn_layer"], [57, 0, 0, "-", "scone"], [58, 0, 0, "-", "scone_layer"]], "topomodelx.nn.simplicial.dist2cycle": [[38, 1, 1, "", "Dist2Cycle"]], "topomodelx.nn.simplicial.dist2cycle.Dist2Cycle": [[38, 2, 1, "", "forward"]], "topomodelx.nn.simplicial.dist2cycle_layer": [[39, 1, 1, "", "Dist2CycleLayer"]], "topomodelx.nn.simplicial.dist2cycle_layer.Dist2CycleLayer": [[39, 2, 1, "", "forward"], [39, 2, 1, "", "reset_parameters"]], "topomodelx.nn.simplicial.hsn": [[40, 1, 1, "", "HSN"]], "topomodelx.nn.simplicial.hsn.HSN": [[40, 2, 1, "", "forward"]], "topomodelx.nn.simplicial.hsn_layer": [[41, 1, 1, "", "HSNLayer"]], "topomodelx.nn.simplicial.hsn_layer.HSNLayer": [[41, 2, 1, "", "forward"], [41, 2, 1, "", "reset_parameters"]], "topomodelx.nn.simplicial.san": [[43, 1, 1, "", "SAN"]], "topomodelx.nn.simplicial.san.SAN": [[43, 2, 1, "", "compute_projection_matrix"], [43, 2, 1, "", "forward"]], "topomodelx.nn.simplicial.san_layer": [[44, 1, 1, "", "SANConv"], [44, 1, 1, "", "SANLayer"]], "topomodelx.nn.simplicial.san_layer.SANConv": [[44, 2, 1, "", "forward"]], "topomodelx.nn.simplicial.san_layer.SANLayer": [[44, 2, 1, "", "forward"], [44, 2, 1, "", "reset_parameters"]], "topomodelx.nn.simplicial.sca_cmps": [[45, 1, 1, "", "SCACMPS"]], "topomodelx.nn.simplicial.sca_cmps.SCACMPS": [[45, 2, 1, "", "forward"]], "topomodelx.nn.simplicial.sca_cmps_layer": [[46, 1, 1, "", "SCACMPSLayer"]], "topomodelx.nn.simplicial.sca_cmps_layer.SCACMPSLayer": [[46, 2, 1, "", "forward"], [46, 2, 1, "", "intra_aggr"], [46, 2, 1, "", "reset_parameters"], [46, 2, 1, "", "weight_func"]], "topomodelx.nn.simplicial.sccn": [[47, 1, 1, "", "SCCN"]], "topomodelx.nn.simplicial.sccn.SCCN": [[47, 2, 1, "", "forward"]], "topomodelx.nn.simplicial.sccn_layer": [[48, 1, 1, "", "SCCNLayer"]], "topomodelx.nn.simplicial.sccn_layer.SCCNLayer": [[48, 2, 1, "", "forward"], [48, 2, 1, "", "reset_parameters"]], "topomodelx.nn.simplicial.sccnn": [[49, 1, 1, "", "SCCNN"], [49, 1, 1, "", "SCCNNComplex"]], "topomodelx.nn.simplicial.sccnn.SCCNN": [[49, 2, 1, "", "forward"]], "topomodelx.nn.simplicial.sccnn.SCCNNComplex": [[49, 2, 1, "", "forward"]], "topomodelx.nn.simplicial.sccnn_layer": [[50, 1, 1, "", "SCCNNLayer"]], "topomodelx.nn.simplicial.sccnn_layer.SCCNNLayer": [[50, 2, 1, "", "aggr_norm_func"], [50, 2, 1, "", "chebyshev_conv"], [50, 2, 1, "", "forward"], [50, 2, 1, "", "reset_parameters"], [50, 2, 1, "", "update"]], "topomodelx.nn.simplicial.scconv": [[51, 1, 1, "", "SCConv"]], "topomodelx.nn.simplicial.scconv.SCConv": [[51, 2, 1, "", "forward"]], "topomodelx.nn.simplicial.scconv_layer": [[52, 1, 1, "", "SCConvLayer"]], "topomodelx.nn.simplicial.scconv_layer.SCConvLayer": [[52, 2, 1, "", "forward"], [52, 2, 1, "", "reset_parameters"]], "topomodelx.nn.simplicial.scn2": [[53, 1, 1, "", "SCN2"]], "topomodelx.nn.simplicial.scn2.SCN2": [[53, 2, 1, "", "forward"]], "topomodelx.nn.simplicial.scn2_layer": [[54, 1, 1, "", "SCN2Layer"]], "topomodelx.nn.simplicial.scn2_layer.SCN2Layer": [[54, 2, 1, "", "forward"], [54, 2, 1, "", "reset_parameters"]], "topomodelx.nn.simplicial.scnn": [[55, 1, 1, "", "SCNN"]], "topomodelx.nn.simplicial.scnn.SCNN": [[55, 2, 1, "", "forward"]], "topomodelx.nn.simplicial.scnn_layer": [[56, 1, 1, "", "SCNNLayer"]], "topomodelx.nn.simplicial.scnn_layer.SCNNLayer": [[56, 2, 1, "", "aggr_norm_func"], [56, 2, 1, "", "chebyshev_conv"], [56, 2, 1, "", "forward"], [56, 2, 1, "", "reset_parameters"], [56, 2, 1, "", "update"]], "topomodelx.nn.simplicial.scone": [[57, 1, 1, "", "SCoNe"], [57, 1, 1, "", "TrajectoriesDataset"], [57, 3, 1, "", "generate_complex"], [57, 3, 1, "", "generate_trajectories"]], "topomodelx.nn.simplicial.scone.SCoNe": [[57, 2, 1, "", "forward"]], "topomodelx.nn.simplicial.scone.TrajectoriesDataset": [[57, 2, 1, "", "vectorize_path"]], "topomodelx.nn.simplicial.scone_layer": [[58, 1, 1, "", "SCoNeLayer"]], "topomodelx.nn.simplicial.scone_layer.SCoNeLayer": [[58, 2, 1, "", "forward"], [58, 2, 1, "", "reset_parameters"]], "topomodelx.utils": [[59, 0, 0, "-", "scatter"]], "topomodelx.utils.scatter": [[59, 3, 1, "", "broadcast"], [59, 3, 1, "", "scatter"], [59, 3, 1, "", "scatter_add"], [59, 3, 1, "", "scatter_mean"], [59, 3, 1, "", "scatter_sum"]]}, "objtypes": {"0": "py:module", "1": "py:class", "2": "py:method", "3": "py:function"}, "objnames": {"0": ["py", "module", "Python module"], "1": ["py", "class", "Python class"], "2": ["py", "method", "Python method"], "3": ["py", "function", "Python function"]}, "titleterms": {"aggreg": 0, "conv": 1, "base": 2, "messag": [3, 69, 82], "pass": [3, 69, 82], "api": 4, "refer": [4, 62, 86], "packag": 4, "modul": 4, "can": [5, 63], "can_lay": 6, "ccxn": [7, 64], "ccxn_layer": 8, "cwn": [9, 65], "cwn_layer": 10, "cell": [11, 63, 64], "allset": 12, "allset_lay": 13, "allset_transform": 14, "allset_transformer_lay": 15, "dhgcn": [16, 68], "dhgcn_layer": 17, "hmpnn": [18, 69], "hmpnn_layer": 19, "hnhn": [20, 70, 71], "hnhn_layer": 21, "hnhn_layer_bi": 22, "hypergat": 23, "hypergat_lay": 24, "hypersag": [25, 73], "hypersage_lay": 26, "hypergraph": [27, 68, 69, 70, 71, 72, 74, 76, 89], "template_lay": 28, "unigcn": [29, 75], "unigcn_lay": 30, "unigcnii": [31, 76], "unigcnii_lay": 32, "neural": [37, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89], "network": [37, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88], "simplici": [42, 79, 80, 81, 82, 83, 84, 85, 87, 88, 89], "util": 59, "icml": 60, "2023": 60, "topolog": [60, 89], "deep": 60, "learn": 60, "challeng": 60, "descript": 60, "public": 60, "outcom": 60, "particip": 60, "deadlin": 60, "how": 60, "submit": 60, "guidelin": 60, "submiss": 60, "requir": 60, "evalu": [60, 88], "question": 60, "contribut": 61, "make": 61, "chang": 61, "write": 61, "test": [61, 82, 85, 87, 88], "run": 61, "document": 61, "intro": 61, "docstr": 61, "The": [61, 63, 64, 65, 81], "anatomi": 61, "exampl": 61, "topomodelx": 62, "tmx": 62, "get": 62, "start": 62, "train": [63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88], "attent": [63, 64, 81], "abstract": [63, 81], "task": [63, 64, 65, 81], "set": [63, 64, 65, 66, 67], "up": [63, 64, 65], "pre": [63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87], "process": [63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87], "creat": [63, 64, 65, 66, 68, 69, 70, 71, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 87, 88], "convolut": [64, 83, 84, 85, 86, 87], "complex": [64, 82, 83, 84, 85, 88, 89], "cw": 65, "an": [66, 67], "all": [66, 67], "tnn": [66, 67, 68, 73, 75, 77, 78], "addit": [66, 67, 73], "theoret": [66, 67, 73], "clarif": [66, 67, 73], "transform": 67, "defin": [67, 68, 70, 72, 73, 74, 79, 80, 83, 84, 85, 86, 87], "import": [68, 70, 72, 74, 75, 77, 78, 79, 80, 82, 83, 84, 85, 86, 87], "data": [68, 72, 74, 75, 76, 77, 78, 82, 85, 88], "neighborhood": [68, 70, 72, 74, 79, 80, 83, 84, 86, 87], "structur": [68, 70, 72, 74, 79, 80, 83, 85, 86], "lift": [68, 72, 74], "domain": [68, 72, 74], "hyperedg": [70, 71], "neuron": [70, 71], "dataset": [70, 79, 80, 83, 84, 85, 86, 87, 88], "signal": [70, 79, 80, 83, 84, 87], "templat": 74, "us": 76, "layer": 76, "load": 76, "unigin": 77, "uni": 78, "sage": 78, "homologi": 79, "local": 79, "dist2cycl": 79, "binari": [79, 80, 83, 84, 87], "label": [79, 80, 83, 84, 87], "featur": 79, "high": 80, "skip": 80, "hsn": 80, "san": 81, "autoencod": 82, "sca": 82, "coadjac": 82, "scheme": 82, "cmp": 82, "prepar": [82, 85, 87], "input": 82, "each": 82, "split": [82, 87], "model": [82, 84, 87, 88], "sccn": 83, "sccnn": 84, "we": [84, 87], "perform": [84, 87], "1": [84, 87], "classif": [84, 87], "shrec": 84, "strcture": [84, 87], "2": [84, 85, 86, 87], "node": [84, 87], "scconv": 85, "helper": 85, "function": 85, "neighbourhood": 85, "simplex": 86, "scn": 86, "rank": 86, "scnn": 87, "compl": 87, "karat": 87, "weight": 87, "hodg": 87, "laplacian": 87, "net": [88, 89], "scone": 88, "tabl": 88, "content": 88, "gener": 88, "trajectori": 88, "pytorch": 88, "dataload": 88, "suggest": 88, "further": 88, "experiment": 88, "tutori": 89, "cellular": 89}, "envversion": {"sphinx.domains.c": 3, "sphinx.domains.changeset": 1, "sphinx.domains.citation": 1, "sphinx.domains.cpp": 9, "sphinx.domains.index": 1, "sphinx.domains.javascript": 3, "sphinx.domains.math": 2, "sphinx.domains.python": 4, "sphinx.domains.rst": 2, "sphinx.domains.std": 2, "nbsphinx": 4, "sphinx.ext.viewcode": 1, "sphinx": 60}, "alltitles": {"Aggregation": [[0, "module-topomodelx.base.aggregation"]], "Conv": [[1, "module-topomodelx.base.conv"]], "Base": [[2, "base"]], "Message Passing": [[3, "module-topomodelx.base.message_passing"]], "API Reference": [[4, "api-reference"]], "Packages & Modules": [[4, null]], "CAN": [[5, "module-topomodelx.nn.cell.can"]], "Can_Layer": [[6, "module-topomodelx.nn.cell.can_layer"]], "CCXN": [[7, "module-topomodelx.nn.cell.ccxn"]], "CCXN_Layer": [[8, "module-topomodelx.nn.cell.ccxn_layer"]], "CWN": [[9, "module-topomodelx.nn.cell.cwn"]], "Cwn_Layer": [[10, "module-topomodelx.nn.cell.cwn_layer"]], "Cell": [[11, "cell"]], "AllSet": [[12, "module-topomodelx.nn.hypergraph.allset"]], "AllSet_Layer": [[13, "module-topomodelx.nn.hypergraph.allset_layer"]], "AllSet_Transformer": [[14, "module-topomodelx.nn.hypergraph.allset_transformer"]], "AllSet_Transformer_Layer": [[15, "module-topomodelx.nn.hypergraph.allset_transformer_layer"]], "DHGCN": [[16, "dhgcn"]], "DHGCN_Layer": [[17, "dhgcn-layer"]], "HMPNN": [[18, "module-topomodelx.nn.hypergraph.hmpnn"]], "HMPNN_Layer": [[19, "module-topomodelx.nn.hypergraph.hmpnn_layer"]], "HNHN": [[20, "module-topomodelx.nn.hypergraph.hnhn"]], "HNHN_Layer": [[21, "module-topomodelx.nn.hypergraph.hnhn_layer"]], "HNHN_Layer_Bis": [[22, "module-topomodelx.nn.hypergraph.hnhn_layer_bis"]], "Hypergat": [[23, "module-topomodelx.nn.hypergraph.hypergat"]], "Hypergat_Layer": [[24, "module-topomodelx.nn.hypergraph.hypergat_layer"]], "Hypersage": [[25, "module-topomodelx.nn.hypergraph.hypersage"]], "Hypersage_Layer": [[26, "module-topomodelx.nn.hypergraph.hypersage_layer"]], "Hypergraph": [[27, "hypergraph"]], "Template_Layer": [[28, "module-topomodelx.nn.hypergraph.template_layer"]], "Unigcn": [[29, "module-topomodelx.nn.hypergraph.unigcn"]], "Unigcn_Layer": [[30, "module-topomodelx.nn.hypergraph.unigcn_layer"]], "Unigcnii": [[31, "module-topomodelx.nn.hypergraph.unigcnii"]], "Unigcnii_Layer": [[32, "module-topomodelx.nn.hypergraph.unigcnii_layer"]], "Neural Networks": [[37, "neural-networks"]], "Simplicial": [[42, "simplicial"]], "Utils": [[59, "utils"]], "ICML 2023 Topological Deep Learning Challenge": [[60, "icml-2023-topological-deep-learning-challenge"]], "Description of the Challenge": [[60, "description-of-the-challenge"]], "\u2b50\ufe0f Publication Outcomes for Participants \u2b50\ufe0f": [[60, "publication-outcomes-for-participants"]], "Deadline": [[60, "deadline"]], "How to Submit": [[60, "how-to-submit"]], "Guidelines": [[60, "guidelines"]], "Submission Requirements": [[60, "submission-requirements"]], "Evaluation": [[60, "evaluation"]], "Questions": [[60, "questions"]], "Contributing": [[61, "contributing"]], "Making Changes": [[61, "making-changes"]], "Write Tests": [[61, "write-tests"]], "Run Tests": [[61, "run-tests"]], "Write Documentation": [[61, "write-documentation"]], "Intro to Docstrings": [[61, "intro-to-docstrings"]], "The Anatomy of a Docstring": [[61, "the-anatomy-of-a-docstring"]], "Docstring Examples": [[61, "docstring-examples"]], "\ud83c\udf10 TopoModelX (TMX) \ud83c\udf69": [[62, "topomodelx-tmx"]], "\ud83d\udd0d References": [[62, "references"]], "\ud83e\uddbe Getting Started": [[62, "getting-started"]], "Train a Cell Attention Network (CAN)": [[63, "Train-a-Cell-Attention-Network-(CAN)"]], "Abstract:": [[63, "Abstract:"]], "The Neural Network:": [[63, "The-Neural-Network:"], [64, "The-Neural-Network:"], [65, "The-Neural-Network:"]], "The Task:": [[63, "The-Task:"], [64, "The-Task:"], [65, "The-Task:"], [81, "The-Task:"]], "Set-up": [[63, "Set-up"], [64, "Set-up"], [65, "Set-up"]], "Pre-processing": [[63, "Pre-processing"], [64, "Pre-processing"], [65, "Pre-processing"], [66, "Pre-processing"], [67, "Pre-processing"], [68, "Pre-processing"], [69, "Pre-processing"], [70, "Pre-processing"], [71, "Pre-processing"], [72, "Pre-processing"], [73, "Pre-processing"], [74, "Pre-processing"], [75, "Pre-processing"], [77, "Pre-processing"], [78, "Pre-processing"], [79, "Pre-processing"], [80, "Pre-processing"], [81, "Pre-processing"], [82, "Pre-processing"], [83, "Pre-processing"], [84, "Pre-processing"], [85, "Pre-processing"], [86, "Pre-processing"], [87, "Pre-processing"]], "Create the Neural Network": [[63, "Create-the-Neural-Network"], [64, "Create-the-Neural-Network"], [65, "Create-the-Neural-Network"], [66, "Create-the-Neural-Network"], [68, "Create-the-Neural-Network"], [69, "Create-the-Neural-Network"], [70, "Create-the-Neural-Network"], [71, "Create-the-Neural-Network"], [74, "Create-the-Neural-Network"], [75, "Create-the-Neural-Network"], [77, "Create-the-Neural-Network"], [78, "Create-the-Neural-Network"], [79, "Create-the-Neural-Network"], [80, "Create-the-Neural-Network"], [81, "Create-the-Neural-Network"], [83, "Create-the-Neural-Network"]], "Train the Neural Network": [[63, "Train-the-Neural-Network"], [64, "Train-the-Neural-Network"], [65, "Train-the-Neural-Network"], [66, "Train-the-Neural-Network"], [67, "Train-the-Neural-Network"], [68, "Train-the-Neural-Network"], [69, "Train-the-Neural-Network"], [70, "Train-the-Neural-Network"], [71, "Train-the-Neural-Network"], [72, "Train-the-Neural-Network"], [73, "Train-the-Neural-Network"], [74, "Train-the-Neural-Network"], [75, "Train-the-Neural-Network"], [77, "Train-the-Neural-Network"], [78, "Train-the-Neural-Network"], [79, "Train-the-Neural-Network"], [80, "Train-the-Neural-Network"], [81, "Train-the-Neural-Network"], [83, "Train-the-Neural-Network"], [86, "Train-the-Neural-Network"], [87, "Train-the-Neural-Network"]], "Train a Convolutional Cell Complex Network (CCXN)": [[64, "Train-a-Convolutional-Cell-Complex-Network-(CCXN)"]], "Train the Neural Network with Attention": [[64, "Train-the-Neural-Network-with-Attention"]], "Train a CW Network (CWN)": [[65, "Train-a-CW-Network-(CWN)"]], "Train an All-Set TNN": [[66, "Train-an-All-Set-TNN"]], "Additional theoretical clarifications": [[66, "Additional-theoretical-clarifications"], [67, "Additional-theoretical-clarifications"], [73, "Additional-theoretical-clarifications"]], "Train an All-Set-Transformer TNN": [[67, "Train-an-All-Set-Transformer-TNN"]], "Define the Neural Network": [[67, "Define-the-Neural-Network"], [72, "Define-the-Neural-Network"], [73, "Define-the-Neural-Network"]], "Train a DHGCN TNN": [[68, "Train-a-DHGCN-TNN"]], "Import data": [[68, "Import-data"], [72, "Import-data"], [74, "Import-data"], [75, "Import-data"], [77, "Import-data"], [78, "Import-data"], [82, "Import-data"]], "Define neighborhood structures and lift into hypergraph domain.": [[68, "Define-neighborhood-structures-and-lift-into-hypergraph-domain."], [72, "Define-neighborhood-structures-and-lift-into-hypergraph-domain."], [74, "Define-neighborhood-structures-and-lift-into-hypergraph-domain."]], "Train a Hypergraph Message Passing Neural Network (HMPNN)": [[69, "Train-a-Hypergraph-Message-Passing-Neural-Network-(HMPNN)"]], "Train a Hypergraph Networks with Hyperedge Neurons (HNHN)": [[70, "Train-a-Hypergraph-Networks-with-Hyperedge-Neurons-(HNHN)"]], "Import dataset": [[70, "Import-dataset"], [79, "Import-dataset"], [80, "Import-dataset"], [83, "Import-dataset"], [85, "Import-dataset"], [86, "Import-dataset"]], "Define neighborhood structures.": [[70, "Define-neighborhood-structures."], [79, "Define-neighborhood-structures."], [80, "Define-neighborhood-structures."], [83, "Define-neighborhood-structures."], [86, "Define-neighborhood-structures."]], "Import signal": [[70, "Import-signal"], [79, "Import-signal"], [80, "Import-signal"], [83, "Import-signal"], [84, "Import-signal"]], "Train a Hypergraph Network with Hyperedge Neurons (HNHN)": [[71, "Train-a-Hypergraph-Network-with-Hyperedge-Neurons-(HNHN)"]], "Train a Hypergraph Neural Network": [[72, "Train-a-Hypergraph-Neural-Network"]], "Train a Hypersage TNN": [[73, "Train-a-Hypersage-TNN"]], "Train a (template) Hypergraph Neural Network": [[74, "Train-a-(template)-Hypergraph-Neural-Network"]], "Train a UNIGCN TNN": [[75, "Train-a-UNIGCN-TNN"]], "Train a hypergraph neural network using UniGCNII layers": [[76, "Train-a-hypergraph-neural-network-using-UniGCNII-layers"]], "Loading the data": [[76, "Loading-the-data"]], "Creating a neural network": [[76, "Creating-a-neural-network"]], "Training the neural network": [[76, "Training-the-neural-network"]], "Train a UNIGIN TNN": [[77, "Train-a-UNIGIN-TNN"]], "Train a Uni-sage TNN": [[78, "Train-a-Uni-sage-TNN"]], "Train a Simplicial Neural Network for Homology Localization (Dist2Cycle)": [[79, "Train-a-Simplicial-Neural-Network-for-Homology-Localization-(Dist2Cycle)"]], "Define binary labels": [[79, "Define-binary-labels"], [80, "Define-binary-labels"], [83, "Define-binary-labels"], [84, "Define-binary-labels"]], "Create Features": [[79, "Create-Features"]], "Train a Simplicial High-Skip Network (HSN)": [[80, "Train-a-Simplicial-High-Skip-Network-(HSN)"]], "Train a Simplicial Attention Network (SAN)": [[81, "Train-a-Simplicial-Attention-Network-(SAN)"]], "Abstract": [[81, "Abstract"]], "The Neural Network": [[81, "The-Neural-Network"]], "Train a Simplicial Complex Autoencoder (SCA) with Coadjacency Message Passing Scheme (CMPS)": [[82, "Train-a-Simplicial-Complex-Autoencoder-(SCA)-with-Coadjacency-Message-Passing-Scheme-(CMPS)"]], "Preparing the inputs to test each message passing scheme:": [[82, "Preparing-the-inputs-to-test-each-message-passing-scheme:"]], "Coadjacency Message Passing Scheme (CMPS):": [[82, "Coadjacency-Message-Passing-Scheme-(CMPS):"]], "Create the Neural Networks": [[82, "Create-the-Neural-Networks"]], "Train and Test Split": [[82, "Train-and-Test-Split"]], "Training and Testing Model": [[82, "Training-and-Testing-Model"]], "Train a Simplicial Complex Convolutional Network (SCCN)": [[83, "Train-a-Simplicial-Complex-Convolutional-Network-(SCCN)"]], "Train a SCCNN": [[84, "Train-a-SCCNN"]], "We train the model to perform:": [[84, "We-train-the-model-to-perform:"], [87, "We-train-the-model-to-perform:"]], "Simplicial Complex Convolutional Neural Networks [SCCNN]": [[84, "Simplicial-Complex-Convolutional-Neural-Networks-[SCCNN]"]], "1. Complex Classification": [[84, "1.-Complex-Classification"]], "Import shrec dataset": [[84, "Import-shrec-dataset"]], "Define Neighborhood Strctures": [[84, "Define-Neighborhood-Strctures"], [84, "id1"], [87, "Define-Neighborhood-Strctures"], [87, "id1"]], "Create and Train the Neural Network": [[84, "Create-and-Train-the-Neural-Network"], [84, "id2"], [85, "Create-and-Train-the-Neural-Network"]], "2. Node Classification": [[84, "2.-Node-Classification"], [87, "2.-Node-Classification"]], "Train a Simplicial 2-complex convolutional neural network (SCConv)": [[85, "Train-a-Simplicial-2-complex-convolutional-neural-network-(SCConv)"]], "Helper functions": [[85, "Helper-functions"]], "Define Neighbourhood Structures": [[85, "Define-Neighbourhood-Structures"]], "prepare training and test data": [[85, "prepare-training-and-test-data"]], "Train a Simplex Convolutional Network (SCN) of Rank 2": [[86, "Train-a-Simplex-Convolutional-Network-(SCN)-of-Rank-2"]], "References": [[86, "References"]], "Train a Simplicial Convolutional Neural Network (SCNN)": [[87, "Train-a-Simplicial-Convolutional-Neural-Network-(SCNN)"]], "Simplicial Convolutional Neural Networks [SCNN]": [[87, "Simplicial-Convolutional-Neural-Networks-[SCNN]"]], "1. Comples Classification": [[87, "1.-Comples-Classification"]], "Import Karate dataset": [[87, "Import-Karate-dataset"]], "Weighted Hodge Laplacians": [[87, "Weighted-Hodge-Laplacians"]], "Import signals": [[87, "Import-signals"]], "Define binary labels and Prepare the training-testing split": [[87, "Define-binary-labels-and-Prepare-the-training-testing-split"]], "Create the SCNN for node classification": [[87, "Create-the-SCNN-for-node-classification"]], "Train the SCNN": [[87, "Train-the-SCNN"]], "Train a Simplicial Complex Net (SCoNe)": [[88, "Train-a-Simplicial-Complex-Net-(SCoNe)"]], "Table of contents": [[88, "Table-of-contents"]], "Dataset generation": [[88, "Dataset-generation"]], "Generating trajectories": [[88, "Generating-trajectories"]], "Creating PyTorch dataloaders": [[88, "Creating-PyTorch-dataloaders"]], "Creating the Neural Network": [[88, "Creating-the-Neural-Network"]], "Training the Neural Network": [[88, "Training-the-Neural-Network"]], "Evaluating the model on test data": [[88, "Evaluating-the-model-on-test-data"]], "Suggestions for further experimentation": [[88, "Suggestions-for-further-experimentation"]], "Tutorials": [[89, "tutorials"]], "Tutorials for Topological Neural Nets on Cellular Complex": [[89, "tutorials-for-topological-neural-nets-on-cellular-complex"]], "Tutorials for Topological Neural Nets on Hypergraphs": [[89, "tutorials-for-topological-neural-nets-on-hypergraphs"]], "Tutorials for Topological Neural Nets on Simplicial Complex": [[89, "tutorials-for-topological-neural-nets-on-simplicial-complex"]]}, "indexentries": {"aggregation (class in topomodelx.base.aggregation)": [[0, "topomodelx.base.aggregation.Aggregation"]], "forward() (topomodelx.base.aggregation.aggregation method)": [[0, "topomodelx.base.aggregation.Aggregation.forward"]], "module": [[0, "module-topomodelx.base.aggregation"], [1, "module-topomodelx.base.conv"], [3, "module-topomodelx.base.message_passing"], [5, "module-topomodelx.nn.cell.can"], [6, "module-topomodelx.nn.cell.can_layer"], [7, "module-topomodelx.nn.cell.ccxn"], [8, "module-topomodelx.nn.cell.ccxn_layer"], [9, "module-topomodelx.nn.cell.cwn"], [10, "module-topomodelx.nn.cell.cwn_layer"], [12, "module-topomodelx.nn.hypergraph.allset"], [13, "module-topomodelx.nn.hypergraph.allset_layer"], [14, "module-topomodelx.nn.hypergraph.allset_transformer"], [15, "module-topomodelx.nn.hypergraph.allset_transformer_layer"], [18, "module-topomodelx.nn.hypergraph.hmpnn"], [19, "module-topomodelx.nn.hypergraph.hmpnn_layer"], [20, "module-topomodelx.nn.hypergraph.hnhn"], [21, "module-topomodelx.nn.hypergraph.hnhn_layer"], [22, "module-topomodelx.nn.hypergraph.hnhn_layer_bis"], [23, "module-topomodelx.nn.hypergraph.hypergat"], [24, "module-topomodelx.nn.hypergraph.hypergat_layer"], [25, "module-topomodelx.nn.hypergraph.hypersage"], [26, "module-topomodelx.nn.hypergraph.hypersage_layer"], [28, "module-topomodelx.nn.hypergraph.template_layer"], [29, "module-topomodelx.nn.hypergraph.unigcn"], [30, "module-topomodelx.nn.hypergraph.unigcn_layer"], [31, "module-topomodelx.nn.hypergraph.unigcnii"], [32, "module-topomodelx.nn.hypergraph.unigcnii_layer"], [33, "module-topomodelx.nn.hypergraph.unigin"], [34, "module-topomodelx.nn.hypergraph.unigin_layer"], [35, "module-topomodelx.nn.hypergraph.unisage"], [36, "module-topomodelx.nn.hypergraph.unisage_layer"], [38, "module-topomodelx.nn.simplicial.dist2cycle"], [39, "module-topomodelx.nn.simplicial.dist2cycle_layer"], [40, "module-topomodelx.nn.simplicial.hsn"], [41, "module-topomodelx.nn.simplicial.hsn_layer"], [43, "module-topomodelx.nn.simplicial.san"], [44, "module-topomodelx.nn.simplicial.san_layer"], [45, "module-topomodelx.nn.simplicial.sca_cmps"], [46, "module-topomodelx.nn.simplicial.sca_cmps_layer"], [47, "module-topomodelx.nn.simplicial.sccn"], [48, "module-topomodelx.nn.simplicial.sccn_layer"], [49, "module-topomodelx.nn.simplicial.sccnn"], [50, "module-topomodelx.nn.simplicial.sccnn_layer"], [51, "module-topomodelx.nn.simplicial.scconv"], [52, "module-topomodelx.nn.simplicial.scconv_layer"], [53, "module-topomodelx.nn.simplicial.scn2"], [54, "module-topomodelx.nn.simplicial.scn2_layer"], [55, "module-topomodelx.nn.simplicial.scnn"], [56, "module-topomodelx.nn.simplicial.scnn_layer"], [57, "module-topomodelx.nn.simplicial.scone"], [58, "module-topomodelx.nn.simplicial.scone_layer"], [59, "module-topomodelx.utils.scatter"]], "topomodelx.base.aggregation": [[0, "module-topomodelx.base.aggregation"]], "update() (topomodelx.base.aggregation.aggregation method)": [[0, "topomodelx.base.aggregation.Aggregation.update"]], "conv (class in topomodelx.base.conv)": [[1, "topomodelx.base.conv.Conv"]], "forward() (topomodelx.base.conv.conv method)": [[1, "topomodelx.base.conv.Conv.forward"]], "topomodelx.base.conv": [[1, "module-topomodelx.base.conv"]], "update() (topomodelx.base.conv.conv method)": [[1, "topomodelx.base.conv.Conv.update"]], "messagepassing (class in topomodelx.base.message_passing)": [[3, "topomodelx.base.message_passing.MessagePassing"]], "aggregate() (topomodelx.base.message_passing.messagepassing method)": [[3, "topomodelx.base.message_passing.MessagePassing.aggregate"]], "attention() (topomodelx.base.message_passing.messagepassing method)": [[3, "topomodelx.base.message_passing.MessagePassing.attention"]], "forward() (topomodelx.base.message_passing.messagepassing method)": [[3, "topomodelx.base.message_passing.MessagePassing.forward"]], "message() (topomodelx.base.message_passing.messagepassing method)": [[3, "topomodelx.base.message_passing.MessagePassing.message"]], "reset_parameters() (topomodelx.base.message_passing.messagepassing method)": [[3, "topomodelx.base.message_passing.MessagePassing.reset_parameters"]], "topomodelx.base.message_passing": [[3, "module-topomodelx.base.message_passing"]], "can (class in topomodelx.nn.cell.can)": [[5, "topomodelx.nn.cell.can.CAN"]], "forward() (topomodelx.nn.cell.can.can method)": [[5, "topomodelx.nn.cell.can.CAN.forward"]], "topomodelx.nn.cell.can": [[5, "module-topomodelx.nn.cell.can"]], "canlayer (class in topomodelx.nn.cell.can_layer)": [[6, "topomodelx.nn.cell.can_layer.CANLayer"]], "liftlayer (class in topomodelx.nn.cell.can_layer)": [[6, "topomodelx.nn.cell.can_layer.LiftLayer"]], "multiheadcellattention (class in topomodelx.nn.cell.can_layer)": [[6, "topomodelx.nn.cell.can_layer.MultiHeadCellAttention"]], "multiheadcellattention_v2 (class in topomodelx.nn.cell.can_layer)": [[6, "topomodelx.nn.cell.can_layer.MultiHeadCellAttention_v2"]], "multiheadliftlayer (class in topomodelx.nn.cell.can_layer)": [[6, "topomodelx.nn.cell.can_layer.MultiHeadLiftLayer"]], "poollayer (class in topomodelx.nn.cell.can_layer)": [[6, "topomodelx.nn.cell.can_layer.PoolLayer"]], "add_self_loops() (in module topomodelx.nn.cell.can_layer)": [[6, "topomodelx.nn.cell.can_layer.add_self_loops"]], "attention() (topomodelx.nn.cell.can_layer.multiheadcellattention method)": [[6, "topomodelx.nn.cell.can_layer.MultiHeadCellAttention.attention"]], "attention() (topomodelx.nn.cell.can_layer.multiheadcellattention_v2 method)": [[6, "topomodelx.nn.cell.can_layer.MultiHeadCellAttention_v2.attention"]], "forward() (topomodelx.nn.cell.can_layer.canlayer method)": [[6, "topomodelx.nn.cell.can_layer.CANLayer.forward"]], "forward() (topomodelx.nn.cell.can_layer.liftlayer method)": [[6, "topomodelx.nn.cell.can_layer.LiftLayer.forward"]], "forward() (topomodelx.nn.cell.can_layer.multiheadcellattention method)": [[6, "topomodelx.nn.cell.can_layer.MultiHeadCellAttention.forward"]], "forward() (topomodelx.nn.cell.can_layer.multiheadcellattention_v2 method)": [[6, "topomodelx.nn.cell.can_layer.MultiHeadCellAttention_v2.forward"]], "forward() (topomodelx.nn.cell.can_layer.multiheadliftlayer method)": [[6, "topomodelx.nn.cell.can_layer.MultiHeadLiftLayer.forward"]], "forward() (topomodelx.nn.cell.can_layer.poollayer method)": [[6, "topomodelx.nn.cell.can_layer.PoolLayer.forward"]], "message() (topomodelx.nn.cell.can_layer.liftlayer method)": [[6, "topomodelx.nn.cell.can_layer.LiftLayer.message"]], "message() (topomodelx.nn.cell.can_layer.multiheadcellattention method)": [[6, "topomodelx.nn.cell.can_layer.MultiHeadCellAttention.message"]], "message() (topomodelx.nn.cell.can_layer.multiheadcellattention_v2 method)": [[6, "topomodelx.nn.cell.can_layer.MultiHeadCellAttention_v2.message"]], "reset_parameters() (topomodelx.nn.cell.can_layer.canlayer method)": [[6, "topomodelx.nn.cell.can_layer.CANLayer.reset_parameters"]], "reset_parameters() (topomodelx.nn.cell.can_layer.liftlayer method)": [[6, "topomodelx.nn.cell.can_layer.LiftLayer.reset_parameters"]], "reset_parameters() (topomodelx.nn.cell.can_layer.multiheadcellattention method)": [[6, "topomodelx.nn.cell.can_layer.MultiHeadCellAttention.reset_parameters"]], "reset_parameters() (topomodelx.nn.cell.can_layer.multiheadcellattention_v2 method)": [[6, "topomodelx.nn.cell.can_layer.MultiHeadCellAttention_v2.reset_parameters"]], "reset_parameters() (topomodelx.nn.cell.can_layer.multiheadliftlayer method)": [[6, "topomodelx.nn.cell.can_layer.MultiHeadLiftLayer.reset_parameters"]], "reset_parameters() (topomodelx.nn.cell.can_layer.poollayer method)": [[6, "topomodelx.nn.cell.can_layer.PoolLayer.reset_parameters"]], "softmax() (in module topomodelx.nn.cell.can_layer)": [[6, "topomodelx.nn.cell.can_layer.softmax"]], "topomodelx.nn.cell.can_layer": [[6, "module-topomodelx.nn.cell.can_layer"]], "ccxn (class in topomodelx.nn.cell.ccxn)": [[7, "topomodelx.nn.cell.ccxn.CCXN"]], "forward() (topomodelx.nn.cell.ccxn.ccxn method)": [[7, "topomodelx.nn.cell.ccxn.CCXN.forward"]], "topomodelx.nn.cell.ccxn": [[7, "module-topomodelx.nn.cell.ccxn"]], "ccxnlayer (class in topomodelx.nn.cell.ccxn_layer)": [[8, "topomodelx.nn.cell.ccxn_layer.CCXNLayer"]], "forward() (topomodelx.nn.cell.ccxn_layer.ccxnlayer method)": [[8, "topomodelx.nn.cell.ccxn_layer.CCXNLayer.forward"]], "topomodelx.nn.cell.ccxn_layer": [[8, "module-topomodelx.nn.cell.ccxn_layer"]], "cwn (class in topomodelx.nn.cell.cwn)": [[9, "topomodelx.nn.cell.cwn.CWN"]], "forward() (topomodelx.nn.cell.cwn.cwn method)": [[9, "topomodelx.nn.cell.cwn.CWN.forward"]], "topomodelx.nn.cell.cwn": [[9, "module-topomodelx.nn.cell.cwn"]], "cwnlayer (class in topomodelx.nn.cell.cwn_layer)": [[10, "topomodelx.nn.cell.cwn_layer.CWNLayer"]], "forward() (topomodelx.nn.cell.cwn_layer.cwnlayer method)": [[10, "topomodelx.nn.cell.cwn_layer.CWNLayer.forward"]], "topomodelx.nn.cell.cwn_layer": [[10, "module-topomodelx.nn.cell.cwn_layer"]], "allset (class in topomodelx.nn.hypergraph.allset)": [[12, "topomodelx.nn.hypergraph.allset.AllSet"]], "forward() (topomodelx.nn.hypergraph.allset.allset method)": [[12, "topomodelx.nn.hypergraph.allset.AllSet.forward"]], "topomodelx.nn.hypergraph.allset": [[12, "module-topomodelx.nn.hypergraph.allset"]], "allsetblock (class in topomodelx.nn.hypergraph.allset_layer)": [[13, "topomodelx.nn.hypergraph.allset_layer.AllSetBlock"]], "allsetlayer (class in topomodelx.nn.hypergraph.allset_layer)": [[13, "topomodelx.nn.hypergraph.allset_layer.AllSetLayer"]], "mlp (class in topomodelx.nn.hypergraph.allset_layer)": [[13, "topomodelx.nn.hypergraph.allset_layer.MLP"]], "forward() (topomodelx.nn.hypergraph.allset_layer.allsetblock method)": [[13, "topomodelx.nn.hypergraph.allset_layer.AllSetBlock.forward"]], "forward() (topomodelx.nn.hypergraph.allset_layer.allsetlayer method)": [[13, "topomodelx.nn.hypergraph.allset_layer.AllSetLayer.forward"]], "reset_parameters() (topomodelx.nn.hypergraph.allset_layer.allsetblock method)": [[13, "topomodelx.nn.hypergraph.allset_layer.AllSetBlock.reset_parameters"]], "reset_parameters() (topomodelx.nn.hypergraph.allset_layer.allsetlayer method)": [[13, "topomodelx.nn.hypergraph.allset_layer.AllSetLayer.reset_parameters"]], "topomodelx.nn.hypergraph.allset_layer": [[13, "module-topomodelx.nn.hypergraph.allset_layer"]], "allsettransformer (class in topomodelx.nn.hypergraph.allset_transformer)": [[14, "topomodelx.nn.hypergraph.allset_transformer.AllSetTransformer"]], "forward() (topomodelx.nn.hypergraph.allset_transformer.allsettransformer method)": [[14, "topomodelx.nn.hypergraph.allset_transformer.AllSetTransformer.forward"]], "topomodelx.nn.hypergraph.allset_transformer": [[14, "module-topomodelx.nn.hypergraph.allset_transformer"]], "allsettransformerblock (class in topomodelx.nn.hypergraph.allset_transformer_layer)": [[15, "topomodelx.nn.hypergraph.allset_transformer_layer.AllSetTransformerBlock"]], "allsettransformerlayer (class in topomodelx.nn.hypergraph.allset_transformer_layer)": [[15, "topomodelx.nn.hypergraph.allset_transformer_layer.AllSetTransformerLayer"]], "mlp (class in topomodelx.nn.hypergraph.allset_transformer_layer)": [[15, "topomodelx.nn.hypergraph.allset_transformer_layer.MLP"]], "multiheadattention (class in topomodelx.nn.hypergraph.allset_transformer_layer)": [[15, "topomodelx.nn.hypergraph.allset_transformer_layer.MultiHeadAttention"]], "attention() (topomodelx.nn.hypergraph.allset_transformer_layer.multiheadattention method)": [[15, "topomodelx.nn.hypergraph.allset_transformer_layer.MultiHeadAttention.attention"]], "forward() (topomodelx.nn.hypergraph.allset_transformer_layer.allsettransformerblock method)": [[15, "topomodelx.nn.hypergraph.allset_transformer_layer.AllSetTransformerBlock.forward"]], "forward() (topomodelx.nn.hypergraph.allset_transformer_layer.allsettransformerlayer method)": [[15, "topomodelx.nn.hypergraph.allset_transformer_layer.AllSetTransformerLayer.forward"]], "forward() (topomodelx.nn.hypergraph.allset_transformer_layer.multiheadattention method)": [[15, "topomodelx.nn.hypergraph.allset_transformer_layer.MultiHeadAttention.forward"]], "reset_parameters() (topomodelx.nn.hypergraph.allset_transformer_layer.allsettransformerblock method)": [[15, "topomodelx.nn.hypergraph.allset_transformer_layer.AllSetTransformerBlock.reset_parameters"]], "reset_parameters() (topomodelx.nn.hypergraph.allset_transformer_layer.allsettransformerlayer method)": [[15, "topomodelx.nn.hypergraph.allset_transformer_layer.AllSetTransformerLayer.reset_parameters"]], "reset_parameters() (topomodelx.nn.hypergraph.allset_transformer_layer.multiheadattention method)": [[15, "topomodelx.nn.hypergraph.allset_transformer_layer.MultiHeadAttention.reset_parameters"]], "topomodelx.nn.hypergraph.allset_transformer_layer": [[15, "module-topomodelx.nn.hypergraph.allset_transformer_layer"]], "hmpnn (class in topomodelx.nn.hypergraph.hmpnn)": [[18, "topomodelx.nn.hypergraph.hmpnn.HMPNN"]], "forward() (topomodelx.nn.hypergraph.hmpnn.hmpnn method)": [[18, "topomodelx.nn.hypergraph.hmpnn.HMPNN.forward"]], "topomodelx.nn.hypergraph.hmpnn": [[18, "module-topomodelx.nn.hypergraph.hmpnn"]], "hmpnnlayer (class in topomodelx.nn.hypergraph.hmpnn_layer)": [[19, "topomodelx.nn.hypergraph.hmpnn_layer.HMPNNLayer"]], "apply_regular_dropout() (topomodelx.nn.hypergraph.hmpnn_layer.hmpnnlayer method)": [[19, "topomodelx.nn.hypergraph.hmpnn_layer.HMPNNLayer.apply_regular_dropout"]], "forward() (topomodelx.nn.hypergraph.hmpnn_layer.hmpnnlayer method)": [[19, "topomodelx.nn.hypergraph.hmpnn_layer.HMPNNLayer.forward"]], "topomodelx.nn.hypergraph.hmpnn_layer": [[19, "module-topomodelx.nn.hypergraph.hmpnn_layer"]], "hnhn (class in topomodelx.nn.hypergraph.hnhn)": [[20, "topomodelx.nn.hypergraph.hnhn.HNHN"]], "forward() (topomodelx.nn.hypergraph.hnhn.hnhn method)": [[20, "topomodelx.nn.hypergraph.hnhn.HNHN.forward"]], "topomodelx.nn.hypergraph.hnhn": [[20, "module-topomodelx.nn.hypergraph.hnhn"]], "hnhnlayer (class in topomodelx.nn.hypergraph.hnhn_layer)": [[21, "topomodelx.nn.hypergraph.hnhn_layer.HNHNLayer"]], "compute_normalization_matrices() (topomodelx.nn.hypergraph.hnhn_layer.hnhnlayer method)": [[21, "topomodelx.nn.hypergraph.hnhn_layer.HNHNLayer.compute_normalization_matrices"]], "forward() (topomodelx.nn.hypergraph.hnhn_layer.hnhnlayer method)": [[21, "topomodelx.nn.hypergraph.hnhn_layer.HNHNLayer.forward"]], "init_biases() (topomodelx.nn.hypergraph.hnhn_layer.hnhnlayer method)": [[21, "topomodelx.nn.hypergraph.hnhn_layer.HNHNLayer.init_biases"]], "normalize_incidence_matrices() (topomodelx.nn.hypergraph.hnhn_layer.hnhnlayer method)": [[21, "topomodelx.nn.hypergraph.hnhn_layer.HNHNLayer.normalize_incidence_matrices"]], "reset_parameters() (topomodelx.nn.hypergraph.hnhn_layer.hnhnlayer method)": [[21, "topomodelx.nn.hypergraph.hnhn_layer.HNHNLayer.reset_parameters"]], "topomodelx.nn.hypergraph.hnhn_layer": [[21, "module-topomodelx.nn.hypergraph.hnhn_layer"]], "hnhnlayer (class in topomodelx.nn.hypergraph.hnhn_layer_bis)": [[22, "topomodelx.nn.hypergraph.hnhn_layer_bis.HNHNLayer"]], "forward() (topomodelx.nn.hypergraph.hnhn_layer_bis.hnhnlayer method)": [[22, "topomodelx.nn.hypergraph.hnhn_layer_bis.HNHNLayer.forward"]], "topomodelx.nn.hypergraph.hnhn_layer_bis": [[22, "module-topomodelx.nn.hypergraph.hnhn_layer_bis"]], "hypergat (class in topomodelx.nn.hypergraph.hypergat)": [[23, "topomodelx.nn.hypergraph.hypergat.HyperGAT"]], "forward() (topomodelx.nn.hypergraph.hypergat.hypergat method)": [[23, "topomodelx.nn.hypergraph.hypergat.HyperGAT.forward"]], "topomodelx.nn.hypergraph.hypergat": [[23, "module-topomodelx.nn.hypergraph.hypergat"]], "hypergatlayer (class in topomodelx.nn.hypergraph.hypergat_layer)": [[24, "topomodelx.nn.hypergraph.hypergat_layer.HyperGATLayer"]], "attention() (topomodelx.nn.hypergraph.hypergat_layer.hypergatlayer method)": [[24, "topomodelx.nn.hypergraph.hypergat_layer.HyperGATLayer.attention"]], "forward() (topomodelx.nn.hypergraph.hypergat_layer.hypergatlayer method)": [[24, "topomodelx.nn.hypergraph.hypergat_layer.HyperGATLayer.forward"]], "reset_parameters() (topomodelx.nn.hypergraph.hypergat_layer.hypergatlayer method)": [[24, "topomodelx.nn.hypergraph.hypergat_layer.HyperGATLayer.reset_parameters"]], "topomodelx.nn.hypergraph.hypergat_layer": [[24, "module-topomodelx.nn.hypergraph.hypergat_layer"]], "update() (topomodelx.nn.hypergraph.hypergat_layer.hypergatlayer method)": [[24, "topomodelx.nn.hypergraph.hypergat_layer.HyperGATLayer.update"]], "hypersage (class in topomodelx.nn.hypergraph.hypersage)": [[25, "topomodelx.nn.hypergraph.hypersage.HyperSAGE"]], "forward() (topomodelx.nn.hypergraph.hypersage.hypersage method)": [[25, "topomodelx.nn.hypergraph.hypersage.HyperSAGE.forward"]], "topomodelx.nn.hypergraph.hypersage": [[25, "module-topomodelx.nn.hypergraph.hypersage"]], "generalizedmean (class in topomodelx.nn.hypergraph.hypersage_layer)": [[26, "topomodelx.nn.hypergraph.hypersage_layer.GeneralizedMean"]], "hypersagelayer (class in topomodelx.nn.hypergraph.hypersage_layer)": [[26, "topomodelx.nn.hypergraph.hypersage_layer.HyperSAGELayer"]], "aggregate() (topomodelx.nn.hypergraph.hypersage_layer.hypersagelayer method)": [[26, "topomodelx.nn.hypergraph.hypersage_layer.HyperSAGELayer.aggregate"]], "forward() (topomodelx.nn.hypergraph.hypersage_layer.generalizedmean method)": [[26, "topomodelx.nn.hypergraph.hypersage_layer.GeneralizedMean.forward"]], "forward() (topomodelx.nn.hypergraph.hypersage_layer.hypersagelayer method)": [[26, "topomodelx.nn.hypergraph.hypersage_layer.HyperSAGELayer.forward"]], "topomodelx.nn.hypergraph.hypersage_layer": [[26, "module-topomodelx.nn.hypergraph.hypersage_layer"]], "update() (topomodelx.nn.hypergraph.hypersage_layer.hypersagelayer method)": [[26, "topomodelx.nn.hypergraph.hypersage_layer.HyperSAGELayer.update"]], "templatelayer (class in topomodelx.nn.hypergraph.template_layer)": [[28, "topomodelx.nn.hypergraph.template_layer.TemplateLayer"]], "forward() (topomodelx.nn.hypergraph.template_layer.templatelayer method)": [[28, "topomodelx.nn.hypergraph.template_layer.TemplateLayer.forward"]], "reset_parameters() (topomodelx.nn.hypergraph.template_layer.templatelayer method)": [[28, "topomodelx.nn.hypergraph.template_layer.TemplateLayer.reset_parameters"]], "topomodelx.nn.hypergraph.template_layer": [[28, "module-topomodelx.nn.hypergraph.template_layer"]], "unigcn (class in topomodelx.nn.hypergraph.unigcn)": [[29, "topomodelx.nn.hypergraph.unigcn.UniGCN"]], "forward() (topomodelx.nn.hypergraph.unigcn.unigcn method)": [[29, "topomodelx.nn.hypergraph.unigcn.UniGCN.forward"]], "topomodelx.nn.hypergraph.unigcn": [[29, "module-topomodelx.nn.hypergraph.unigcn"]], "unigcnlayer (class in topomodelx.nn.hypergraph.unigcn_layer)": [[30, "topomodelx.nn.hypergraph.unigcn_layer.UniGCNLayer"]], "forward() (topomodelx.nn.hypergraph.unigcn_layer.unigcnlayer method)": [[30, "topomodelx.nn.hypergraph.unigcn_layer.UniGCNLayer.forward"]], "reset_parameters() (topomodelx.nn.hypergraph.unigcn_layer.unigcnlayer method)": [[30, "topomodelx.nn.hypergraph.unigcn_layer.UniGCNLayer.reset_parameters"]], "topomodelx.nn.hypergraph.unigcn_layer": [[30, "module-topomodelx.nn.hypergraph.unigcn_layer"]], "unigcnii (class in topomodelx.nn.hypergraph.unigcnii)": [[31, "topomodelx.nn.hypergraph.unigcnii.UniGCNII"]], "forward() (topomodelx.nn.hypergraph.unigcnii.unigcnii method)": [[31, "topomodelx.nn.hypergraph.unigcnii.UniGCNII.forward"]], "topomodelx.nn.hypergraph.unigcnii": [[31, "module-topomodelx.nn.hypergraph.unigcnii"]], "unigcniilayer (class in topomodelx.nn.hypergraph.unigcnii_layer)": [[32, "topomodelx.nn.hypergraph.unigcnii_layer.UniGCNIILayer"]], "forward() (topomodelx.nn.hypergraph.unigcnii_layer.unigcniilayer method)": [[32, "topomodelx.nn.hypergraph.unigcnii_layer.UniGCNIILayer.forward"]], "reset_parameters() (topomodelx.nn.hypergraph.unigcnii_layer.unigcniilayer method)": [[32, "topomodelx.nn.hypergraph.unigcnii_layer.UniGCNIILayer.reset_parameters"]], "topomodelx.nn.hypergraph.unigcnii_layer": [[32, "module-topomodelx.nn.hypergraph.unigcnii_layer"]], "unigin (class in topomodelx.nn.hypergraph.unigin)": [[33, "topomodelx.nn.hypergraph.unigin.UniGIN"]], "forward() (topomodelx.nn.hypergraph.unigin.unigin method)": [[33, "topomodelx.nn.hypergraph.unigin.UniGIN.forward"]], "topomodelx.nn.hypergraph.unigin": [[33, "module-topomodelx.nn.hypergraph.unigin"]], "uniginlayer (class in topomodelx.nn.hypergraph.unigin_layer)": [[34, "topomodelx.nn.hypergraph.unigin_layer.UniGINLayer"]], "forward() (topomodelx.nn.hypergraph.unigin_layer.uniginlayer method)": [[34, "topomodelx.nn.hypergraph.unigin_layer.UniGINLayer.forward"]], "topomodelx.nn.hypergraph.unigin_layer": [[34, "module-topomodelx.nn.hypergraph.unigin_layer"]], "unisage (class in topomodelx.nn.hypergraph.unisage)": [[35, "topomodelx.nn.hypergraph.unisage.UniSAGE"]], "forward() (topomodelx.nn.hypergraph.unisage.unisage method)": [[35, "topomodelx.nn.hypergraph.unisage.UniSAGE.forward"]], "topomodelx.nn.hypergraph.unisage": [[35, "module-topomodelx.nn.hypergraph.unisage"]], "unisagelayer (class in topomodelx.nn.hypergraph.unisage_layer)": [[36, "topomodelx.nn.hypergraph.unisage_layer.UniSAGELayer"]], "forward() (topomodelx.nn.hypergraph.unisage_layer.unisagelayer method)": [[36, "topomodelx.nn.hypergraph.unisage_layer.UniSAGELayer.forward"]], "reset_parameters() (topomodelx.nn.hypergraph.unisage_layer.unisagelayer method)": [[36, "topomodelx.nn.hypergraph.unisage_layer.UniSAGELayer.reset_parameters"]], "topomodelx.nn.hypergraph.unisage_layer": [[36, "module-topomodelx.nn.hypergraph.unisage_layer"]], "dist2cycle (class in topomodelx.nn.simplicial.dist2cycle)": [[38, "topomodelx.nn.simplicial.dist2cycle.Dist2Cycle"]], "forward() (topomodelx.nn.simplicial.dist2cycle.dist2cycle method)": [[38, "topomodelx.nn.simplicial.dist2cycle.Dist2Cycle.forward"]], "topomodelx.nn.simplicial.dist2cycle": [[38, "module-topomodelx.nn.simplicial.dist2cycle"]], "dist2cyclelayer (class in topomodelx.nn.simplicial.dist2cycle_layer)": [[39, "topomodelx.nn.simplicial.dist2cycle_layer.Dist2CycleLayer"]], "forward() (topomodelx.nn.simplicial.dist2cycle_layer.dist2cyclelayer method)": [[39, "topomodelx.nn.simplicial.dist2cycle_layer.Dist2CycleLayer.forward"]], "reset_parameters() (topomodelx.nn.simplicial.dist2cycle_layer.dist2cyclelayer method)": [[39, "topomodelx.nn.simplicial.dist2cycle_layer.Dist2CycleLayer.reset_parameters"]], "topomodelx.nn.simplicial.dist2cycle_layer": [[39, "module-topomodelx.nn.simplicial.dist2cycle_layer"]], "hsn (class in topomodelx.nn.simplicial.hsn)": [[40, "topomodelx.nn.simplicial.hsn.HSN"]], "forward() (topomodelx.nn.simplicial.hsn.hsn method)": [[40, "topomodelx.nn.simplicial.hsn.HSN.forward"]], "topomodelx.nn.simplicial.hsn": [[40, "module-topomodelx.nn.simplicial.hsn"]], "hsnlayer (class in topomodelx.nn.simplicial.hsn_layer)": [[41, "topomodelx.nn.simplicial.hsn_layer.HSNLayer"]], "forward() (topomodelx.nn.simplicial.hsn_layer.hsnlayer method)": [[41, "topomodelx.nn.simplicial.hsn_layer.HSNLayer.forward"]], "reset_parameters() (topomodelx.nn.simplicial.hsn_layer.hsnlayer method)": [[41, "topomodelx.nn.simplicial.hsn_layer.HSNLayer.reset_parameters"]], "topomodelx.nn.simplicial.hsn_layer": [[41, "module-topomodelx.nn.simplicial.hsn_layer"]], "san (class in topomodelx.nn.simplicial.san)": [[43, "topomodelx.nn.simplicial.san.SAN"]], "compute_projection_matrix() (topomodelx.nn.simplicial.san.san method)": [[43, "topomodelx.nn.simplicial.san.SAN.compute_projection_matrix"]], "forward() (topomodelx.nn.simplicial.san.san method)": [[43, "topomodelx.nn.simplicial.san.SAN.forward"]], "topomodelx.nn.simplicial.san": [[43, "module-topomodelx.nn.simplicial.san"]], "sanconv (class in topomodelx.nn.simplicial.san_layer)": [[44, "topomodelx.nn.simplicial.san_layer.SANConv"]], "sanlayer (class in topomodelx.nn.simplicial.san_layer)": [[44, "topomodelx.nn.simplicial.san_layer.SANLayer"]], "forward() (topomodelx.nn.simplicial.san_layer.sanconv method)": [[44, "topomodelx.nn.simplicial.san_layer.SANConv.forward"]], "forward() (topomodelx.nn.simplicial.san_layer.sanlayer method)": [[44, "topomodelx.nn.simplicial.san_layer.SANLayer.forward"]], "reset_parameters() (topomodelx.nn.simplicial.san_layer.sanlayer method)": [[44, "topomodelx.nn.simplicial.san_layer.SANLayer.reset_parameters"]], "topomodelx.nn.simplicial.san_layer": [[44, "module-topomodelx.nn.simplicial.san_layer"]], "scacmps (class in topomodelx.nn.simplicial.sca_cmps)": [[45, "topomodelx.nn.simplicial.sca_cmps.SCACMPS"]], "forward() (topomodelx.nn.simplicial.sca_cmps.scacmps method)": [[45, "topomodelx.nn.simplicial.sca_cmps.SCACMPS.forward"]], "topomodelx.nn.simplicial.sca_cmps": [[45, "module-topomodelx.nn.simplicial.sca_cmps"]], "scacmpslayer (class in topomodelx.nn.simplicial.sca_cmps_layer)": [[46, "topomodelx.nn.simplicial.sca_cmps_layer.SCACMPSLayer"]], "forward() (topomodelx.nn.simplicial.sca_cmps_layer.scacmpslayer method)": [[46, "topomodelx.nn.simplicial.sca_cmps_layer.SCACMPSLayer.forward"]], "intra_aggr() (topomodelx.nn.simplicial.sca_cmps_layer.scacmpslayer method)": [[46, "topomodelx.nn.simplicial.sca_cmps_layer.SCACMPSLayer.intra_aggr"]], "reset_parameters() (topomodelx.nn.simplicial.sca_cmps_layer.scacmpslayer method)": [[46, "topomodelx.nn.simplicial.sca_cmps_layer.SCACMPSLayer.reset_parameters"]], "topomodelx.nn.simplicial.sca_cmps_layer": [[46, "module-topomodelx.nn.simplicial.sca_cmps_layer"]], "weight_func() (topomodelx.nn.simplicial.sca_cmps_layer.scacmpslayer method)": [[46, "topomodelx.nn.simplicial.sca_cmps_layer.SCACMPSLayer.weight_func"]], "sccn (class in topomodelx.nn.simplicial.sccn)": [[47, "topomodelx.nn.simplicial.sccn.SCCN"]], "forward() (topomodelx.nn.simplicial.sccn.sccn method)": [[47, "topomodelx.nn.simplicial.sccn.SCCN.forward"]], "topomodelx.nn.simplicial.sccn": [[47, "module-topomodelx.nn.simplicial.sccn"]], "sccnlayer (class in topomodelx.nn.simplicial.sccn_layer)": [[48, "topomodelx.nn.simplicial.sccn_layer.SCCNLayer"]], "forward() (topomodelx.nn.simplicial.sccn_layer.sccnlayer method)": [[48, "topomodelx.nn.simplicial.sccn_layer.SCCNLayer.forward"]], "reset_parameters() (topomodelx.nn.simplicial.sccn_layer.sccnlayer method)": [[48, "topomodelx.nn.simplicial.sccn_layer.SCCNLayer.reset_parameters"]], "topomodelx.nn.simplicial.sccn_layer": [[48, "module-topomodelx.nn.simplicial.sccn_layer"]], "sccnn (class in topomodelx.nn.simplicial.sccnn)": [[49, "topomodelx.nn.simplicial.sccnn.SCCNN"]], "sccnncomplex (class in topomodelx.nn.simplicial.sccnn)": [[49, "topomodelx.nn.simplicial.sccnn.SCCNNComplex"]], "forward() (topomodelx.nn.simplicial.sccnn.sccnn method)": [[49, "topomodelx.nn.simplicial.sccnn.SCCNN.forward"]], "forward() (topomodelx.nn.simplicial.sccnn.sccnncomplex method)": [[49, "topomodelx.nn.simplicial.sccnn.SCCNNComplex.forward"]], "topomodelx.nn.simplicial.sccnn": [[49, "module-topomodelx.nn.simplicial.sccnn"]], "sccnnlayer (class in topomodelx.nn.simplicial.sccnn_layer)": [[50, "topomodelx.nn.simplicial.sccnn_layer.SCCNNLayer"]], "aggr_norm_func() (topomodelx.nn.simplicial.sccnn_layer.sccnnlayer method)": [[50, "topomodelx.nn.simplicial.sccnn_layer.SCCNNLayer.aggr_norm_func"]], "chebyshev_conv() (topomodelx.nn.simplicial.sccnn_layer.sccnnlayer method)": [[50, "topomodelx.nn.simplicial.sccnn_layer.SCCNNLayer.chebyshev_conv"]], "forward() (topomodelx.nn.simplicial.sccnn_layer.sccnnlayer method)": [[50, "topomodelx.nn.simplicial.sccnn_layer.SCCNNLayer.forward"]], "reset_parameters() (topomodelx.nn.simplicial.sccnn_layer.sccnnlayer method)": [[50, "topomodelx.nn.simplicial.sccnn_layer.SCCNNLayer.reset_parameters"]], "topomodelx.nn.simplicial.sccnn_layer": [[50, "module-topomodelx.nn.simplicial.sccnn_layer"]], "update() (topomodelx.nn.simplicial.sccnn_layer.sccnnlayer method)": [[50, "topomodelx.nn.simplicial.sccnn_layer.SCCNNLayer.update"]], "scconv (class in topomodelx.nn.simplicial.scconv)": [[51, "topomodelx.nn.simplicial.scconv.SCConv"]], "forward() (topomodelx.nn.simplicial.scconv.scconv method)": [[51, "topomodelx.nn.simplicial.scconv.SCConv.forward"]], "topomodelx.nn.simplicial.scconv": [[51, "module-topomodelx.nn.simplicial.scconv"]], "scconvlayer (class in topomodelx.nn.simplicial.scconv_layer)": [[52, "topomodelx.nn.simplicial.scconv_layer.SCConvLayer"]], "forward() (topomodelx.nn.simplicial.scconv_layer.scconvlayer method)": [[52, "topomodelx.nn.simplicial.scconv_layer.SCConvLayer.forward"]], "reset_parameters() (topomodelx.nn.simplicial.scconv_layer.scconvlayer method)": [[52, "topomodelx.nn.simplicial.scconv_layer.SCConvLayer.reset_parameters"]], "topomodelx.nn.simplicial.scconv_layer": [[52, "module-topomodelx.nn.simplicial.scconv_layer"]], "scn2 (class in topomodelx.nn.simplicial.scn2)": [[53, "topomodelx.nn.simplicial.scn2.SCN2"]], "forward() (topomodelx.nn.simplicial.scn2.scn2 method)": [[53, "topomodelx.nn.simplicial.scn2.SCN2.forward"]], "topomodelx.nn.simplicial.scn2": [[53, "module-topomodelx.nn.simplicial.scn2"]], "scn2layer (class in topomodelx.nn.simplicial.scn2_layer)": [[54, "topomodelx.nn.simplicial.scn2_layer.SCN2Layer"]], "forward() (topomodelx.nn.simplicial.scn2_layer.scn2layer method)": [[54, "topomodelx.nn.simplicial.scn2_layer.SCN2Layer.forward"]], "reset_parameters() (topomodelx.nn.simplicial.scn2_layer.scn2layer method)": [[54, "topomodelx.nn.simplicial.scn2_layer.SCN2Layer.reset_parameters"]], "topomodelx.nn.simplicial.scn2_layer": [[54, "module-topomodelx.nn.simplicial.scn2_layer"]], "scnn (class in topomodelx.nn.simplicial.scnn)": [[55, "topomodelx.nn.simplicial.scnn.SCNN"]], "forward() (topomodelx.nn.simplicial.scnn.scnn method)": [[55, "topomodelx.nn.simplicial.scnn.SCNN.forward"]], "topomodelx.nn.simplicial.scnn": [[55, "module-topomodelx.nn.simplicial.scnn"]], "scnnlayer (class in topomodelx.nn.simplicial.scnn_layer)": [[56, "topomodelx.nn.simplicial.scnn_layer.SCNNLayer"]], "aggr_norm_func() (topomodelx.nn.simplicial.scnn_layer.scnnlayer method)": [[56, "topomodelx.nn.simplicial.scnn_layer.SCNNLayer.aggr_norm_func"]], "chebyshev_conv() (topomodelx.nn.simplicial.scnn_layer.scnnlayer method)": [[56, "topomodelx.nn.simplicial.scnn_layer.SCNNLayer.chebyshev_conv"]], "forward() (topomodelx.nn.simplicial.scnn_layer.scnnlayer method)": [[56, "topomodelx.nn.simplicial.scnn_layer.SCNNLayer.forward"]], "reset_parameters() (topomodelx.nn.simplicial.scnn_layer.scnnlayer method)": [[56, "topomodelx.nn.simplicial.scnn_layer.SCNNLayer.reset_parameters"]], "topomodelx.nn.simplicial.scnn_layer": [[56, "module-topomodelx.nn.simplicial.scnn_layer"]], "update() (topomodelx.nn.simplicial.scnn_layer.scnnlayer method)": [[56, "topomodelx.nn.simplicial.scnn_layer.SCNNLayer.update"]], "scone (class in topomodelx.nn.simplicial.scone)": [[57, "topomodelx.nn.simplicial.scone.SCoNe"]], "trajectoriesdataset (class in topomodelx.nn.simplicial.scone)": [[57, "topomodelx.nn.simplicial.scone.TrajectoriesDataset"]], "forward() (topomodelx.nn.simplicial.scone.scone method)": [[57, "topomodelx.nn.simplicial.scone.SCoNe.forward"]], "generate_complex() (in module topomodelx.nn.simplicial.scone)": [[57, "topomodelx.nn.simplicial.scone.generate_complex"]], "generate_trajectories() (in module topomodelx.nn.simplicial.scone)": [[57, "topomodelx.nn.simplicial.scone.generate_trajectories"]], "topomodelx.nn.simplicial.scone": [[57, "module-topomodelx.nn.simplicial.scone"]], "vectorize_path() (topomodelx.nn.simplicial.scone.trajectoriesdataset method)": [[57, "topomodelx.nn.simplicial.scone.TrajectoriesDataset.vectorize_path"]], "sconelayer (class in topomodelx.nn.simplicial.scone_layer)": [[58, "topomodelx.nn.simplicial.scone_layer.SCoNeLayer"]], "forward() (topomodelx.nn.simplicial.scone_layer.sconelayer method)": [[58, "topomodelx.nn.simplicial.scone_layer.SCoNeLayer.forward"]], "reset_parameters() (topomodelx.nn.simplicial.scone_layer.sconelayer method)": [[58, "topomodelx.nn.simplicial.scone_layer.SCoNeLayer.reset_parameters"]], "topomodelx.nn.simplicial.scone_layer": [[58, "module-topomodelx.nn.simplicial.scone_layer"]], "broadcast() (in module topomodelx.utils.scatter)": [[59, "topomodelx.utils.scatter.broadcast"]], "scatter() (in module topomodelx.utils.scatter)": [[59, "topomodelx.utils.scatter.scatter"]], "scatter_add() (in module topomodelx.utils.scatter)": [[59, "topomodelx.utils.scatter.scatter_add"]], "scatter_mean() (in module topomodelx.utils.scatter)": [[59, "topomodelx.utils.scatter.scatter_mean"]], "scatter_sum() (in module topomodelx.utils.scatter)": [[59, "topomodelx.utils.scatter.scatter_sum"]], "topomodelx.utils.scatter": [[59, "module-topomodelx.utils.scatter"]]}})