Search.setIndex({"docnames": ["api/base/aggregation", "api/base/conv", "api/base/index", "api/base/message_passing", "api/index", "api/nn/cell/can", "api/nn/cell/can_layer", "api/nn/cell/ccxn", "api/nn/cell/ccxn_layer", "api/nn/cell/cwn", "api/nn/cell/cwn_layer", "api/nn/cell/index", "api/nn/hypergraph/allset", "api/nn/hypergraph/allset_layer", "api/nn/hypergraph/allset_transformer", "api/nn/hypergraph/allset_transformer_layer", "api/nn/hypergraph/dhgcn", "api/nn/hypergraph/dhgcn_layer", "api/nn/hypergraph/hmpnn", "api/nn/hypergraph/hmpnn_layer", "api/nn/hypergraph/hnhn", "api/nn/hypergraph/hnhn_layer", "api/nn/hypergraph/hnhn_layer_bis", "api/nn/hypergraph/hypergat", "api/nn/hypergraph/hypergat_layer", "api/nn/hypergraph/hypersage", "api/nn/hypergraph/hypersage_layer", "api/nn/hypergraph/index", "api/nn/hypergraph/unigcn", "api/nn/hypergraph/unigcn_layer", "api/nn/hypergraph/unigcnii", "api/nn/hypergraph/unigcnii_layer", "api/nn/hypergraph/unigin", "api/nn/hypergraph/unigin_layer", "api/nn/hypergraph/unisage", "api/nn/hypergraph/unisage_layer", "api/nn/index", "api/nn/simplicial/dist2cycle", "api/nn/simplicial/dist2cycle_layer", "api/nn/simplicial/hsn", "api/nn/simplicial/hsn_layer", "api/nn/simplicial/index", "api/nn/simplicial/san", "api/nn/simplicial/san_layer", "api/nn/simplicial/sca_cmps", "api/nn/simplicial/sca_cmps_layer", "api/nn/simplicial/sccn", "api/nn/simplicial/sccn_layer", "api/nn/simplicial/sccnn", "api/nn/simplicial/sccnn_layer", "api/nn/simplicial/scconv", "api/nn/simplicial/scconv_layer", "api/nn/simplicial/scn2", "api/nn/simplicial/scn2_layer", "api/nn/simplicial/scnn", "api/nn/simplicial/scnn_layer", "api/nn/simplicial/scone", "api/nn/simplicial/scone_layer", "api/utils/index", "challenge/index", "contributing/index", "index", "notebooks/cell/can_train", "notebooks/cell/ccxn_train", "notebooks/cell/cwn_train", "notebooks/hypergraph/allset_train", "notebooks/hypergraph/allset_transformer_train", "notebooks/hypergraph/dhgcn_train", "notebooks/hypergraph/hmpnn_train", "notebooks/hypergraph/hnhn_train", "notebooks/hypergraph/hnhn_train_bis", "notebooks/hypergraph/hypergat_train", "notebooks/hypergraph/hypersage_train", "notebooks/hypergraph/unigcn_train", "notebooks/hypergraph/unigcnii_train", "notebooks/hypergraph/unigin_train", "notebooks/hypergraph/unisage_train", "notebooks/simplicial/dist2cycle_train", "notebooks/simplicial/hsn_train", "notebooks/simplicial/san_train", "notebooks/simplicial/sca_cmps_train", "notebooks/simplicial/sccn_train", "notebooks/simplicial/sccnn_train", "notebooks/simplicial/scconv_train", "notebooks/simplicial/scn2_train", "notebooks/simplicial/scnn_train", "notebooks/simplicial/scone_train", "tutorials/index"], "filenames": ["api/base/aggregation.rst", "api/base/conv.rst", "api/base/index.rst", "api/base/message_passing.rst", "api/index.rst", "api/nn/cell/can.rst", "api/nn/cell/can_layer.rst", "api/nn/cell/ccxn.rst", "api/nn/cell/ccxn_layer.rst", "api/nn/cell/cwn.rst", "api/nn/cell/cwn_layer.rst", "api/nn/cell/index.rst", "api/nn/hypergraph/allset.rst", "api/nn/hypergraph/allset_layer.rst", "api/nn/hypergraph/allset_transformer.rst", "api/nn/hypergraph/allset_transformer_layer.rst", "api/nn/hypergraph/dhgcn.rst", "api/nn/hypergraph/dhgcn_layer.rst", "api/nn/hypergraph/hmpnn.rst", "api/nn/hypergraph/hmpnn_layer.rst", "api/nn/hypergraph/hnhn.rst", "api/nn/hypergraph/hnhn_layer.rst", "api/nn/hypergraph/hnhn_layer_bis.rst", "api/nn/hypergraph/hypergat.rst", "api/nn/hypergraph/hypergat_layer.rst", "api/nn/hypergraph/hypersage.rst", "api/nn/hypergraph/hypersage_layer.rst", "api/nn/hypergraph/index.rst", "api/nn/hypergraph/unigcn.rst", "api/nn/hypergraph/unigcn_layer.rst", "api/nn/hypergraph/unigcnii.rst", "api/nn/hypergraph/unigcnii_layer.rst", "api/nn/hypergraph/unigin.rst", "api/nn/hypergraph/unigin_layer.rst", "api/nn/hypergraph/unisage.rst", "api/nn/hypergraph/unisage_layer.rst", "api/nn/index.rst", "api/nn/simplicial/dist2cycle.rst", "api/nn/simplicial/dist2cycle_layer.rst", "api/nn/simplicial/hsn.rst", "api/nn/simplicial/hsn_layer.rst", "api/nn/simplicial/index.rst", "api/nn/simplicial/san.rst", "api/nn/simplicial/san_layer.rst", "api/nn/simplicial/sca_cmps.rst", "api/nn/simplicial/sca_cmps_layer.rst", "api/nn/simplicial/sccn.rst", "api/nn/simplicial/sccn_layer.rst", "api/nn/simplicial/sccnn.rst", "api/nn/simplicial/sccnn_layer.rst", "api/nn/simplicial/scconv.rst", "api/nn/simplicial/scconv_layer.rst", "api/nn/simplicial/scn2.rst", "api/nn/simplicial/scn2_layer.rst", "api/nn/simplicial/scnn.rst", "api/nn/simplicial/scnn_layer.rst", "api/nn/simplicial/scone.rst", "api/nn/simplicial/scone_layer.rst", "api/utils/index.rst", "challenge/index.rst", "contributing/index.rst", "index.rst", "notebooks/cell/can_train.ipynb", "notebooks/cell/ccxn_train.ipynb", "notebooks/cell/cwn_train.ipynb", "notebooks/hypergraph/allset_train.ipynb", "notebooks/hypergraph/allset_transformer_train.ipynb", "notebooks/hypergraph/dhgcn_train.ipynb", "notebooks/hypergraph/hmpnn_train.ipynb", "notebooks/hypergraph/hnhn_train.ipynb", "notebooks/hypergraph/hnhn_train_bis.ipynb", "notebooks/hypergraph/hypergat_train.ipynb", "notebooks/hypergraph/hypersage_train.ipynb", "notebooks/hypergraph/unigcn_train.ipynb", "notebooks/hypergraph/unigcnii_train.ipynb", "notebooks/hypergraph/unigin_train.ipynb", "notebooks/hypergraph/unisage_train.ipynb", "notebooks/simplicial/dist2cycle_train.ipynb", "notebooks/simplicial/hsn_train.ipynb", "notebooks/simplicial/san_train.ipynb", "notebooks/simplicial/sca_cmps_train.ipynb", "notebooks/simplicial/sccn_train.ipynb", "notebooks/simplicial/sccnn_train.ipynb", "notebooks/simplicial/scconv_train.ipynb", "notebooks/simplicial/scn2_train.ipynb", "notebooks/simplicial/scnn_train.ipynb", "notebooks/simplicial/scone_train.ipynb", "tutorials/index.rst"], "titles": ["Aggregation", "Conv", "Base", "Message Passing", "API Reference", "CAN", "Can_Layer", "CCXN", "CCXN_Layer", "CWN", "Cwn_Layer", "Cell", "AllSet", "AllSet_Layer", "AllSet_Transformer", "AllSet_Transformer_Layer", "DHGCN", "DHGCN_Layer", "HMPNN", "HMPNN_Layer", "HNHN", "HNHN_Layer", "HNHN_Layer_Bis", "Hypergat", "Hypergat_Layer", "Hypersage", "Hypersage_Layer", "Hypergraph", "Unigcn", "Unigcn_Layer", "Unigcnii", "Unigcnii_Layer", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "Neural Networks", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "Simplicial", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "Utils", "ICML 2023 Topological Deep Learning Challenge", "Contributing", "\ud83c\udf10 TopoModelX (TMX) \ud83c\udf69", "Train a Cell Attention Network (CAN)", "Train a Convolutional Cell Complex Network (CCXN)", "Train a CW Network (CWN)", "Train an All-Set TNN", "Train an All-Set-Transformer TNN", "Train a DHGCN TNN", "Train a Hypergraph Message Passing Neural Network (HMPNN)", "Train a Hypergraph Networks with Hyperedge Neurons (HNHN)", "Train a Hypergraph Network with Hyperedge Neurons (HNHN)", "Train a Hypergraph Neural Network", "Train a Hypersage TNN", "Train a UNIGCN TNN", "Train a hypergraph neural network using UniGCNII layers", "Train a UNIGIN TNN", "Train a Uni-sage TNN", "Train a Simplicial Neural Network for Homology Localization (Dist2Cycle)", "Train a Simplicial High-Skip Network (HSN)", "Train a Simplicial Attention Network (SAN)", "Train a Simplicial Complex Autoencoder (SCA) with Coadjacency Message Passing Scheme (CMPS)", "Train a Simplicial Complex Convolutional Network (SCCN)", "Train a SCCNN", "Train a Simplicial 2-complex convolutional neural network (SCConv)", "Train a Simplex Convolutional Network (SCN) of Rank 2", "Train a Simplicial Convolutional Neural Network (SCNN)", "Train a Simplicial Complex Net (SCoNe)", "Tutorials"], "terms": {"modul": [0, 3, 5, 6, 10, 12, 13, 14, 15, 19, 33, 36, 59, 60, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 82, 85], "class": [0, 1, 2, 3, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 86], "topomodelx": [0, 1, 3, 4, 5, 6, 7, 8, 9, 10, 12, 13, 14, 15, 18, 19, 20, 21, 22, 23, 24, 25, 26, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86], "base": [0, 1, 3, 4, 10, 11, 15, 27, 41, 45, 59, 66, 82, 83, 85, 86], "aggr_func": [0, 3, 6, 19, 47, 62], "liter": [0, 1, 3, 6, 15, 19, 21, 24, 26, 35, 43, 47, 57], "mean": [0, 3, 6, 19, 26, 35, 47, 49, 54, 58, 60, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 74, 75, 77, 78, 79, 81, 82, 83, 84, 85, 86], "sum": [0, 3, 6, 19, 29, 31, 33, 35, 47, 49, 62, 63, 64, 69, 71, 79, 81, 83, 86], "update_func": [0, 1, 6, 15, 24, 26, 46, 47, 48, 49, 50, 54, 55, 57, 62, 81, 86], "relu": [0, 1, 6, 13, 15, 22, 24, 26, 47, 57, 62, 71, 75, 86], "sigmoid": [0, 1, 6, 19, 26, 46, 47, 57, 62, 65, 66, 71, 72, 73, 75, 76, 81, 86], "tanh": [0, 6, 47, 57], "none": [0, 1, 3, 6, 8, 10, 12, 13, 15, 19, 21, 24, 26, 29, 31, 35, 38, 40, 43, 45, 47, 48, 49, 51, 53, 54, 55, 57, 58, 60, 65, 85, 86], "sourc": [0, 1, 3, 5, 6, 7, 8, 9, 10, 12, 13, 14, 15, 18, 19, 20, 21, 22, 23, 24, 25, 26, 28, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 40, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59], "messag": [0, 1, 2, 4, 6, 8, 10, 15, 18, 19, 20, 21, 23, 24, 25, 26, 28, 29, 30, 31, 32, 33, 34, 35, 37, 39, 42, 43, 45, 46, 47, 50, 52, 53, 59, 62, 63, 64, 65, 66, 67, 69, 71, 72, 73, 74, 75, 76, 79, 81], "pass": [0, 1, 2, 4, 6, 8, 10, 15, 18, 19, 20, 21, 23, 24, 25, 26, 28, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 40, 42, 43, 45, 46, 47, 50, 51, 52, 53, 54, 56, 57, 59, 62, 63, 65, 66, 67, 69, 71, 72, 73, 74, 75, 76, 79, 81, 85, 86], "layer": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 12, 13, 14, 15, 18, 19, 20, 21, 22, 23, 24, 25, 26, 28, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 40, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 57, 59, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86], "paramet": [0, 1, 3, 5, 6, 7, 8, 9, 10, 12, 13, 14, 15, 18, 19, 20, 21, 22, 23, 24, 25, 26, 28, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 40, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 57, 60, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86], "default": [0, 1, 3, 5, 6, 8, 10, 12, 13, 14, 15, 19, 21, 24, 25, 26, 30, 35, 42, 43, 47, 54, 60, 62, 65, 66, 70, 72, 74], "method": [0, 1, 3, 6, 15, 21, 24, 26, 43, 49, 55, 59, 60, 68, 70], "inter": [0, 26, 72, 82], "neighborhood": [0, 1, 2, 3, 6, 8, 10, 15, 29, 43, 59, 62, 63, 64, 65, 66, 72, 79, 80, 86], "updat": [0, 1, 2, 3, 6, 10, 15, 24, 26, 29, 31, 33, 35, 41, 43, 49, 55, 57, 62, 65, 66, 70, 79, 86], "appli": [0, 1, 3, 5, 6, 10, 12, 14, 15, 18, 19, 24, 26, 47, 57, 62, 65, 66, 68, 71, 79, 83, 86], "merg": 0, "forward": [0, 1, 2, 3, 5, 6, 7, 8, 9, 10, 12, 13, 14, 15, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 86], "x": [0, 1, 3, 6, 8, 10, 12, 13, 14, 15, 19, 21, 24, 25, 26, 29, 31, 33, 35, 38, 40, 42, 43, 45, 47, 48, 49, 51, 53, 54, 55, 56, 57, 60, 62, 63, 64, 65, 66, 68, 69, 70, 71, 72, 73, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86], "list": [0, 12, 13, 14, 15, 44, 45, 56, 60, 65, 66, 67, 74, 86], "len": [0, 68, 69, 70, 73, 74, 75, 76, 77, 78, 79, 81, 82, 83, 85, 86], "n_messages_to_merg": 0, "each": [0, 1, 3, 6, 24, 26, 40, 44, 45, 46, 47, 48, 49, 55, 57, 59, 60, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 81, 82, 86], "ha": [0, 1, 6, 60, 63, 64, 65, 66, 67, 69, 71, 72, 74, 77, 78, 79, 80, 81, 82, 83, 84, 85], "shape": [0, 1, 3, 5, 6, 7, 8, 9, 10, 13, 15, 18, 19, 20, 21, 22, 23, 24, 25, 26, 28, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 40, 42, 43, 44, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 57, 58, 60, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85], "n_skeleton_in": 0, "channel": [0, 5, 6, 8, 13, 15, 31, 37, 38, 39, 40, 43, 46, 47, 48, 50, 51, 52, 53, 54, 55, 62, 74, 77, 78, 79, 81], "input": [0, 1, 3, 5, 6, 7, 8, 9, 10, 12, 13, 14, 15, 18, 19, 21, 22, 24, 26, 29, 30, 31, 33, 35, 38, 40, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 57, 60, 62, 63, 64, 65, 66, 67, 68, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 81, 82, 85], "step": [0, 1, 3, 6, 8, 10, 21, 24, 26, 29, 33, 35, 49, 55, 59, 60, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86], "4": [0, 1, 10, 14, 15, 24, 26, 49, 55, 60, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 81, 82, 83, 84, 85, 86], "h": [0, 3, 24, 31, 40, 47, 51, 62, 66, 71, 72, 74, 78, 79, 81, 82, 83, 85], "arrai": [0, 60, 63, 64, 65, 66, 67, 69, 71, 72, 74, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86], "like": [0, 49, 55, 57, 60, 62, 63, 79, 81, 82, 85], "n_skeleton_out": 0, "out_channel": [0, 1, 3, 5, 6, 10, 12, 14, 15, 23, 24, 25, 26, 29, 32, 33, 35, 42, 43, 49, 54, 55, 57, 62, 64, 65, 66, 67, 71, 72, 73, 75, 76, 79, 85], "featur": [0, 1, 3, 5, 6, 7, 8, 9, 10, 12, 13, 14, 15, 18, 19, 20, 21, 22, 23, 24, 25, 26, 28, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 40, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 57, 59, 60, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 78, 79, 80, 81, 82, 83, 84, 85, 86], "skeleton": [0, 45, 62, 86], "out": [0, 6, 58, 59, 60, 61, 62, 67, 69, 70, 79, 80, 81, 86], "return": [0, 1, 3, 5, 6, 7, 8, 9, 10, 12, 13, 14, 15, 18, 19, 20, 21, 22, 23, 24, 25, 26, 28, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 40, 42, 43, 44, 45, 46, 47, 48, 49, 50, 52, 53, 54, 55, 57, 58, 60, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 81, 82, 83, 85, 86], "convolut": [1, 6, 8, 9, 10, 21, 42, 43, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 59, 64, 79], "in_channel": [1, 3, 6, 12, 13, 14, 15, 23, 24, 25, 26, 29, 30, 31, 33, 35, 42, 43, 49, 54, 55, 57, 62, 65, 66, 67, 71, 72, 73, 74, 75, 76, 77, 78, 79, 81, 82, 85], "aggr_norm": [1, 15, 29, 48, 49, 54, 55], "bool": [1, 3, 5, 6, 7, 8, 12, 13, 14, 15, 21, 29, 33, 35, 44, 45, 49, 54, 55, 60, 62, 63, 65, 66], "fals": [1, 3, 6, 7, 8, 12, 13, 14, 15, 29, 33, 35, 44, 45, 48, 49, 54, 55, 62, 63, 64, 65, 66, 67, 69, 71, 72, 73, 75, 76, 80, 82, 83, 84, 85], "att": [1, 3, 7, 8, 24, 43, 44, 45, 49, 63, 71, 80], "initi": [1, 3, 6, 8, 10, 15, 21, 24, 26, 29, 33, 35, 40, 43, 45, 47, 49, 55, 57, 62, 63, 65, 66, 68, 69, 70, 72, 79, 82, 83], "xavier_uniform": [1, 3, 6, 15, 21, 24, 26, 43, 55, 72], "xavier_norm": [1, 3, 6, 15, 21, 24, 26, 43, 49], "initialization_gain": [1, 3, 15, 24], "float": [1, 3, 5, 6, 12, 13, 14, 15, 19, 21, 22, 24, 30, 31, 33, 42, 49, 55, 57, 62, 63, 64, 65, 66, 67, 69, 70, 71, 72, 73, 74, 76, 77, 78, 79, 81, 82, 83, 84, 85, 86], "1": [1, 3, 5, 6, 7, 8, 9, 10, 12, 13, 14, 15, 18, 19, 20, 21, 22, 23, 24, 25, 26, 28, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 40, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 53, 54, 55, 57, 58, 59, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 83, 84, 86], "414": [1, 3, 15, 21, 24, 49, 55, 69, 73], "with_linear_transform": 1, "true": [1, 5, 6, 21, 62, 63, 64, 65, 66, 67, 69, 71, 72, 73, 75, 76, 77, 78, 79, 81, 82, 83, 84, 85, 86], "2": [1, 3, 5, 6, 7, 8, 9, 10, 12, 13, 14, 18, 20, 21, 23, 25, 26, 28, 29, 30, 32, 33, 34, 35, 37, 39, 40, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 59, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 86], "3": [1, 5, 6, 8, 10, 21, 26, 29, 33, 35, 40, 45, 47, 51, 53, 55, 57, 60, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86], "build": [1, 2, 22, 56, 59, 60, 86], "rout": 1, "given": [1, 3, 8, 10, 19, 21, 22, 29, 33, 35, 40, 45, 47, 57, 59, 62, 63, 64, 65, 66, 69, 71, 72, 74, 77, 78, 79, 81, 82, 83, 85, 86], "one": [1, 3, 15, 21, 24, 47, 53, 54, 57, 59, 60, 63, 69, 74, 77, 78, 79, 80, 81, 82, 83, 86], "matrix": [1, 3, 6, 7, 8, 9, 10, 13, 15, 18, 19, 20, 21, 22, 23, 24, 25, 26, 28, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 40, 42, 43, 44, 49, 50, 51, 53, 55, 57, 60, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 84, 85, 86], "includ": [1, 59, 60, 86], "an": [1, 4, 6, 8, 10, 49, 55, 59, 60, 62, 63, 64, 67, 69, 71, 72, 73, 75, 76, 77, 78, 79, 81, 82, 83, 84, 85, 86], "option": [1, 3, 5, 6, 7, 8, 10, 12, 13, 14, 15, 31, 42, 43, 59, 60, 62, 63, 65, 66], "specif": [1, 9, 59, 60, 64, 69, 71, 74], "function": [1, 3, 5, 6, 12, 13, 14, 15, 19, 22, 26, 31, 35, 45, 46, 47, 49, 50, 51, 55, 57, 58, 59, 60, 62, 64, 65, 66, 72, 74, 77, 78, 79, 81, 82, 85, 86], "int": [1, 5, 6, 7, 8, 9, 10, 12, 13, 14, 15, 18, 19, 20, 21, 22, 23, 24, 25, 26, 28, 29, 30, 31, 32, 33, 34, 35, 37, 39, 40, 42, 43, 44, 45, 46, 47, 48, 49, 50, 52, 53, 54, 55, 56, 57, 58, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 80, 86], "dimens": [1, 6, 7, 8, 9, 10, 12, 13, 14, 15, 18, 19, 20, 21, 22, 23, 24, 25, 26, 28, 29, 32, 33, 34, 35, 37, 39, 40, 42, 44, 45, 46, 47, 48, 49, 50, 52, 53, 54, 55, 56, 57, 60, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86], "output": [1, 3, 5, 6, 8, 10, 12, 13, 14, 15, 19, 21, 22, 24, 26, 29, 31, 33, 35, 38, 40, 42, 43, 45, 47, 48, 49, 53, 54, 55, 57, 59, 60, 62, 65, 66, 67, 71, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 85, 86], "whether": [1, 3, 5, 6, 7, 8, 12, 13, 14, 15, 21, 29, 33, 35, 44, 45, 54, 62, 63, 65, 66], "normal": [1, 6, 12, 13, 14, 15, 21, 22, 29, 31, 47, 49, 50, 51, 53, 55, 65, 66, 81, 84], "aggreg": [1, 2, 3, 4, 6, 10, 15, 19, 26, 29, 35, 45, 46, 47, 49, 50, 54, 55, 59, 62, 71, 72, 79, 83, 85], "size": [1, 12, 14, 15, 29, 62, 63, 64, 65, 66, 67, 69, 71, 72, 74, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86], "us": [1, 3, 5, 6, 7, 8, 10, 19, 21, 22, 29, 30, 31, 33, 35, 42, 44, 45, 46, 47, 50, 51, 56, 57, 59, 60, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 75, 76, 77, 78, 79, 80, 81, 82, 84, 85, 86], "attent": [1, 2, 3, 5, 6, 7, 8, 15, 24, 42, 43, 44, 45, 59, 66, 71], "learnabl": [1, 3, 6, 13, 15, 21, 29, 35, 38, 40, 43, 47, 49, 53, 55, 57, 62, 66, 79, 85], "linear": [1, 6, 7, 9, 19, 23, 25, 28, 29, 31, 32, 34, 35, 44, 54, 59, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 81, 82, 85, 86], "transform": [1, 6, 14, 15, 29, 31, 35, 60, 70, 74, 79, 85], "nb": 1, "equal": [1, 60, 71, 86], "x_sourc": [1, 3, 6, 15, 24, 43], "x_target": [1, 3, 6, 24, 26], "tensor": [1, 3, 4, 5, 6, 7, 8, 9, 10, 12, 13, 14, 15, 18, 19, 20, 21, 22, 23, 24, 25, 26, 28, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 40, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 81, 82, 83, 84, 85, 86], "thi": [1, 2, 3, 6, 8, 10, 19, 21, 22, 26, 29, 33, 35, 40, 43, 45, 47, 48, 49, 53, 54, 55, 57, 58, 59, 60, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 74, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86], "implement": [1, 3, 4, 6, 8, 9, 10, 15, 18, 20, 21, 23, 24, 25, 26, 28, 29, 31, 32, 33, 34, 35, 37, 39, 40, 42, 43, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 59, 62, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 75, 76, 79, 80, 81, 86], "from": [1, 3, 5, 6, 8, 10, 15, 18, 19, 21, 26, 29, 33, 35, 43, 45, 56, 57, 58, 59, 60, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86], "cell": [1, 3, 5, 6, 7, 8, 9, 10, 15, 24, 26, 36, 40, 43, 46, 47, 49, 50, 53, 54, 55, 59, 64, 65, 66, 67, 72, 73, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 86], "via": [1, 3, 38, 40, 43, 46, 47], "defin": [1, 2, 3, 33, 43, 56, 59, 60, 62, 63, 64, 65, 74, 79, 80, 86], "where": [1, 3, 43, 45, 49, 55, 59, 60, 62, 63, 64, 66, 69, 71, 72, 77, 78, 79, 80, 81, 82, 83, 85, 86], "can": [1, 3, 6, 8, 11, 21, 43, 47, 48, 55, 57, 59, 60, 61, 71, 72, 74, 77, 78, 79, 81, 83, 85, 86], "target": [1, 3, 6, 15, 24, 26, 43, 49, 55, 63, 82, 85], "In": [1, 3, 19, 22, 43, 48, 59, 60, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86], "practic": [1, 3, 43, 65, 66], "If": [1, 3, 6, 10, 19, 24, 26, 31, 46, 60, 62, 63, 64, 65, 66, 67, 68, 69, 70, 74, 79, 80], "provid": [1, 3, 6, 31, 49, 55, 59, 60, 80, 82, 85], "i": [1, 2, 3, 5, 6, 8, 10, 11, 19, 21, 24, 25, 26, 27, 29, 30, 31, 33, 35, 38, 40, 41, 42, 45, 47, 53, 55, 57, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86], "assum": [1, 3, 15, 24, 26, 43], "e": [1, 3, 6, 12, 14, 19, 33, 49, 54, 55, 59, 60, 61, 62, 63, 64, 65, 66, 67, 69, 71, 72, 74, 77, 78, 79, 82, 84, 85, 86], "send": [1, 3, 8, 10, 21, 59, 62, 63, 64, 65, 66, 67, 69, 71, 72, 77, 78, 79, 84], "themselv": [1, 3, 81], "n_source_cel": [1, 3, 15, 24, 43], "all": [1, 3, 6, 15, 24, 26, 30, 31, 43, 48, 58, 59, 60, 62, 67, 71, 72, 74, 77, 78, 79, 81, 82, 85, 86], "have": [1, 3, 15, 19, 24, 26, 43, 49, 55, 59, 60, 63, 65, 66, 67, 69, 71, 72, 77, 78, 81, 82, 85, 86], "same": [1, 3, 6, 15, 24, 26, 43, 47, 48, 49, 53, 59, 60, 62, 63, 79, 80, 82, 85, 86], "rank": [1, 3, 7, 9, 15, 20, 23, 24, 25, 26, 28, 31, 32, 34, 37, 39, 42, 43, 47, 50, 51, 53, 59, 62, 63, 64, 65, 66, 67, 69, 71, 72, 73, 75, 76, 77, 78, 79, 80, 81, 82, 83, 85], "r": [1, 3, 6, 8, 10, 15, 24, 42, 43, 45, 46, 47, 53, 60, 62, 63, 64, 65, 66, 71, 79, 80, 81, 86], "torch": [1, 3, 5, 6, 7, 8, 9, 10, 12, 13, 14, 15, 18, 19, 20, 21, 22, 24, 26, 29, 30, 31, 33, 35, 38, 40, 43, 44, 45, 46, 47, 49, 50, 51, 53, 55, 57, 59, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86], "spars": [1, 3, 4, 6, 8, 10, 13, 15, 18, 19, 20, 21, 22, 24, 29, 33, 35, 38, 40, 43, 46, 47, 49, 53, 55, 57, 60, 65, 66, 68, 69, 70, 71, 72, 74, 77, 79, 81, 82, 83], "n_target_cel": [1, 3, 15, 24, 26, 43, 49, 55], "": [1, 3, 15, 18, 19, 26, 43, 59, 60, 62, 63, 64, 65, 66, 67, 68, 70, 74, 77, 78, 80, 81, 82, 86], "x_message_on_target": [1, 24, 26, 49, 55], "embed": [1, 24, 26, 44, 49, 55], "The": [2, 3, 4, 6, 8, 10, 11, 19, 20, 21, 22, 26, 27, 29, 31, 33, 35, 36, 40, 41, 42, 45, 47, 53, 55, 57, 59, 65, 66, 67, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 80, 81, 82, 83, 84, 85, 86], "compos": [2, 8, 10, 11, 21, 27, 29, 33, 35, 41], "primarili": [2, 11, 27, 36, 41], "three": [2, 29, 33, 35, 36, 41, 59, 60, 74], "conv": [2, 4, 21], "structur": [2, 62, 63, 64, 65, 66, 72, 79, 82, 85], "messagepass": [2, 3, 49, 55], "reset_paramet": [2, 3, 6, 13, 15, 21, 24, 27, 29, 31, 35, 38, 40, 41, 43, 45, 47, 49, 51, 53, 55, 57], "message_pass": 3, "add": [3, 6, 13, 15, 19, 58, 60, 74, 85], "uniform": [3, 6, 26, 86], "through": [3, 5, 7, 9, 10, 18, 23, 25, 28, 30, 32, 34, 44, 56, 59, 63, 64, 67, 68, 70, 71, 72, 73, 74, 75, 76, 85, 86], "singl": [3, 26, 59, 64], "n": [3, 6, 10, 43, 45, 56, 59, 60, 61, 62, 64, 71, 72, 79, 80, 86], "decompos": 3, "creat": [3, 10, 56, 59, 60, 66, 71, 72], "go": [3, 9, 10, 61, 62, 64, 79, 86], "come": 3, "differ": [3, 22, 47, 53, 60, 63, 69, 77, 78, 79, 81, 82, 85, 86], "onto": [3, 79], "should": [3, 6, 59, 60, 86], "instanti": [3, 64], "directli": [3, 74], "rather": [3, 59, 60], "inherit": [3, 59], "subclass": [3, 49, 55], "effect": [3, 19], "doe": [3, 59, 60, 82, 85, 86], "trainabl": [3, 33, 49, 55, 71], "weight": [3, 6, 15, 22, 24, 43, 45, 49, 55, 60, 66, 71, 79, 81, 82, 86], "its": [3, 19, 29, 31, 33, 35, 59, 60, 68, 70, 71], "gain": [3, 15, 21, 49, 55, 57], "refer": [3, 5, 6, 7, 8, 9, 10, 12, 13, 14, 15, 18, 19, 20, 21, 22, 23, 24, 25, 26, 28, 29, 30, 31, 32, 33, 34, 35, 38, 40, 43, 45, 47, 49, 51, 53, 55, 57, 60, 62, 72, 79], "hajij": [3, 7, 8, 10, 21, 26, 29, 33, 35, 38, 40, 43, 45, 47, 49, 51, 53, 55, 57, 59, 61, 62, 63, 78, 79], "zamzmi": [3, 7, 8, 40, 45, 61], "papamark": [3, 45, 59, 61], "miolan": [3, 8, 10, 21, 26, 29, 33, 35, 38, 40, 43, 45, 47, 49, 51, 53, 55, 57, 59, 61], "guzm\u00e1n": [3, 40, 59, 61], "s\u00e1enz": [3, 40, 59, 61], "ramamurthi": [3, 40, 59, 61], "birdal": [3, 59, 61], "dei": [3, 59, 61], "mukherje": [3, 59, 61], "samaga": [3, 59, 61], "livesai": [3, 59, 61], "walter": [3, 59, 61], "rosen": [3, 59, 61], "schaub": [3, 59, 61], "topolog": [3, 4, 7, 8, 10, 21, 26, 29, 33, 35, 38, 40, 43, 45, 47, 49, 51, 53, 55, 57, 61, 62, 63, 64, 69, 77, 78, 79, 80, 81, 83, 85, 86], "deep": [3, 8, 10, 21, 26, 29, 33, 35, 40, 45, 47, 51, 53, 55, 57, 61, 62, 63, 64, 66, 69, 77, 78, 79, 80, 81, 83, 85, 86], "learn": [3, 8, 10, 20, 21, 22, 25, 26, 29, 31, 33, 35, 40, 45, 47, 51, 53, 55, 57, 60, 61, 62, 63, 64, 69, 70, 71, 72, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86], "beyond": [3, 7, 8, 20, 21, 22, 51, 61, 62, 79], "graph": [3, 5, 6, 13, 15, 20, 21, 22, 28, 29, 30, 31, 32, 33, 34, 35, 48, 49, 59, 61, 62, 63, 64, 65, 66, 67, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 84, 85, 86], "data": [3, 7, 8, 47, 53, 60, 61, 62, 68, 70, 79, 81, 84], "2023": [3, 8, 10, 21, 26, 29, 33, 35, 38, 40, 43, 45, 47, 49, 51, 53, 55, 57, 61, 62, 63, 64, 69, 74, 77, 78, 79, 80, 81, 82, 83, 85, 86], "http": [3, 5, 6, 7, 8, 9, 10, 12, 13, 14, 15, 18, 19, 20, 21, 22, 23, 24, 25, 26, 28, 29, 30, 31, 32, 33, 34, 35, 40, 43, 45, 47, 51, 53, 55, 57, 62, 69, 70, 74, 76, 77, 78, 79, 81, 82, 84], "arxiv": [3, 5, 6, 7, 8, 9, 10, 12, 13, 14, 15, 18, 19, 21, 25, 26, 28, 29, 30, 31, 32, 33, 34, 35, 40, 43, 45, 47, 51, 53, 55, 57, 61], "org": [3, 5, 6, 7, 8, 9, 10, 12, 13, 14, 15, 18, 19, 21, 23, 24, 25, 26, 28, 29, 30, 31, 32, 33, 34, 35, 40, 43, 45, 47, 51, 53, 55, 57, 69, 77, 78, 79, 81, 82], "ab": [3, 8, 9, 10, 12, 13, 14, 15, 18, 19, 21, 25, 26, 29, 33, 35, 40, 43, 45, 47, 51, 53, 55, 57, 83], "2206": [3, 61, 77], "00606": [3, 61], "papillon": [3, 8, 10, 21, 26, 29, 33, 35, 38, 40, 43, 45, 47, 49, 51, 53, 55, 57, 59, 61, 62, 63, 64, 69, 77, 78, 79, 80, 81, 83, 85, 86], "sanborn": [3, 8, 10, 21, 26, 29, 33, 35, 38, 40, 43, 45, 47, 49, 51, 53, 55, 57, 59, 61], "architectur": [3, 8, 10, 15, 21, 26, 29, 33, 35, 40, 43, 45, 47, 51, 53, 55, 57, 59, 61, 62, 63, 64, 65, 66, 67, 69, 71, 72, 77, 78, 79, 80, 81, 83, 85, 86], "survei": [3, 8, 10, 21, 26, 29, 33, 35, 40, 45, 47, 51, 53, 55, 57, 59, 61, 62, 63, 64, 69, 77, 78, 79, 80, 81, 83, 85, 86], "neural": [3, 4, 7, 8, 10, 12, 13, 14, 15, 18, 19, 21, 23, 25, 26, 28, 29, 30, 31, 32, 33, 34, 35, 38, 40, 43, 45, 47, 49, 51, 53, 54, 55, 56, 57, 59, 61], "network": [3, 4, 5, 6, 7, 8, 9, 10, 12, 13, 14, 15, 18, 19, 20, 21, 22, 23, 25, 26, 28, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 40, 42, 43, 45, 46, 47, 49, 50, 51, 52, 53, 54, 55, 56, 57, 59, 61], "2304": [3, 8, 10, 21, 26, 29, 33, 35, 40, 45, 47, 51, 53, 55, 57, 61, 69], "10031": [3, 8, 10, 21, 26, 29, 33, 35, 40, 45, 47, 51, 53, 55, 57, 61], "x_messag": [3, 26], "A": [3, 8, 10, 12, 13, 14, 15, 18, 21, 26, 33, 38, 51, 59, 60, 61, 62, 63, 64, 65, 66, 68, 69, 77, 78, 79, 80, 81, 82, 83, 85, 86], "receiv": [3, 10, 19, 26, 59], "sever": [3, 4, 26, 59], "per": [3, 15, 24, 26, 59, 60], "correspond": [3, 10, 26, 47, 53, 55, 62, 66, 68, 70, 74], "within": [3, 60, 62, 68, 70, 79], "n_messag": [3, 24, 26], "associ": [3, 26, 60, 62, 63, 64, 65, 66, 67, 71, 72, 73, 75, 76, 77, 78, 79, 80, 81, 82], "One": [3, 26, 37, 39, 42, 48, 52, 59, 81, 85, 86], "sent": [3, 26, 74], "comput": [3, 4, 5, 6, 7, 9, 12, 13, 14, 15, 18, 19, 20, 21, 22, 23, 24, 25, 26, 28, 31, 32, 34, 37, 39, 42, 44, 46, 48, 49, 50, 52, 54, 55, 58, 59, 60, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 79, 85, 86], "scheme": [3, 6, 8, 45, 59, 63, 79], "altern": [3, 59], "user": [3, 63, 77, 81, 82, 85], "overwrit": 3, "order": [3, 40, 42, 43, 47, 48, 49, 53, 54, 55, 57, 59, 62, 78, 79, 81, 82, 84, 85, 86], "replac": 3, "own": 3, "mechan": [3, 5, 6, 7, 8, 15, 24, 62, 63, 66, 71, 79], "follow": [3, 6, 10, 56, 59, 60, 61, 62, 65, 66, 67, 72, 73, 74, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86], "scalar": [3, 15, 21, 24, 63, 64, 80, 86], "between": [3, 6, 7, 8, 10, 13, 15, 19, 24, 26, 47, 53, 62, 63, 79, 81], "two": [3, 8, 10, 19, 21, 31, 49, 62, 65, 66, 67, 69, 71, 72, 77, 78, 79, 81, 82, 85, 86], "m_": [3, 6, 8, 10, 13, 15, 19, 21, 24, 26, 29, 31, 33, 35, 38, 40, 43, 45, 47, 49, 51, 53, 55, 57, 62, 63, 64, 65, 66, 69, 71, 72, 77, 78, 79, 80, 81, 83, 85, 86], "y": [3, 6, 8, 10, 13, 15, 19, 21, 24, 26, 29, 31, 33, 35, 38, 40, 43, 45, 47, 49, 51, 53, 55, 57, 60, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86], "rightarrow": [3, 6, 8, 10, 13, 15, 19, 21, 24, 26, 29, 31, 33, 35, 38, 40, 43, 45, 47, 49, 51, 53, 55, 57, 62, 63, 64, 65, 66, 69, 71, 72, 77, 78, 79, 80, 81, 83, 85, 86], "left": [3, 10, 26, 45, 56, 60, 64, 71, 72, 86], "right": [3, 10, 26, 56, 60, 64, 71, 72, 86], "travel": 3, "denot": [3, 65, 66, 71, 72, 85, 86], "mathcal": [3, 6, 8, 10, 13, 15, 19, 21, 24, 26, 29, 31, 33, 35, 38, 40, 43, 45, 47, 49, 51, 53, 55, 57, 62, 63, 64, 65, 66, 69, 71, 72, 77, 78, 79, 80, 81, 83, 85, 86], "mathbf": [3, 82, 85], "_x": [3, 38, 51, 53, 62, 77, 79, 83], "_y": [3, 31, 40, 47, 78, 81], "theta": [3, 6, 8, 21, 24, 26, 29, 33, 38, 40, 43, 45, 47, 49, 51, 53, 57, 62, 63, 66, 69, 71, 72, 77, 78, 80, 81, 82, 83, 85, 86], "ar": [3, 6, 8, 10, 12, 13, 14, 15, 19, 21, 22, 29, 33, 35, 40, 43, 45, 47, 49, 57, 59, 60, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 74, 77, 78, 79, 80, 81, 82, 83, 85, 86], "call": [3, 19, 22, 49, 55, 60, 66, 86], "leftarrow": [3, 72], "across": [3, 22], "belong": [3, 60, 69, 77, 78, 79, 81, 82], "m_x": [3, 6, 8, 10, 19, 21, 24, 26, 29, 31, 33, 35, 38, 40, 43, 45, 47, 49, 51, 53, 55, 57, 63, 64, 69, 71, 72, 77, 78, 80, 81, 83, 85, 86], "text": [3, 8, 10, 15, 45, 49, 60, 65, 66, 67, 68, 69, 70, 71, 72, 77, 78, 79, 80, 81, 82], "agg": [3, 8, 10, 19, 45, 80], "_": [3, 6, 7, 8, 9, 10, 21, 23, 24, 25, 26, 28, 31, 32, 34, 35, 37, 38, 39, 40, 42, 43, 44, 45, 46, 47, 48, 50, 51, 52, 53, 55, 57, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86], "result": [3, 60, 62, 63, 66, 69, 82, 85], "detail": [3, 10, 59, 60, 62, 66, 79], "found": [3, 70, 74], "construct": [3, 6, 68, 70, 74, 86], "reset": [3, 6, 13, 15, 21, 24, 29, 31, 35, 38, 40, 43, 45, 47, 49, 51, 53, 55, 57], "note": [3, 6, 8, 10, 21, 40, 43, 45, 48, 49, 51, 53, 54, 55, 57, 59, 60, 65, 66, 67, 71, 72, 77, 78, 82, 86], "give": [4, 62, 66, 79], "overview": 4, "which": [4, 6, 19, 26, 43, 44, 48, 59, 60, 65, 66, 68, 70, 71, 74, 77, 78, 79, 81, 82, 85, 86], "consist": [4, 31, 36, 58, 59, 60, 74, 86], "core": 4, "mathemat": 4, "concept": 4, "nn": [4, 5, 6, 7, 8, 9, 10, 12, 13, 14, 15, 18, 19, 20, 21, 22, 23, 24, 25, 26, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 59, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86], "organ": [4, 59, 81], "domain": [4, 59, 61, 62, 63, 64, 65, 66, 68, 69, 70, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86], "util": [4, 6, 30, 62, 65, 66, 73, 74, 75, 76, 79, 86], "broadcast": [4, 58, 63, 82, 85], "scatter": [4, 58, 70, 86], "scatter_add": [4, 58], "scatter_mean": [4, 58], "scatter_sum": [4, 58], "in_channels_0": [5, 6, 7, 8, 9, 10, 49, 52, 53, 62, 63, 64, 82, 84, 85], "in_channels_1": [5, 6, 7, 8, 9, 10, 49, 52, 53, 62, 63, 64, 82, 84, 85], "num_class": [5, 7, 8, 9, 18, 30, 48, 52, 62, 63, 64, 68, 70, 74, 82, 84], "dropout": [5, 6, 12, 13, 14, 15, 18, 19, 62, 65, 66, 68, 70], "0": [5, 6, 7, 8, 9, 12, 13, 14, 15, 18, 19, 21, 22, 24, 26, 29, 30, 31, 33, 35, 37, 39, 40, 42, 43, 45, 49, 50, 51, 53, 55, 57, 59, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86], "5": [5, 18, 19, 21, 30, 42, 60, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 81, 82, 83, 84, 85, 86], "head": [5, 6, 14, 15, 62, 66], "concat": [5, 6, 49, 55, 62], "skip_connect": [5, 6, 62], "att_activ": [5, 6, 62], "leakyrelu": [5, 6, 62, 71, 79], "negative_slop": [5, 6, 62], "n_layer": [5, 7, 9, 12, 14, 18, 20, 23, 25, 28, 30, 32, 34, 37, 39, 42, 44, 46, 48, 50, 52, 54, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 85], "att_lift": [5, 62], "classif": [5, 8, 10, 20, 21, 23, 25, 28, 30, 32, 34, 37, 39, 40, 42, 43, 45, 46, 48, 50, 52, 53, 54, 56, 59, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 83, 84], "number": [5, 6, 7, 9, 12, 13, 14, 15, 18, 19, 20, 30, 31, 42, 43, 46, 48, 50, 52, 54, 59, 62, 63, 64, 65, 66, 68, 69, 70, 74, 77, 78, 79, 80, 81, 83, 84, 86], "node": [5, 6, 7, 8, 9, 10, 13, 15, 18, 19, 20, 21, 22, 23, 24, 25, 26, 28, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 40, 42, 44, 45, 46, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 59, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 83, 84, 86], "level": [5, 22, 24, 30, 45, 59, 62, 65, 66, 67, 69, 71, 72, 74, 77, 78, 79, 80, 81, 82, 85], "edg": [5, 6, 7, 8, 9, 10, 12, 13, 14, 15, 20, 21, 23, 24, 25, 28, 29, 31, 32, 33, 34, 35, 38, 40, 42, 44, 48, 49, 50, 51, 52, 53, 54, 55, 57, 59, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86], "num_classest": 5, "probabl": [5, 6, 12, 13, 14, 15, 62, 65, 66], "concaten": [5, 6, 19, 62, 66, 71, 86], "skip": [5, 6, 31, 37, 39, 40, 62, 74, 86], "connect": [5, 6, 31, 40, 56, 62, 71, 74, 78, 82, 86], "activ": [5, 6, 13, 15, 46, 47, 50, 62, 66], "lift": [5, 6, 59, 62, 63, 64, 65, 66, 69, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 85], "signal": [5, 6, 62, 63, 64, 65, 66, 67, 71, 72, 73, 75, 76, 79, 80], "giusti": [5, 6, 43, 59, 62, 79], "battiloro": [5, 6, 43, 59, 79], "testa": [5, 6], "di": [5, 6, 43], "lorenzo": [5, 6, 43], "sardellitti": [5, 6, 43], "barbarossa": [5, 6, 43], "2022": [5, 6, 12, 13, 14, 15, 18, 19, 40, 43, 45, 47, 53, 59, 62, 68, 77, 78, 79, 81, 85], "paper": [5, 6, 15, 19, 20, 21, 22, 59, 62, 68, 69, 70, 77, 78, 79, 82, 83, 84, 85, 86], "pdf": [5, 6, 7, 8, 20, 21, 22, 23, 24, 28, 29, 30, 31, 32, 33, 34, 35, 40, 45, 55, 60], "2209": [5, 6], "08179": [5, 6], "repositori": [5, 6, 59, 60], "lrnzgiusti": [5, 6], "x_0": [5, 6, 7, 8, 9, 10, 12, 14, 18, 19, 20, 21, 22, 29, 30, 31, 32, 33, 35, 37, 39, 40, 49, 50, 51, 52, 53, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 74, 75, 77, 78, 79, 80, 81, 82, 83, 84, 85], "x_1": [5, 6, 7, 8, 9, 10, 18, 19, 20, 21, 22, 23, 28, 34, 49, 50, 51, 52, 53, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85], "neighborhood_0_to_0": [5, 6, 7, 8, 62, 63], "lower_neighborhood": [5, 6, 62], "upper_neighborhood": [5, 6, 62], "n_node": [5, 7, 9, 13, 15, 18, 19, 20, 21, 22, 23, 25, 28, 29, 32, 33, 34, 35, 37, 38, 39, 40, 42, 46, 48, 49, 50, 51, 52, 53, 54, 57, 63, 64, 67, 68, 69, 70, 71, 72, 73, 75, 76], "n_edg": [5, 7, 9, 19, 20, 21, 22, 23, 25, 28, 29, 32, 33, 34, 35, 37, 38, 39, 40, 42, 48, 49, 50, 51, 53, 54, 55, 57, 63, 64, 69, 71, 72, 73, 75, 76], "canlay": [6, 11, 62], "01": [6, 68, 74, 77, 82, 85, 86], "add_self_loop": [6, 11], "version": [6, 8, 9, 21, 63, 64, 82, 85], "v1": 6, "v2": 6, "share_weight": 6, "kwarg": [6, 25, 26, 72], "model": [6, 30, 37, 39, 59, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 81, 83, 84], "consid": [6, 42, 48, 49, 54, 55, 62, 79, 82], "though": 6, "upper": [6, 9, 10, 19, 42, 43, 46, 47, 48, 49, 50, 51, 54, 55, 56, 62, 64, 79, 82, 85, 86], "lower": [6, 43, 46, 47, 49, 54, 55, 56, 62, 79, 80, 82, 85, 86], "addition": [6, 65, 66, 81], "ad": [6, 19, 59, 60], "prefer": [6, 59, 60], "necessari": [6, 58, 59, 62, 79], "self": [6, 30, 31, 60, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 79, 81, 82, 85, 86], "loop": [6, 30, 31, 59, 65, 66, 67, 68, 70, 72, 73, 74, 76, 77, 78, 79, 81, 83, 84, 86], "preprocess": 6, "coeffici": [6, 62, 71], "otherwis": [6, 62, 63, 64, 65, 66, 67, 68, 69, 70, 74, 80, 86], "averag": [6, 9, 54, 63, 64, 67, 86], "callabl": [6, 13, 15, 19, 22], "origin": [6, 31, 59, 60, 62, 63, 64, 74, 79, 81, 82, 84, 85, 86], "while": [6, 59, 60, 79], "attet": 6, "gatv2": [6, 62], "valid": [6, 60, 73, 74, 75, 76, 86], "onli": [6, 8, 19, 53, 59, 60, 62, 67, 74, 79, 82, 84, 86], "share": [6, 19, 59, 62], "n_1": [6, 43, 62, 79], "n_2": [6, 43, 62, 79], "a_": [6, 10, 40, 43, 62, 63, 64, 71, 77, 78, 79], "uparrow": [6, 8, 10, 38, 40, 43, 47, 51, 53, 55, 57, 62, 63, 64, 77, 78, 79, 81, 82, 83, 85, 86], "downarrow": [6, 38, 43, 45, 47, 51, 53, 55, 57, 62, 77, 79, 80, 81, 82, 83, 85, 86], "begin": [6, 8, 10, 13, 15, 19, 21, 24, 26, 29, 31, 33, 35, 38, 40, 43, 45, 47, 49, 51, 53, 55, 57, 60, 62, 79], "align": [6, 8, 10, 13, 15, 19, 21, 24, 26, 29, 31, 33, 35, 38, 40, 43, 45, 47, 49, 51, 53, 55, 57, 62, 79, 85], "quad": [6, 8, 10, 13, 15, 19, 21, 24, 26, 29, 31, 33, 35, 38, 40, 43, 45, 47, 49, 51, 53, 55, 57, 62, 63, 64, 65, 66, 69, 71, 72, 77, 78, 79, 80, 81, 83, 85, 86], "k": [6, 10, 19, 43, 45, 49, 55, 61, 62, 64, 66, 69, 71, 77, 78, 79, 80, 81, 83, 85, 86], "alpha_k": [6, 43, 62, 79], "h_x": [6, 8, 10, 13, 15, 19, 21, 24, 26, 29, 31, 33, 35, 38, 40, 43, 45, 47, 49, 53, 55, 57, 62, 63, 64, 65, 66, 69, 71, 72, 77, 78, 79, 80, 81, 85, 86], "t": [6, 8, 10, 13, 15, 19, 21, 24, 26, 29, 31, 33, 35, 38, 40, 43, 45, 47, 49, 51, 53, 55, 57, 61, 62, 63, 64, 65, 66, 69, 70, 71, 72, 74, 77, 78, 79, 80, 81, 82, 83, 85, 86], "h_y": [6, 8, 10, 13, 15, 19, 21, 26, 29, 33, 35, 40, 43, 49, 51, 55, 57, 62, 63, 64, 65, 66, 69, 72, 78, 79, 83, 85, 86], "a_k": [6, 43, 62, 79, 86], "cdot": [6, 21, 24, 26, 29, 31, 33, 35, 38, 40, 43, 47, 49, 51, 53, 55, 57, 62, 69, 71, 72, 77, 78, 79, 81, 83, 85, 86], "psi_k": [6, 43, 62, 79], "foral": [6, 43, 62, 71, 79], "n_k": [6, 43, 62, 79], "bigoplus_": [6, 43, 62, 79], "_k": [6, 10, 43, 45, 62, 64, 79, 80], "m": [6, 8, 38, 43, 45, 51, 53, 55, 57, 60, 63, 71, 77, 80, 83, 85, 86], "bigotimes_": [6, 43, 62, 79], "phi": [6, 43, 62, 79], "end": [6, 8, 10, 13, 15, 19, 21, 24, 26, 29, 31, 33, 35, 38, 40, 43, 45, 47, 49, 51, 53, 55, 57, 60, 62, 77, 78, 79, 81, 86], "n_k_cell": 6, "complex": [6, 7, 8, 9, 10, 23, 25, 28, 32, 34, 38, 40, 44, 45, 46, 47, 48, 49, 50, 51, 53, 54, 55, 56, 57, 59, 60, 62, 64, 65, 66, 67, 71, 72, 73, 75, 76, 77, 78, 79, 84], "map": [6, 8, 10, 13, 15, 19, 20, 21, 22, 29, 33, 35, 38, 40, 46, 47, 57, 69, 83, 86], "a_k_low": 6, "a_k_up": 6, "liftlay": [6, 11, 62], "signal_lift_activ": 6, "signal_lift_dropout": [6, 62], "adapt": [6, 58], "offici": [6, 70], "rate": [6, 18, 19, 68, 70], "num_nod": [6, 30, 31, 74], "num_edg": [6, 30, 31, 74], "reiniti": 6, "xavier": 6, "multiheadcellattent": [6, 11, 62], "propos": [6, 8, 10, 15, 21, 24, 26, 29, 33, 35, 40, 43, 45, 47, 51, 53, 57, 62, 63, 64, 69, 72, 77, 78, 79, 82, 83, 84, 85, 86], "gat": [6, 62, 79], "adjac": [6, 7, 8, 9, 10, 18, 19, 37, 38, 39, 40, 46, 47, 49, 50, 51, 55, 62, 63, 64, 68, 77, 78, 79, 81, 82, 86], "non": [6, 62, 79], "zero": [6, 62, 65, 66, 74, 77, 78, 79, 82, 85, 86], "valu": [6, 42, 58, 63, 64, 65, 66, 67, 68, 70, 71, 72, 74, 77, 80, 82, 83, 84, 85, 86], "empti": 6, "veli\u010dkovi\u0107": 6, "cucurul": 6, "casanova": 6, "romero": 6, "li\u00f2": 6, "bengio": [6, 20, 21, 22], "2017": [6, 66, 77], "1710": [6, 69], "10903": 6, "up": [6, 7, 10, 29, 31, 33, 35, 37, 39, 43, 54, 55, 60, 79, 81, 86], "down": [6, 42, 43, 44, 45, 48, 50, 51, 54, 55, 79], "multiheadcellattention_v2": [6, 11], "brodi": 6, "alon": 6, "yahav": 6, "how": [6, 60, 66, 77, 78, 79], "2105": [6, 28, 29, 30, 31, 32, 33, 34, 35, 69], "14491": 6, "alpha": [6, 21, 30, 31, 55, 62, 72, 74, 79, 86], "multiheadliftlay": [6, 11, 62], "type": [6, 13, 15, 60], "built": [6, 59, 60], "object": [6, 19, 60], "signal_lift_readout": 6, "str": [6, 13, 15, 24, 26, 46, 49, 50, 55, 58, 60], "cat": 6, "multi": [6, 13, 15, 60, 66], "readout": [6, 48, 62], "z": [6, 8, 10, 13, 15, 19, 24, 26, 29, 31, 33, 35, 40, 43, 49, 55, 57, 62, 63, 64, 65, 66, 71, 72, 78, 85, 86], "h_z": [6, 10, 13, 15, 19, 43, 62, 64, 65, 66], "index": [6, 58, 60, 81, 86], "poollay": [6, 11, 62], "k_pool": [6, 62], "signal_pool_activ": [6, 62], "pool": [6, 7, 9, 23, 25, 28, 32, 34, 44, 62, 63, 64, 67, 71, 72, 73, 75, 76], "ratio": 6, "fraction": [6, 69], "keep": [6, 60, 62, 63, 64, 65, 66, 67, 68, 70, 72, 73, 76, 77, 78, 79, 81, 82], "after": [6, 29, 35, 60, 62, 70, 75], "oper": [6, 13, 15, 57, 62, 66, 71, 79, 81, 82, 86], "tupl": [6, 18, 43, 48, 49, 56, 68, 86], "gamma": [6, 62, 70], "tau": [6, 62], "c_r": [6, 62], "num_pooled_nod": 6, "file": [6, 59, 60], "sparse_coo_tensor": [6, 68, 70], "softmax": [6, 11, 57, 69, 74, 79, 85, 86], "src": [6, 58, 74], "num_cel": 6, "There": [6, 59, 60, 68, 69, 70, 77, 78, 79, 81, 85], "subtract": 6, "maximum": [6, 47, 81, 86], "element": [6, 60, 66], "avoid": [6, 49, 55], "overflow": 6, "underflow": 6, "indic": [6, 43, 58, 70, 77, 78, 79, 81, 82, 86], "batch": [6, 86], "in_channels_2": [7, 8, 9, 10, 49, 52, 53, 62, 63, 64, 82, 84, 85], "face": [7, 8, 9, 10, 44, 48, 49, 50, 51, 52, 53, 54, 59, 62, 63, 64, 65, 66, 67, 71, 72, 77, 78, 79, 80, 81, 82, 83, 84, 85], "istvan": [7, 8], "analysi": [7, 8], "workshop": [7, 8, 20, 21, 22, 40, 51, 59], "neurip": [7, 8, 9, 10, 51], "2020": [7, 8, 20, 21, 22, 23, 24, 25, 26, 51, 59, 63, 68, 69, 71, 72, 83], "2010": [7, 8, 25, 26], "00743": [7, 8], "neighborhood_1_to_2": [7, 8, 63], "avg": [7, 44, 63], "n_face": [7, 9, 48, 49, 50, 51, 53, 63, 64], "transpos": [7, 44, 45, 63, 80, 83], "boundari": [7, 9, 10, 20, 23, 25, 28, 32, 34, 37, 39, 57, 63, 64, 65, 66, 67, 69, 71, 72, 73, 75, 76, 77, 78, 81, 86], "x_2": [7, 8, 9, 10, 49, 50, 51, 52, 53, 63, 64, 65, 66, 67, 71, 72, 77, 78, 79, 80, 81, 82, 83, 84, 85], "label": [7, 9, 23, 25, 28, 32, 34, 37, 39, 42, 44, 47, 48, 50, 52, 53, 60, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 79, 80, 83, 84, 86], "assign": [7, 9, 23, 25, 28, 32, 34, 37, 39, 42, 44, 46, 48, 50, 52, 60, 63, 64, 67, 71, 72, 73, 74, 75, 76, 77, 78, 79, 81, 82, 86], "whole": [7, 9, 23, 25, 28, 32, 34, 44, 48, 50, 54, 63, 64, 67, 71, 72, 73, 75, 76], "simplifi": [8, 21, 63], "ccxn": [8, 11], "et": [8, 9, 10, 19, 22, 29, 33, 35, 53, 59, 61, 62, 63, 64, 65, 66, 68, 69, 70, 71, 72, 77, 78, 79, 80, 81, 82, 83, 85, 86], "al": [8, 9, 10, 22, 29, 33, 35, 53, 59, 61, 62, 63, 64, 65, 66, 69, 70, 71, 72, 77, 78, 79, 80, 81, 82, 83, 85, 86], "ccxnlayer": [8, 11, 63], "entir": [8, 10, 62, 63, 64], "equat": [8, 10, 21, 26, 29, 33, 35, 38, 40, 43, 45, 47, 49, 51, 53, 55, 57, 60, 61, 62, 63, 64, 69, 77, 78, 79, 81, 83, 86], "awesom": [8, 10, 21, 26, 29, 33, 35, 38, 40, 43, 45, 47, 49, 51, 53, 55, 57, 61, 66, 72], "tnn": [8, 10, 21, 26, 29, 33, 35, 38, 40, 43, 45, 47, 49, 51, 53, 55, 57, 59, 61], "wa": [8, 10, 21, 40, 45, 47, 57, 80], "Its": [8, 10, 21, 29, 33, 35, 40, 45, 47, 57], "graphic": [8, 10, 21, 29, 33, 35, 40, 45, 47, 57, 61], "illustr": [8, 10, 21, 29, 33, 35, 40, 45, 47, 57, 84], "amp": [8, 63], "l": [8, 10, 18, 19, 38, 40, 42, 43, 45, 47, 51, 53, 55, 57, 62, 63, 64, 71, 72, 77, 78, 79, 80, 81, 82, 83, 85, 86], "u": [8, 10, 19, 43, 45, 55, 59, 63, 64, 65, 66, 71, 80, 86], "cohomologi": [8, 63], "coboundari": [8, 9, 63, 64], "t_": [8, 26, 63, 72], "c": [8, 13, 15, 19, 21, 24, 26, 29, 31, 33, 35, 40, 47, 49, 51, 61, 62, 63, 65, 66, 69, 70, 71, 72, 77, 78, 81, 83, 86], "h_": [8, 24, 38, 45, 47, 53, 63, 71, 77, 80, 81], "n_0_cell": 8, "n_1_cell": 8, "a_0_up": 8, "n_2_cell": 8, "b_2": [8, 49, 50, 51, 57, 63, 79, 83], "requir": [8, 60, 62, 80], "predict": [8, 12, 14, 18, 20, 57, 60, 62, 65, 66, 68, 69, 70, 86], "hid_channel": [9, 64], "cw": [9, 10, 59], "hidden": [9, 12, 13, 14, 15, 18, 42, 64, 65, 66, 68, 70, 71, 79, 86], "bodnar": [9, 10, 59, 64], "weisfeil": [9, 10, 64], "lehman": [9, 10, 64], "cellular": [9, 10, 59, 64], "2021": [9, 10, 28, 29, 30, 31, 32, 33, 34, 35, 55, 57, 59, 64, 65, 66, 86], "2106": [9, 10, 12, 13, 14, 15], "12575": [9, 10], "neighborhood_1_to_1": [9, 10, 64], "neighborhood_2_to_1": [9, 10, 64], "neighborhood_0_to_1": [9, 10, 64], "project": [9, 42, 44, 60, 64, 79], "cwn": [10, 11, 59], "cwnlayer": [10, 11, 64], "conv_1_to_1": 10, "conv_0_to_1": 10, "aggregate_fn": 10, "update_fn": 10, "represent": [10, 18, 19, 20, 21, 22, 25, 26, 40, 47, 53, 56, 59, 62, 65, 66, 68, 69, 70, 71, 81, 84, 86], "case": [10, 59, 62, 63, 64, 65, 66, 67, 69, 71, 72, 77, 78, 79, 84, 86], "convolv": 10, "neighbor": [10, 19, 22, 62, 72, 85, 86], "co": [10, 59, 74], "check": [10, 59, 60, 61, 69, 77, 78], "docstr": [10, 59], "_cwndefaultfirstconv": 10, "more": [10, 19, 59, 60, 61, 77, 81, 82, 86], "_cwndefaultsecondconv": 10, "obtain": [10, 54, 72, 74, 79, 82, 85, 86], "_cwndefaultaggreg": 10, "_cwndefaultupd": 10, "final": [10, 19, 22, 54, 57, 59, 68, 69, 70, 74, 79, 85, 86], "first": [10, 29, 31, 35, 60, 62, 63, 64, 65, 66, 67, 69, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86], "exploit": [10, 62], "second": [10, 29, 31, 35, 59, 65, 66, 80], "b": [10, 13, 15, 19, 21, 24, 26, 29, 31, 33, 35, 45, 47, 49, 50, 51, 60, 64, 65, 66, 69, 71, 72, 80, 81, 82, 83], "Then": [10, 60, 63, 64], "agg_": [10, 13, 15, 19, 45, 63, 64, 65, 66, 80], "n_": [10, 43, 65, 66, 67, 68, 69, 70, 71, 72, 77, 78, 79, 81, 82, 85], "_cell": 10, "in_channels_": 10, "b_": [10, 47, 64, 80, 81], "t_r": 10, "six": 11, "can_lay": [11, 62], "ccxn_layer": [11, 63], "cwn_layer": [11, 64], "hypergraph": [12, 13, 14, 15, 18, 19, 20, 21, 22, 23, 24, 25, 26, 28, 29, 30, 31, 32, 33, 34, 35, 36, 59, 65, 66, 72, 73, 75, 76], "hidden_channel": [12, 13, 14, 15, 42, 65, 66, 79], "mlp_num_lay": [12, 13, 14, 15, 65, 66], "mlp_activ": [12, 13, 15, 65], "mlp_dropout": [12, 13, 14, 15, 65, 66], "mlp_norm": [12, 13, 14, 15, 65, 66], "combin": [12, 14, 31, 62, 65, 66], "multipl": [12, 14, 59, 60, 65, 66, 86], "form": [12, 14, 56, 59, 65, 66, 77, 78, 81, 82, 86], "in_dim": [12, 14, 65, 66], "hid_dim": [12, 14, 65, 66], "out_dim": [12, 14, 65, 66, 71, 75], "input_dropout": [12, 14, 65, 66], "mlp": [12, 13, 14, 15, 27, 65, 66, 75], "chien": [12, 13, 14, 15, 59, 65, 66], "pan": [12, 13, 14, 15], "peng": [12, 13, 14, 15], "milenkov": [12, 13, 14, 15], "you": [12, 13, 14, 15, 51, 59, 60, 74], "multiset": [12, 13, 14, 15, 65, 66], "framework": [12, 13, 14, 15, 28, 29, 30, 31, 32, 33, 34, 35, 65, 66], "iclr": [12, 13, 14, 15, 40], "13264": [12, 13, 14, 15], "incidence_1": [12, 13, 14, 15, 18, 19, 20, 21, 22, 23, 28, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 40, 48, 50, 51, 57, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 82, 83, 85], "edge_index": [12, 14, 62, 65, 66, 68, 71, 72], "allset": [13, 14, 15, 27, 65, 66], "allsetblock": [13, 27], "block": [13, 15, 59], "bipartit": [13, 15], "incid": [13, 15, 18, 19, 20, 21, 22, 24, 25, 26, 29, 30, 31, 33, 35, 38, 40, 44, 45, 46, 47, 48, 49, 50, 51, 57, 65, 66, 67, 68, 69, 70, 71, 72, 74, 77, 78, 79, 80, 81, 82, 85], "hyperedg": [13, 15, 18, 19, 20, 21, 22, 24, 26, 35, 59, 65, 66, 67, 68, 71, 72], "allsetlay": [13, 27, 65], "vertex": [13, 15, 65, 66, 86], "sigma": [13, 21, 24, 26, 38, 40, 43, 47, 51, 53, 55, 57, 65, 69, 71, 72, 77, 78, 81, 82, 83, 85, 86], "n_hyperedg": [13, 15, 18, 22, 68, 70], "b_1": [13, 15, 19, 20, 21, 22, 24, 26, 29, 31, 33, 35, 38, 40, 49, 50, 51, 57, 65, 66, 67, 68, 69, 70, 71, 72, 77, 78, 79, 83], "norm_lay": [13, 15], "activation_lay": [13, 15], "inplac": [13, 15], "bia": [13, 15, 21, 62, 82, 85], "perceptron": [13, 15, 66], "do": [13, 15, 59, 68, 70, 77, 78, 81, 86], "place": [13, 15, 59, 60, 65, 66], "allsettransform": [14, 15, 27, 59, 66], "allsettransformerblock": [15, 27], "number_queri": 15, "queri": 15, "allsettransformerlay": [15, 27, 66], "ln": [15, 66], "multiheadattent": [15, 27], "qk": 15, "v": [15, 60, 65, 66, 69, 71, 72, 74, 77, 78, 79, 81, 82, 85], "mh": [15, 66], "eq": [15, 77, 78, 79, 82, 85], "7": [15, 18, 19, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86], "factor": 15, "in_featur": [18, 19, 22, 30, 62, 68, 70, 74, 82, 85], "hidden_featur": [18, 68, 70], "adjacency_dropout_r": [18, 68], "regular_dropout_r": [18, 68], "gradual": [18, 68], "reduc": [18, 19, 68, 82], "last": [18, 54, 57, 68, 81, 86], "item": [18, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86], "hmpnnlayer": [18, 19, 27, 68], "regular": [18, 19, 68], "heydari": [18, 19, 59, 68], "livi": [18, 19, 68], "icann": [18, 19], "2203": [18, 19, 43], "16995": [18, 19], "b1": [18, 48, 49, 68, 69, 77, 78, 81, 82, 83, 85], "y_pred": [18, 68, 70, 74, 77, 78, 79, 81, 82, 85], "logit": [18, 20, 30, 46, 68, 69, 70, 74], "hmpnn": [19, 27, 59], "introduc": [19, 22, 62, 68, 70, 71, 79], "node_to_hyperedge_messaging_func": 19, "hyperedge_to_node_messaging_func": 19, "adjacency_dropout": [19, 68], "updating_dropout": [19, 68], "updating_func": 19, "compris": 19, "make": [19, 22, 33, 59, 62, 63, 64, 65, 66, 67, 68, 70, 74, 80, 86], "new": [19, 22, 59, 60, 63], "reprsent": 19, "them": [19, 49, 62, 63, 64, 65, 66, 67, 68, 70, 71, 74, 79, 80, 81], "also": [19, 55, 60, 62, 63, 64, 65, 66, 67, 69, 70, 71, 72, 73, 75, 76, 77, 78, 79, 80, 84], "reciev": 19, "beforehand": [19, 22], "wai": [19, 60, 79], "could": [19, 62, 79, 81], "explicit": 19, "rightarrow1": [19, 31, 49, 51, 83], "rightarrow0": [19, 24, 29, 31, 33, 35, 49, 51, 71, 83], "m_z": [19, 24, 26, 29, 31, 33, 35, 49, 71, 72], "plu": [19, 79], "accord": [19, 45, 70, 84], "It": [19, 30, 31, 59, 68, 70, 74], "get": [19, 57, 59, 79, 82, 85, 86], "back": 19, "retriev": [19, 62, 63, 64, 65, 66, 67, 69, 71, 72, 73, 75, 76, 77, 78, 79, 80, 81, 82, 84], "apply_regular_dropout": 19, "unmask": 19, "vector": [19, 56, 60, 66, 68, 70, 71, 86], "scale": [19, 86], "d": [19, 43, 55, 60, 71, 77, 83], "mask": [19, 56, 62, 68, 79, 86], "total": [19, 79], "node_in_featur": 19, "hyperedge_in_featur": 19, "channels_nod": [20, 21, 23, 25, 28, 34, 65, 67, 69, 71, 72, 73, 76, 77, 78, 79, 81, 82, 85], "channels_edg": [20, 21, 23, 25, 28, 34, 65, 67, 69, 71, 72, 73, 76], "n_class": [20, 44, 46, 50, 69, 80, 83], "neuron": [20, 21, 22, 59], "multiclass": [20, 69], "dong": [20, 21, 22, 59, 69, 70], "sawin": [20, 21, 22], "icml": [20, 21, 22, 57], "grlplu": [20, 21, 22], "github": [20, 21, 22, 59, 66, 70, 72, 74], "io": [20, 21, 22, 70], "40": [20, 21, 22, 67, 68, 69, 70, 73, 74, 75, 76, 77, 81], "hypernod": [20, 21, 69], "templat": [21, 23, 60, 71], "hnhnlayer": [21, 22, 27, 69, 70], "use_bia": 21, "use_normalized_incid": 21, "beta": [21, 30, 31, 74, 86], "bias_gain": 21, "bias_init": 21, "hnhn": [21, 22, 27, 59], "matric": [21, 44, 45, 46, 47, 48, 51, 59, 62, 63, 64, 65, 66, 67, 69, 71, 72, 77, 78, 79, 81, 82, 84, 85], "usign": 21, "cardin": 21, "hyperparamet": [21, 31, 68, 70, 74], "control": 21, "strenght": 21, "support": [21, 74, 79, 82, 85], "train": [21, 26, 44, 59], "term": [21, 59, 79], "flag": 21, "import": [21, 22, 31, 59, 60, 62, 63, 64, 65, 66, 68, 70, 72, 74, 79, 86], "compute_normalization_matric": 21, "w": [21, 29, 31, 69, 71, 79], "xy": [21, 38, 40, 43, 47, 51, 53, 55, 57, 69, 77, 78, 81, 83, 85, 86], "sum_": [21, 24, 26, 29, 31, 33, 35, 38, 40, 43, 47, 49, 51, 53, 55, 57, 69, 71, 72, 77, 78, 81, 82, 83, 85, 86], "init_bias": 21, "normalize_incidence_matric": 21, "activation_func": 22, "normalization_param_alpha": [22, 70], "normalization_param_beta": [22, 70], "relai": 22, "other": [22, 58, 60, 86], "word": [22, 68, 70], "intermediari": 22, "those": [22, 71, 79], "dure": 22, "multipli": [22, 79], "reflect": 22, "param": 22, "power": [22, 26, 83], "amount": [23, 25, 28, 32, 34, 37, 39, 52, 59, 63, 64, 65, 66, 67, 68, 70, 71, 72, 73, 75, 76, 82], "ding": [23, 24, 71], "wang": [23, 24], "li": [23, 24], "huan": [23, 24], "liu": [23, 24], "emnlp": [23, 24], "aclanthologi": [23, 24], "main": [23, 24, 60, 74], "399": [23, 24, 69, 73], "global": [23, 25, 28, 32, 34, 67, 71, 72, 73, 75, 76], "max": [23, 25, 28, 32, 34, 62, 65, 66, 71, 72, 73, 75, 76, 81], "hypergat": [24, 27, 71], "hypergatlay": [24, 27, 71], "string": [24, 26, 60, 72], "set": [24, 26, 67, 69, 72, 73, 74, 75, 76, 79, 81, 86], "see": [24, 26, 49, 51, 53, 59, 60, 61, 70, 86], "t_1": [24, 31, 71], "odot": [24, 38, 43, 49, 71, 77], "zy": [24, 26, 31, 40, 71, 72, 78], "xz": [24, 26, 40, 71, 72, 78], "arya": [25, 26, 72], "gupta": [25, 26], "rudinac": [25, 26], "wor": [25, 26], "gener": [25, 26, 40, 56, 60, 62, 65, 66, 74, 78, 79], "induct": [25, 26], "04558": [25, 26], "features_nod": [25, 72], "hypersag": [26, 27], "generalizedmean": [26, 27], "hypersagelay": [26, 27, 72], "aggr_func_intra": 26, "aggr_func_int": 26, "devic": [26, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 74, 79, 80, 83, 84], "cpu": [26, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 74, 79, 80, 83, 84], "p": [26, 43, 55, 62, 71, 72, 79], "name": [26, 59, 60, 62, 65, 66, 68, 73, 75, 76], "mode": [26, 74], "intra": [26, 45, 72], "either": [26, 59, 60, 79, 86], "w_y": [26, 72], "frac": [26, 31, 49, 71, 72], "vert": [26, 66, 72, 86], "w_z": [26, 72], "lvert": [26, 72], "rvert": [26, 72], "n_target_nod": 26, "allset_lay": [27, 65], "allset_transformer_lay": [27, 66], "allset_transform": 27, "dhgcn_layer": [27, 67], "dhgcn": 27, "hmpnn_layer": [27, 68], "hnhn_layer_bi": [27, 70], "hnhn_layer": [27, 69], "hypergat_lay": [27, 71], "hypersage_lay": [27, 72], "unigcn_lay": [27, 73], "unigcnlay": [27, 29, 73], "unigcn": [27, 29, 59], "unigcnii_lay": [27, 74], "unigcniilay": [27, 31, 74], "unigcnii": [27, 31, 32], "uniginlay": [27, 33, 75], "unigin": [27, 32, 33], "unisagelay": [27, 35, 76], "unisag": [27, 34, 35, 76], "huang": [28, 29, 30, 31, 32, 33, 34, 35, 59], "yang": [28, 29, 30, 31, 32, 33, 34, 35, 47, 53, 55, 59, 81, 82, 84, 85], "unignn": [28, 29, 30, 31, 32, 33, 34, 35], "unifi": [28, 29, 30, 31, 32, 33, 34, 35], "ijcai": [28, 29, 30, 31, 32, 33, 34, 35], "00956": [28, 29, 30, 31, 32, 33, 34, 35], "use_bn": [29, 35], "boolean": [29, 35, 60], "bathnorm": [29, 35], "everi": [29, 30, 31, 33, 35, 59, 74, 75], "hyper": [29, 31, 33, 35, 74, 86], "constitu": [29, 31, 33, 35], "third": [29, 31, 35], "num_lay": [30, 74, 82, 85], "expect": [30, 31, 74, 81, 83], "contain": [30, 31, 56, 59, 62, 65, 66, 68, 70, 74, 82, 86], "y_hat": [30, 62, 63, 64, 65, 66, 67, 69, 71, 72, 74, 77, 78, 79, 81, 82, 83, 84, 85], "determin": [31, 86], "theta_2": 31, "theta_1": 31, "x_skip": 31, "degre": 31, "sqrt": 31, "d_x": 31, "d_z": 31, "in_channels_nod": [32, 75], "intermediate_channel": [32, 54, 67, 75, 85], "unigin_lay": [33, 75], "ep": 33, "train_ep": 33, "g": [33, 49, 54, 55, 60, 65, 66, 71, 79, 86], "sequenti": [33, 68, 70, 75], "constant": 33, "gin": 33, "unisage_lay": [35, 76], "e_aggr": 35, "amax": 35, "amin": 35, "v_aggr": 35, "operatornam": [35, 43, 71], "sage": 35, "submodul": 36, "simplici": [36, 37, 38, 39, 40, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 59, 63, 64, 65, 66, 67, 69, 71, 72, 84], "dist2cycl": [37, 38, 41], "binari": [37, 39, 42, 46, 50, 52, 62, 68, 69, 70, 73, 75, 76, 79], "high": [37, 39, 40], "x_1e": [37, 77], "linv": [37, 38, 77], "adjacency_0": [37, 38, 39, 40, 62, 63, 77, 78], "hot": [37, 39, 42, 48, 52, 69, 77, 78, 79, 82], "dist2cycle_lay": 38, "dist2cyclelay": [38, 41], "x_e": 38, "a_0": [38, 40], "hsn": [39, 40, 41, 59], "hsn_layer": [40, 59], "hsnlayer": [40, 41, 59, 78, 81], "complic": [40, 43], "higher": [40, 47, 53, 59, 62, 78, 81, 84, 85], "geometr": [40, 59], "openreview": [40, 51], "net": [40, 51, 57, 59], "id": [40, 51, 60], "sc8glb": 40, "k6e9": 40, "sanconv": [41, 43], "sanlay": [41, 43], "san": [41, 42, 43, 59, 62], "compute_projection_matrix": [41, 42], "scacmpslay": [41, 45, 80], "intra_aggr": [41, 45], "weight_func": [41, 45], "scacmp": [41, 44, 80], "sccnlayer": [41, 47, 53, 81], "sccn": [41, 46, 47, 53, 59], "sccnnlayer": [41, 49, 82], "aggr_norm_func": [41, 49, 55], "chebyshev_conv": [41, 49, 55], "sccnn": [41, 48, 49], "sccnncomplex": [41, 48, 82], "scconvlay": [41, 51], "scconv": [41, 50, 51], "scn2layer": [41, 47, 53], "scn2": [41, 47, 52, 84], "scnnlayer": [41, 55, 85], "scnn": [41, 54, 55, 82], "sconelay": [41, 57, 86], "scone": [41, 56, 57, 59], "trajectoriesdataset": [41, 56, 86], "vectorize_path": [41, 56, 86], "generate_complex": [41, 56, 86], "generate_trajectori": [41, 56, 86], "n_filter": [42, 43], "order_harmon": 42, "epsilon_harmon": 42, "simplex_order_k": [42, 79], "simplic": [42, 45, 48, 55, 79, 81, 82, 85, 86], "approxim": [42, 43, 86], "filter": [42, 43, 49, 82], "harmon": 42, "epsilon": 42, "1e": [42, 69, 73, 76, 86], "laplacian": [42, 43, 44, 45, 48, 49, 53, 54, 55, 79, 80, 81, 82, 84], "calcul": [42, 60], "compon": [42, 60, 79], "hodg": [42, 49, 53, 55, 79, 81, 82], "laplacian_up": [42, 43, 54, 55, 79, 85], "laplacian_down": [42, 43, 54, 55, 79, 85], "channels_in": 42, "ld": [42, 77], "san_lay": 43, "07485": 43, "l_": [43, 55, 57, 79, 80, 85, 86], "wh_1": 43, "simplex": [43, 44, 52, 53, 63, 64, 65, 66, 67, 71, 72, 77, 78, 79, 81, 82, 83, 85, 86], "projection_mat": 43, "2p": [43, 79], "q_r": [43, 79], "n_cell": 43, "down_indic": 43, "n_cells_down": 43, "n_neighbor": 43, "up_indic": 43, "n_cells_up": 43, "sca": [44, 45, 59], "cmp": [44, 45], "sca_cmp": [44, 80], "channels_list": [44, 45, 80], "complex_dim": [44, 45, 80], "tetahedron": 44, "respect": [44, 59, 64, 65, 66, 71, 72, 79, 82, 85, 86], "complex_dimens": 44, "highest": [44, 45, 86], "being": [44, 59, 79], "x_list": [44, 45], "laplacian_down_list": 44, "incidence_t_list": 44, "etc": [44, 55, 60, 74], "start": [44, 59, 60, 74, 86], "autoencod": [45, 59], "sca_cmps_lay": 45, "coadjac": 45, "chain": [45, 57, 86], "maroula": 45, "cai": 45, "2103": 45, "04046": 45, "down_lap_list": 45, "incidencet_list": 45, "qquad": [45, 49, 80], "hold": [45, 60], "untouch": 45, "max_rank": [46, 47, 81, 82, 85], "dict": [46, 47], "length": [46, 47, 60, 68, 70, 74], "n_rank_r_cel": [46, 47], "n_rank_r_minus_1_cel": [46, 47], "b_r": [46, 47, 64, 81], "h_r": [46, 47, 53, 81], "sccn_layer": [47, 53], "ani": [47, 48, 60, 62, 63, 72, 86], "leftmost": 47, "diagram": [47, 53, 59], "yang22c": [47, 53, 84], "figur": [47, 53, 69], "11": [47, 53, 59, 62, 63, 64, 68, 69, 70, 71, 73, 74, 75, 76, 77, 78, 79, 81, 82, 83, 85, 86], "scn2_layer": [47, 53], "scn": [47, 53], "abov": [47, 53, 59, 60, 85, 86], "below": [47, 53, 68, 75, 86], "sala": [47, 53, 84], "bogdan": [47, 53, 84], "effici": [47, 53, 77, 81, 82, 84], "proceed": [47, 53, 57, 84], "mlr": [47, 53, 57, 84], "press": [47, 53, 57, 84], "v198": [47, 53, 84], "yang22a": [47, 53, 84], "html": [47, 53, 57, 70, 84], "describ": [47, 60, 66], "unnorm": 47, "bigcup": [47, 81], "out_featur": [47, 62, 82, 85], "in_channels_al": [48, 82], "intermediate_channels_al": [48, 82], "out_channels_al": [48, 82], "conv_ord": [48, 49, 55, 82], "sc_order": [48, 49, 82], "task": [48, 54, 59, 65, 66, 68, 69, 70, 74, 77, 78, 81, 82, 86], "we": [48, 49, 54, 55, 57, 59, 60, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 83, 84, 86], "cours": [48, 60], "amend": 48, "intermedi": [48, 54], "sc": [48, 49, 56, 80, 82, 85, 86], "numer": 48, "x_all": [48, 49, 82], "laplacian_al": [48, 49, 82], "incidence_al": [48, 49, 82], "entri": [48, 79], "n_simplic": [48, 49, 54, 55], "l0": 48, "l1_d": 48, "l1_u": 48, "l2": 48, "pf": 48, "b2": [48, 49, 81, 82, 83, 85], "sccnn_layer": 49, "triangl": [49, 55, 56, 57, 79, 82, 86], "ndoe": 49, "too": 49, "mani": [49, 60, 62], "exampl": [49, 55, 59, 79, 82, 85, 86], "here": [49, 51, 55, 59, 60, 62, 67, 68, 70, 74, 77, 78, 81, 82, 86], "pseudocod": [49, 55], "l_0": 49, "lap_down": [49, 55], "l_1_down": 49, "lap_up": [49, 55], "l_1_up": 49, "lap": 49, "l_2": 49, "y_0": 49, "y_1": 49, "y_2": 49, "look": [49, 55, 60, 86], "einsum": [49, 55], "weight_0": 49, "weight_1": 49, "weight_2": 49, "total_order_0": 49, "total_order_1": 49, "total_order_2": 49, "chebyshev": [49, 55], "conv_oper": [49, 55], "perform": [49, 54, 55, 59, 60, 62, 63, 64, 65, 66, 67, 69, 71, 72, 73, 74, 76, 77, 78, 79, 81, 83, 84, 86], "num_channel": [49, 55], "repres": [49, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 74, 77, 78, 79, 84, 86], "n_triangl": [49, 57], "laplacian_0": [49, 52, 53, 82, 84, 85], "laplacian_down_1": [49, 80, 82, 85], "laplacian_up_1": [49, 82, 85], "laplacian_2": [49, 52, 53, 82, 84, 85], "part": [49, 55, 62, 77, 78, 79, 81, 82], "node_channel": [50, 51, 67, 83], "edge_channel": [50, 51, 83], "face_channel": [50, 51, 83], "incidence_1_norm": [50, 51, 83], "incidence_2": [50, 51, 57, 64, 82, 83, 85], "incidence_2_norm": [50, 51, 83], "adjacency_up_0_norm": [50, 51, 83], "adjacency_up_1_norm": [50, 51, 83], "adjacency_down_1_norm": [50, 51, 83], "adjacency_down_2_norm": [50, 51, 83], "_1": [50, 51, 82, 83, 85], "_2": [50, 51, 82], "scconv_lay": 51, "bunch": [51, 83], "fung": 51, "singh": [51, 59], "tda": 51, "forum": 51, "tlbnskrt6j": 51, "tild": [51, 83], "For": [51, 54, 57, 60, 61, 69, 72, 77, 78, 82, 85, 86], "mai": [51, 59], "helper": 51, "pyt": 51, "team": [51, 59], "laplacian_1": [52, 53, 84], "log": [53, 60, 81, 86], "rightmost": 53, "pshm23": 53, "2i": [53, 81], "node_featur": 53, "edge_featur": 53, "face_featur": 53, "l_upper": 53, "l_lower": 53, "conv_order_down": [54, 55, 85], "conv_order_up": [54, 55, 85], "aggr": [54, 85], "At": [54, 82, 85], "simplci": 54, "To": [54, 61, 65, 69, 76, 77, 78, 85, 86], "challeng": 54, "one_dimensional_cells_mean": [54, 63, 64], "dimension": [54, 66, 86], "scnn_layer": 55, "total_ord": 55, "isufi": 55, "leu": 55, "2110": 55, "02585": 55, "n_simplex": 55, "simplicialcomplex": [56, 73, 75, 76, 86], "hidden_dim": [56, 86], "trajectori": [56, 57], "dataset": [56, 59, 62, 63, 64, 65, 66, 67, 68, 70, 71, 72, 73, 74, 75, 76, 79, 80], "path": [56, 86], "100": [56, 67, 68, 69, 70, 73, 74, 75, 76, 80, 81, 83, 84, 86], "ndarrai": [56, 60, 86], "uniformli": [56, 86], "sampl": [56, 60, 62, 69, 86], "random": [56, 62, 70, 86], "point": [56, 86], "unit": [56, 59, 60, 86], "squar": [56, 60, 86], "delaunai": [56, 86], "triangul": [56, 86], "delet": [56, 86], "some": [56, 60, 81, 86], "pre": [56, 59, 86], "disk": [56, 86], "coord": [56, 86], "n_max": [56, 86], "1000": [56, 69, 86], "corner": [56, 86], "middl": [56, 86], "scone_lay": 57, "stack": [57, 62, 63, 64, 65, 67, 68, 69, 70, 73, 75, 76, 77, 78, 79, 81, 82, 85, 86], "befor": [57, 59, 60, 74, 86], "neighbour": [57, 86], "next": [57, 60, 69, 71, 74, 86], "when": [57, 59, 60, 62, 79, 86], "roddenberri": [57, 59, 86], "mitchel": 57, "glaze": 57, "principl": [57, 86], "v139": 57, "roddenberry21a": 57, "variou": 58, "librari": 58, "torch_scatt": 58, "py": [58, 59, 60, 63, 70, 74, 77, 81, 82, 83, 85], "rusty1": 58, "pytorch_scatt": 58, "dim": [58, 60, 62, 63, 64, 65, 66, 67, 68, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 82, 85, 86], "dim_siz": 58, "welcom": [59, 60], "host": 59, "annual": 59, "topologi": [59, 67, 79], "geometri": 59, "tag": 59, "machin": [59, 84], "review": [59, 60, 61], "contributor": [59, 60], "mathild": [59, 61], "mustafa": [59, 61], "nina": [59, 61], "florian": 59, "frantzen": 59, "ghada": [59, 61], "alzamzmi": 59, "theodor": [59, 61], "michael": [59, 61], "scholkemp": 59, "josef": 59, "hopp": 59, "karthikeyan": [59, 61], "natesan": [59, 61], "johan": 59, "math": [59, 60, 62, 66, 71, 72, 85, 86], "audun": 59, "myer": 59, "helen": 59, "jenn": 59, "tim": 59, "doster": 59, "tegan": 59, "emerson": 59, "henri": 59, "kving": 59, "bastian": [59, 84], "rieck": [59, 84], "sophia": [59, 61], "jan": 59, "meissner": 59, "paul": [59, 61, 84], "tolga": [59, 61], "vincent": 59, "grand": 59, "aldo": [59, 61], "tamal": [59, 61], "soham": [59, 61], "shreya": [59, 61], "neal": [59, 61], "robin": [59, 61], "edit": [59, 60], "now": [59, 62, 63, 65, 66, 67, 68, 69, 70, 71, 72, 74, 77, 78, 79, 84, 86], "over": [59, 62, 65, 66, 67, 68, 70, 72, 73, 76, 77, 78, 79, 81, 83, 84, 86], "thank": 59, "stellar": 59, "contirbut": 59, "foster": 59, "reproduc": [59, 61], "open": [59, 74], "research": [59, 84], "winner": 59, "announc": 59, "luca": 59, "scofano": 59, "claudio": 59, "guillermo": 59, "bernardez": 59, "simon": 59, "fiorellino": 59, "indro": 59, "spinelli": 59, "scardapan": 59, "lev": 59, "telyatninkov": 59, "olga": 59, "zaghen": 59, "sadrodin": 59, "barikbin": [59, 70], "odin": 59, "hoff": 59, "gardaa": 59, "dmitrii": 59, "gavrilev": 59, "gleb": 59, "bazhenov": 59, "suraj": 59, "combinatori": 59, "rub\u00e9n": 59, "ballest": 59, "manuel": 59, "lecha": 59, "sergio": 59, "escalera": 59, "hoan": 59, "aiden": 59, "brent": 59, "honor": 59, "mention": 59, "jen": 59, "agerberg": 59, "georg": 59, "b\u00f6kman": 59, "pavlo": 59, "melnyk": 59, "alessandro": 59, "salatiello": 59, "alexand": 59, "nikitin": 59, "purpos": [59, 60, 63, 64, 65, 66, 67, 68, 70, 72, 73, 76, 77, 78, 79, 81, 82], "crowdsourc": 59, "ask": 59, "contribut": [59, 71, 82], "code": [59, 60, 86], "previous": 59, "exist": 59, "benchmark": [59, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 80, 81, 82, 83, 84, 85], "python": [59, 60, 61], "packag": [59, 61, 63, 70, 74, 77, 81, 82, 83, 85], "take": [59, 62, 63, 64, 79, 86], "pull": [59, 60], "request": [59, 60, 74, 86], "literatur": [59, 61, 74], "leverag": [59, 79], "infrastructur": 59, "invit": 59, "regularli": 59, "white": 59, "summar": 59, "find": [59, 62, 86], "publish": 59, "qualifi": 59, "opportun": 59, "author": [59, 61, 70, 86], "top": [59, 62, 82], "8": [59, 62, 63, 64, 66, 68, 69, 70, 72, 73, 74, 75, 76, 77, 78, 79, 81, 82, 84, 85, 86], "best": [59, 66], "addit": 59, "softwar": [59, 60], "journal": 59, "special": 59, "recognit": 59, "date": 59, "time": [59, 62, 65, 66, 67, 68, 69, 70, 71, 72, 77, 78, 79, 81, 82, 85, 86], "must": [59, 65, 66, 67, 69, 71, 72, 77, 78, 79, 81, 82, 85], "juli": 59, "13": [59, 63, 68, 69, 70, 71, 73, 74, 75, 76, 77, 78, 80, 81, 82, 83, 84, 85, 86], "16": [59, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 75, 76, 77, 79, 80, 81, 82, 83, 84, 85, 86], "59": [59, 68, 69, 73, 77, 81], "pacif": 59, "standard": [59, 60, 74, 79], "modifi": [59, 60, 80], "until": 59, "everyon": [59, 60], "free": [59, 86], "suffici": 59, "accept": 59, "automat": [59, 79], "subscrib": 59, "encourag": 59, "earli": 59, "help": [59, 60], "debug": 59, "fail": 59, "test": [59, 62, 63, 64, 65, 66, 67, 68, 69, 70, 72, 73, 74, 75, 76, 77, 78, 79, 81, 82], "address": 59, "potenti": 59, "issu": [59, 74], "similar": [59, 81], "qualiti": 59, "earlier": [59, 81], "prioriti": 59, "consider": 59, "restrict": 59, "member": 59, "than": [59, 85, 86], "princip": 59, "develop": [59, 60], "allow": [59, 81], "fig": [59, 86], "compli": 59, "action": 59, "workflow": 59, "successfulli": 59, "lint": 59, "format": [59, 60, 74, 82], "black": [59, 86], "isort": 59, "flake8": 59, "_layer": 59, "ex": 59, "store": [59, 69], "directori": [59, 60], "primit": 59, "equival": [59, 60], "depict": 59, "_train": 59, "ipynb": 59, "hsn_train": 59, "tutori": [59, 61, 74], "process": [59, 60, 74], "well": [59, 60], "load": [59, 63, 64, 65, 66, 67, 70, 71, 72, 76, 77, 78, 80, 81, 82, 83, 84, 85], "toponetx": [59, 62, 63, 64, 65, 66, 67, 69, 71, 72, 73, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86], "shrec16": [59, 63, 64, 65, 66, 67, 71, 72, 80, 82, 85], "suitabl": [59, 69], "template_lay": 59, "karat": [59, 69, 77, 78, 79, 81, 82], "club": [59, 69, 77, 78, 79, 81, 82], "choic": [59, 62, 63, 64, 65, 66, 67, 71, 72, 73, 75, 76, 80], "along": [59, 62, 79], "simpl": [59, 85], "depend": 59, "accuraci": [59, 69, 70, 73, 74, 75, 76, 77, 78, 79, 81, 82, 85, 86], "test_": [59, 60], "name_of_model": 59, "test_hsn_lay": 59, "testhsnlay": 59, "pleas": [59, 60, 62, 63, 70, 74, 79, 82, 85], "pytest": [59, 60], "unittest": 59, "further": [59, 62, 79], "manipul": 59, "modif": 59, "accompani": 59, "appropri": [59, 60], "locat": [59, 60, 74], "With": [59, 71], "said": 59, "highli": 59, "most": [59, 60, 79, 81], "resort": 59, "absolut": 59, "condorcet": 59, "decid": [59, 79], "criteria": 59, "chosen": [59, 81], "correctli": 59, "need": [59, 62, 63, 64, 65, 66, 67, 69, 71, 72, 77, 78, 79, 84, 86], "match": 59, "readabl": [59, 60], "clean": 59, "api": [59, 60], "written": 59, "clearli": 59, "explain": 59, "robust": 59, "reward": 59, "nor": 59, "goal": 59, "accur": 59, "our": [59, 60, 61, 62, 63, 64, 65, 66, 67, 69, 71, 72, 73, 75, 76, 77, 78, 79, 80, 81, 82, 84, 85, 86], "field": 59, "select": [59, 81, 85], "maintain": 59, "collabor": 59, "whose": [59, 60], "vote": 59, "onc": [59, 65, 66, 67, 71, 72], "googl": [59, 60], "express": [59, 71], "even": 59, "link": [59, 60], "record": [59, 60, 65, 66, 67, 71, 72], "email": 59, "identifi": 59, "voter": 59, "ident": [59, 66, 77, 78, 82], "remain": [59, 86], "secret": 59, "feel": [59, 86], "contact": 59, "slack": 59, "ucsb": 59, "edu": 59, "guid": 60, "aim": [60, 62, 79], "eas": 60, "both": [60, 62, 79, 86], "novic": 60, "experienc": 60, "commun": 60, "effort": 60, "fork": 60, "upstream": 60, "submit": [60, 74], "pr": 60, "synchron": 60, "your": 60, "branch": 60, "git": 60, "checkout": 60, "sure": 60, "section": [60, 86], "re": [60, 77, 86], "done": [60, 63, 64, 65, 66, 67, 71, 72, 76, 80, 82, 83, 84, 85, 86], "commit": 60, "modified_fil": 60, "my": [60, 80], "push": 60, "toponextx": 60, "instruct": 60, "repeat": 60, "folder": 60, "filenam": 60, "test_add": 60, "def": [60, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 81, 82, 83, 85, 86], "test_capital_cas": 60, "assert": [60, 86], "9": [60, 62, 63, 64, 66, 68, 69, 70, 72, 73, 74, 75, 76, 77, 78, 79, 81, 82, 84, 85, 86], "statement": 60, "under": 60, "correct": [60, 62, 73, 75, 76, 86], "instal": 60, "tool": 60, "pip": 60, "dev": 60, "verifi": 60, "break": 60, "doc": 60, "descript": [60, 86], "usag": 60, "inform": [60, 65, 66, 68, 71, 72, 85], "markdown": 60, "languag": 60, "common": [60, 62], "restructuredtext": 60, "numpi": [60, 62, 63, 64, 65, 66, 67, 69, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86], "style": 60, "understand": 60, "role": 60, "syntax": 60, "autom": 60, "pars": 60, "inclus": 60, "print": [60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86], "__doc__": 60, "attribut": 60, "try": [60, 62, 63, 79, 86], "np": [60, 62, 63, 64, 65, 66, 67, 69, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86], "good": [60, 84], "These": 60, "ones": [60, 68, 70], "summari": 60, "line": 60, "79": [60, 67, 68, 69, 73, 81, 85], "char": 60, "immedi": 60, "capit": 60, "letter": 60, "period": 60, "verb": 60, "imper": 60, "mood": 60, "possibl": [60, 74], "uncertain": 60, "oppos": 60, "evalu": [60, 62, 68, 69, 70, 74, 75], "separ": 60, "blank": 60, "argument": [60, 65, 66, 79], "On": 60, "state": [60, 72, 74], "rest": 60, "space": [60, 82, 86], "side": 60, "default_valu": 60, "indent": 60, "esp": 60, "would": [60, 77, 78], "want": [60, 74, 86], "veri": [60, 74], "rais": [60, 82, 85], "latex": 60, "cite": [60, 74], "my_method": 60, "my_param_1": 60, "my_param_2": 60, "big": 60, "o": [60, 66, 69, 79], "short": 60, "my_result": 60, "relev": 60, "snippet": 60, "show": [60, 69, 77, 78, 86], "script": 60, "wikipedia": 60, "page": [60, 84], "And": 60, "fill": 60, "scikit": 60, "fit_predict": 60, "sample_weight": 60, "cluster": [60, 80], "center": [60, 86], "conveni": 60, "fit": 60, "sparse_matrix": 60, "n_featur": 60, "ignor": [60, 73, 76], "Not": 60, "present": [60, 62], "convent": [60, 62], "observ": 60, "labels_": 60, "mind": 60, "instead": [60, 74, 82], "vari": 60, "notat": [60, 62, 63, 64, 69, 77, 78, 79, 80, 81, 83, 85, 86], "axi": [60, 86], "bracket": 60, "multinomi": 60, "1d": [60, 63, 64], "2d": [60, 63, 64], "subset": [60, 62, 68, 70], "datafram": 60, "explicitli": 60, "relat": [60, 68], "colon": 60, "explan": 60, "_weight_boost": 60, "adaboost": 60, "great": 60, "ve": 60, "discuss": 60, "Of": 60, "verbos": 60, "thei": [60, 62, 63, 64, 65, 66, 77, 78, 79, 81, 82, 85], "rst": 60, "80": [60, 67, 68, 69, 73, 80, 81], "charact": 60, "except": [60, 62, 79], "tabl": 60, "tdl": 61, "blue": 61, "laid": 61, "extend": [61, 82], "avail": [61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 74, 79, 80], "about": 61, "blueprint": 61, "misc": 61, "hajij2023topolog": 61, "titl": [61, 69], "year": 61, "eprint": 61, "archiveprefix": 61, "primaryclass": 61, "lg": 61, "papillon2023architectur": 61, "notebook": [62, 65, 66, 67, 68, 69, 70, 71, 72, 77, 78, 79, 82, 83, 84, 85, 86], "didact": [62, 79], "clear": [62, 79], "technic": [62, 79], "document": [62, 68, 70, 74, 79], "sinc": [62, 77, 78, 81, 82, 86], "introduct": 62, "achiev": [62, 66, 81, 86], "outstand": 62, "howev": [62, 74, 82, 85], "pairwis": [62, 65, 66, 67, 71, 72, 86], "relationship": 62, "among": 62, "abl": [62, 79], "fulli": 62, "interact": 62, "real": [62, 86], "world": [62, 86], "vertic": [62, 86], "captur": 62, "particular": 62, "encod": [62, 69, 77, 78, 79, 82, 86], "design": 62, "independ": [62, 66], "thu": [62, 79, 84], "strategi": [62, 72], "approach": 62, "hierarch": 62, "incorpor": 62, "algorithm": 62, "ii": 62, "optim": [62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86], "iii": 62, "extract": [62, 65, 66, 71, 72, 76], "compact": 62, "meaning": 62, "remark": [62, 79], "custom": [62, 79], "symbol": [62, 79], "involv": [62, 79], "made": [62, 69, 77, 78, 79, 81, 82, 84, 85], "stage": 62, "nbsphinx": [62, 66, 71, 72, 85], "textrm": [62, 79], "parameter": 62, "mathbb": [62, 65, 66, 71, 86], "2f_0": 62, "f_0": 62, "textbf": [62, 65, 66, 72, 79], "bigg": [62, 79, 82, 85], "f": [62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86], "respons": [62, 74], "reciproc": 62, "round": [62, 83], "tcdot": 62, "xin": 62, "score": [62, 86], "_r": 62, "coars": 62, "mutag": [62, 73, 75, 76], "tudataset": [62, 73, 75, 76], "paperswithcod": 62, "com": [62, 70, 74, 76], "__": 62, "188": [62, 69, 73, 81], "chemic": 62, "compound": 62, "discret": 62, "mutagen": 62, "salmonella": 62, "typhimurium": 62, "sklearn": [62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 75, 76, 80, 82, 84, 85], "model_select": [62, 63, 64, 65, 66, 67, 69, 71, 72, 73, 75, 76, 80, 82, 84, 85], "train_test_split": [62, 63, 64, 65, 66, 67, 69, 71, 72, 73, 75, 76, 80, 82, 84, 85], "cell_complex": [62, 63, 64], "cellcomplex": 62, "torch_geometr": [62, 68, 70, 73, 75, 76, 79], "convert": [62, 63, 64, 69, 73, 75, 76, 77, 78, 79, 82, 84], "to_networkx": [62, 73, 75, 76, 79], "gpu": [62, 63, 64, 65, 66, 67, 68, 69, 70, 74, 80], "run": [62, 63, 64, 65, 66, 67, 68, 69, 70, 74, 81, 86], "cuda": [62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 74, 79, 80, 83, 84], "is_avail": [62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 74, 79, 80, 83, 84], "els": [62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 74, 79, 80, 82, 83, 84, 85], "root": [62, 73, 75, 76], "tmp": [62, 73, 74, 75, 76], "use_edge_attr": [62, 73, 75, 76], "use_node_attr": 62, "cc_list": [62, 63, 64], "x_0_list": 62, "x_1_list": [62, 73, 75, 76], "y_list": [62, 73, 75, 76], "append": [62, 63, 64, 65, 66, 67, 69, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86], "edge_attr": 62, "i_cc": 62, "th": [62, 63, 64, 65, 66, 67, 71, 72, 79, 80, 83, 84], "36": [62, 63, 68, 69, 70, 73, 75, 76, 77, 81], "0th": [62, 83], "17": [62, 68, 69, 70, 73, 75, 76, 77, 80, 81, 82, 85], "38": [62, 68, 69, 70, 73, 75, 76, 77, 81], "lower_neighborhood_list": 62, "upper_neighborhood_list": 62, "adjacency_0_list": [62, 63], "adjacency_matrix": [62, 63, 64, 77, 78, 81, 82, 83, 86], "from_numpi": [62, 63, 64, 65, 66, 67, 69, 70, 71, 72, 73, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85], "todens": [62, 63, 64, 65, 66, 67, 69, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85], "to_spars": [62, 63, 64, 65, 66, 67, 69, 71, 72, 75, 77, 78, 79, 80, 81, 82, 83, 84, 85], "lower_neighborhood_t": 62, "down_laplacian_matrix": [62, 77, 79, 80, 82, 83, 85], "upper_neighborhood_t": 62, "up_laplacian_matrix": [62, 79, 82, 83, 85], "6": [62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86], "__init__": [62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 83, 86], "arg": [62, 82, 83], "super": [62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76], "lift_lay": 62, "rang": [62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 81, 82, 83, 84, 85, 86], "modulelist": [62, 63, 64, 65, 66, 67, 68, 69, 71, 72, 73, 74, 75, 76, 82, 85], "lin_0": [62, 63, 64], "128": [62, 69, 73, 81], "lin_1": [62, 63, 64], "hasattr": 62, "isinst": 62, "feed": [62, 65, 66, 67, 71, 72, 73, 75, 76, 86], "foward": 62, "specifi": [62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 81, 82, 84, 85], "loss": [62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 81, 82, 83, 84, 85, 86], "without": [62, 63, 85], "32": [62, 65, 68, 69, 70, 71, 73, 75, 76, 77, 81, 86], "crit": [62, 63, 73, 75, 76], "crossentropyloss": [62, 63, 68, 69, 70, 74, 75], "opt": [62, 63, 65, 66, 67, 71, 72, 80, 81, 82, 83, 85], "adam": [62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86], "lr": [62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86], "001": [62, 75, 84], "lower_att": 62, "lin": 62, "64": [62, 65, 66, 68, 69, 73, 77, 81], "upper_att": 62, "split": [62, 63, 64, 65, 66, 67, 69, 70, 72, 73, 74, 75, 76, 83, 86], "test_siz": [62, 63, 64, 65, 66, 67, 69, 71, 72, 73, 75, 76, 80, 82, 84, 85, 86], "x_1_train": [62, 63, 64, 73, 75, 76, 80, 82, 83], "x_1_test": [62, 63, 64, 73, 75, 76, 80, 82], "shuffl": [62, 63, 64, 65, 66, 67, 71, 72, 73, 75, 76, 80, 82, 84, 85, 86], "x_0_train": [62, 63, 64, 65, 66, 67, 70, 71, 72, 80, 82, 83], "x_0_test": [62, 63, 64, 65, 66, 67, 70, 71, 72, 80, 82], "lower_neighborhood_train": 62, "lower_neighborhood_test": 62, "upper_neighborhood_train": 62, "upper_neighborhood_test": 62, "adjacency_0_train": [62, 63], "adjacency_0_test": [62, 63], "y_train": [62, 63, 64, 65, 66, 67, 69, 71, 72, 73, 75, 76, 77, 78, 79, 80, 81, 82, 84, 85], "y_test": [62, 63, 64, 65, 66, 67, 69, 71, 72, 73, 75, 76, 77, 78, 79, 80, 81, 82, 84, 85], "test_interv": [62, 63, 64, 65, 66, 67, 69, 71, 72, 74, 77, 78, 79, 81, 82, 83, 84, 85], "num_epoch": [62, 63, 64, 65, 66, 67, 69, 71, 72, 73, 74, 75, 76, 77, 78, 79, 81, 82, 83, 84, 85], "epoch_i": [62, 63, 64, 65, 66, 67, 69, 71, 72, 77, 78, 79, 81, 82, 83, 84, 85], "epoch_loss": [62, 63, 64, 65, 66, 67, 71, 72, 77, 78, 79, 81, 82, 83, 84, 85], "num_sampl": 62, "zip": [62, 63, 64, 65, 66, 67, 71, 72, 73, 75, 76, 82, 83, 84, 85], "dtype": [62, 65, 66, 67, 68, 69, 70, 71, 72, 74, 82, 85], "long": [62, 68, 70, 81], "zero_grad": [62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 81, 82, 83, 84, 85, 86], "argmax": [62, 68, 69, 70, 74, 75, 85, 86], "backward": [62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 81, 82, 83, 84, 85, 86], "train_acc": [62, 68, 69, 74, 77, 78, 79, 81, 82, 85, 86], "epoch": [62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 81, 82, 83, 84, 85, 86], "4f": [62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 77, 78, 79, 81, 82, 84, 85], "flush": [62, 63, 64, 65, 66, 67, 69, 71, 72, 77, 78, 79, 81, 82, 83, 84, 85], "no_grad": [62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 81, 82, 83, 84, 85], "test_acc": [62, 68, 69, 74, 77, 78, 79, 81, 82, 85], "6234": 62, "6794": 62, "5965": 62, "6047": 62, "6947": 62, "5953": 62, "5879": 62, "7099": 62, "5614": 62, "5801": 62, "6316": 62, "5755": [62, 69], "7405": 62, "7544": 62, "5624": [62, 77, 84], "5565": [62, 69], "7719": 62, "small": [63, 64, 65, 66, 67, 71, 72, 80, 81, 82, 83, 84, 85], "3d": [63, 64, 65, 66, 67, 71, 72, 80, 83, 84], "mesh": [63, 64, 65, 66, 67, 71, 72, 80, 82, 83, 84, 85], "shrec": [63, 64, 65, 66, 67, 71, 72, 80, 83, 84, 85], "shrec_16": [63, 64, 65, 66, 67, 71, 72, 80, 82, 83, 84, 85], "kei": [63, 64, 65, 66, 67, 71, 72, 80, 82, 83, 84, 85], "node_feat": [63, 64, 65, 66, 67, 69, 71, 72, 77, 78, 79, 80, 81, 82, 83, 84, 85], "edge_feat": [63, 64, 65, 66, 67, 69, 71, 72, 77, 78, 79, 80, 81, 82, 83, 84, 85], "face_feat": [63, 64, 65, 66, 67, 71, 72, 77, 78, 79, 80, 81, 82, 83, 84, 85], "i_complex": [63, 64, 65, 66, 67, 71, 72, 80, 83, 84], "6th": [63, 64, 65, 66, 67, 71, 72, 80, 84], "252": [63, 64, 65, 66, 67, 69, 71, 72, 73, 80, 83, 84], "750": [63, 64, 65, 66, 67, 69, 71, 72, 80, 83, 84], "10": [63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86], "500": [63, 64, 65, 66, 67, 69, 71, 72, 73, 80, 83, 84], "messg": [63, 65, 66, 71, 72, 77, 78, 84], "incidence_2_t_list": 63, "to_cell_complex": [63, 64], "incidence_2_t": [63, 80], "incidence_matrix": [63, 64, 65, 66, 67, 69, 71, 72, 73, 75, 76, 77, 78, 79, 80, 81, 82, 83, 85], "lin_2": [63, 64], "0d": [63, 64], "nan": [63, 64, 69, 83], "two_dimensional_cells_mean": [63, 64], "nanmean": [63, 64], "isnan": [63, 64], "zero_dimensional_cells_mean": [63, 64], "loss_fn": [63, 65, 66, 67, 68, 70, 71, 72, 74, 80, 82, 83, 84, 85], "mseloss": [63, 64, 65, 66, 67, 71, 72, 80, 82, 83, 84, 85], "incidence_2_t_train": 63, "incidence_2_t_test": 63, "low": [63, 64, 65, 66, 67, 68, 70, 72, 73, 76, 77, 78, 79, 81, 82, 83, 84], "minim": [63, 64, 65, 66, 67, 68, 70, 72, 73, 76, 82], "rapid": [63, 64, 65, 66, 67, 68, 70, 72, 73, 76, 82], "test_loss": [63, 64, 65, 66, 67, 68, 71, 72, 82, 83, 84, 85], "96": [63, 68, 69, 70, 73, 81], "4544": 63, "ninamiolan": [63, 81, 82, 85], "anaconda3": [63, 77, 81, 82, 83, 85], "env": [63, 77, 81, 82, 83, 85], "tmxtest": 63, "lib": [63, 70, 77, 81, 82, 83, 85], "python3": [63, 70, 81, 82, 83, 85], "site": [63, 70, 77, 81, 82, 83, 85], "536": [63, 69, 82, 85], "userwarn": [63, 74, 82, 83, 85], "lead": [63, 82, 85], "incorrect": [63, 82, 85], "due": [63, 81, 82, 85], "ensur": [63, 82, 85], "mse_loss": [63, 82, 85], "reduct": [63, 82, 85], "82": [63, 67, 68, 69, 73, 74, 81], "0496": [63, 69], "4422": [63, 69], "83": [63, 67, 68, 69, 70, 73, 81], "8916": 63, "9388": 63, "49": [63, 68, 69, 70, 73, 75, 76, 77, 81], "7630": 63, "12": [63, 68, 69, 70, 71, 73, 74, 75, 76, 77, 78, 79, 81, 82, 84, 85, 86], "99": [63, 68, 69, 70, 73, 74, 81, 83, 86], "6948": 63, "84": [63, 64, 67, 68, 69, 73, 81, 85], "4177": 63, "39": [63, 68, 69, 70, 73, 74, 75, 76, 77, 81, 82, 83], "5379": [63, 77], "85": [63, 64, 67, 68, 69, 70, 73, 81, 85], "5503": 63, "8946": 63, "6596": 63, "interc": 64, "incidence_2_list": [64, 82, 83, 85], "adjacency_1_list": 64, "incidence_1_t_list": 64, "adjacency_1": [64, 83], "incidence_1_t": [64, 80], "proj_0": 64, "proj_1": 64, "proj_2": 64, "elu": 64, "05": [64, 86], "criterion": [64, 69], "x_2_train": [64, 80, 82, 83], "x_2_test": [64, 80, 82], "adjacency_1_train": 64, "adjacency_1_test": 64, "incidence_2_train": [64, 82], "incidence_2_test": [64, 82], "incidence_1_t_train": 64, "incidence_1_t_test": 64, "106": [64, 69, 73, 81], "5665": 64, "4893": [64, 68], "54": [64, 68, 69, 73, 74, 77, 81], "3770": 64, "0177": 64, "6247": 64, "51": [64, 68, 69, 70, 73, 74, 77, 81], "4964": 64, "collect": [65, 66, 67, 68, 70, 71, 72, 85], "let": [65, 66, 71, 72, 86], "v_": [65, 66, 71], "e_": [65, 66], "rule": [65, 66], "put": [65, 66, 81], "f_": [65, 66], "permut": [65, 66], "invari": [65, 66], "parametr": [65, 66], "learnt": [65, 66], "25": [65, 68, 69, 70, 73, 74, 75, 76, 77, 81, 86], "load_ext": [65, 66, 73, 76, 79], "autoreload": [65, 66, 73, 76, 79], "extens": [65, 76], "alreadi": [65, 76], "reload": [65, 76], "reload_ext": [65, 76], "26": [65, 68, 69, 70, 73, 75, 76, 77, 81], "what": [65, 66, 67, 71, 72, 73, 75, 76, 81], "27": [65, 68, 69, 70, 73, 75, 76, 77, 81, 83], "28": [65, 68, 69, 70, 73, 75, 76, 77, 81], "amtric": [65, 66, 67, 71, 72], "unsign": [65, 66, 67, 71, 72], "becom": [65, 66, 67, 71, 72, 79], "simplciial": [65, 66, 67, 71, 72], "wise": [65, 66, 67, 71, 72], "29": [65, 68, 69, 70, 73, 75, 76, 77, 81], "hg_list": [65, 66, 67, 71, 72, 73, 75, 76], "incidence_1_list": [65, 66, 67, 71, 72, 73, 75, 76, 82, 83, 85], "sign": [65, 66, 67, 69, 71, 72, 83, 86], "hg": [65, 66, 67, 71, 72, 73, 75, 76], "to_hypergraph": [65, 66, 67, 69, 71, 72, 73, 75, 76], "30": [65, 68, 69, 70, 73, 74, 75, 76, 77, 78, 79, 81, 82, 83, 85, 86], "1250": [65, 66, 69, 71, 72], "templatelay": [65, 67], "31": [65, 68, 69, 70, 73, 75, 76, 77, 81, 82], "allsetnn": 65, "cidx": 65, "min": 65, "reversed_edge_index": 65, "pooled_x": [65, 66, 67, 71, 72, 73, 76], "33": [65, 68, 69, 70, 73, 75, 76, 77, 81], "34": [65, 68, 69, 70, 73, 75, 76, 77, 78, 79, 81, 82, 83, 85], "incidence_1_train": [65, 66, 71, 72, 73, 75, 76, 82], "incidence_1_test": [65, 66, 71, 72, 73, 75, 76, 82], "35": [65, 68, 69, 70, 73, 74, 75, 76, 77, 81], "to_edge_index": [65, 66, 71, 72], "274": [65, 66, 69, 72, 73], "8233": 65, "529": [65, 66, 69, 71, 72], "0000": [65, 66, 69, 71, 72, 77, 78, 79, 81, 82, 85], "6125": [65, 66, 72], "repo": [66, 70, 72], "rise": 66, "so": [66, 77, 78, 79, 81, 82, 85, 86], "dive": 66, "iter": 66, "Their": 66, "omega": 66, "overset": [66, 79], "delta": 66, "mathbin": 66, "ba": 66, "2016": 66, "hf_": 66, "multihead": 66, "vaswani": 66, "row": 66, "q_n": 66, "9018": 66, "75": [67, 68, 69, 73, 74, 75, 76, 77, 81], "dhgcnlayer": 67, "76": [67, 68, 69, 73, 77, 81], "77": [67, 68, 69, 73, 77, 81, 84], "78": [67, 68, 69, 73, 77, 78, 79, 81, 82, 85], "dir": 67, "81": [67, 68, 69, 73, 81], "dynam": 67, "86": [67, 68, 69, 70, 73, 81], "87": [67, 68, 69, 73, 81, 84], "88": [67, 68, 69, 73, 81, 85], "8821578": 67, "6946": 67, "36521": 67, "6562": 67, "17477": 67, "7685": 67, "212": [67, 69, 73], "8590": 67, "152": [67, 69, 73, 81], "9723": 67, "0999": 67, "177": [67, 69, 73, 81, 85], "0583": [67, 69], "163": [67, 69, 73, 81], "9508": 67, "202": [67, 69, 73], "9814": 67, "22": [67, 68, 69, 70, 73, 75, 76, 77, 81], "2991": 67, "cora": [68, 74], "2708": 68, "academ": [68, 70], "5429": 68, "citat": [68, 74], "categori": [68, 70], "case_bas": 68, "genetic_algorithm": 68, "neural_network": 68, "probabilistic_method": 68, "reinforcement_learn": 68, "rule_learn": 68, "theori": 68, "1433": [68, 69], "stand": [68, 70], "uniqu": [68, 69, 70, 83], "presenc": [68, 70], "planetoid": 68, "metric": [68, 69, 70], "accuracy_scor": [68, 70], "24": [68, 69, 70, 73, 75, 76, 77, 81], "download": [68, 70, 74, 76], "val": [68, 73, 75, 76, 86], "to_hidden_linear": [68, 70], "to_categories_linear": [68, 70], "manual_se": [68, 86], "41": [68, 69, 70, 73, 75, 76, 77, 81], "256": [68, 69, 73], "train_y_tru": [68, 70], "train_mask": [68, 70], "val_y_tru": 68, "val_mask": 68, "initial_x_1": 68, "zeros_lik": 68, "y_pred_logit": [68, 70], "train_loss": [68, 86], "eval": [68, 70, 73, 74, 75, 76, 86], "val_loss": [68, 86], "val_acc": [68, 86], "acc": [68, 70], "2f": [68, 70], "1079": [68, 69], "14": [68, 69, 70, 71, 73, 74, 75, 76, 77, 80, 81, 82, 85, 86], "1436": [68, 69], "0234": 68, "15": [68, 69, 70, 73, 74, 75, 76, 77, 80, 81, 82, 85, 86], "1016": [68, 69], "9800": 68, "0681": [68, 69], "9504": 68, "18": [68, 69, 70, 73, 74, 75, 76, 77, 80, 81, 85], "0389": [68, 69], "9194": 68, "21": [68, 69, 70, 73, 75, 76, 77, 80, 81, 84], "0137": 68, "9241": 68, "19": [68, 69, 70, 73, 75, 76, 77, 80, 81, 83, 86], "9917": 68, "8917": 68, "9729": 68, "8710": 68, "23": [68, 69, 70, 73, 75, 76, 77, 81], "9556": 68, "8574": 68, "9402": 68, "8646": 68, "9265": 68, "8540": 68, "9136": 68, "8430": 68, "9012": 68, "8336": 68, "8886": 68, "8405": 68, "8775": 68, "8264": 68, "8668": 68, "8065": 68, "8562": 68, "37": [68, 69, 70, 73, 74, 75, 76, 77, 81], "8158": 68, "8456": 68, "7957": 68, "44": [68, 69, 70, 73, 75, 76, 77, 81], "8346": 68, "8028": 68, "8249": 68, "20": [68, 69, 70, 73, 74, 75, 76, 77, 80, 81, 86], "7882": 68, "8156": 68, "42": [68, 69, 70, 73, 75, 76, 77, 81, 82], "7912": 68, "8070": 68, "7610": 68, "46": [68, 69, 70, 73, 75, 76, 77, 81], "7987": 68, "7617": 68, "47": [68, 69, 70, 73, 75, 76, 77, 81], "7905": 68, "7596": 68, "7830": 68, "7391": 68, "7740": 68, "7315": 68, "7655": 68, "7365": 68, "7565": 68, "43": [68, 69, 70, 73, 75, 76, 77, 81, 83], "7184": 68, "48": [68, 69, 70, 73, 75, 76, 77, 81], "7459": 68, "7085": 68, "7367": 68, "45": [68, 69, 70, 73, 74, 75, 76, 77, 78, 79, 81, 82, 83, 85], "6815": [68, 69], "7279": 68, "6673": 68, "50": [68, 69, 70, 73, 74, 75, 76, 77, 81], "7178": 68, "6846": 68, "7077": 68, "6483": 68, "7000": [68, 81, 82], "6436": 68, "6971": 68, "6353": 68, "6991": 68, "6336": 68, "6982": 68, "5938": 68, "60": [68, 69, 73, 77, 81, 86], "6980": 68, "5886": 68, "56": [68, 69, 73, 77, 81], "6979": 68, "5974": 68, "55": [68, 69, 73, 77, 81], "6881": [68, 69], "5600": 68, "52": [68, 69, 73, 74, 77, 81], "6694": 68, "5445": 68, "6513": 68, "5501": 68, "6308": 68, "5397": [68, 77], "6141": 68, "5096": 68, "6020": 68, "4992": 68, "5915": [68, 69], "5020": 68, "58": [68, 69, 73, 77, 81], "5829": 68, "4710": 68, "5747": 68, "4608": 68, "67": [68, 69, 70, 73, 77, 81], "5703": 68, "4341": 68, "62": [68, 69, 73, 77, 81], "5632": 68, "4428": 68, "66": [68, 69, 70, 73, 77, 81], "5630": 68, "4209": 68, "5502": 68, "4151": 68, "63": [68, 69, 73, 81], "5303": 68, "53": [68, 69, 73, 74, 77, 81], "4090": 68, "5051": 68, "4021": 68, "3847": 68, "65": [68, 69, 73, 77, 81], "4842": 68, "3907": 68, "61": [68, 69, 73, 77, 81], "4849": 68, "57": [68, 69, 70, 73, 77, 81], "3434": 68, "70": [68, 69, 73, 77, 81], "4866": 68, "3253": 68, "69": [68, 69, 73, 77, 81], "4864": 68, "3380": 68, "4896": 68, "2933": 68, "4921": 68, "3124": 68, "68": [68, 69, 73, 77, 81], "4948": 68, "3091": 68, "4931": 68, "2768": 68, "4881": 68, "2749": 68, "4827": [68, 69], "2740": 68, "4833": 68, "2773": 68, "4744": 68, "2430": 68, "4646": 68, "4648": 68, "1958": [68, 69], "4734": 68, "1895": [68, 69], "71": [68, 69, 73, 77, 81], "4748": 68, "2008": 68, "4760": 68, "72": [68, 69, 73, 77, 81], "1573": [68, 69], "74": [68, 69, 73, 77, 81], "4385": 68, "73": [68, 69, 73, 77, 81], "1751": [68, 69], "4242": 68, "1889": [68, 69], "4183": 68, "1762": [68, 69], "4250": 68, "1737": [68, 69], "4471": 68, "1242": [68, 69], "4559": 68, "0648": [68, 69], "4498": 68, "0717": [68, 69], "4506": 68, "0568": [68, 69, 70], "4410": 68, "0650": [68, 69, 77], "4375": 68, "0475": [68, 69], "4441": 68, "0293": [68, 81], "4450": 68, "0494": [68, 69], "4622": 68, "0200": 68, "4609": 68, "0358": [68, 69], "4645": 68, "9877": 68, "0173": 68, "4788": 68, "89": [68, 69, 70, 73, 81], "9744": 68, "4970": 68, "90": [68, 69, 73, 81, 85], "9497": 68, "4981": 68, "91": [68, 69, 70, 73, 81], "9345": 68, "4736": 68, "92": [68, 69, 73, 81], "9636": 68, "4339": 68, "93": [68, 69, 70, 73, 74, 81, 83], "9197": 68, "4162": 68, "94": [68, 69, 70, 73, 81], "8984": 68, "3813": 68, "95": [68, 69, 70, 73, 81], "8895": 68, "3547": 68, "9048": 68, "3499": 68, "97": [68, 69, 70, 73, 81, 83, 84], "8737": 68, "3502": [68, 70], "98": [68, 69, 70, 73, 81], "9479": 68, "3518": 68, "8906": 68, "3639": 68, "8589": 68, "3789": 68, "against": [68, 70], "test_y_tru": [68, 70], "test_mask": [68, 70], "2982": 68, "karateclub": [69, 77, 78, 79, 81, 82], "matplotlib": [69, 70, 86], "pyplot": [69, 70, 86], "plt": [69, 70, 86], "www": [69, 76, 77, 78, 79, 81, 82], "jstor": [69, 77, 78, 79, 81, 82], "stabl": [69, 70, 77, 78, 79, 81, 82], "3629752": [69, 77, 78, 79, 81, 82], "singular": [69, 77, 78, 79, 81, 82], "social": [69, 77, 78, 79, 81, 82], "group": [69, 77, 78, 79, 81, 82], "dataset_sim": 69, "karate_club": [69, 77, 78, 79, 81, 82, 85], "complex_typ": [69, 77, 78, 79, 81, 82, 85], "dataset_hyp": 69, "santii": [69, 77, 78], "classifi": [69, 70, 74], "channels_": 69, "get_simplex_attribut": [69, 77, 78, 79, 81, 82, 85], "y_1h": 69, "ey": [69, 74, 77, 81], "astyp": 69, "stratifi": 69, "ind_train": 69, "ind_test": 69, "arang": 69, "random_st": 69, "float32": [69, 74], "int32": 69, "hnhnnetwork": 69, "2000": 69, "get_accuraci": 69, "lambda": 69, "yhat": 69, "ytrue": 69, "full": 69, "y_hat_cl": 69, "nloss": 69, "ntrain_acc": 69, "7157": 69, "5000": [69, 81], "7135": 69, "7113": 69, "7093": 69, "7074": 69, "7056": 69, "7039": [69, 78], "7024": 69, "7010": 69, "6997": 69, "6985": 69, "6975": 69, "6966": 69, "6958": 69, "6951": 69, "6945": 69, "6940": 69, "6936": 69, "6933": 69, "6931": 69, "6930": 69, "6929": 69, "6928": 69, "6932": 69, "6927": 69, "6926": 69, "6925": 69, "6924": 69, "6429": 69, "6923": 69, "6922": 69, "6921": 69, "6920": 69, "6919": 69, "8571": 69, "9643": 69, "6918": [69, 79], "7500": [69, 77, 81, 82], "6917": 69, "5714": 69, "5357": 69, "6916": 69, "6915": 69, "6914": 69, "6913": 69, "6912": 69, "6911": 69, "6910": 69, "6909": 69, "6908": 69, "6667": [69, 81, 82], "101": [69, 73, 81], "6907": 69, "102": [69, 73, 81], "103": [69, 73, 81, 83, 85], "6906": 69, "104": [69, 73, 81, 84], "105": [69, 73, 81, 83], "6905": 69, "107": [69, 73, 81], "6904": 69, "108": [69, 73, 74, 81], "109": [69, 73, 74, 81], "6903": 69, "110": [69, 73, 74, 81], "6902": 69, "111": [69, 73, 74, 81], "112": [69, 73, 81], "6901": 69, "113": [69, 73, 81, 83], "114": [69, 73, 81], "6900": 69, "115": [69, 73, 81], "6899": 69, "116": [69, 73, 81], "117": [69, 73, 81], "6898": 69, "118": [69, 73, 81], "6897": 69, "119": [69, 73, 81], "120": [69, 73, 81], "6896": 69, "121": [69, 73, 74, 81, 85], "6895": 69, "122": [69, 73, 81], "6894": 69, "123": [69, 73, 81], "124": [69, 73, 81], "6893": 69, "125": [69, 73, 81], "6892": 69, "126": [69, 73, 81], "6891": [69, 81], "127": [69, 73, 81], "6890": 69, "129": [69, 73, 81], "6889": 69, "130": [69, 73, 81], "6888": 69, "131": [69, 73, 81], "6887": 69, "132": [69, 73, 81], "6886": 69, "133": [69, 73, 74, 81], "134": [69, 73, 81], "6885": 69, "135": [69, 73, 81], "6884": 69, "136": [69, 73, 81], "6883": 69, "137": [69, 73, 81], "6882": 69, "138": [69, 73, 81], "139": [69, 73, 81], "6880": 69, "140": [69, 73, 74, 81], "6879": [69, 77], "141": [69, 73, 81], "6878": 69, "142": [69, 73, 81], "6877": 69, "143": [69, 73, 77, 81, 82], "6876": 69, "144": [69, 73, 81], "6875": [69, 73], "145": [69, 73, 81], "6874": 69, "146": [69, 73, 81], "6873": 69, "147": [69, 73, 81, 84], "6871": 69, "148": [69, 73, 81], "6870": 69, "149": [69, 73, 81], "6869": 69, "150": [69, 73, 81, 86], "6868": 69, "151": [69, 73, 81], "6867": 69, "6865": 69, "153": [69, 73, 81], "6864": 69, "154": [69, 73, 81], "6863": 69, "155": [69, 73, 81], "6861": 69, "156": [69, 73, 81], "6860": [69, 85], "157": [69, 73, 81], "6859": 69, "158": [69, 73, 81], "6857": 69, "159": [69, 73, 81], "6856": 69, "160": [69, 73, 81], "6855": 69, "161": [69, 73, 81], "6853": 69, "162": [69, 73, 81], "6852": 69, "6850": 69, "164": [69, 73, 81], "6849": 69, "165": [69, 73, 81], "6847": 69, "166": [69, 73, 81], "6845": 69, "167": [69, 73, 81], "6844": 69, "168": [69, 73, 81], "6842": 69, "169": [69, 73, 81], "6840": 69, "170": [69, 73, 81], "6839": 69, "171": [69, 73, 81], "6837": 69, "172": [69, 73, 81], "6835": 69, "173": [69, 73, 81], "6833": 69, "174": [69, 73, 81], "6831": 69, "175": [69, 73, 81], "6829": 69, "176": [69, 73, 81], "6827": 69, "6825": 69, "178": [69, 73, 81], "6823": 69, "179": [69, 73, 81], "6821": 69, "180": [69, 73, 81], "6819": 69, "181": [69, 73, 81], "6817": 69, "182": [69, 73, 81], "183": [69, 73, 81], "6813": 69, "184": [69, 73, 81], "6810": 69, "185": [69, 73, 74, 81], "6808": 69, "186": [69, 73, 81], "6806": 69, "187": [69, 73, 81], "6803": 69, "6801": 69, "189": [69, 73, 81], "6799": 69, "190": [69, 73, 81], "6796": 69, "191": [69, 73, 81], "6793": 69, "192": [69, 73, 81], "6791": 69, "193": [69, 73, 81], "6788": 69, "194": [69, 73, 81], "6785": 69, "195": [69, 73, 81], "6783": 69, "196": [69, 73, 81], "6780": 69, "197": [69, 73, 81], "6777": 69, "198": [69, 73, 81, 84, 85], "6774": 69, "199": [69, 73, 74, 81], "6771": 69, "200": [69, 70, 73, 74, 81], "6768": 69, "201": [69, 73], "6765": 69, "6762": 69, "203": [69, 73], "6758": 69, "204": [69, 73], "6755": 69, "205": [69, 73], "6752": 69, "206": [69, 73], "6748": 69, "207": [69, 73], "6745": 69, "208": [69, 73], "6741": 69, "209": [69, 73], "6738": 69, "210": [69, 73], "6734": 69, "211": [69, 73], "6730": 69, "6726": 69, "213": [69, 73], "6723": 69, "214": [69, 73, 82], "6719": 69, "215": [69, 73], "6715": 69, "216": [69, 73], "6710": 69, "217": [69, 73], "6706": 69, "218": [69, 73], "6702": 69, "6786": 69, "219": [69, 73], "6698": 69, "220": [69, 73], "6693": 69, "221": [69, 73], "6689": 69, "222": [69, 73], "6684": 69, "223": [69, 73], "6679": 69, "224": [69, 73], "6675": 69, "225": [69, 73], "6670": 69, "226": [69, 73], "6665": 69, "227": [69, 73, 84], "6660": 69, "228": [69, 73], "6655": 69, "229": [69, 73], "6649": 69, "230": [69, 73], "6644": 69, "231": [69, 73], "6639": 69, "232": [69, 73, 84], "6633": 69, "233": [69, 73], "6628": 69, "234": [69, 73], "6622": 69, "235": [69, 73], "6616": 69, "236": [69, 73], "6610": 69, "237": [69, 73], "6604": 69, "238": [69, 73], "6598": 69, "239": [69, 73], "6592": 69, "240": [69, 73], "6585": 69, "7143": 69, "241": [69, 73], "6579": 69, "242": [69, 73], "6572": 69, "243": [69, 73], "6566": 69, "244": [69, 73], "6559": 69, "245": [69, 73], "6552": 69, "246": [69, 73], "6545": 69, "247": [69, 73], "6538": 69, "248": [69, 73], "6531": 69, "249": [69, 73], "6523": 69, "250": [69, 73], "6516": 69, "8214": 69, "251": [69, 73], "6508": 69, "6500": 69, "253": [69, 73], "6492": 69, "254": [69, 73], "6484": 69, "255": [69, 73], "6476": 69, "6468": 69, "257": [69, 73], "6459": 69, "258": [69, 73], "6451": 69, "259": [69, 73], "6442": 69, "260": [69, 73], "6433": 69, "261": [69, 73], "6424": 69, "262": [69, 73], "6415": 69, "263": [69, 73], "6406": 69, "264": [69, 73], "6397": 69, "265": [69, 73], "6387": 69, "266": [69, 73], "6378": 69, "267": [69, 73], "6368": 69, "268": [69, 73], "6358": 69, "269": [69, 73], "6348": 69, "270": [69, 73, 77], "6337": 69, "271": [69, 73], "6327": 69, "272": [69, 73], "6317": 69, "273": [69, 73], "6306": 69, "6295": 69, "275": [69, 72, 73], "6284": [69, 81], "276": [69, 73], "6273": 69, "277": [69, 73], "6262": 69, "278": [69, 71, 73], "6250": 69, "279": [69, 73], "6239": 69, "280": [69, 73], "6227": 69, "281": [69, 71, 73], "6215": 69, "282": [69, 71, 73, 84], "6203": 69, "283": [69, 73], "6191": 69, "284": [69, 73], "6178": 69, "285": [69, 73], "6166": 69, "286": [69, 73], "6153": 69, "287": [69, 73], "6140": 69, "288": [69, 73], "6127": 69, "289": [69, 73], "6114": 69, "290": [69, 73], "6101": 69, "291": [69, 73], "6088": 69, "292": [69, 73], "6074": 69, "293": [69, 73], "6060": 69, "294": [69, 73], "6046": [69, 70], "295": [69, 73], "6032": 69, "296": [69, 73], "6018": 69, "297": [69, 73], "6004": 69, "298": [69, 73], "5989": 69, "299": [69, 73], "5975": 69, "300": [69, 73], "5960": 69, "301": [69, 73], "5945": 69, "302": [69, 73, 74], "5930": 69, "303": [69, 73], "304": [69, 73, 84], "5900": 69, "305": [69, 73], "5884": 69, "306": [69, 73], "5868": 69, "307": [69, 73], "5853": 69, "308": [69, 73], "5837": 69, "309": [69, 73], "5821": 69, "310": [69, 73], "5805": [69, 77], "311": [69, 73], "5788": 69, "312": [69, 73], "5772": 69, "313": [69, 73], "314": [69, 73], "5739": 69, "315": [69, 73], "5722": 69, "316": [69, 73], "5705": 69, "317": [69, 73], "5688": 69, "318": [69, 73], "5671": 69, "319": [69, 73], "5653": 69, "320": [69, 73], "5636": 69, "321": [69, 73], "5618": 69, "322": [69, 73], "5601": 69, "323": [69, 73], "5583": 69, "324": [69, 73], "325": [69, 73], "5547": 69, "326": [69, 73], "5529": 69, "327": [69, 73], "5511": 69, "328": [69, 73], "5492": 69, "329": [69, 73], "5474": 69, "330": [69, 73], "5455": 69, "331": [69, 73], "5437": 69, "332": [69, 73], "5418": 69, "333": [69, 73], "5399": 69, "334": [69, 73], "5381": 69, "335": [69, 73], "5362": [69, 77], "336": [69, 73], "5343": 69, "337": [69, 73], "5323": 69, "338": [69, 73], "5304": 69, "339": [69, 73], "5285": 69, "340": [69, 73], "5266": 69, "8929": 69, "341": [69, 73], "5246": [69, 77], "342": [69, 73], "5227": 69, "343": [69, 73], "5207": 69, "344": [69, 73], "5188": [69, 77], "345": [69, 73], "5168": 69, "346": [69, 73], "5148": 69, "347": [69, 73], "5129": 69, "348": [69, 73], "5109": 69, "349": [69, 73], "5089": 69, "350": [69, 73], "5069": 69, "9286": 69, "351": [69, 73], "5049": 69, "352": [69, 73], "5029": 69, "353": [69, 73], "5009": 69, "354": [69, 73], "4989": 69, "355": [69, 73], "4969": 69, "356": [69, 73], "4949": 69, "357": [69, 73], "4928": 69, "358": [69, 73], "4908": 69, "359": [69, 73], "4888": 69, "360": [69, 73], "4868": 69, "361": [69, 73], "4847": 69, "362": [69, 73], "363": [69, 73], "4807": 69, "364": [69, 73], "4787": 69, "365": [69, 73], "4766": 69, "366": [69, 73], "4746": 69, "367": [69, 73], "4726": 69, "368": [69, 73], "4705": 69, "369": [69, 73], "4685": 69, "370": [69, 73], "4665": 69, "371": [69, 73], "4644": 69, "372": [69, 73], "4624": 69, "373": [69, 73], "4604": 69, "374": [69, 73], "4583": 69, "375": [69, 73], "4563": 69, "376": [69, 73], "4543": 69, "377": [69, 73], "4523": 69, "378": [69, 73], "4503": 69, "379": [69, 73], "4482": 69, "380": [69, 73], "4462": 69, "381": [69, 73], "4442": 69, "382": [69, 73], "383": [69, 73], "4402": 69, "384": [69, 73], "4382": 69, "385": [69, 73], "4362": 69, "386": [69, 73], "4342": 69, "387": [69, 73], "4322": 69, "388": [69, 73], "4302": 69, "389": [69, 73], "4282": 69, "390": [69, 73], "4262": 69, "391": [69, 73], "4243": 69, "392": [69, 73], "4223": 69, "393": [69, 73], "4203": 69, "394": [69, 73], "4184": 69, "395": [69, 73, 74], "4164": 69, "396": [69, 73], "4145": 69, "397": [69, 73], "4125": 69, "398": [69, 73], "4106": 69, "4086": 69, "400": [69, 70, 73], "4067": 69, "401": [69, 73], "4048": 69, "402": [69, 73], "4029": 69, "403": [69, 73], "4010": 69, "404": [69, 73], "3991": 69, "405": [69, 73], "3972": 69, "406": [69, 73], "3953": 69, "407": [69, 73], "3934": 69, "408": [69, 73, 82], "3915": 69, "409": [69, 73], "3897": 69, "410": [69, 73], "3878": 69, "411": [69, 73], "3859": 69, "412": [69, 73], "3841": 69, "413": [69, 73], "3823": 69, "3804": 69, "415": [69, 73], "3786": 69, "416": [69, 73], "3768": 69, "417": [69, 73], "3750": [69, 71], "418": [69, 73], "3732": 69, "419": [69, 73], "3714": 69, "420": [69, 73], "3696": 69, "421": [69, 73], "3678": 69, "422": [69, 73], "3661": 69, "423": [69, 73], "3643": 69, "424": [69, 73], "3626": 69, "425": [69, 73], "3608": 69, "426": [69, 73], "3591": 69, "427": [69, 73], "3573": 69, "428": [69, 73], "3556": 69, "429": [69, 73], "3539": 69, "430": [69, 73], "3522": 69, "431": [69, 73], "3505": 69, "432": [69, 73], "3488": 69, "433": [69, 73], "3472": 69, "434": [69, 73], "3455": 69, "435": [69, 73], "3438": 69, "436": [69, 73], "3422": 69, "437": [69, 73], "3406": 69, "438": [69, 73], "3389": 69, "439": [69, 73], "3373": 69, "440": [69, 73], "3357": 69, "441": [69, 73], "3341": 69, "442": [69, 73], "3325": 69, "443": [69, 73, 74], "3309": 69, "444": [69, 73], "3293": 69, "445": [69, 73], "3278": 69, "446": [69, 73], "3262": 69, "447": [69, 73], "3247": 69, "448": [69, 73], "3231": 69, "449": [69, 73], "3216": 69, "450": [69, 73], "3201": 69, "451": [69, 73], "3185": 69, "452": [69, 73], "3170": 69, "453": [69, 73], "3155": 69, "454": [69, 73], "3140": 69, "455": [69, 73], "3126": 69, "456": [69, 73], "3111": 69, "457": [69, 73], "3096": 69, "458": [69, 73], "3082": 69, "459": [69, 73], "3067": 69, "460": [69, 73], "3053": 69, "461": [69, 73], "3039": 69, "462": [69, 73], "3025": 69, "463": [69, 73], "3011": 69, "464": [69, 73], "2997": 69, "465": [69, 73], "2983": 69, "466": [69, 73], "2969": 69, "467": [69, 73], "2955": 69, "468": [69, 73], "2941": 69, "469": [69, 73], "2928": 69, "470": [69, 73], "2914": [69, 70], "471": [69, 73], "2901": 69, "472": [69, 73], "2888": 69, "473": [69, 73], "2874": 69, "474": [69, 73], "2861": 69, "475": [69, 73], "2848": 69, "476": [69, 73], "2835": 69, "477": [69, 73], "2822": 69, "478": [69, 73], "2810": 69, "479": [69, 73], "2797": 69, "480": [69, 73], "2784": 69, "481": [69, 73], "2772": 69, "482": [69, 73], "2759": 69, "483": [69, 73], "2747": 69, "484": [69, 73], "2735": 69, "485": [69, 73], "2722": 69, "486": [69, 73], "2710": 69, "487": [69, 73], "2698": 69, "488": [69, 73], "2686": 69, "489": [69, 73], "2674": 69, "490": [69, 73], "2662": 69, "491": [69, 73], "2651": 69, "492": [69, 73], "2639": 69, "493": [69, 73], "2627": 69, "494": [69, 73], "2616": 69, "495": [69, 73], "2604": 69, "496": [69, 73], "2593": 69, "497": [69, 73], "2582": 69, "498": [69, 73], "2571": 69, "499": [69, 73], "2559": 69, "2548": 69, "501": 69, "2537": 69, "502": 69, "2526": 69, "503": 69, "2516": 69, "504": 69, "2505": 69, "505": 69, "2494": 69, "506": 69, "2484": 69, "507": 69, "2473": 69, "508": 69, "2463": 69, "509": 69, "2452": [69, 70], "510": 69, "2442": 69, "511": 69, "2432": 69, "512": 69, "2421": 69, "513": 69, "2411": 69, "514": [69, 84], "2401": 69, "515": 69, "2391": 69, "516": 69, "2381": 69, "517": 69, "2371": 69, "518": 69, "2362": 69, "519": 69, "2352": 69, "520": 69, "2342": 69, "521": 69, "2333": 69, "522": 69, "2323": 69, "523": 69, "2314": 69, "524": 69, "525": 69, "2295": 69, "526": 69, "2286": 69, "527": 69, "2276": 69, "528": 69, "2267": 69, "2258": 69, "530": 69, "2249": 69, "531": 69, "2240": 69, "532": 69, "2231": [69, 70], "533": 69, "2223": 69, "534": 69, "2214": 69, "535": 69, "2205": 69, "2196": 69, "537": 69, "2188": 69, "538": 69, "2179": 69, "539": 69, "2171": 69, "540": 69, "2162": 69, "541": 69, "2154": 69, "542": 69, "2146": 69, "543": 69, "2137": 69, "544": 69, "2129": 69, "545": 69, "2121": 69, "546": [69, 83], "2113": 69, "547": 69, "548": 69, "2097": 69, "549": 69, "2089": 69, "550": 69, "2081": 69, "551": 69, "2073": 69, "552": 69, "2066": 69, "553": 69, "2058": 69, "554": 69, "2050": 69, "555": 69, "2043": 69, "556": 69, "2035": 69, "557": 69, "2028": 69, "558": 69, "559": 69, "2013": 69, "560": 69, "2005": 69, "561": 69, "1998": 69, "562": 69, "1991": 69, "563": 69, "1983": 69, "564": 69, "1976": 69, "565": 69, "1969": 69, "566": 69, "1962": [69, 77], "567": 69, "1955": 69, "568": 69, "1948": 69, "569": 69, "1941": 69, "570": 69, "1934": 69, "571": 69, "1927": 69, "572": 69, "1921": 69, "573": 69, "1914": 69, "574": 69, "1907": 69, "575": 69, "1900": 69, "576": 69, "1894": 69, "577": 69, "1887": 69, "578": 69, "1881": 69, "579": 69, "1874": 69, "580": 69, "1868": 69, "581": 69, "1861": 69, "582": 69, "1855": 69, "583": 69, "1849": 69, "584": 69, "1842": 69, "585": 69, "1836": [69, 77], "586": 69, "1830": 69, "587": 69, "1824": 69, "588": 69, "1818": 69, "589": 69, "1811": 69, "590": 69, "1805": 69, "591": 69, "1799": 69, "592": 69, "1793": 69, "593": 69, "1788": 69, "594": 69, "1782": 69, "595": 69, "1776": 69, "596": 69, "1770": 69, "597": 69, "1764": 69, "598": 69, "1758": 69, "599": 69, "1753": 69, "600": 69, "1747": 69, "601": 69, "1741": 69, "602": 69, "1736": 69, "603": 69, "1730": [69, 77], "604": 69, "1725": [69, 77], "605": 69, "1719": 69, "606": 69, "1714": 69, "607": 69, "1708": 69, "608": 69, "1703": 69, "609": 69, "1697": 69, "610": 69, "1692": 69, "611": 69, "1687": 69, "612": 69, "1682": 69, "613": [69, 86], "1676": 69, "614": 69, "1671": 69, "615": 69, "1666": 69, "616": 69, "1661": 69, "617": 69, "1656": 69, "618": 69, "1651": 69, "619": 69, "1646": 69, "620": 69, "1640": 69, "621": 69, "1636": 69, "622": 69, "1631": 69, "623": 69, "1626": 69, "624": 69, "1621": 69, "625": [69, 73], "1616": 69, "626": 69, "1611": 69, "627": 69, "1606": 69, "628": 69, "1601": 69, "629": 69, "1597": 69, "630": 69, "1592": 69, "631": 69, "1587": 69, "632": 69, "1583": 69, "633": 69, "1578": 69, "634": 69, "635": [69, 82], "1569": 69, "636": 69, "1564": 69, "637": 69, "1560": [69, 77], "638": 69, "1555": 69, "639": 69, "1551": 69, "640": 69, "1546": 69, "641": 69, "1542": 69, "642": 69, "1537": 69, "643": 69, "1533": 69, "644": 69, "1529": 69, "645": 69, "1524": 69, "646": 69, "1520": 69, "647": 69, "1516": [69, 81], "648": 69, "1511": [69, 77], "649": 69, "1507": 69, "650": 69, "1503": 69, "651": 69, "1499": 69, "652": 69, "1495": [69, 77], "653": 69, "1491": 69, "654": 69, "1487": [69, 77], "655": 69, "1482": 69, "656": 69, "1478": 69, "657": 69, "1474": [69, 77], "658": 69, "1470": 69, "659": 69, "1466": 69, "660": 69, "1462": 69, "661": 69, "1458": 69, "662": 69, "1454": 69, "663": 69, "1451": 69, "664": 69, "1447": 69, "665": 69, "1443": 69, "666": 69, "1439": 69, "667": 69, "1435": 69, "668": 69, "1431": 69, "669": 69, "1428": 69, "670": 69, "1424": 69, "671": 69, "1420": 69, "672": 69, "1416": 69, "673": 69, "1413": 69, "674": 69, "1409": 69, "675": 69, "1405": 69, "676": 69, "1402": 69, "677": 69, "1398": 69, "678": 69, "1395": 69, "679": 69, "1391": 69, "680": 69, "1387": 69, "681": 69, "1384": 69, "682": 69, "1380": [69, 77], "683": 69, "1377": 69, "684": 69, "1373": 69, "685": 69, "1370": 69, "686": 69, "1367": 69, "687": 69, "1363": 69, "688": 69, "1360": 69, "689": 69, "1356": [69, 77], "690": 69, "1353": 69, "691": 69, "1350": 69, "692": 69, "1346": 69, "693": 69, "1343": [69, 77], "694": 69, "1340": 69, "695": 69, "1336": 69, "696": 69, "1333": 69, "697": 69, "1330": 69, "698": 69, "1327": 69, "699": 69, "1323": 69, "700": 69, "1320": 69, "701": 69, "1317": 69, "702": 69, "1314": 69, "703": 69, "1311": 69, "704": 69, "1308": 69, "705": 69, "1304": 69, "706": 69, "1301": 69, "707": 69, "1298": 69, "708": 69, "1295": 69, "709": 69, "1292": 69, "710": 69, "1289": 69, "711": 69, "1286": 69, "712": 69, "1283": [69, 81], "713": 69, "1280": 69, "714": 69, "1277": 69, "715": 69, "1274": 69, "716": 69, "1271": 69, "717": 69, "1268": 69, "718": 69, "1265": 69, "719": 69, "1263": 69, "720": 69, "1260": 69, "721": 69, "1257": [69, 81], "722": 69, "1254": 69, "723": 69, "1251": 69, "724": 69, "1248": 69, "725": 69, "1245": 69, "726": 69, "1243": 69, "727": 69, "1240": 69, "728": 69, "1237": 69, "729": 69, "1234": 69, "730": 69, "1232": 69, "731": 69, "1229": 69, "732": 69, "1226": 69, "733": 69, "1223": 69, "734": 69, "1221": 69, "735": 69, "1218": 69, "736": 69, "1215": 69, "737": 69, "1213": 69, "738": 69, "1210": 69, "739": 69, "1207": 69, "740": 69, "1205": 69, "741": 69, "1202": 69, "742": 69, "1200": [69, 86], "743": 69, "1197": 69, "744": 69, "1194": 69, "745": 69, "1192": 69, "746": 69, "1189": 69, "747": 69, "1187": 69, "748": 69, "1184": 69, "749": 69, "1182": 69, "1179": [69, 77], "751": 69, "1177": 69, "752": 69, "1174": 69, "753": 69, "1172": 69, "754": 69, "1169": 69, "755": 69, "1167": 69, "756": 69, "1165": 69, "757": 69, "1162": 69, "758": 69, "1160": 69, "759": 69, "1157": 69, "760": 69, "1155": 69, "761": 69, "1153": 69, "762": 69, "1150": 69, "763": 69, "1148": 69, "764": 69, "1146": 69, "765": 69, "1143": 69, "766": 69, "1141": 69, "767": 69, "1139": [69, 70], "768": 69, "1136": [69, 81], "769": 69, "1134": 69, "770": 69, "1132": 69, "771": 69, "1130": 69, "772": 69, "1127": 69, "773": 69, "1125": 69, "774": 69, "1123": 69, "775": 69, "1121": 69, "776": 69, "1118": 69, "777": 69, "1116": 69, "778": 69, "1114": 69, "779": 69, "1112": 69, "780": 69, "1110": 69, "781": 69, "1107": [69, 70], "782": 69, "1105": 69, "783": 69, "1103": 69, "784": 69, "1101": [69, 81], "785": 69, "1099": 69, "786": 69, "1097": 69, "787": 69, "1095": 69, "788": 69, "1093": 69, "789": 69, "1090": 69, "790": 69, "1088": 69, "791": 69, "1086": 69, "792": 69, "1084": 69, "793": 69, "1082": 69, "794": 69, "1080": [69, 77], "795": [69, 70], "1078": 69, "796": 69, "1076": 69, "797": 69, "1074": 69, "798": 69, "1072": 69, "799": 69, "1070": 69, "800": 69, "1068": 69, "801": 69, "1066": 69, "802": 69, "1064": 69, "803": 69, "1062": 69, "804": 69, "1060": [69, 82], "805": [69, 70], "1058": 69, "806": 69, "1056": 69, "807": 69, "1054": 69, "808": 69, "1052": 69, "809": 69, "1050": 69, "810": 69, "1049": 69, "811": 69, "1047": 69, "812": 69, "1045": 69, "813": 69, "1043": 69, "814": 69, "1041": 69, "815": 69, "1039": 69, "816": 69, "1037": 69, "817": 69, "1035": 69, "818": 69, "1033": 69, "819": 69, "1032": 69, "820": 69, "1030": 69, "821": 69, "1028": 69, "822": 69, "1026": 69, "823": 69, "1024": 69, "824": 69, "1023": [69, 77], "825": 69, "1021": 69, "826": 69, "1019": [69, 77], "827": 69, "1017": 69, "828": 69, "1015": 69, "829": 69, "1014": 69, "830": 69, "1012": [69, 77], "831": 69, "1010": 69, "832": 69, "1008": 69, "833": 69, "1007": 69, "834": 69, "1005": 69, "835": 69, "1003": 69, "836": 69, "1001": 69, "837": 69, "838": 69, "0998": [69, 77], "839": 69, "0996": [69, 77], "840": 69, "0995": 69, "841": 69, "0993": 69, "842": 69, "0991": 69, "843": 69, "0990": 69, "844": 69, "0988": 69, "845": 69, "0986": 69, "846": 69, "0985": 69, "847": 69, "0983": 69, "848": 69, "0981": 69, "849": 69, "0980": 69, "850": 69, "0978": 69, "851": 69, "0976": 69, "852": 69, "0975": 69, "853": 69, "0973": 69, "854": 69, "0972": 69, "855": 69, "0970": 69, "856": 69, "0968": [69, 77], "857": 69, "0967": 69, "858": 69, "0965": 69, "859": 69, "0964": 69, "860": 69, "0962": 69, "861": 69, "0961": 69, "862": 69, "0959": [69, 77], "863": 69, "0957": 69, "864": 69, "0956": 69, "865": 69, "0954": [69, 77], "866": 69, "0953": 69, "867": 69, "0951": 69, "868": 69, "0950": 69, "869": 69, "0948": [69, 77], "870": 69, "0947": 69, "871": 69, "0945": 69, "872": 69, "0944": 69, "873": 69, "0942": 69, "874": 69, "0941": 69, "875": 69, "0939": 69, "876": 69, "0938": 69, "877": 69, "0936": 69, "878": 69, "0935": 69, "879": 69, "0933": 69, "880": 69, "0932": 69, "881": 69, "0930": [69, 77], "882": 69, "0929": 69, "883": 69, "0928": 69, "884": 69, "0926": 69, "885": 69, "0925": 69, "886": 69, "0923": [69, 77], "887": 69, "0922": 69, "888": 69, "0920": [69, 77], "889": 69, "0919": 69, "890": 69, "0918": 69, "891": 69, "0916": 69, "892": 69, "0915": 69, "893": 69, "0913": 69, "894": 69, "0912": 69, "895": 69, "0911": 69, "896": 69, "0909": 69, "897": 69, "0908": 69, "898": 69, "0907": 69, "899": 69, "0905": 69, "900": 69, "0904": 69, "901": 69, "0902": 69, "902": 69, "0901": 69, "903": 69, "0900": 69, "904": 69, "0898": 69, "905": 69, "0897": [69, 77], "906": 69, "0896": 69, "907": 69, "0894": 69, "908": 69, "0893": 69, "909": 69, "0892": [69, 77], "910": 69, "0891": [69, 77], "911": 69, "0889": 69, "912": 69, "0888": 69, "913": 69, "0887": 69, "914": 69, "0885": 69, "915": 69, "0884": 69, "916": 69, "0883": 69, "917": 69, "0881": 69, "918": 69, "0880": 69, "919": 69, "0879": 69, "920": 69, "0878": 69, "921": 69, "0876": 69, "922": 69, "0875": 69, "923": 69, "0874": 69, "924": 69, "0873": [69, 81], "925": 69, "0871": [69, 84], "926": 69, "0870": 69, "927": 69, "0869": 69, "928": 69, "0868": 69, "929": 69, "0866": 69, "930": 69, "0865": 69, "931": 69, "0864": [69, 77], "932": 69, "0863": 69, "933": 69, "0862": 69, "934": 69, "0860": 69, "935": 69, "0859": 69, "936": 69, "0858": 69, "937": 69, "0857": 69, "938": 69, "0856": 69, "939": 69, "0854": 69, "940": 69, "0853": 69, "941": 69, "0852": 69, "942": 69, "0851": [69, 81], "943": 69, "0850": 69, "944": 69, "0848": 69, "945": 69, "0847": 69, "946": 69, "0846": 69, "947": 69, "0845": 69, "948": 69, "0844": 69, "949": 69, "0843": 69, "950": 69, "0842": 69, "951": 69, "0840": 69, "952": 69, "0839": 69, "953": 69, "0838": 69, "954": 69, "0837": 69, "955": 69, "0836": 69, "956": 69, "0835": 69, "957": 69, "0834": 69, "958": 69, "0833": 69, "959": 69, "0831": 69, "960": 69, "0830": 69, "961": 69, "0829": 69, "962": 69, "0828": 69, "963": 69, "0827": 69, "964": 69, "0826": 69, "965": 69, "0825": 69, "966": 69, "0824": 69, "967": 69, "0823": 69, "968": 69, "0822": 69, "969": 69, "0820": 69, "970": 69, "0819": [69, 77], "971": 69, "0818": 69, "972": 69, "0817": 69, "973": 69, "0816": 69, "974": 69, "0815": 69, "975": 69, "0814": 69, "976": 69, "0813": 69, "977": 69, "0812": 69, "978": 69, "0811": 69, "979": 69, "0810": 69, "980": 69, "0809": 69, "981": 69, "0808": 69, "982": 69, "0807": 69, "983": 69, "0806": 69, "984": 69, "0805": 69, "985": 69, "0804": [69, 77], "986": 69, "0803": [69, 77], "987": 69, "0802": 69, "988": 69, "0800": 69, "989": 69, "0799": 69, "990": 69, "0798": 69, "991": 69, "0797": 69, "992": 69, "0796": 69, "993": 69, "0795": 69, "994": 69, "0794": 69, "995": 69, "0793": 69, "996": 69, "0792": 69, "997": 69, "0791": 69, "998": 69, "0790": 69, "999": 69, "0789": [69, 77], "0788": 69, "0787": 69, "1002": 69, "0786": [69, 70], "1004": 69, "0785": 69, "0784": 69, "1006": [69, 77], "0783": 69, "0782": 69, "0781": 69, "1009": [69, 77], "0780": 69, "0779": 69, "1011": 69, "0778": 69, "0777": 69, "1013": 69, "0776": 69, "0775": 69, "0774": 69, "0773": 69, "0772": 69, "1018": 69, "0771": 69, "0770": 69, "1020": 69, "0769": 69, "0768": 69, "1022": 69, "0767": 69, "0766": 69, "1025": 69, "0765": 69, "0764": 69, "1027": 69, "0763": 69, "0762": [69, 77], "1029": 69, "0761": 69, "0760": 69, "1031": 69, "0759": [69, 77], "0758": 69, "1034": 69, "0757": 69, "0756": 69, "1036": 69, "0755": 69, "0754": [69, 77], "1038": 69, "0753": 69, "0752": 69, "1040": 69, "0751": 69, "0750": 69, "1042": 69, "0749": [69, 77], "1044": 69, "0748": 69, "0747": 69, "1046": [69, 77], "0746": [69, 77], "0745": 69, "1048": 69, "0744": 69, "0743": 69, "1051": 69, "0742": 69, "0741": 69, "1053": 69, "0740": 69, "0739": 69, "1055": 69, "0738": 69, "1057": 69, "0737": 69, "0736": 69, "1059": 69, "0735": 69, "0734": 69, "1061": 69, "0733": 69, "1063": 69, "0732": 69, "0731": [69, 77], "1065": 69, "0730": 69, "0729": 69, "1067": 69, "0728": 69, "1069": 69, "0727": 69, "0726": 69, "1071": 69, "0725": [69, 77], "0724": [69, 77], "1073": 69, "0723": 69, "1075": 69, "0722": 69, "0721": [69, 77], "1077": 69, "0720": [69, 77], "0719": 69, "0718": 69, "1081": 69, "0716": 69, "1083": 69, "0715": 69, "1085": 69, "0714": 69, "0713": 69, "1087": 69, "0712": 69, "1089": 69, "0711": 69, "0710": 69, "1091": 69, "0709": 69, "1092": 69, "0708": 69, "1094": [69, 85], "0707": 69, "0706": 69, "1096": 69, "0705": 69, "1098": 69, "0704": 69, "0703": 69, "1100": 69, "0702": 69, "1102": 69, "0701": [69, 77], "0700": 69, "1104": 69, "0699": [69, 77], "1106": [69, 77], "0698": 69, "0697": 69, "1108": 69, "1109": 69, "0696": 69, "0695": 69, "1111": 69, "0694": 69, "1113": 69, "0693": [69, 77], "0692": 69, "1115": 69, "0691": 69, "1117": 69, "0690": 69, "0689": 69, "1119": 69, "1120": 69, "0688": 69, "0687": [69, 77], "1122": 69, "0686": 69, "1124": 69, "0685": [69, 77], "0684": 69, "1126": 69, "0683": 69, "1128": 69, "0682": 69, "1129": 69, "1131": 69, "0680": 69, "0679": 69, "1133": 69, "0678": 69, "1135": 69, "0677": 69, "1137": 69, "0676": 69, "1138": 69, "0675": 69, "0674": [69, 81], "1140": 69, "0673": 69, "1142": 69, "0672": 69, "1144": 69, "0671": 69, "1145": 69, "0670": 69, "1147": 69, "0669": 69, "0668": 69, "1149": 69, "0667": 69, "1151": 69, "0666": 69, "1152": 69, "0665": 69, "1154": 69, "0664": 69, "1156": 69, "0663": [69, 77], "0662": 69, "1158": 69, "1159": 69, "0661": 69, "0660": 69, "1161": 69, "0659": 69, "1163": 69, "0658": 69, "1164": 69, "0657": 69, "1166": 69, "0656": 69, "1168": 69, "0655": 69, "1170": 69, "0654": 69, "1171": 69, "0653": 69, "1173": 69, "0652": 69, "0651": 69, "1175": 69, "1176": 69, "1178": 69, "0649": 69, "1180": 69, "1181": 69, "0647": 69, "0646": 69, "1183": 69, "0645": 69, "1185": 69, "1186": 69, "0644": 69, "0643": 69, "1188": 69, "0642": 69, "1190": 69, "0641": 69, "1191": 69, "0640": 69, "1193": 69, "0639": 69, "1195": 69, "0638": 69, "1196": 69, "0637": 69, "1198": 69, "1199": 69, "0636": 69, "0635": 69, "1201": 69, "0634": 69, "1203": 69, "1204": 69, "0633": 69, "0632": 69, "1206": 69, "0631": 69, "1208": 69, "1209": 69, "0630": 69, "0629": 69, "1211": 69, "1212": 69, "0628": 69, "1214": 69, "0627": 69, "1216": 69, "0626": 69, "1217": 69, "0625": 69, "1219": 69, "0624": 69, "1220": 69, "0623": 69, "1222": 69, "0622": 69, "1224": 69, "0621": 69, "1225": 69, "0620": 69, "1227": 69, "1228": 69, "0619": 69, "1230": 69, "0618": 69, "1231": 69, "0617": 69, "1233": 69, "0616": 69, "1235": 69, "0615": 69, "1236": 69, "0614": 69, "1238": 69, "0613": 69, "1239": 69, "0612": 69, "1241": 69, "0611": 69, "1244": 69, "0610": 69, "1246": 69, "0609": 69, "1247": 69, "0608": 69, "1249": 69, "0607": [69, 77], "0606": 69, "1252": 69, "1253": 69, "0605": 69, "1255": [69, 77], "0604": 69, "1256": 69, "0603": 69, "1258": 69, "1259": 69, "0602": 69, "1261": 69, "0601": 69, "1262": 69, "0600": 69, "1264": 69, "0599": 69, "1266": 69, "0598": 69, "1267": 69, "0597": 69, "1269": 69, "1270": 69, "0596": 69, "1272": 69, "0595": 69, "1273": 69, "0594": 69, "1275": 69, "1276": 69, "0593": 69, "1278": 69, "0592": 69, "1279": 69, "0591": 69, "1281": 69, "1282": 69, "0590": 69, "1284": 69, "0589": 69, "1285": 69, "0588": 69, "1287": 69, "0587": 69, "1288": [69, 77], "0586": 69, "1290": 69, "1291": 69, "0585": 69, "1293": [69, 77], "0584": 69, "1294": [69, 77], "1296": 69, "1297": 69, "0582": 69, "1299": 69, "0581": 69, "1300": 69, "0580": [69, 77], "1302": 69, "1303": 69, "0579": 69, "1305": 69, "1306": 69, "0578": 69, "1307": 69, "0577": 69, "1309": 69, "1310": 69, "0576": [69, 77], "1312": 69, "0575": 69, "1313": 69, "0574": 69, "1315": 69, "1316": [69, 77], "0573": 69, "1318": 69, "0572": 69, "1319": 69, "0571": 69, "1321": 69, "1322": 69, "0570": 69, "1324": 69, "0569": 69, "1325": 69, "1326": 69, "1328": 69, "0567": 69, "1329": 69, "1331": 69, "0566": 69, "1332": [69, 77], "0565": 69, "1334": 69, "1335": 69, "0564": 69, "1337": 69, "0563": 69, "1338": 69, "1339": 69, "0562": [69, 81], "1341": 69, "0561": [69, 70], "1342": 69, "1344": 69, "0560": [69, 70], "1345": 69, "0559": [69, 70], "1347": 69, "1348": [69, 81], "0558": 69, "1349": 69, "0557": 69, "1351": 69, "1352": 69, "0556": 69, "1354": 69, "1355": 69, "0555": 69, "1357": 69, "0554": 69, "1358": 69, "1359": 69, "0553": [69, 77], "1361": 69, "0552": 69, "1362": 69, "0551": 69, "1364": 69, "1365": 69, "1366": 69, "0550": 69, "1368": 69, "0549": 69, "1369": 69, "0548": 69, "1371": 69, "1372": 69, "0547": 69, "1374": 69, "1375": 69, "0546": [69, 77], "1376": 69, "0545": 69, "1378": 69, "1379": 69, "0544": 69, "1381": 69, "1382": 69, "0543": 69, "1383": 69, "0542": 69, "1385": 69, "1386": 69, "0541": 69, "1388": 69, "1389": 69, "0540": 69, "1390": 69, "0539": 69, "1392": 69, "1393": 69, "1394": 69, "0538": 69, "1396": 69, "0537": [69, 77], "1397": 69, "0536": 69, "1399": 69, "1400": 69, "1401": 69, "0535": 69, "1403": 69, "0534": 69, "1404": 69, "0533": 69, "1406": 69, "1407": 69, "1408": 69, "0532": 69, "1410": 69, "0531": 69, "1411": 69, "1412": 69, "0530": 69, "1414": 69, "1415": 69, "0529": [69, 81], "1417": 69, "1418": 69, "0528": 69, "1419": 69, "0527": 69, "1421": 69, "1422": 69, "1423": 69, "0526": 69, "1425": 69, "0525": 69, "1426": [69, 77], "1427": 69, "0524": 69, "1429": [69, 77], "1430": 69, "0523": 69, "1432": 69, "0522": 69, "1434": 69, "0521": 69, "1437": 69, "1438": 69, "0520": 69, "1440": 69, "0519": 69, "1441": 69, "1442": 69, "0518": 69, "1444": 69, "1445": 69, "0517": 69, "1446": 69, "1448": 69, "0516": 69, "1449": 69, "1450": 69, "0515": 69, "1452": 69, "1453": 69, "0514": 69, "1455": 69, "0513": 69, "1456": 69, "1457": [69, 77], "0512": [69, 81], "1459": 69, "1460": 69, "1461": 69, "0511": 69, "1463": 69, "0510": 69, "1464": 69, "1465": 69, "0509": 69, "1467": 69, "1468": 69, "1469": 69, "0508": 69, "1471": 69, "0507": 69, "1472": 69, "1473": [69, 77], "0506": 69, "1475": [69, 77], "1476": 69, "0505": 69, "1477": 69, "1479": 69, "0504": 69, "1480": 69, "1481": 69, "0503": 69, "1483": 69, "1484": [69, 77], "1485": 69, "0502": 69, "1486": 69, "0501": 69, "1488": 69, "1489": 69, "1490": 69, "0500": 69, "1492": 69, "1493": 69, "0499": 69, "1494": 69, "0498": 69, "1496": 69, "1497": 69, "1498": [69, 70], "0497": 69, "1500": 69, "1501": 69, "1502": 69, "1504": 69, "0495": 69, "1505": 69, "1506": 69, "1508": 69, "1509": 69, "0493": 69, "1510": 69, "1512": 69, "0492": 69, "1513": 69, "1514": 69, "1515": 69, "0491": 69, "1517": 69, "1518": [69, 77], "0490": 69, "1519": [69, 77], "0489": 69, "1521": 69, "1522": 69, "1523": 69, "0488": 69, "1525": 69, "1526": 69, "0487": 69, "1527": 69, "1528": 69, "0486": 69, "1530": 69, "1531": 69, "1532": 69, "0485": 69, "1534": 69, "1535": 69, "0484": 69, "1536": 69, "1538": 69, "0483": 69, "1539": 69, "1540": 69, "1541": 69, "0482": 69, "1543": 69, "1544": 69, "0481": 69, "1545": [69, 77], "0480": 69, "1547": 69, "1548": 69, "1549": 69, "0479": 69, "1550": 69, "1552": 69, "0478": 69, "1553": 69, "1554": 69, "0477": 69, "1556": 69, "1557": 69, "1558": 69, "0476": 69, "1559": 69, "1561": 69, "1562": 69, "1563": 69, "0474": 69, "1565": 69, "1566": 69, "1567": 69, "0473": 69, "1568": 69, "1570": 69, "0472": 69, "1571": 69, "1572": 69, "0471": 69, "1574": 69, "1575": [69, 77], "1576": 69, "0470": 69, "1577": 69, "1579": 69, "0469": [69, 81], "1580": 69, "1581": 69, "1582": 69, "0468": 69, "1584": 69, "1585": 69, "1586": 69, "0467": 69, "1588": 69, "1589": 69, "0466": 69, "1590": 69, "1591": 69, "0465": 69, "1593": 69, "1594": 69, "1595": 69, "0464": 69, "1596": 69, "1598": 69, "0463": 69, "1599": 69, "1600": 69, "0462": 69, "1602": 69, "1603": 69, "1604": 69, "0461": 69, "1605": 69, "1607": 69, "0460": 69, "1608": 69, "1609": 69, "1610": 69, "0459": 69, "1612": 69, "1613": 69, "1614": 69, "0458": 69, "1615": 69, "1617": 69, "0457": 69, "1618": 69, "1619": 69, "1620": 69, "0456": 69, "1622": 69, "1623": 69, "0455": 69, "1624": 69, "1625": 69, "0454": 69, "1627": 69, "1628": [69, 77], "1629": 69, "1630": 69, "0453": 69, "1632": 69, "1633": 69, "0452": 69, "1634": 69, "1635": 69, "0451": 69, "1637": 69, "1638": 69, "1639": 69, "0450": 69, "1641": 69, "1642": 69, "1643": 69, "0449": 69, "1644": 69, "1645": 69, "0448": 69, "1647": 69, "1648": 69, "1649": 69, "0447": [69, 70], "1650": 69, "1652": 69, "0446": 69, "1653": 69, "1654": [69, 77], "1655": 69, "0445": 69, "1657": 69, "1658": 69, "1659": 69, "0444": 69, "1660": 69, "1662": 69, "1663": 69, "0443": 69, "1664": 69, "1665": 69, "0442": 69, "1667": 69, "1668": 69, "1669": 69, "0441": 69, "1670": 69, "1672": 69, "1673": 69, "0440": 69, "1674": 69, "1675": 69, "0439": 69, "1677": 69, "1678": 69, "1679": 69, "0438": 69, "1680": 69, "1681": 69, "1683": 69, "0437": 69, "1684": 69, "1685": 69, "1686": 69, "0436": 69, "1688": 69, "1689": 69, "1690": 69, "0435": 69, "1691": 69, "1693": 69, "0434": 69, "1694": 69, "1695": 69, "1696": 69, "0433": 69, "1698": 69, "1699": 69, "1700": 69, "0432": 69, "1701": 69, "1702": 69, "1704": 69, "0431": 69, "1705": 69, "1706": 69, "1707": 69, "0430": 69, "1709": 69, "1711": 69, "0429": 69, "1712": 69, "1713": 69, "0428": 69, "1715": 69, "1716": 69, "1717": 69, "1718": 69, "0427": 69, "1720": 69, "1721": 69, "0426": 69, "1722": 69, "1723": 69, "1724": [69, 77], "0425": 69, "1726": 69, "1727": 69, "1728": 69, "0424": 69, "1729": 69, "1731": 69, "1732": 69, "0423": 69, "1733": 69, "1734": [69, 77], "1735": 69, "0422": 69, "1738": [69, 77], "1739": 69, "0421": 69, "1740": 69, "1742": 69, "1743": 69, "0420": 69, "1744": 69, "1745": 69, "1746": 69, "0419": 69, "1748": 69, "1749": 69, "1750": 69, "0418": 69, "1752": 69, "1754": 69, "0417": 69, "1755": 69, "1756": 69, "1757": 69, "0416": 69, "1759": 69, "1760": 69, "1761": 69, "0415": 69, "1763": [69, 77], "1765": 69, "0414": 69, "1766": 69, "1767": 69, "1768": 69, "1769": 69, "0413": 69, "1771": 69, "1772": 69, "0412": 69, "1773": 69, "1774": 69, "1775": 69, "0411": 69, "1777": 69, "1778": 69, "1779": 69, "1780": [69, 77], "0410": 69, "1781": 69, "1783": 69, "1784": 69, "0409": 69, "1785": 69, "1786": 69, "1787": 69, "0408": 69, "1789": 69, "1790": 69, "1791": 69, "0407": 69, "1792": 69, "1794": 69, "1795": 69, "0406": [69, 70], "1796": 69, "1797": 69, "1798": 69, "0405": 69, "1800": 69, "1801": 69, "1802": 69, "1803": 69, "0404": 69, "1804": [69, 77], "1806": 69, "1807": 69, "0403": 69, "1808": 69, "1809": 69, "1810": 69, "0402": 69, "1812": [69, 77], "1813": 69, "1814": 69, "0401": 69, "1815": 69, "1816": 69, "1817": 69, "0400": 69, "1819": 69, "1820": 69, "1821": [69, 77], "1822": 69, "0399": 69, "1823": 69, "1825": 69, "1826": 69, "0398": 69, "1827": 69, "1828": 69, "1829": 69, "0397": 69, "1831": [69, 77], "1832": 69, "1833": 69, "1834": 69, "0396": [69, 70], "1835": 69, "1837": 69, "1838": 69, "0395": 69, "1839": 69, "1840": 69, "1841": 69, "0394": 69, "1843": 69, "1844": 69, "1845": 69, "1846": 69, "0393": 69, "1847": 69, "1848": 69, "1850": 69, "0392": 69, "1851": [69, 77], "1852": 69, "1853": 69, "1854": 69, "0391": [69, 84], "1856": 69, "1857": 69, "1858": 69, "0390": 69, "1859": 69, "1860": 69, "1862": 69, "1863": 69, "1864": 69, "1865": 69, "1866": 69, "0388": 69, "1867": 69, "1869": 69, "1870": 69, "0387": 69, "1871": 69, "1872": 69, "1873": 69, "0386": 69, "1875": 69, "1876": 69, "1877": 69, "1878": 69, "0385": 69, "1879": 69, "1880": 69, "1882": 69, "0384": 69, "1883": 69, "1884": 69, "1885": 69, "1886": 69, "0383": 69, "1888": 69, "1890": 69, "1891": 69, "0382": 69, "1892": 69, "1893": 69, "0381": 69, "1896": 69, "1897": 69, "1898": 69, "1899": 69, "0380": 69, "1901": 69, "1902": 69, "1903": 69, "0379": 69, "1904": 69, "1905": 69, "1906": 69, "0378": 69, "1908": 69, "1909": 69, "1910": 69, "1911": 69, "1912": 69, "0377": 69, "1913": [69, 81], "1915": 69, "1916": 69, "0376": 69, "1917": 69, "1918": 69, "1919": 69, "1920": 69, "0375": 69, "1922": 69, "1923": 69, "1924": 69, "0374": 69, "1925": 69, "1926": [69, 85], "1928": 69, "1929": 69, "0373": 69, "1930": 69, "1931": 69, "1932": 69, "1933": 69, "0372": 69, "1935": 69, "1936": 69, "1937": 69, "0371": 69, "1938": 69, "1939": 69, "1940": 69, "1942": 69, "0370": 69, "1943": 69, "1944": 69, "1945": 69, "1946": 69, "0369": 69, "1947": 69, "1949": 69, "1950": 69, "0368": 69, "1951": 69, "1952": 69, "1953": 69, "1954": 69, "0367": 69, "1956": 69, "1957": 69, "1959": 69, "0366": 69, "1960": 69, "1961": 69, "1963": 69, "0365": 69, "1964": 69, "1965": 69, "1966": 69, "1967": 69, "1968": 69, "0364": 69, "1970": 69, "1971": 69, "1972": 69, "0363": [69, 81], "1973": 69, "1974": 69, "1975": 69, "1977": 69, "0362": 69, "1978": 69, "1979": 69, "1980": 69, "1981": 69, "0361": 69, "1982": 69, "1984": 69, "1985": 69, "0360": 69, "1986": 69, "1987": 69, "1988": 69, "1989": 69, "1990": 69, "0359": 69, "1992": 69, "1993": 69, "1994": 69, "1995": 69, "1996": 69, "1997": 69, "1999": 69, "0357": 69, "plot": [69, 86], "dpi": 69, "test_epoch": 69, "isfinit": 69, "linestyl": 69, "marker": 69, "legend": [69, 86], "xlabel": 69, "cites": 70, "3703": 70, "manifold": 70, "tsne": 70, "randomnodesplit": 70, "home": [70, 83], "sadra": 70, "local": 70, "tqdm": [70, 86], "auto": 70, "tqdmwarn": 70, "iprogress": 70, "jupyt": 70, "ipywidget": 70, "readthedoc": 70, "en": 70, "user_instal": 70, "autonotebook": 70, "notebook_tqdm": 70, "wget": [70, 74], "twistedcub": 70, "raw": [70, 74], "master": [70, 74], "citeseer6cls3703": 70, "pt": [70, 86], "paper_x": 70, "longtensor": [70, 74], "paper_author": 70, "train_test_splitt": 70, "num_test": 70, "num_val": 70, "dropout_r": 70, "enumer": [70, 74, 86], "schedul": 70, "initial_lr": 70, "04": [70, 77, 86], "lr_schedul": 70, "steplr": 70, "7889": 70, "6347": 70, "6458": 70, "9118": 70, "5491": 70, "7203": 70, "5956": 70, "4068": 70, "4721": 70, "7964": 70, "3431": 70, "5075": 70, "0124": 70, "9682": 70, "2804": 70, "2483": 70, "2004": 70, "2019": [70, 81], "0248": 70, "0219": 70, "0217": 70, "0328": 70, "0180": 70, "0220": 70, "0232": 70, "0241": 70, "0130": 70, "0239": 70, "0240": 70, "0284": 70, "0295": 70, "0163": 70, "0195": 70, "0323": 70, "8441": 70, "worth": 70, "visual": [70, 86], "n_compon": 70, "fit_transform": 70, "ax1": 70, "ax2": 70, "subplot": [70, 86], "suptitl": 70, "set_titl": [70, 86], "_t_sne": 70, "futurewarn": 70, "chang": [70, 77, 81, 82, 86], "pca": 70, "warn": [70, 73, 76, 82, 83], "As": 71, "j": [71, 72, 77, 81, 82, 83, 86], "highlight": 71, "formal": [71, 79], "alpha_": 71, "jk": 71, "nonlinear": [71, 85], "exp": 71, "u_": 71, "limits_": 71, "context": 71, "again": [71, 79, 80], "vi": [71, 72], "beta_": 71, "ij": 71, "anoth": 71, "measur": 71, "7875": 71, "9625": 71, "interpret": 72, "propag": 72, "problem": [72, 74], "divid": 72, "hypersagemodel": 72, "2431": 72, "simplicial_complex": [73, 75, 76, 86], "filterwarn": [73, 76], "to_sparse_csr": [73, 74, 76], "bceloss": [73, 76], "x_1_val": [73, 75, 76], "incidence_1_v": [73, 75, 76], "y_val": [73, 75, 76], "pred": [73, 75, 76, 86], "0971908569336": 73, "94380187988281": 73, "2369270324707": 73, "26074981689453": 73, "98492431640625": 73, "20859909057617": 73, "80668640136719": 73, "686824798583984": 73, "83675765991211": 73, "03213882446289": 73, "238582611083984": 73, "33390426635742": 73, "644731521606445": 73, "585487365722656": 73, "487186431884766": 73, "787471771240234": 73, "63153076171875": 73, "06392478942871": 73, "550783157348633": 73, "08787727355957": 73, "46900749206543": 73, "992172241210938": 73, "090089797973633": 73, "6256160736084": 73, "858888626098633": 73, "25303077697754": 73, "14973258972168": 73, "443086624145508": 73, "693538665771484": 73, "53269386291504": 73, "046640396118164": 73, "61376190185547": 73, "491796493530273": 73, "60036277770996": 73, "690528869628906": 73, "603723526000977": 73, "351343154907227": 73, "073862075805664": 73, "908754348754883": 73, "88480567932129": 73, "900497436523438": 73, "822755813598633": 73, "63096809387207": 73, "415300369262695": 73, "28574562072754": 73, "239614486694336": 73, "227548599243164": 73, "16373634338379": 73, "053255081176758": 73, "935653686523438": 73, "852752685546875": 73, "81441307067871": 73, "792282104492188": 73, "750215530395508": 73, "678442001342773": 73, "601423263549805": 73, "548017501831055": 73, "518722534179688": 73, "489871978759766": 73, "43247413635254": 73, "345685958862305": 73, "245248794555664": 73, "150096893310547": 73, "066505432128906": 73, "986852645874023": 73, "90121841430664": 73, "803401947021484": 73, "703289031982422": 73, "632183074951172": 73, "587385177612305": 73, "54228401184082": 73, "49706268310547": 73, "451065063476562": 73, "407331466674805": 73, "371335983276367": 73, "337526321411133": 73, "306472778320312": 73, "273452758789062": 73, "237478256225586": 73, "20127296447754": 73, "167097091674805": 73, "132389068603516": 73, "098857879638672": 73, "060476303100586": 73, "02267837524414": 73, "9860782623291": 73, "950490951538086": 73, "914758682250977": 73, "878108978271484": 73, "842838287353516": 73, "811080932617188": 73, "780370712280273": 73, "74917221069336": 73, "71664810180664": 73, "699419021606445": 73, "676870346069336": 73, "647939682006836": 73, "61389923095703": 73, "5761775970459": 73, "549442291259766": 73, "5289249420166": 73, "49772071838379": 73, "474157333374023": 73, "45038414001465": 73, "424612045288086": 73, "40214729309082": 73, "378002166748047": 73, "351882934570312": 73, "324098587036133": 73, "298376083374023": 73, "274303436279297": 73, "24971580505371": 73, "228342056274414": 73, "197002410888672": 73, "17634391784668": 73, "151491165161133": 73, "128210067749023": 73, "113391876220703": 73, "088733673095703": 73, "06772232055664": 73, "04320526123047": 73, "0169734954834": 73, "99879264831543": 73, "976442337036133": 73, "94951820373535": 73, "926555633544922": 73, "90591812133789": 73, "884706497192383": 73, "859689712524414": 73, "834184646606445": 73, "812297821044922": 73, "79071807861328": 73, "770198822021484": 73, "74526596069336": 73, "72126579284668": 73, "695659637451172": 73, "671920776367188": 73, "650121688842773": 73, "63254737854004": 73, "603469848632812": 73, "58234405517578": 73, "55916976928711": 73, "540271759033203": 73, "513303756713867": 73, "49028968811035": 73, "466527938842773": 73, "449338912963867": 73, "419330596923828": 73, "39695167541504": 73, "375646591186523": 73, "3538875579834": 73, "334510803222656": 73, "312015533447266": 73, "285579681396484": 73, "269054412841797": 73, "242639541625977": 73, "22045135498047": 73, "203929901123047": 73, "183347702026367": 73, "160404205322266": 73, "133930206298828": 73, "104745864868164": 73, "098697662353516": 73, "078676223754883": 73, "040504455566406": 73, "02244758605957": 73, "011486053466797": 73, "993385314941406": 73, "96799087524414": 73, "939449310302734": 73, "912992477416992": 73, "884111404418945": 73, "86220359802246": 73, "8441219329834": 73, "813901901245117": 73, "801708221435547": 73, "786155700683594": 73, "761951446533203": 73, "740060806274414": 73, "71567726135254": 73, "68885040283203": 73, "670719146728516": 73, "652807235717773": 73, "618999481201172": 73, "5964298248291": 73, "57932472229004": 73, "557270050048828": 73, "530651092529297": 73, "504606246948242": 73, "476253509521484": 73, "45449447631836": 73, "434545516967773": 73, "411243438720703": 73, "382360458374023": 73, "362293243408203": 73, "335590362548828": 73, "30755043029785": 73, "284061431884766": 73, "25484275817871": 73, "22318458557129": 73, "19426155090332": 73, "173973083496094": 73, "14513397216797": 73, "10979652404785": 73, "082422256469727": 73, "0560359954834": 73, "033998489379883": 73, "013517379760742": 73, "987682342529297": 73, "960020065307617": 73, "938085556030273": 73, "911584854125977": 73, "887819290161133": 73, "859230041503906": 73, "83808708190918": 73, "825475692749023": 73, "791316986083984": 73, "769617080688477": 73, "746116638183594": 73, "724519729614258": 73, "69559097290039": 73, "676664352416992": 73, "659330368041992": 73, "623565673828125": 73, "596616744995117": 73, "573455810546875": 73, "547086715698242": 73, "527324676513672": 73, "50086784362793": 73, "473081588745117": 73, "447193145751953": 73, "41757583618164": 73, "402799606323242": 73, "38097381591797": 73, "355859756469727": 73, "32827377319336": 73, "316627502441406": 73, "297752380371094": 73, "263805389404297": 73, "23656463623047": 73, "23184585571289": 73, "2138671875": 73, "183242797851562": 73, "142995834350586": 73, "137985229492188": 73, "11527442932129": 73, "083709716796875": 73, "5625": [73, 75, 76], "04819107055664": 73, "046104431152344": 73, "027341842651367": 73, "99266242980957": 73, "961795806884766": 73, "930912017822266": 73, "911731719970703": 73, "880786895751953": 73, "845874786376953": 73, "83226203918457": 73, "812429428100586": 73, "780649185180664": 73, "75445556640625": 73, "74269676208496": 73, "706310272216797": 73, "682571411132812": 73, "67528533935547": 73, "641508102416992": 73, "615623474121094": 73, "612245559692383": 73, "574350357055664": 73, "543176651000977": 73, "523324966430664": 73, "500341415405273": 73, "460468292236328": 73, "45850944519043": 73, "433353424072266": 73, "38702392578125": 73, "373823165893555": 73, "356149673461914": 73, "33302116394043": 73, "2984619140625": 73, "27631950378418": 73, "242095947265625": 73, "220375061035156": 73, "19377326965332": 73, "166545867919922": 73, "145954132080078": 73, "118263244628906": 73, "103313446044922": 73, "090181350708008": 73, "05240821838379": 73, "038013458251953": 73, "01729965209961": 73, "008516311645508": 73, "97707748413086": 73, "94068717956543": 73, "90737533569336": 73, "87880516052246": 73, "850223541259766": 73, "817041397094727": 73, "79216194152832": 73, "768423080444336": 73, "73453712463379": 73, "714162826538086": 73, "683536529541016": 73, "661930084228516": 73, "627168655395508": 73, "591276168823242": 73, "575401306152344": 73, "538806915283203": 73, "532752990722656": 73, "491992950439453": 73, "470623016357422": 73, "447834014892578": 73, "417621612548828": 73, "395700454711914": 73, "371519088745117": 73, "34141731262207": 73, "317161560058594": 73, "290822982788086": 73, "27621078491211": 73, "252424240112305": 73, "223106384277344": 73, "191679000854492": 73, "167285919189453": 73, "146617889404297": 73, "1218204498291": 73, "092159271240234": 73, "068817138671875": 73, "043107986450195": 73, "020431518554688": 73, "99953842163086": 73, "965152740478516": 73, "948270797729492": 73, "9256591796875": 73, "89617347717285": 73, "867271423339844": 73, "838720321655273": 73, "820175170898438": 73, "80241584777832": 73, "77659797668457": 73, "735746383666992": 73, "71509552001953": 73, "694164276123047": 73, "66806411743164": 73, "638553619384766": 73, "61126136779785": 73, "588682174682617": 73, "57335090637207": 73, "55010223388672": 73, "518211364746094": 73, "497529983520508": 73, "474842071533203": 73, "450790405273438": 73, "426084518432617": 73, "395307540893555": 73, "374881744384766": 73, "35700798034668": 73, "3305721282959": 73, "315614700317383": 73, "27733039855957": 73, "254539489746094": 73, "226396560668945": 73, "21523666381836": 73, "177637100219727": 73, "150047302246094": 73, "121498107910156": 73, "07767677307129": 73, "025562286376953": 73, "97791290283203": 73, "93244743347168": 73, "88832664489746": 73, "836246490478516": 73, "796043395996094": 73, "74243927001953": 73, "702007293701172": 73, "6556339263916": 73, "615976333618164": 73, "584707260131836": 73, "56226348876953": 73, "522308349609375": 73, "483394622802734": 73, "425535202026367": 73, "371274948120117": 73, "33834457397461": 73, "30695152282715": 73, "257896423339844": 73, "215682983398438": 73, "17093849182129": 73, "115076065063477": 73, "079204559326172": 73, "051490783691406": 73, "033987045288086": 73, "01789093017578": 73, "02878189086914": 73, "052005767822266": 73, "115055084228516": 73, "044801712036133": 73, "891642570495605": 73, "728387832641602": 73, "788483619689941": 73, "005611419677734": 73, "045494079589844": 73, "028085708618164": 73, "70627212524414": 73, "557747840881348": 73, "639880180358887": 73, "732135772705078": 73, "785037994384766": 73, "548952102661133": 73, "409181594848633": 73, "439441680908203": 73, "51244068145752": 73, "506211280822754": 73, "33906364440918": 73, "261138916015625": 73, "3150053024292": 73, "368926048278809": 73, "32858943939209": 73, "185369491577148": 73, "1485595703125": 73, "187846183776855": 73, "209299087524414": 73, "182910919189453": 73, "098752975463867": 73, "044387817382812": 73, "052579879760742": 73, "05774211883545": 73, "031035423278809": 73, "977952003479004": 73, "939553260803223": 73, "92505931854248": 73, "922098159790039": 73, "910082817077637": 73, "877153396606445": 73, "838932991027832": 73, "809442520141602": 73, "788487434387207": 73, "78007698059082": 73, "799392700195312": 73, "814967155456543": 73, "780838966369629": 73, "738446235656738": 73, "707448959350586": 73, "675198554992676": 73, "641422271728516": 73, "6210355758667": 73, "609297752380371": 73, "609857559204102": 73, "6266508102417": 73, "633983612060547": 73, "6951904296875": 73, "658025741577148": 73, "600869178771973": 73, "535354614257812": 73, "497736930847168": 73, "458619117736816": 73, "456388473510742": 73, "473320007324219": 73, "49333667755127": 73, "535053253173828": 73, "544401168823242": 73, "582549095153809": 73, "526555061340332": 73, "50399112701416": 73, "38758373260498": 73, "308337211608887": 73, "299043655395508": 73, "327571868896484": 73, "388229370117188": 73, "417525291442871": 73, "455561637878418": 73, "416516304016113": 73, "392356872558594": 73, "28682804107666": 73, "192612648010254": 73, "12002182006836": 73, "091236114501953": 73, "14907455444336": 73, "283432006835938": 73, "588083267211914": 73, "7717866897583": 73, "171638488769531": 73, "575143814086914": 73, "168379783630371": 73, "044833183288574": 73, "277763366699219": 73, "634512901306152": 73, "24776554107666": 73, "931353569030762": 73, "041153907775879": 73, "249833106994629": 73, "302783966064453": 73, "985976219177246": 73, "880131721496582": 73, "02563762664795": 73, "059005737304688": 73, "942398071289062": 73, "796463012695312": 73, "87881088256836": 73, "039618492126465": 73, "918492317199707": 73, "420305252075195": 73, "406696319580078": 73, "394020080566406": 73, "37169075012207": 73, "35594940185547": 73, "34318733215332": 73, "33356475830078": 73, "31667137145996": 73, "302318572998047": 73, "28500747680664": 73, "26559066772461": 73, "25322914123535": 73, "238706588745117": 73, "220836639404297": 73, "208791732788086": 73, "1856746673584": 73, "171249389648438": 73, "15633201599121": 73, "145591735839844": 73, "12535285949707": 73, "112903594970703": 73, "096086502075195": 73, "082990646362305": 73, "068315505981445": 73, "050025939941406": 73, "046539306640625": 73, "030738830566406": 73, "007352828979492": 73, "004419326782227": 73, "990276336669922": 73, "96859359741211": 73, "95843505859375": 73, "941267013549805": 73, "93852424621582": 73, "93297004699707": 73, "905284881591797": 73, "90688705444336": 73, "88941764831543": 73, "867488861083984": 73, "852458953857422": 73, "82878303527832": 73, "848604202270508": 73, "816743850708008": 73, "798707962036133": 73, "782867431640625": 73, "770641326904297": 73, "765010833740234": 73, "74397087097168": 73, "730905532836914": 73, "71582794189453": 73, "692882537841797": 73, "678205490112305": 73, "659757614135742": 73, "66231918334961": 73, "63972282409668": 73, "637910842895508": 73, "620912551879883": 73, "613643646240234": 73, "592449188232422": 73, "578832626342773": 73, "5690860748291": 73, "5478572845459": 73, "536046981811523": 73, "518606185913086": 73, "507850646972656": 73, "49575424194336": 73, "47629737854004": 73, "466197967529297": 73, "446958541870117": 73, "431520462036133": 73, "42148780822754": 73, "418920516967773": 73, "382413864135742": 73, "38419532775879": 73, "382558822631836": 73, "373292922973633": 73, "359771728515625": 73, "339229583740234": 73, "32771873474121": 73, "31146240234375": 73, "291799545288086": 73, "266311645507812": 73, "25176429748535": 73, "275617599487305": 73, "243261337280273": 73, "198863983154297": 73, "190418243408203": 73, "180404663085938": 73, "159381866455078": 73, "132848739624023": 73, "08831024169922": 73, "07340431213379": 73, "040719985961914": 73, "018062591552734": 73, "98824119567871": 73, "994766235351562": 73, "97658348083496": 73, "954269409179688": 73, "931133270263672": 73, "923803329467773": 73, "900142669677734": 73, "883092880249023": 73, "874980926513672": 73, "859506607055664": 73, "83385467529297": 73, "821224212646484": 73, "808631896972656": 73, "790834426879883": 73, "770950317382812": 73, "753965377807617": 73, "74176597595215": 73, "726638793945312": 73, "71209144592285": 73, "717884063720703": 73, "69021987915039": 73, "684720993041992": 73, "65319061279297": 73, "64687728881836": 73, "641027450561523": 73, "62809181213379": 73, "599239349365234": 73, "57975196838379": 73, "56479835510254": 73, "56086540222168": 73, "538368225097656": 73, "528959274291992": 73, "50667953491211": 73, "503355026245117": 73, "473203659057617": 73, "480627059936523": 73, "455434799194336": 73, "448516845703125": 73, "433578491210938": 73, "42472267150879": 73, "41015625": 73, "397563934326172": 73, "366233825683594": 73, "36186981201172": 73, "347782135009766": 73, "334121704101562": 73, "319826126098633": 73, "311002731323242": 73, "29153060913086": 73, "269283294677734": 73, "26325225830078": 73, "24094581604004": 73, "23046112060547": 73, "2217960357666": 73, "21772575378418": 73, "204160690307617": 73, "186527252197266": 73, "18400764465332": 73, "16082763671875": 73, "150056838989258": 73, "157442092895508": 73, "121925354003906": 73, "12659454345703": 73, "10972023010254": 73, "108051300048828": 73, "09576988220215": 73, "07231903076172": 73, "065282821655273": 73, "032222747802734": 73, "040668487548828": 73, "02615737915039": 73, "99028778076172": 73, "992895126342773": 73, "986125946044922": 73, "971839904785156": 73, "947101593017578": 73, "93064308166504": 73, "940488815307617": 73, "926761627197266": 73, "89728355407715": 73, "92677116394043": 73, "90224266052246": 73, "888980865478516": 73, "871950149536133": 73, "850805282592773": 73, "85380744934082": 73, "83152961730957": 73, "820480346679688": 73, "813871383666992": 73, "811283111572266": 73, "79723358154297": 73, "773174285888672": 73, "76585578918457": 73, "763370513916016": 73, "74306297302246": 73, "733776092529297": 73, "727794647216797": 73, "726064682006836": 73, "721574783325195": 73, "69578742980957": 73, "671518325805664": 73, "67192268371582": 73, "675018310546875": 73, "652587890625": 73, "63589859008789": 73, "632600784301758": 73, "618101119995117": 73, "61069679260254": 73, "603355407714844": 73, "587169647216797": 73, "568763732910156": 73, "55999183654785": 73, "554792404174805": 73, "530359268188477": 73, "52699851989746": 73, "524051666259766": 73, "511272430419922": 73, "492382049560547": 73, "48703384399414": 73, "472692489624023": 73, "478933334350586": 73, "467594146728516": 73, "44580841064453": 73, "4395809173584": 73, "439208984375": 73, "427417755126953": 73, "416553497314453": 73, "407917022705078": 73, "396162033081055": 73, "377273559570312": 73, "370101928710938": 73, "381824493408203": 73, "365995407104492": 73, "33931541442871": 73, "341108322143555": 73, "34080696105957": 73, "326269149780273": 73, "295074462890625": 73, "290189743041992": 73, "294099807739258": 73, "279863357543945": 73, "259613037109375": 73, "255352020263672": 73, "243770599365234": 73, "24730682373047": 73, "235071182250977": 73, "221952438354492": 73, "20149803161621": 73, "19734001159668": 73, "199737548828125": 73, "179275512695312": 73, "176069259643555": 73, "170211791992188": 73, "16652488708496": 73, "136791229248047": 73, "125789642333984": 73, "123292922973633": 73, "109601974487305": 73, "100919723510742": 73, "0986270904541": 73, "09308624267578": 73, "078474044799805": 73, "067218780517578": 73, "05537223815918": 73, "03988265991211": 73, "03138542175293": 73, "026723861694336": 73, "008121490478516": 73, "018474578857422": 73, "014162063598633": 73, "001453399658203": 73, "981800079345703": 73, "987455368041992": 73, "983600616455078": 73, "cicitationcora": 74, "hgnn": 74, "utlil": 74, "neccessari": 74, "pickl": 74, "scipi": [74, 77, 81, 82, 83, 86], "sp": 74, "computation": 74, "expens": [74, 77, 81, 82], "malllabiisc": 74, "hypergcn": 74, "cocit": 74, "07": [74, 77, 86], "ca": 74, "certif": 74, "ssl": 74, "cert": 74, "crt": 74, "resolv": 74, "await": 74, "githubusercont": 74, "ok": 74, "404937": 74, "395k": 74, "applic": 74, "octet": 74, "stream": 74, "save": 74, "gt": [74, 86], "45k": 74, "kb": 74, "mb": 74, "101905": 74, "100k": 74, "52k": 74, "02": [74, 77, 86], "5436": 74, "3k": 74, "31k": 74, "51582": 74, "50k": 74, "37k": 74, "rb": 74, "handl": 74, "ipykernel_14655": 74, "121206761": 74, "deprecationwarn": 74, "csr_matrix": [74, 77, 81, 82], "namespac": 74, "csr": 74, "deprec": [74, 82], "pytorch": 74, "floattensor": 74, "num": 74, "gcnii": 74, "h2": [74, 81], "hstack": 74, "2226475299": 74, "miss": 74, "trigger": 74, "intern": 74, "aten": 74, "sparsecsrtensorimpl": 74, "cpp": 74, "predefin": 74, "train_idx": 74, "test_idx": 74, "unigcniimodel": 74, "num_featur": 74, "copi": 74, "x_0_skip": 74, "clone": 74, "ommit": 74, "cross": 74, "entropi": 74, "current": [74, 79, 82, 85], "readi": [74, 86], "7071428298950195": 74, "39291277527809143": 74, "4376946985721588": 74, "9357143044471741": 74, "6137071847915649": 74, "8714285492897034": 74, "45210281014442444": 74, "9071428775787354": 74, "5502336621284485": 74, "9785714149475098": 74, "5463395714759827": 74, "5747663378715515": 74, "9857142567634583": 74, "5673676133155823": 74, "9428571462631226": 74, "552570104598999": 74, "9571428298950195": 74, "5607476830482483": 74, "unigin_nn": 75, "inp_emb": 75, "out_decod": 75, "pooled_x_0": 75, "node_dim": 75, "seper": 75, "unsqueez": [75, 86], "3916015625": 75, "91788864135742": 75, "573387145996094": 75, "32501220703125": 75, "29070281982422": 75, "48554229736328": 75, "905006408691406": 75, "502838134765625": 75, "23980712890625": 75, "084415435791016": 75, "003536224365234": 75, "96804428100586": 75, "95846939086914": 75, "9611930847168": 75, "96879196166992": 75, "977806091308594": 75, "98638916015625": 75, "99394989013672": 75, "00029754638672": 75, "00556945800781": 75, "00981521606445": 75, "01326370239258": 75, "01602554321289": 75, "018226623535156": 75, "01999282836914": 75, "0213737487793": 75, "02241897583008": 75, "02317810058594": 75, "023704528808594": 75, "02401351928711": 75, "02414321899414": 75, "024078369140625": 75, "023826599121094": 75, "02338790893555": 75, "02278137207031": 75, "021995544433594": 75, "02100372314453": 75, "019805908203125": 75, "018394470214844": 75, "01671600341797": 75, "0147590637207": 75, "01249313354492": 75, "00986862182617": 75, "0068359375": 75, "003334045410156": 75, "999298095703125": 75, "99464416503906": 75, "98926544189453": 75, "98302459716797": 75, "975791931152344": 75, "chrsmrr": 76, "graphkerneldataset": 76, "unisagenn": 76, "38711929321289": 76, "50642013549805": 76, "88081741333008": 76, "313419342041016": 76, "208885192871094": 76, "00963592529297": 76, "59610366821289": 76, "292537689208984": 76, "19595718383789": 76, "106815338134766": 76, "913330078125": 76, "710723876953125": 76, "6346549987793": 76, "63870620727539": 76, "57096481323242": 76, "44948196411133": 76, "391658782958984": 76, "39373779296875": 76, "34821319580078": 76, "241302490234375": 76, "159812927246094": 76, "131492614746094": 76, "08507537841797": 76, "99526023864746": 76, "924638748168945": 76, "894010543823242": 76, "868173599243164": 76, "829978942871094": 76, "808109283447266": 76, "807811737060547": 76, "785655975341797": 76, "746475219726562": 76, "724843978881836": 76, "700727462768555": 76, "660417556762695": 76, "624387741088867": 76, "606855392456055": 76, "585779190063477": 76, "568805694580078": 76, "556062698364258": 76, "53620147705078": 76, "51841163635254": 76, "507102966308594": 76, "491756439208984": 76, "478620529174805": 76, "475711822509766": 76, "467418670654297": 76, "449710845947266": 76, "429576873779297": 76, "42317771911621": 76, "alexandro": 77, "kero": 77, "linalg": 77, "npla": 77, "a0": [77, 78], "becaus": [77, 78, 80, 82, 85], "serv": [77, 78], "simpli": [77, 78, 86], "demonstr": [77, 78], "similarli": [77, 78, 81], "emerg": [77, 78, 79, 81, 82], "four": [77, 78, 79, 81, 82], "y_true": [77, 78, 79, 82, 85], "l_tilde_pinv": 77, "pinv": 77, "invers": 77, "0971": 77, "0937": 77, "2140": 77, "2069": 77, "2927": 77, "3018": 77, "2309": 77, "0992": 77, "0943": 77, "0927": 77, "2678": 77, "3090": 77, "0960": 77, "2077": 77, "2056": 77, "2813": 77, "nnz": 77, "layout": 77, "sparse_coo": 77, "56771909e": 77, "49643084e": 77, "13434650e": 77, "60154799e": 77, "03": [77, 86], "73820292e": 77, "65885226e": 77, "04038181e": 77, "08": [77, 86], "51925802e": 77, "09": [77, 84, 86], "73643677e": 77, "95577741e": 77, "09312067e": 77, "39698386e": 77, "11006736e": 77, "25540316e": 77, "87149896e": 77, "65674657e": 77, "43987098e": 77, "79396772e": 77, "00662204e": 77, "45058060e": 77, "36910174e": 77, "82942520e": 77, "24798042e": 77, "85055751e": 77, "78386103e": 77, "24821486e": 77, "81510593e": 77, "07917011e": 77, "30485535e": 77, "19925834e": 77, "56662779e": 77, "25658545e": 77, "29514395e": 77, "73054542e": 77, "57650283e": 77, "87089108e": 77, "31973699e": 77, "45874534e": 77, "78385898e": 77, "24821523e": 77, "38282800e": 77, "29527006e": 77, "24821542e": 77, "45585343e": 77, "20149602e": 77, "39614227e": 77, "52603984e": 77, "02427802e": 77, "38569428e": 77, "20058507e": 77, "89658767e": 77, "67997003e": 77, "90682733e": 77, "88636552e": 77, "61071175e": 77, "75768661e": 77, "22418800e": 77, "07488209e": 77, "26928225e": 77, "52925774e": 77, "50903371e": 77, "71863856e": 77, "40345353e": 77, "36909867e": 77, "82943824e": 77, "90223058e": 77, "08467136e": 77, "43380561e": 77, "27135092e": 77, "31898531e": 77, "01219751e": 77, "78963115e": 77, "97890193e": 77, "49229891e": 77, "67953214e": 77, "75078206e": 77, "75904313e": 77, "03583546e": 77, "12457962e": 77, "10897127e": 77, "18870673e": 77, "28672193e": 77, "61245163e": 77, "48166016e": 77, "75217551e": 77, "67996958e": 77, "90682673e": 77, "44834775e": 77, "90006804e": 77, "59747154e": 77, "69860917e": 77, "59747209e": 77, "69862127e": 77, "59747284e": 77, "69861429e": 77, "59747191e": 77, "59747247e": 77, "69860823e": 77, "59747135e": 77, "11979373e": 77, "90869734e": 77, "59747228e": 77, "69860637e": 77, "59747303e": 77, "69861010e": 77, "59747116e": 77, "17587730e": 77, "43268425e": 77, "43105909e": 77, "32787512e": 77, "03376685e": 77, "44168448e": 77, "62169540e": 77, "41996737e": 77, "73246880e": 77, "97727704e": 77, "03496753e": 77, "71378374e": 77, "92902595e": 77, "15740368e": 77, "94057676e": 77, "48602486e": 77, "40909785e": 77, "14646482e": 77, "38315065e": 77, "76777497e": 77, "38311899e": 77, "76780128e": 77, "37373477e": 77, "49392605e": 77, "30545244e": 77, "10224779e": 77, "69429579e": 77, "59057510e": 77, "11831834e": 77, "86165255e": 77, "07662510e": 77, "53556532e": 77, "82225195e": 77, "76254632e": 77, "62731223e": 77, "63466549e": 77, "16528196e": 77, "62805045e": 77, "36022410e": 77, "48832843e": 77, "19494419e": 77, "13972221e": 77, "zia003": 77, "topox2": 77, "_index": [77, 81, 82], "sparseefficiencywarn": [77, 81, 82], "sparsiti": [77, 81, 82], "lil_matrix": [77, 81, 82], "_set_arrayxarrai": [77, 81, 82], "produc": [77, 78, 79, 81, 85], "compar": [77, 78, 79, 81, 85], "binary_cross_entropy_with_logit": [77, 78, 79, 81, 85], "y_hat_test": [77, 78, 79, 81, 82, 85], "y_pred_test": [77, 78, 79, 81, 82, 85], "test_accuraci": [77, 78, 79, 81, 82, 85], "7231": 77, "6000": [77, 82], "6989": 77, "5667": [77, 78, 81, 82], "2500": 77, "6737": [77, 82], "6564": 77, "6434": 77, "6362": 77, "6290": 77, "4333": 77, "6199": 77, "6117": 77, "6057": 77, "6011": 77, "5964": 77, "5911": 77, "5855": 77, "5764": 77, "4000": 77, "5730": 77, "5696": 77, "5660": 77, "5593": 77, "5567": 77, "5544": 77, "5520": 77, "5496": 77, "5472": 77, "5451": 77, "5432": 77, "5414": 77, "5346": 77, "5333": 77, "5320": 77, "5308": 77, "5295": 77, "5284": 77, "5273": 77, "5264": 77, "5255": 77, "5238": 77, "5230": 77, "5223": 77, "5216": 77, "5210": 77, "5204": 77, "5198": 77, "5193": 77, "5183": 77, "5178": 77, "5174": 77, "5170": 77, "5166": 77, "5163": 77, "5159": 77, "5156": 77, "7216": 78, "7169": 78, "7151": 78, "7109": 78, "work": 79, "novel": 79, "hing": 79, "proper": 79, "orient": 79, "fashion": 79, "kernel": 79, "l_r": 79, "widetild": 79, "hy": 79, "neq": [79, 86], "affin": 79, "therefor": 79, "hop": [79, 85], "suppos": 79, "_j": 79, "underset": 79, "w_": 79, "q_": 79, "tb_1": 79, "b_2b_2": 79, "notic": 79, "pattern": 79, "just": [79, 81, 86], "maxium": 79, "valueerror": [79, 82, 85], "gradient": 79, "tx_0": 79, "estim": 79, "diverg": 79, "deriv": 79, "seen": 79, "incidence_0_1": 79, "accordingli": 79, "mm": 79, "henc": 79, "y_hat_edg": 79, "fn": 79, "y_hat_edge_test": 79, "_pred_test": 79, "ge": 79, "7322": 79, "3667": 79, "7208": 79, "7333": [79, 81], "7070": 79, "6814": 79, "6753": 79, "6717": 79, "6695": 79, "6682": 79, "6674": 79, "laplacian_down_1_list": [80, 82, 85], "laplacian_down_2_list": 80, "incidence1_t_list": 80, "incidence2_t_list": 80, "laplacian_down_2": [80, 82, 85], "laplacian_down_1_train": [80, 82], "laplacian_down_1_test": [80, 82], "laplacian_down_2_train": 80, "laplacian_down_2_test": 80, "incidence1_t_train": 80, "incidence1_t_test": 80, "incidence2_t_train": 80, "incidence2_t_test": 80, "hzpmc22": 80, "did": 80, "la": 81, "r_": 81, "mathrm": 81, "leq": [81, 86], "feat_dim": 81, "arbitrari": 81, "choos": [81, 86], "dictionari": 81, "tha": 81, "arbitrarili": 81, "formul": 81, "quit": 81, "close": 81, "usual": 81, "suggest": 81, "refrain": 81, "tetrahedron": 81, "sparse_to_torch": 81, "rank_": 81, "rank_0": 81, "coadjacency_matrix": [81, 82], "h0": 81, "h1": 81, "h3": 81, "b3": 81, "tmx": [81, 82, 85], "x_3": 81, "tetrahedron_feat": 81, "track": 81, "rank_1": 81, "rank_2": 81, "rank_3": 81, "typic": 81, "6721": 81, "6333": [81, 82], "6173": 81, "6110": 81, "5831": 81, "5695": 81, "5638": 81, "5493": 81, "5384": 81, "7667": 81, "5141": 81, "5201": 81, "5038": 81, "5016": 81, "4906": 81, "4763": 81, "4545": 81, "4483": 81, "4153": 81, "8000": 81, "4062": 81, "3790": 81, "8333": 81, "3916": 81, "3529": 81, "8667": 81, "2900": 81, "2359": 81, "9333": 81, "2002": 81, "9667": 81, "2970": 81, "9000": 81, "2032": 81, "2329": 81, "0272": 81, "0264": 81, "0245": 81, "0207": 81, "0165": 81, "0132": 81, "0114": 81, "0113": 81, "0117": 81, "0101": 81, "0081": 81, "0071": 81, "0065": 81, "0061": 81, "0057": 81, "0054": 81, "0050": 81, "0046": 81, "0043": 81, "0040": 81, "0038": 81, "0036": 81, "0034": 81, "0033": 81, "0032": 81, "0031": 81, "0030": 81, "0029": 81, "0028": 81, "0027": 81, "0026": 81, "0025": 81, "0024": 81, "0023": 81, "0022": 81, "0021": 81, "0020": 81, "0019": 81, "0018": 81, "0017": 81, "0016": 81, "0015": 81, "0014": 81, "0013": 81, "0012": 81, "0011": 81, "0010": 81, "0009": 81, "0008": 81, "account": 82, "_t": [82, 85], "p_d": [82, 85], "p_u": [82, 85], "likewis": 82, "essenti": 82, "_0": 82, "yet": [82, 85], "laplacian_0_list": [82, 85], "laplacian_up_1_list": [82, 85], "laplacian_2_list": [82, 85], "hodge_laplacian_matrix": [82, 85], "size_averag": 82, "in_linear_0": 82, "in_linear_1": 82, "in_linear_2": 82, "out_linear_0": 82, "out_linear_1": 82, "out_linear_2": 82, "_reduct": 82, "ret": 82, "laplacian_0_train": 82, "laplacian_0_test": 82, "laplacian_up_1_train": 82, "laplacian_up_1_test": 82, "laplacian_2_train": 82, "laplacian_2_test": 82, "944857": 82, "9959": 82, "5353": 82, "2055": 82, "6187": 82, "7969": 82, "4612": 82, "5951": 82, "8349": 82, "8397": 82, "9482": 82, "laplacian_up_2": [82, 85], "get_simplicial_featur": [82, 85], "which_feat": [82, 85], "elif": [82, 85], "binary_cross_entropi": 82, "6361": 82, "7911": 82, "coo_matrix": 83, "diag": 83, "return_count": 83, "normalize_higher_order_adj": 83, "a_opt": 83, "cochain": 83, "num_of_k_simplic": 83, "num_of_j_simplic": 83, "rowsum": 83, "r_inv_sqrt": 83, "flatten": 83, "isinf": 83, "r_mat_inv_sqrt": 83, "a_opt_to": 83, "dot": 83, "neigborood": 83, "ssconv": 83, "get_neighborhood": 83, "incidence_1_norm_list": 83, "incidence_2_norm_list": 83, "adjacency_up_0_norm_list": 83, "adjacency_up_1_norm_list": 83, "adjacency_down_1_norm_list": 83, "adjacency_down_2_norm_list": 83, "up_laplacian_1_list": 83, "up_laplacian_2_list": 83, "down_laplacian_1_list": 83, "down_laplacian_2_list": 83, "up_laplacian_1": 83, "up_laplacian_2": 83, "down_laplacian_1": 83, "down_laplacian_2": 83, "todo": 83, "kha053": 83, "nvml": 83, "incid1": 83, "incid1_norm": 83, "incid2": 83, "incid2_norm": 83, "adj0_up_norm": 83, "adj1_up_norm": 83, "adj1_down_norm": 83, "adj2_down_norm": 83, "correct_count": 83, "x_0t": 83, "x_1t": 83, "x_2t": 83, "incid1t": 83, "incid1_normt": 83, "incid2t": 83, "incid2_normt": 83, "adj0_up_normt": 83, "adj1_up_normt": 83, "adj1_down_normt": 83, "adj2_down_normt": 83, "yt": 83, "ysb22": 84, "ruochen": 84, "freder": 84, "razvan": 84, "pascanu": 84, "editor": 84, "confer": 84, "volum": 84, "pmlr": 84, "dec": 84, "2022a": 84, "chose": 84, "reshap": 84, "normalized_laplacian_matrix": 84, "x_0s_train": 84, "x_0s_test": 84, "x_1s_train": 84, "x_1s_test": 84, "x_2s_train": 84, "x_2s_test": 84, "laplacian_0s_train": 84, "laplacian_0s_test": 84, "laplacian_1s_train": 84, "laplacian_1s_test": 84, "laplacian_2s_train": 84, "laplacian_2s_test": 84, "6056": 84, "2707": 84, "9831": 84, "8605": 84, "0164": 84, "0106": 84, "9957": 84, "0872": 84, "5802": 84, "definit": 85, "itself": 85, "larger": 85, "x_train": 85, "x_test": 85, "laplacian_down_train": 85, "laplacian_down_test": 85, "laplacian_up_train": 85, "laplacian_up_test": 85, "simplex_order_select": 85, "7131": 85, "5218": 85, "6109": 85, "7219": 85, "8411": 85, "7747": 85, "2382": 85, "0308": 85, "maxim": 85, "chennel_edg": 85, "channel_fac": 85, "certain": 85, "classm": 85, "rm": 85, "channels_x": 85, "squeez": [85, 86], "8799": 85, "rgs21": 86, "spend": 86, "synthet": 86, "ahead": 86, "itertool": 86, "product": 86, "tnx": 86, "networkx": 86, "nx": 86, "random_split": 86, "spatial": 86, "distanc": 86, "seed": 86, "lt": 86, "_c": 86, "0x1664f0f50": 86, "less": 86, "cloud": 86, "insid": 86, "remov": 86, "centroid": 86, "sort": 86, "coordin": 86, "argsort": 86, "tri": 86, "disk_cent": 86, "disk_radiu": 86, "indices_includ": 86, "cdist": 86, "idx_dict": 86, "instanc": 86, "euclidean": 86, "shortest": 86, "plot_complex": 86, "plane": 86, "idx": 86, "poli": 86, "polygon": 86, "color": 86, "green": 86, "gca": 86, "add_patch": 86, "vstack": 86, "i_1": 86, "i_2": 86, "ldot": 86, "i_m": 86, "i_j": 86, "i_": 86, "ground": 86, "truth": 86, "supervis": 86, "setup": 86, "subsect": 86, "randomli": 86, "pick": 86, "triplet": 86, "around": 86, "anti": 86, "diagon": 86, "mid": 86, "region": 86, "start_nod": 86, "mid_nod": 86, "end_nod": 86, "all_triplet": 86, "increas": 86, "underli": 86, "distance_matrix": 86, "squareform": 86, "pdist": 86, "toarrai": 86, "from_numpy_arrai": 86, "path_1": 86, "shortest_path": 86, "path_2": 86, "plot_path": 86, "red": 86, "arrow": 86, "quiver": 86, "scale_unit": 86, "angl": 86, "yield": 86, "vectorized_trajectori": 86, "neigbors_mask": 86, "last_nod": 86, "turn": 86, "a_1": 86, "a_2": 86, "a_j": 86, "i_n": 86, "later": 86, "lookup": 86, "speed": 86, "edge_lookup_t": 86, "__getitem__": 86, "discard": 86, "neighbors_mask": 86, "__len__": 86, "c0": 86, "loader": 86, "batch_siz": 86, "val_siz": 86, "train_siz": 86, "train_d": 86, "val_d": 86, "test_d": 86, "train_dl": 86, "val_dl": 86, "test_dl": 86, "c_1": 86, "partial_1": 86, "c_0": 86, "That": 86, "hat": 86, "_m": 86, "neg": 86, "likelihood": 86, "penal": 86, "weight_decai": 86, "5e": 86, "loss_funct": 86, "nllloss": 86, "nll": 86, "training_histori": 86, "training_loss": 86, "traj": 86, "06": 86, "quick": 86, "confirm": 86, "everyth": 86, "reason": 86, "ax": 86, "ncol": 86, "figsiz": 86, "better": 86, "guess": 86, "3f": 86, "constructor": 86, "affect": 86, "capabl": 86, "revers": 86, "Or": 86, "ocean": 86, "drifter": 86}, "objects": {"topomodelx.base": [[0, 0, 0, "-", "aggregation"], [1, 0, 0, "-", "conv"], [3, 0, 0, "-", "message_passing"]], "topomodelx.base.aggregation": [[0, 1, 1, "", "Aggregation"]], "topomodelx.base.aggregation.Aggregation": [[0, 2, 1, "", "forward"], [0, 2, 1, "", "update"]], "topomodelx.base.conv": [[1, 1, 1, "", "Conv"]], "topomodelx.base.conv.Conv": [[1, 2, 1, "", "forward"], [1, 2, 1, "", "update"]], "topomodelx.base.message_passing": [[3, 1, 1, "", "MessagePassing"]], "topomodelx.base.message_passing.MessagePassing": [[3, 2, 1, "", "aggregate"], [3, 2, 1, "", "attention"], [3, 2, 1, "", "forward"], [3, 2, 1, "", "message"], [3, 2, 1, "", "reset_parameters"]], "topomodelx.nn.cell": [[5, 0, 0, "-", "can"], [6, 0, 0, "-", "can_layer"], [7, 0, 0, "-", "ccxn"], [8, 0, 0, "-", "ccxn_layer"], [9, 0, 0, "-", "cwn"], [10, 0, 0, "-", "cwn_layer"]], "topomodelx.nn.cell.can": [[5, 1, 1, "", "CAN"]], "topomodelx.nn.cell.can.CAN": [[5, 2, 1, "", "forward"]], "topomodelx.nn.cell.can_layer": [[6, 1, 1, "", "CANLayer"], [6, 1, 1, "", "LiftLayer"], [6, 1, 1, "", "MultiHeadCellAttention"], [6, 1, 1, "", "MultiHeadCellAttention_v2"], [6, 1, 1, "", "MultiHeadLiftLayer"], [6, 1, 1, "", "PoolLayer"], [6, 3, 1, "", "add_self_loops"], [6, 3, 1, "", "softmax"]], "topomodelx.nn.cell.can_layer.CANLayer": [[6, 2, 1, "", "forward"], [6, 2, 1, "", "reset_parameters"]], "topomodelx.nn.cell.can_layer.LiftLayer": [[6, 2, 1, "", "forward"], [6, 2, 1, "", "message"], [6, 2, 1, "", "reset_parameters"]], "topomodelx.nn.cell.can_layer.MultiHeadCellAttention": [[6, 2, 1, "", "attention"], [6, 2, 1, "", "forward"], [6, 2, 1, "", "message"], [6, 2, 1, "", "reset_parameters"]], "topomodelx.nn.cell.can_layer.MultiHeadCellAttention_v2": [[6, 2, 1, "", "attention"], [6, 2, 1, "", "forward"], [6, 2, 1, "", "message"], [6, 2, 1, "", "reset_parameters"]], "topomodelx.nn.cell.can_layer.MultiHeadLiftLayer": [[6, 2, 1, "", "forward"], [6, 2, 1, "", "reset_parameters"]], "topomodelx.nn.cell.can_layer.PoolLayer": [[6, 2, 1, "", "forward"], [6, 2, 1, "", "reset_parameters"]], "topomodelx.nn.cell.ccxn": [[7, 1, 1, "", "CCXN"]], "topomodelx.nn.cell.ccxn.CCXN": [[7, 2, 1, "", "forward"]], "topomodelx.nn.cell.ccxn_layer": [[8, 1, 1, "", "CCXNLayer"]], "topomodelx.nn.cell.ccxn_layer.CCXNLayer": [[8, 2, 1, "", "forward"]], "topomodelx.nn.cell.cwn": [[9, 1, 1, "", "CWN"]], "topomodelx.nn.cell.cwn.CWN": [[9, 2, 1, "", "forward"]], "topomodelx.nn.cell.cwn_layer": [[10, 1, 1, "", "CWNLayer"]], "topomodelx.nn.cell.cwn_layer.CWNLayer": [[10, 2, 1, "", "forward"]], "topomodelx.nn.hypergraph": [[12, 0, 0, "-", "allset"], [13, 0, 0, "-", "allset_layer"], [14, 0, 0, "-", "allset_transformer"], [15, 0, 0, "-", "allset_transformer_layer"], [18, 0, 0, "-", "hmpnn"], [19, 0, 0, "-", "hmpnn_layer"], [20, 0, 0, "-", "hnhn"], [21, 0, 0, "-", "hnhn_layer"], [22, 0, 0, "-", "hnhn_layer_bis"], [23, 0, 0, "-", "hypergat"], [24, 0, 0, "-", "hypergat_layer"], [25, 0, 0, "-", "hypersage"], [26, 0, 0, "-", "hypersage_layer"], [28, 0, 0, "-", "unigcn"], [29, 0, 0, "-", "unigcn_layer"], [30, 0, 0, "-", "unigcnii"], [31, 0, 0, "-", "unigcnii_layer"], [32, 0, 0, "-", "unigin"], [33, 0, 0, "-", "unigin_layer"], [34, 0, 0, "-", "unisage"], [35, 0, 0, "-", "unisage_layer"]], "topomodelx.nn.hypergraph.allset": [[12, 1, 1, "", "AllSet"]], "topomodelx.nn.hypergraph.allset.AllSet": [[12, 2, 1, "", "forward"]], "topomodelx.nn.hypergraph.allset_layer": [[13, 1, 1, "", "AllSetBlock"], [13, 1, 1, "", "AllSetLayer"], [13, 1, 1, "", "MLP"]], "topomodelx.nn.hypergraph.allset_layer.AllSetBlock": [[13, 2, 1, "", "forward"], [13, 2, 1, "", "reset_parameters"]], "topomodelx.nn.hypergraph.allset_layer.AllSetLayer": [[13, 2, 1, "", "forward"], [13, 2, 1, "", "reset_parameters"]], "topomodelx.nn.hypergraph.allset_transformer": [[14, 1, 1, "", "AllSetTransformer"]], "topomodelx.nn.hypergraph.allset_transformer.AllSetTransformer": [[14, 2, 1, "", "forward"]], "topomodelx.nn.hypergraph.allset_transformer_layer": [[15, 1, 1, "", "AllSetTransformerBlock"], [15, 1, 1, "", "AllSetTransformerLayer"], [15, 1, 1, "", "MLP"], [15, 1, 1, "", "MultiHeadAttention"]], "topomodelx.nn.hypergraph.allset_transformer_layer.AllSetTransformerBlock": [[15, 2, 1, "", "forward"], [15, 2, 1, "", "reset_parameters"]], "topomodelx.nn.hypergraph.allset_transformer_layer.AllSetTransformerLayer": [[15, 2, 1, "", "forward"], [15, 2, 1, "", "reset_parameters"]], "topomodelx.nn.hypergraph.allset_transformer_layer.MultiHeadAttention": [[15, 2, 1, "", "attention"], [15, 2, 1, "", "forward"], [15, 2, 1, "", "reset_parameters"]], "topomodelx.nn.hypergraph.hmpnn": [[18, 1, 1, "", "HMPNN"]], "topomodelx.nn.hypergraph.hmpnn.HMPNN": [[18, 2, 1, "", "forward"]], "topomodelx.nn.hypergraph.hmpnn_layer": [[19, 1, 1, "", "HMPNNLayer"]], "topomodelx.nn.hypergraph.hmpnn_layer.HMPNNLayer": [[19, 2, 1, "", "apply_regular_dropout"], [19, 2, 1, "", "forward"]], "topomodelx.nn.hypergraph.hnhn": [[20, 1, 1, "", "HNHN"]], "topomodelx.nn.hypergraph.hnhn.HNHN": [[20, 2, 1, "", "forward"]], "topomodelx.nn.hypergraph.hnhn_layer": [[21, 1, 1, "", "HNHNLayer"]], "topomodelx.nn.hypergraph.hnhn_layer.HNHNLayer": [[21, 2, 1, "", "compute_normalization_matrices"], [21, 2, 1, "", "forward"], [21, 2, 1, "", "init_biases"], [21, 2, 1, "", "normalize_incidence_matrices"], [21, 2, 1, "", "reset_parameters"]], "topomodelx.nn.hypergraph.hnhn_layer_bis": [[22, 1, 1, "", "HNHNLayer"]], "topomodelx.nn.hypergraph.hnhn_layer_bis.HNHNLayer": [[22, 2, 1, "", "forward"]], "topomodelx.nn.hypergraph.hypergat": [[23, 1, 1, "", "HyperGAT"]], "topomodelx.nn.hypergraph.hypergat.HyperGAT": [[23, 2, 1, "", "forward"]], "topomodelx.nn.hypergraph.hypergat_layer": [[24, 1, 1, "", "HyperGATLayer"]], "topomodelx.nn.hypergraph.hypergat_layer.HyperGATLayer": [[24, 2, 1, "", "attention"], [24, 2, 1, "", "forward"], [24, 2, 1, "", "reset_parameters"], [24, 2, 1, "", "update"]], "topomodelx.nn.hypergraph.hypersage": [[25, 1, 1, "", "HyperSAGE"]], "topomodelx.nn.hypergraph.hypersage.HyperSAGE": [[25, 2, 1, "", "forward"]], "topomodelx.nn.hypergraph.hypersage_layer": [[26, 1, 1, "", "GeneralizedMean"], [26, 1, 1, "", "HyperSAGELayer"]], "topomodelx.nn.hypergraph.hypersage_layer.GeneralizedMean": [[26, 2, 1, "", "forward"]], "topomodelx.nn.hypergraph.hypersage_layer.HyperSAGELayer": [[26, 2, 1, "", "aggregate"], [26, 2, 1, "", "forward"], [26, 2, 1, "", "update"]], "topomodelx.nn.hypergraph.unigcn": [[28, 1, 1, "", "UniGCN"]], "topomodelx.nn.hypergraph.unigcn.UniGCN": [[28, 2, 1, "", "forward"]], "topomodelx.nn.hypergraph.unigcn_layer": [[29, 1, 1, "", "UniGCNLayer"]], "topomodelx.nn.hypergraph.unigcn_layer.UniGCNLayer": [[29, 2, 1, "", "forward"], [29, 2, 1, "", "reset_parameters"]], "topomodelx.nn.hypergraph.unigcnii": [[30, 1, 1, "", "UniGCNII"]], "topomodelx.nn.hypergraph.unigcnii.UniGCNII": [[30, 2, 1, "", "forward"]], "topomodelx.nn.hypergraph.unigcnii_layer": [[31, 1, 1, "", "UniGCNIILayer"]], "topomodelx.nn.hypergraph.unigcnii_layer.UniGCNIILayer": [[31, 2, 1, "", "forward"], [31, 2, 1, "", "reset_parameters"]], "topomodelx.nn.hypergraph.unigin": [[32, 1, 1, "", "UniGIN"]], "topomodelx.nn.hypergraph.unigin.UniGIN": [[32, 2, 1, "", "forward"]], "topomodelx.nn.hypergraph.unigin_layer": [[33, 1, 1, "", "UniGINLayer"]], "topomodelx.nn.hypergraph.unigin_layer.UniGINLayer": [[33, 2, 1, "", "forward"]], "topomodelx.nn.hypergraph.unisage": [[34, 1, 1, "", "UniSAGE"]], "topomodelx.nn.hypergraph.unisage.UniSAGE": [[34, 2, 1, "", "forward"]], "topomodelx.nn.hypergraph.unisage_layer": [[35, 1, 1, "", "UniSAGELayer"]], "topomodelx.nn.hypergraph.unisage_layer.UniSAGELayer": [[35, 2, 1, "", "forward"], [35, 2, 1, "", "reset_parameters"]], "topomodelx.nn.simplicial": [[37, 0, 0, "-", "dist2cycle"], [38, 0, 0, "-", "dist2cycle_layer"], [39, 0, 0, "-", "hsn"], [40, 0, 0, "-", "hsn_layer"], [42, 0, 0, "-", "san"], [43, 0, 0, "-", "san_layer"], [44, 0, 0, "-", "sca_cmps"], [45, 0, 0, "-", "sca_cmps_layer"], [46, 0, 0, "-", "sccn"], [47, 0, 0, "-", "sccn_layer"], [48, 0, 0, "-", "sccnn"], [49, 0, 0, "-", "sccnn_layer"], [50, 0, 0, "-", "scconv"], [51, 0, 0, "-", "scconv_layer"], [52, 0, 0, "-", "scn2"], [53, 0, 0, "-", "scn2_layer"], [54, 0, 0, "-", "scnn"], [55, 0, 0, "-", "scnn_layer"], [56, 0, 0, "-", "scone"], [57, 0, 0, "-", "scone_layer"]], "topomodelx.nn.simplicial.dist2cycle": [[37, 1, 1, "", "Dist2Cycle"]], "topomodelx.nn.simplicial.dist2cycle.Dist2Cycle": [[37, 2, 1, "", "forward"]], "topomodelx.nn.simplicial.dist2cycle_layer": [[38, 1, 1, "", "Dist2CycleLayer"]], "topomodelx.nn.simplicial.dist2cycle_layer.Dist2CycleLayer": [[38, 2, 1, "", "forward"], [38, 2, 1, "", "reset_parameters"]], "topomodelx.nn.simplicial.hsn": [[39, 1, 1, "", "HSN"]], "topomodelx.nn.simplicial.hsn.HSN": [[39, 2, 1, "", "forward"]], "topomodelx.nn.simplicial.hsn_layer": [[40, 1, 1, "", "HSNLayer"]], "topomodelx.nn.simplicial.hsn_layer.HSNLayer": [[40, 2, 1, "", "forward"], [40, 2, 1, "", "reset_parameters"]], "topomodelx.nn.simplicial.san": [[42, 1, 1, "", "SAN"]], "topomodelx.nn.simplicial.san.SAN": [[42, 2, 1, "", "compute_projection_matrix"], [42, 2, 1, "", "forward"]], "topomodelx.nn.simplicial.san_layer": [[43, 1, 1, "", "SANConv"], [43, 1, 1, "", "SANLayer"]], "topomodelx.nn.simplicial.san_layer.SANConv": [[43, 2, 1, "", "forward"]], "topomodelx.nn.simplicial.san_layer.SANLayer": [[43, 2, 1, "", "forward"], [43, 2, 1, "", "reset_parameters"]], "topomodelx.nn.simplicial.sca_cmps": [[44, 1, 1, "", "SCACMPS"]], "topomodelx.nn.simplicial.sca_cmps.SCACMPS": [[44, 2, 1, "", "forward"]], "topomodelx.nn.simplicial.sca_cmps_layer": [[45, 1, 1, "", "SCACMPSLayer"]], "topomodelx.nn.simplicial.sca_cmps_layer.SCACMPSLayer": [[45, 2, 1, "", "forward"], [45, 2, 1, "", "intra_aggr"], [45, 2, 1, "", "reset_parameters"], [45, 2, 1, "", "weight_func"]], "topomodelx.nn.simplicial.sccn": [[46, 1, 1, "", "SCCN"]], "topomodelx.nn.simplicial.sccn.SCCN": [[46, 2, 1, "", "forward"]], "topomodelx.nn.simplicial.sccn_layer": [[47, 1, 1, "", "SCCNLayer"]], "topomodelx.nn.simplicial.sccn_layer.SCCNLayer": [[47, 2, 1, "", "forward"], [47, 2, 1, "", "reset_parameters"]], "topomodelx.nn.simplicial.sccnn": [[48, 1, 1, "", "SCCNN"], [48, 1, 1, "", "SCCNNComplex"]], "topomodelx.nn.simplicial.sccnn.SCCNN": [[48, 2, 1, "", "forward"]], "topomodelx.nn.simplicial.sccnn.SCCNNComplex": [[48, 2, 1, "", "forward"]], "topomodelx.nn.simplicial.sccnn_layer": [[49, 1, 1, "", "SCCNNLayer"]], "topomodelx.nn.simplicial.sccnn_layer.SCCNNLayer": [[49, 2, 1, "", "aggr_norm_func"], [49, 2, 1, "", "chebyshev_conv"], [49, 2, 1, "", "forward"], [49, 2, 1, "", "reset_parameters"], [49, 2, 1, "", "update"]], "topomodelx.nn.simplicial.scconv": [[50, 1, 1, "", "SCConv"]], "topomodelx.nn.simplicial.scconv.SCConv": [[50, 2, 1, "", "forward"]], "topomodelx.nn.simplicial.scconv_layer": [[51, 1, 1, "", "SCConvLayer"]], "topomodelx.nn.simplicial.scconv_layer.SCConvLayer": [[51, 2, 1, "", "forward"], [51, 2, 1, "", "reset_parameters"]], "topomodelx.nn.simplicial.scn2": [[52, 1, 1, "", "SCN2"]], "topomodelx.nn.simplicial.scn2.SCN2": [[52, 2, 1, "", "forward"]], "topomodelx.nn.simplicial.scn2_layer": [[53, 1, 1, "", "SCN2Layer"]], "topomodelx.nn.simplicial.scn2_layer.SCN2Layer": [[53, 2, 1, "", "forward"], [53, 2, 1, "", "reset_parameters"]], "topomodelx.nn.simplicial.scnn": [[54, 1, 1, "", "SCNN"]], "topomodelx.nn.simplicial.scnn.SCNN": [[54, 2, 1, "", "forward"]], "topomodelx.nn.simplicial.scnn_layer": [[55, 1, 1, "", "SCNNLayer"]], "topomodelx.nn.simplicial.scnn_layer.SCNNLayer": [[55, 2, 1, "", "aggr_norm_func"], [55, 2, 1, "", "chebyshev_conv"], [55, 2, 1, "", "forward"], [55, 2, 1, "", "reset_parameters"], [55, 2, 1, "", "update"]], "topomodelx.nn.simplicial.scone": [[56, 1, 1, "", "SCoNe"], [56, 1, 1, "", "TrajectoriesDataset"], [56, 3, 1, "", "generate_complex"], [56, 3, 1, "", "generate_trajectories"]], "topomodelx.nn.simplicial.scone.SCoNe": [[56, 2, 1, "", "forward"]], "topomodelx.nn.simplicial.scone.TrajectoriesDataset": [[56, 2, 1, "", "vectorize_path"]], "topomodelx.nn.simplicial.scone_layer": [[57, 1, 1, "", "SCoNeLayer"]], "topomodelx.nn.simplicial.scone_layer.SCoNeLayer": [[57, 2, 1, "", "forward"], [57, 2, 1, "", "reset_parameters"]], "topomodelx.utils": [[58, 0, 0, "-", "scatter"]], "topomodelx.utils.scatter": [[58, 3, 1, "", "broadcast"], [58, 3, 1, "", "scatter"], [58, 3, 1, "", "scatter_add"], [58, 3, 1, "", "scatter_mean"], [58, 3, 1, "", "scatter_sum"]]}, "objtypes": {"0": "py:module", "1": "py:class", "2": "py:method", "3": "py:function"}, "objnames": {"0": ["py", "module", "Python module"], "1": ["py", "class", "Python class"], "2": ["py", "method", "Python method"], "3": ["py", "function", "Python function"]}, "titleterms": {"aggreg": 0, "conv": 1, "base": 2, "messag": [3, 68, 80], "pass": [3, 68, 80], "api": 4, "refer": [4, 61, 84], "packag": 4, "modul": 4, "can": [5, 62], "can_lay": 6, "ccxn": [7, 63], "ccxn_layer": 8, "cwn": [9, 64], "cwn_layer": 10, "cell": [11, 62, 63], "allset": 12, "allset_lay": 13, "allset_transform": 14, "allset_transformer_lay": 15, "dhgcn": [16, 67], "dhgcn_layer": 17, "hmpnn": [18, 68], "hmpnn_layer": 19, "hnhn": [20, 69, 70], "hnhn_layer": 21, "hnhn_layer_bi": 22, "hypergat": 23, "hypergat_lay": 24, "hypersag": [25, 72], "hypersage_lay": 26, "hypergraph": [27, 67, 68, 69, 70, 71, 74, 87], "unigcn": [28, 73], "unigcn_lay": 29, "unigcnii": [30, 74], "unigcnii_lay": 31, "neural": [36, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87], "network": [36, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86], "simplici": [41, 77, 78, 79, 80, 81, 82, 83, 85, 86, 87], "util": 58, "icml": 59, "2023": 59, "topolog": [59, 87], "deep": 59, "learn": 59, "challeng": 59, "descript": 59, "public": 59, "outcom": 59, "particip": 59, "deadlin": 59, "how": 59, "submit": 59, "guidelin": 59, "submiss": 59, "requir": 59, "evalu": [59, 86], "question": 59, "contribut": 60, "make": 60, "chang": 60, "write": 60, "test": [60, 80, 83, 85, 86], "run": 60, "document": 60, "intro": 60, "docstr": 60, "The": [60, 62, 63, 64, 79], "anatomi": 60, "exampl": 60, "topomodelx": 61, "tmx": 61, "get": 61, "start": 61, "train": [62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86], "attent": [62, 63, 79], "abstract": [62, 79], "task": [62, 63, 64, 79], "set": [62, 63, 64, 65, 66], "up": [62, 63, 64], "pre": [62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85], "process": [62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85], "creat": [62, 63, 64, 65, 67, 68, 69, 70, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 85, 86], "convolut": [63, 81, 82, 83, 84, 85], "complex": [63, 80, 81, 82, 83, 85, 86, 87], "cw": 64, "an": [65, 66], "all": [65, 66], "tnn": [65, 66, 67, 72, 73, 75, 76], "addit": [65, 66, 72], "theoret": [65, 66, 72], "clarif": [65, 66, 72], "transform": 66, "defin": [66, 67, 69, 71, 72, 77, 78, 81, 82, 83, 84, 85], "import": [67, 69, 71, 73, 75, 76, 77, 78, 80, 81, 82, 83, 84, 85], "data": [67, 71, 73, 74, 75, 76, 80, 83, 86], "neighborhood": [67, 69, 71, 77, 78, 81, 82, 84, 85], "structur": [67, 69, 71, 77, 78, 81, 83, 84], "lift": [67, 71], "domain": [67, 71], "hyperedg": [69, 70], "neuron": [69, 70], "dataset": [69, 77, 78, 81, 82, 83, 84, 85, 86], "signal": [69, 77, 78, 81, 82, 85], "us": 74, "layer": 74, "load": 74, "unigin": 75, "uni": 76, "sage": 76, "homologi": 77, "local": 77, "dist2cycl": 77, "binari": [77, 78, 81, 82, 85], "label": [77, 78, 81, 82, 85], "featur": 77, "high": 78, "skip": 78, "hsn": 78, "san": 79, "autoencod": 80, "sca": 80, "coadjac": 80, "scheme": 80, "cmp": 80, "prepar": [80, 83, 85], "input": 80, "each": 80, "split": [80, 85], "model": [80, 82, 85, 86], "sccn": 81, "sccnn": 82, "we": [82, 85], "perform": [82, 85], "1": [82, 85], "classif": [82, 85], "shrec": 82, "strcture": [82, 85], "2": [82, 83, 84, 85], "node": [82, 85], "scconv": 83, "helper": 83, "function": 83, "neighbourhood": 83, "simplex": 84, "scn": 84, "rank": 84, "scnn": 85, "karat": 85, "weight": 85, "hodg": 85, "laplacian": 85, "net": [86, 87], "scone": 86, "tabl": 86, "content": 86, "gener": 86, "trajectori": 86, "pytorch": 86, "dataload": 86, "suggest": 86, "further": 86, "experiment": 86, "tutori": 87, "cellular": 87}, "envversion": {"sphinx.domains.c": 3, "sphinx.domains.changeset": 1, "sphinx.domains.citation": 1, "sphinx.domains.cpp": 9, "sphinx.domains.index": 1, "sphinx.domains.javascript": 3, "sphinx.domains.math": 2, "sphinx.domains.python": 4, "sphinx.domains.rst": 2, "sphinx.domains.std": 2, "nbsphinx": 4, "sphinx.ext.viewcode": 1, "sphinx": 60}, "alltitles": {"Aggregation": [[0, "module-topomodelx.base.aggregation"]], "Conv": [[1, "module-topomodelx.base.conv"]], "Base": [[2, "base"]], "Message Passing": [[3, "module-topomodelx.base.message_passing"]], "API Reference": [[4, "api-reference"]], "Packages & Modules": [[4, null]], "CAN": [[5, "module-topomodelx.nn.cell.can"]], "Can_Layer": [[6, "module-topomodelx.nn.cell.can_layer"]], "CCXN": [[7, "module-topomodelx.nn.cell.ccxn"]], "CCXN_Layer": [[8, "module-topomodelx.nn.cell.ccxn_layer"]], "CWN": [[9, "module-topomodelx.nn.cell.cwn"]], "Cwn_Layer": [[10, "module-topomodelx.nn.cell.cwn_layer"]], "Cell": [[11, "cell"]], "AllSet": [[12, "module-topomodelx.nn.hypergraph.allset"]], "AllSet_Layer": [[13, "module-topomodelx.nn.hypergraph.allset_layer"]], "AllSet_Transformer": [[14, "module-topomodelx.nn.hypergraph.allset_transformer"]], "AllSet_Transformer_Layer": [[15, "module-topomodelx.nn.hypergraph.allset_transformer_layer"]], "DHGCN": [[16, "dhgcn"]], "DHGCN_Layer": [[17, "dhgcn-layer"]], "HMPNN": [[18, "module-topomodelx.nn.hypergraph.hmpnn"]], "HMPNN_Layer": [[19, "module-topomodelx.nn.hypergraph.hmpnn_layer"]], "HNHN": [[20, "module-topomodelx.nn.hypergraph.hnhn"]], "HNHN_Layer": [[21, "module-topomodelx.nn.hypergraph.hnhn_layer"]], "HNHN_Layer_Bis": [[22, "module-topomodelx.nn.hypergraph.hnhn_layer_bis"]], "Hypergat": [[23, "module-topomodelx.nn.hypergraph.hypergat"]], "Hypergat_Layer": [[24, "module-topomodelx.nn.hypergraph.hypergat_layer"]], "Hypersage": [[25, "module-topomodelx.nn.hypergraph.hypersage"]], "Hypersage_Layer": [[26, "module-topomodelx.nn.hypergraph.hypersage_layer"]], "Hypergraph": [[27, "hypergraph"]], "Unigcn": [[28, "module-topomodelx.nn.hypergraph.unigcn"]], "Unigcn_Layer": [[29, "module-topomodelx.nn.hypergraph.unigcn_layer"]], "Unigcnii": [[30, "module-topomodelx.nn.hypergraph.unigcnii"]], "Unigcnii_Layer": [[31, "module-topomodelx.nn.hypergraph.unigcnii_layer"]], "Neural Networks": [[36, "neural-networks"]], "Simplicial": [[41, "simplicial"]], "Utils": [[58, "utils"]], "ICML 2023 Topological Deep Learning Challenge": [[59, "icml-2023-topological-deep-learning-challenge"]], "Description of the Challenge": [[59, "description-of-the-challenge"]], "\u2b50\ufe0f Publication Outcomes for Participants \u2b50\ufe0f": [[59, "publication-outcomes-for-participants"]], "Deadline": [[59, "deadline"]], "How to Submit": [[59, "how-to-submit"]], "Guidelines": [[59, "guidelines"]], "Submission Requirements": [[59, "submission-requirements"]], "Evaluation": [[59, "evaluation"]], "Questions": [[59, "questions"]], "Contributing": [[60, "contributing"]], "Making Changes": [[60, "making-changes"]], "Write Tests": [[60, "write-tests"]], "Run Tests": [[60, "run-tests"]], "Write Documentation": [[60, "write-documentation"]], "Intro to Docstrings": [[60, "intro-to-docstrings"]], "The Anatomy of a Docstring": [[60, "the-anatomy-of-a-docstring"]], "Docstring Examples": [[60, "docstring-examples"]], "\ud83c\udf10 TopoModelX (TMX) \ud83c\udf69": [[61, "topomodelx-tmx"]], "\ud83d\udd0d References": [[61, "references"]], "\ud83e\uddbe Getting Started": [[61, "getting-started"]], "Train a Cell Attention Network (CAN)": [[62, "Train-a-Cell-Attention-Network-(CAN)"]], "Abstract:": [[62, "Abstract:"]], "The Neural Network:": [[62, "The-Neural-Network:"], [63, "The-Neural-Network:"], [64, "The-Neural-Network:"]], "The Task:": [[62, "The-Task:"], [63, "The-Task:"], [64, "The-Task:"], [79, "The-Task:"]], "Set-up": [[62, "Set-up"], [63, "Set-up"], [64, "Set-up"]], "Pre-processing": [[62, "Pre-processing"], [63, "Pre-processing"], [64, "Pre-processing"], [65, "Pre-processing"], [66, "Pre-processing"], [67, "Pre-processing"], [68, "Pre-processing"], [69, "Pre-processing"], [70, "Pre-processing"], [71, "Pre-processing"], [72, "Pre-processing"], [73, "Pre-processing"], [75, "Pre-processing"], [76, "Pre-processing"], [77, "Pre-processing"], [78, "Pre-processing"], [79, "Pre-processing"], [80, "Pre-processing"], [81, "Pre-processing"], [82, "Pre-processing"], [83, "Pre-processing"], [84, "Pre-processing"], [85, "Pre-processing"]], "Create the Neural Network": [[62, "Create-the-Neural-Network"], [63, "Create-the-Neural-Network"], [64, "Create-the-Neural-Network"], [65, "Create-the-Neural-Network"], [67, "Create-the-Neural-Network"], [68, "Create-the-Neural-Network"], [69, "Create-the-Neural-Network"], [70, "Create-the-Neural-Network"], [73, "Create-the-Neural-Network"], [75, "Create-the-Neural-Network"], [76, "Create-the-Neural-Network"], [77, "Create-the-Neural-Network"], [78, "Create-the-Neural-Network"], [79, "Create-the-Neural-Network"], [81, "Create-the-Neural-Network"]], "Train the Neural Network": [[62, "Train-the-Neural-Network"], [63, "Train-the-Neural-Network"], [64, "Train-the-Neural-Network"], [65, "Train-the-Neural-Network"], [66, "Train-the-Neural-Network"], [67, "Train-the-Neural-Network"], [68, "Train-the-Neural-Network"], [69, "Train-the-Neural-Network"], [70, "Train-the-Neural-Network"], [71, "Train-the-Neural-Network"], [72, "Train-the-Neural-Network"], [73, "Train-the-Neural-Network"], [75, "Train-the-Neural-Network"], [76, "Train-the-Neural-Network"], [77, "Train-the-Neural-Network"], [78, "Train-the-Neural-Network"], [79, "Train-the-Neural-Network"], [81, "Train-the-Neural-Network"], [84, "Train-the-Neural-Network"], [85, "Train-the-Neural-Network"]], "Train a Convolutional Cell Complex Network (CCXN)": [[63, "Train-a-Convolutional-Cell-Complex-Network-(CCXN)"]], "Train the Neural Network with Attention": [[63, "Train-the-Neural-Network-with-Attention"]], "Train a CW Network (CWN)": [[64, "Train-a-CW-Network-(CWN)"]], "Train an All-Set TNN": [[65, "Train-an-All-Set-TNN"]], "Additional theoretical clarifications": [[65, "Additional-theoretical-clarifications"], [66, "Additional-theoretical-clarifications"], [72, "Additional-theoretical-clarifications"]], "Train an All-Set-Transformer TNN": [[66, "Train-an-All-Set-Transformer-TNN"]], "Define the Neural Network": [[66, "Define-the-Neural-Network"], [71, "Define-the-Neural-Network"], [72, "Define-the-Neural-Network"]], "Train a DHGCN TNN": [[67, "Train-a-DHGCN-TNN"]], "Import data": [[67, "Import-data"], [71, "Import-data"], [73, "Import-data"], [75, "Import-data"], [76, "Import-data"], [80, "Import-data"]], "Define neighborhood structures and lift into hypergraph domain.": [[67, "Define-neighborhood-structures-and-lift-into-hypergraph-domain."], [71, "Define-neighborhood-structures-and-lift-into-hypergraph-domain."]], "Train a Hypergraph Message Passing Neural Network (HMPNN)": [[68, "Train-a-Hypergraph-Message-Passing-Neural-Network-(HMPNN)"]], "Train a Hypergraph Networks with Hyperedge Neurons (HNHN)": [[69, "Train-a-Hypergraph-Networks-with-Hyperedge-Neurons-(HNHN)"]], "Import dataset": [[69, "Import-dataset"], [77, "Import-dataset"], [78, "Import-dataset"], [81, "Import-dataset"], [83, "Import-dataset"], [84, "Import-dataset"]], "Define neighborhood structures.": [[69, "Define-neighborhood-structures."], [77, "Define-neighborhood-structures."], [78, "Define-neighborhood-structures."], [81, "Define-neighborhood-structures."], [84, "Define-neighborhood-structures."]], "Import signal": [[69, "Import-signal"], [77, "Import-signal"], [78, "Import-signal"], [81, "Import-signal"], [82, "Import-signal"]], "Train a Hypergraph Network with Hyperedge Neurons (HNHN)": [[70, "Train-a-Hypergraph-Network-with-Hyperedge-Neurons-(HNHN)"]], "Train a Hypergraph Neural Network": [[71, "Train-a-Hypergraph-Neural-Network"]], "Train a Hypersage TNN": [[72, "Train-a-Hypersage-TNN"]], "Train a UNIGCN TNN": [[73, "Train-a-UNIGCN-TNN"]], "Train a hypergraph neural network using UniGCNII layers": [[74, "Train-a-hypergraph-neural-network-using-UniGCNII-layers"]], "Loading the data": [[74, "Loading-the-data"]], "Creating a neural network": [[74, "Creating-a-neural-network"]], "Training the neural network": [[74, "Training-the-neural-network"]], "Train a UNIGIN TNN": [[75, "Train-a-UNIGIN-TNN"]], "Train a Uni-sage TNN": [[76, "Train-a-Uni-sage-TNN"]], "Train a Simplicial Neural Network for Homology Localization (Dist2Cycle)": [[77, "Train-a-Simplicial-Neural-Network-for-Homology-Localization-(Dist2Cycle)"]], "Define binary labels": [[77, "Define-binary-labels"], [78, "Define-binary-labels"], [81, "Define-binary-labels"], [82, "Define-binary-labels"]], "Create Features": [[77, "Create-Features"]], "Train a Simplicial High-Skip Network (HSN)": [[78, "Train-a-Simplicial-High-Skip-Network-(HSN)"]], "Train a Simplicial Attention Network (SAN)": [[79, "Train-a-Simplicial-Attention-Network-(SAN)"]], "Abstract": [[79, "Abstract"]], "The Neural Network": [[79, "The-Neural-Network"]], "Train a Simplicial Complex Autoencoder (SCA) with Coadjacency Message Passing Scheme (CMPS)": [[80, "Train-a-Simplicial-Complex-Autoencoder-(SCA)-with-Coadjacency-Message-Passing-Scheme-(CMPS)"]], "Preparing the inputs to test each message passing scheme:": [[80, "Preparing-the-inputs-to-test-each-message-passing-scheme:"]], "Coadjacency Message Passing Scheme (CMPS):": [[80, "Coadjacency-Message-Passing-Scheme-(CMPS):"]], "Create the Neural Networks": [[80, "Create-the-Neural-Networks"]], "Train and Test Split": [[80, "Train-and-Test-Split"]], "Training and Testing Model": [[80, "Training-and-Testing-Model"]], "Train a Simplicial Complex Convolutional Network (SCCN)": [[81, "Train-a-Simplicial-Complex-Convolutional-Network-(SCCN)"]], "Train a SCCNN": [[82, "Train-a-SCCNN"]], "We train the model to perform:": [[82, "We-train-the-model-to-perform:"], [85, "We-train-the-model-to-perform:"]], "Simplicial Complex Convolutional Neural Networks [SCCNN]": [[82, "Simplicial-Complex-Convolutional-Neural-Networks-[SCCNN]"]], "1. Complex Classification": [[82, "1.-Complex-Classification"], [85, "1.-Complex-Classification"]], "Import shrec dataset": [[82, "Import-shrec-dataset"]], "Define Neighborhood Strctures": [[82, "Define-Neighborhood-Strctures"], [82, "id1"], [85, "Define-Neighborhood-Strctures"], [85, "id1"]], "Create and Train the Neural Network": [[82, "Create-and-Train-the-Neural-Network"], [82, "id2"], [83, "Create-and-Train-the-Neural-Network"]], "2. Node Classification": [[82, "2.-Node-Classification"], [85, "2.-Node-Classification"]], "Train a Simplicial 2-complex convolutional neural network (SCConv)": [[83, "Train-a-Simplicial-2-complex-convolutional-neural-network-(SCConv)"]], "Helper functions": [[83, "Helper-functions"]], "Define Neighbourhood Structures": [[83, "Define-Neighbourhood-Structures"]], "prepare training and test data": [[83, "prepare-training-and-test-data"]], "Train a Simplex Convolutional Network (SCN) of Rank 2": [[84, "Train-a-Simplex-Convolutional-Network-(SCN)-of-Rank-2"]], "References": [[84, "References"]], "Train a Simplicial Convolutional Neural Network (SCNN)": [[85, "Train-a-Simplicial-Convolutional-Neural-Network-(SCNN)"]], "Simplicial Convolutional Neural Networks [SCNN]": [[85, "Simplicial-Convolutional-Neural-Networks-[SCNN]"]], "Import Karate dataset": [[85, "Import-Karate-dataset"]], "Weighted Hodge Laplacians": [[85, "Weighted-Hodge-Laplacians"]], "Import signals": [[85, "Import-signals"]], "Define binary labels and Prepare the training-testing split": [[85, "Define-binary-labels-and-Prepare-the-training-testing-split"]], "Create the SCNN for node classification": [[85, "Create-the-SCNN-for-node-classification"]], "Train the SCNN": [[85, "Train-the-SCNN"]], "Train a Simplicial Complex Net (SCoNe)": [[86, "Train-a-Simplicial-Complex-Net-(SCoNe)"]], "Table of contents": [[86, "Table-of-contents"]], "Dataset generation": [[86, "Dataset-generation"]], "Generating trajectories": [[86, "Generating-trajectories"]], "Creating PyTorch dataloaders": [[86, "Creating-PyTorch-dataloaders"]], "Creating the Neural Network": [[86, "Creating-the-Neural-Network"]], "Training the Neural Network": [[86, "Training-the-Neural-Network"]], "Evaluating the model on test data": [[86, "Evaluating-the-model-on-test-data"]], "Suggestions for further experimentation": [[86, "Suggestions-for-further-experimentation"]], "Tutorials": [[87, "tutorials"]], "Topological Neural Nets on Cellular Complexes": [[87, "topological-neural-nets-on-cellular-complexes"]], "Topological Neural Nets on Hypergraphs": [[87, "topological-neural-nets-on-hypergraphs"]], "Topological Neural Nets on Simplicial Complexes": [[87, "topological-neural-nets-on-simplicial-complexes"]]}, "indexentries": {"aggregation (class in topomodelx.base.aggregation)": [[0, "topomodelx.base.aggregation.Aggregation"]], "forward() (topomodelx.base.aggregation.aggregation method)": [[0, "topomodelx.base.aggregation.Aggregation.forward"]], "module": [[0, "module-topomodelx.base.aggregation"], [1, "module-topomodelx.base.conv"], [3, "module-topomodelx.base.message_passing"], [5, "module-topomodelx.nn.cell.can"], [6, "module-topomodelx.nn.cell.can_layer"], [7, "module-topomodelx.nn.cell.ccxn"], [8, "module-topomodelx.nn.cell.ccxn_layer"], [9, "module-topomodelx.nn.cell.cwn"], [10, "module-topomodelx.nn.cell.cwn_layer"], [12, "module-topomodelx.nn.hypergraph.allset"], [13, "module-topomodelx.nn.hypergraph.allset_layer"], [14, "module-topomodelx.nn.hypergraph.allset_transformer"], [15, "module-topomodelx.nn.hypergraph.allset_transformer_layer"], [18, "module-topomodelx.nn.hypergraph.hmpnn"], [19, "module-topomodelx.nn.hypergraph.hmpnn_layer"], [20, "module-topomodelx.nn.hypergraph.hnhn"], [21, "module-topomodelx.nn.hypergraph.hnhn_layer"], [22, "module-topomodelx.nn.hypergraph.hnhn_layer_bis"], [23, "module-topomodelx.nn.hypergraph.hypergat"], [24, "module-topomodelx.nn.hypergraph.hypergat_layer"], [25, "module-topomodelx.nn.hypergraph.hypersage"], [26, "module-topomodelx.nn.hypergraph.hypersage_layer"], [28, "module-topomodelx.nn.hypergraph.unigcn"], [29, "module-topomodelx.nn.hypergraph.unigcn_layer"], [30, "module-topomodelx.nn.hypergraph.unigcnii"], [31, "module-topomodelx.nn.hypergraph.unigcnii_layer"], [32, "module-topomodelx.nn.hypergraph.unigin"], [33, "module-topomodelx.nn.hypergraph.unigin_layer"], [34, "module-topomodelx.nn.hypergraph.unisage"], [35, "module-topomodelx.nn.hypergraph.unisage_layer"], [37, "module-topomodelx.nn.simplicial.dist2cycle"], [38, "module-topomodelx.nn.simplicial.dist2cycle_layer"], [39, "module-topomodelx.nn.simplicial.hsn"], [40, "module-topomodelx.nn.simplicial.hsn_layer"], [42, "module-topomodelx.nn.simplicial.san"], [43, "module-topomodelx.nn.simplicial.san_layer"], [44, "module-topomodelx.nn.simplicial.sca_cmps"], [45, "module-topomodelx.nn.simplicial.sca_cmps_layer"], [46, "module-topomodelx.nn.simplicial.sccn"], [47, "module-topomodelx.nn.simplicial.sccn_layer"], [48, "module-topomodelx.nn.simplicial.sccnn"], [49, "module-topomodelx.nn.simplicial.sccnn_layer"], [50, "module-topomodelx.nn.simplicial.scconv"], [51, "module-topomodelx.nn.simplicial.scconv_layer"], [52, "module-topomodelx.nn.simplicial.scn2"], [53, "module-topomodelx.nn.simplicial.scn2_layer"], [54, "module-topomodelx.nn.simplicial.scnn"], [55, "module-topomodelx.nn.simplicial.scnn_layer"], [56, "module-topomodelx.nn.simplicial.scone"], [57, "module-topomodelx.nn.simplicial.scone_layer"], [58, "module-topomodelx.utils.scatter"]], "topomodelx.base.aggregation": [[0, "module-topomodelx.base.aggregation"]], "update() (topomodelx.base.aggregation.aggregation method)": [[0, "topomodelx.base.aggregation.Aggregation.update"]], "conv (class in topomodelx.base.conv)": [[1, "topomodelx.base.conv.Conv"]], "forward() (topomodelx.base.conv.conv method)": [[1, "topomodelx.base.conv.Conv.forward"]], "topomodelx.base.conv": [[1, "module-topomodelx.base.conv"]], "update() (topomodelx.base.conv.conv method)": [[1, "topomodelx.base.conv.Conv.update"]], "messagepassing (class in topomodelx.base.message_passing)": [[3, "topomodelx.base.message_passing.MessagePassing"]], "aggregate() (topomodelx.base.message_passing.messagepassing method)": [[3, "topomodelx.base.message_passing.MessagePassing.aggregate"]], "attention() (topomodelx.base.message_passing.messagepassing method)": [[3, "topomodelx.base.message_passing.MessagePassing.attention"]], "forward() (topomodelx.base.message_passing.messagepassing method)": [[3, "topomodelx.base.message_passing.MessagePassing.forward"]], "message() (topomodelx.base.message_passing.messagepassing method)": [[3, "topomodelx.base.message_passing.MessagePassing.message"]], "reset_parameters() (topomodelx.base.message_passing.messagepassing method)": [[3, "topomodelx.base.message_passing.MessagePassing.reset_parameters"]], "topomodelx.base.message_passing": [[3, "module-topomodelx.base.message_passing"]], "can (class in topomodelx.nn.cell.can)": [[5, "topomodelx.nn.cell.can.CAN"]], "forward() (topomodelx.nn.cell.can.can method)": [[5, "topomodelx.nn.cell.can.CAN.forward"]], "topomodelx.nn.cell.can": [[5, "module-topomodelx.nn.cell.can"]], "canlayer (class in topomodelx.nn.cell.can_layer)": [[6, "topomodelx.nn.cell.can_layer.CANLayer"]], "liftlayer (class in topomodelx.nn.cell.can_layer)": [[6, "topomodelx.nn.cell.can_layer.LiftLayer"]], "multiheadcellattention (class in topomodelx.nn.cell.can_layer)": [[6, "topomodelx.nn.cell.can_layer.MultiHeadCellAttention"]], "multiheadcellattention_v2 (class in topomodelx.nn.cell.can_layer)": [[6, "topomodelx.nn.cell.can_layer.MultiHeadCellAttention_v2"]], "multiheadliftlayer (class in topomodelx.nn.cell.can_layer)": [[6, "topomodelx.nn.cell.can_layer.MultiHeadLiftLayer"]], "poollayer (class in topomodelx.nn.cell.can_layer)": [[6, "topomodelx.nn.cell.can_layer.PoolLayer"]], "add_self_loops() (in module topomodelx.nn.cell.can_layer)": [[6, "topomodelx.nn.cell.can_layer.add_self_loops"]], "attention() (topomodelx.nn.cell.can_layer.multiheadcellattention method)": [[6, "topomodelx.nn.cell.can_layer.MultiHeadCellAttention.attention"]], "attention() (topomodelx.nn.cell.can_layer.multiheadcellattention_v2 method)": [[6, "topomodelx.nn.cell.can_layer.MultiHeadCellAttention_v2.attention"]], "forward() (topomodelx.nn.cell.can_layer.canlayer method)": [[6, "topomodelx.nn.cell.can_layer.CANLayer.forward"]], "forward() (topomodelx.nn.cell.can_layer.liftlayer method)": [[6, "topomodelx.nn.cell.can_layer.LiftLayer.forward"]], "forward() (topomodelx.nn.cell.can_layer.multiheadcellattention method)": [[6, "topomodelx.nn.cell.can_layer.MultiHeadCellAttention.forward"]], "forward() (topomodelx.nn.cell.can_layer.multiheadcellattention_v2 method)": [[6, "topomodelx.nn.cell.can_layer.MultiHeadCellAttention_v2.forward"]], "forward() (topomodelx.nn.cell.can_layer.multiheadliftlayer method)": [[6, "topomodelx.nn.cell.can_layer.MultiHeadLiftLayer.forward"]], "forward() (topomodelx.nn.cell.can_layer.poollayer method)": [[6, "topomodelx.nn.cell.can_layer.PoolLayer.forward"]], "message() (topomodelx.nn.cell.can_layer.liftlayer method)": [[6, "topomodelx.nn.cell.can_layer.LiftLayer.message"]], "message() (topomodelx.nn.cell.can_layer.multiheadcellattention method)": [[6, "topomodelx.nn.cell.can_layer.MultiHeadCellAttention.message"]], "message() (topomodelx.nn.cell.can_layer.multiheadcellattention_v2 method)": [[6, "topomodelx.nn.cell.can_layer.MultiHeadCellAttention_v2.message"]], "reset_parameters() (topomodelx.nn.cell.can_layer.canlayer method)": [[6, "topomodelx.nn.cell.can_layer.CANLayer.reset_parameters"]], "reset_parameters() (topomodelx.nn.cell.can_layer.liftlayer method)": [[6, "topomodelx.nn.cell.can_layer.LiftLayer.reset_parameters"]], "reset_parameters() (topomodelx.nn.cell.can_layer.multiheadcellattention method)": [[6, "topomodelx.nn.cell.can_layer.MultiHeadCellAttention.reset_parameters"]], "reset_parameters() (topomodelx.nn.cell.can_layer.multiheadcellattention_v2 method)": [[6, "topomodelx.nn.cell.can_layer.MultiHeadCellAttention_v2.reset_parameters"]], "reset_parameters() (topomodelx.nn.cell.can_layer.multiheadliftlayer method)": [[6, "topomodelx.nn.cell.can_layer.MultiHeadLiftLayer.reset_parameters"]], "reset_parameters() (topomodelx.nn.cell.can_layer.poollayer method)": [[6, "topomodelx.nn.cell.can_layer.PoolLayer.reset_parameters"]], "softmax() (in module topomodelx.nn.cell.can_layer)": [[6, "topomodelx.nn.cell.can_layer.softmax"]], "topomodelx.nn.cell.can_layer": [[6, "module-topomodelx.nn.cell.can_layer"]], "ccxn (class in topomodelx.nn.cell.ccxn)": [[7, "topomodelx.nn.cell.ccxn.CCXN"]], "forward() (topomodelx.nn.cell.ccxn.ccxn method)": [[7, "topomodelx.nn.cell.ccxn.CCXN.forward"]], "topomodelx.nn.cell.ccxn": [[7, "module-topomodelx.nn.cell.ccxn"]], "ccxnlayer (class in topomodelx.nn.cell.ccxn_layer)": [[8, "topomodelx.nn.cell.ccxn_layer.CCXNLayer"]], "forward() (topomodelx.nn.cell.ccxn_layer.ccxnlayer method)": [[8, "topomodelx.nn.cell.ccxn_layer.CCXNLayer.forward"]], "topomodelx.nn.cell.ccxn_layer": [[8, "module-topomodelx.nn.cell.ccxn_layer"]], "cwn (class in topomodelx.nn.cell.cwn)": [[9, "topomodelx.nn.cell.cwn.CWN"]], "forward() (topomodelx.nn.cell.cwn.cwn method)": [[9, "topomodelx.nn.cell.cwn.CWN.forward"]], "topomodelx.nn.cell.cwn": [[9, "module-topomodelx.nn.cell.cwn"]], "cwnlayer (class in topomodelx.nn.cell.cwn_layer)": [[10, "topomodelx.nn.cell.cwn_layer.CWNLayer"]], "forward() (topomodelx.nn.cell.cwn_layer.cwnlayer method)": [[10, "topomodelx.nn.cell.cwn_layer.CWNLayer.forward"]], "topomodelx.nn.cell.cwn_layer": [[10, "module-topomodelx.nn.cell.cwn_layer"]], "allset (class in topomodelx.nn.hypergraph.allset)": [[12, "topomodelx.nn.hypergraph.allset.AllSet"]], "forward() (topomodelx.nn.hypergraph.allset.allset method)": [[12, "topomodelx.nn.hypergraph.allset.AllSet.forward"]], "topomodelx.nn.hypergraph.allset": [[12, "module-topomodelx.nn.hypergraph.allset"]], "allsetblock (class in topomodelx.nn.hypergraph.allset_layer)": [[13, "topomodelx.nn.hypergraph.allset_layer.AllSetBlock"]], "allsetlayer (class in topomodelx.nn.hypergraph.allset_layer)": [[13, "topomodelx.nn.hypergraph.allset_layer.AllSetLayer"]], "mlp (class in topomodelx.nn.hypergraph.allset_layer)": [[13, "topomodelx.nn.hypergraph.allset_layer.MLP"]], "forward() (topomodelx.nn.hypergraph.allset_layer.allsetblock method)": [[13, "topomodelx.nn.hypergraph.allset_layer.AllSetBlock.forward"]], "forward() (topomodelx.nn.hypergraph.allset_layer.allsetlayer method)": [[13, "topomodelx.nn.hypergraph.allset_layer.AllSetLayer.forward"]], "reset_parameters() (topomodelx.nn.hypergraph.allset_layer.allsetblock method)": [[13, "topomodelx.nn.hypergraph.allset_layer.AllSetBlock.reset_parameters"]], "reset_parameters() (topomodelx.nn.hypergraph.allset_layer.allsetlayer method)": [[13, "topomodelx.nn.hypergraph.allset_layer.AllSetLayer.reset_parameters"]], "topomodelx.nn.hypergraph.allset_layer": [[13, "module-topomodelx.nn.hypergraph.allset_layer"]], "allsettransformer (class in topomodelx.nn.hypergraph.allset_transformer)": [[14, "topomodelx.nn.hypergraph.allset_transformer.AllSetTransformer"]], "forward() (topomodelx.nn.hypergraph.allset_transformer.allsettransformer method)": [[14, "topomodelx.nn.hypergraph.allset_transformer.AllSetTransformer.forward"]], "topomodelx.nn.hypergraph.allset_transformer": [[14, "module-topomodelx.nn.hypergraph.allset_transformer"]], "allsettransformerblock (class in topomodelx.nn.hypergraph.allset_transformer_layer)": [[15, "topomodelx.nn.hypergraph.allset_transformer_layer.AllSetTransformerBlock"]], "allsettransformerlayer (class in topomodelx.nn.hypergraph.allset_transformer_layer)": [[15, "topomodelx.nn.hypergraph.allset_transformer_layer.AllSetTransformerLayer"]], "mlp (class in topomodelx.nn.hypergraph.allset_transformer_layer)": [[15, "topomodelx.nn.hypergraph.allset_transformer_layer.MLP"]], "multiheadattention (class in topomodelx.nn.hypergraph.allset_transformer_layer)": [[15, "topomodelx.nn.hypergraph.allset_transformer_layer.MultiHeadAttention"]], "attention() (topomodelx.nn.hypergraph.allset_transformer_layer.multiheadattention method)": [[15, "topomodelx.nn.hypergraph.allset_transformer_layer.MultiHeadAttention.attention"]], "forward() (topomodelx.nn.hypergraph.allset_transformer_layer.allsettransformerblock method)": [[15, "topomodelx.nn.hypergraph.allset_transformer_layer.AllSetTransformerBlock.forward"]], "forward() (topomodelx.nn.hypergraph.allset_transformer_layer.allsettransformerlayer method)": [[15, "topomodelx.nn.hypergraph.allset_transformer_layer.AllSetTransformerLayer.forward"]], "forward() (topomodelx.nn.hypergraph.allset_transformer_layer.multiheadattention method)": [[15, "topomodelx.nn.hypergraph.allset_transformer_layer.MultiHeadAttention.forward"]], "reset_parameters() (topomodelx.nn.hypergraph.allset_transformer_layer.allsettransformerblock method)": [[15, "topomodelx.nn.hypergraph.allset_transformer_layer.AllSetTransformerBlock.reset_parameters"]], "reset_parameters() (topomodelx.nn.hypergraph.allset_transformer_layer.allsettransformerlayer method)": [[15, "topomodelx.nn.hypergraph.allset_transformer_layer.AllSetTransformerLayer.reset_parameters"]], "reset_parameters() (topomodelx.nn.hypergraph.allset_transformer_layer.multiheadattention method)": [[15, "topomodelx.nn.hypergraph.allset_transformer_layer.MultiHeadAttention.reset_parameters"]], "topomodelx.nn.hypergraph.allset_transformer_layer": [[15, "module-topomodelx.nn.hypergraph.allset_transformer_layer"]], "hmpnn (class in topomodelx.nn.hypergraph.hmpnn)": [[18, "topomodelx.nn.hypergraph.hmpnn.HMPNN"]], "forward() (topomodelx.nn.hypergraph.hmpnn.hmpnn method)": [[18, "topomodelx.nn.hypergraph.hmpnn.HMPNN.forward"]], "topomodelx.nn.hypergraph.hmpnn": [[18, "module-topomodelx.nn.hypergraph.hmpnn"]], "hmpnnlayer (class in topomodelx.nn.hypergraph.hmpnn_layer)": [[19, "topomodelx.nn.hypergraph.hmpnn_layer.HMPNNLayer"]], "apply_regular_dropout() (topomodelx.nn.hypergraph.hmpnn_layer.hmpnnlayer method)": [[19, "topomodelx.nn.hypergraph.hmpnn_layer.HMPNNLayer.apply_regular_dropout"]], "forward() (topomodelx.nn.hypergraph.hmpnn_layer.hmpnnlayer method)": [[19, "topomodelx.nn.hypergraph.hmpnn_layer.HMPNNLayer.forward"]], "topomodelx.nn.hypergraph.hmpnn_layer": [[19, "module-topomodelx.nn.hypergraph.hmpnn_layer"]], "hnhn (class in topomodelx.nn.hypergraph.hnhn)": [[20, "topomodelx.nn.hypergraph.hnhn.HNHN"]], "forward() (topomodelx.nn.hypergraph.hnhn.hnhn method)": [[20, "topomodelx.nn.hypergraph.hnhn.HNHN.forward"]], "topomodelx.nn.hypergraph.hnhn": [[20, "module-topomodelx.nn.hypergraph.hnhn"]], "hnhnlayer (class in topomodelx.nn.hypergraph.hnhn_layer)": [[21, "topomodelx.nn.hypergraph.hnhn_layer.HNHNLayer"]], "compute_normalization_matrices() (topomodelx.nn.hypergraph.hnhn_layer.hnhnlayer method)": [[21, "topomodelx.nn.hypergraph.hnhn_layer.HNHNLayer.compute_normalization_matrices"]], "forward() (topomodelx.nn.hypergraph.hnhn_layer.hnhnlayer method)": [[21, "topomodelx.nn.hypergraph.hnhn_layer.HNHNLayer.forward"]], "init_biases() (topomodelx.nn.hypergraph.hnhn_layer.hnhnlayer method)": [[21, "topomodelx.nn.hypergraph.hnhn_layer.HNHNLayer.init_biases"]], "normalize_incidence_matrices() (topomodelx.nn.hypergraph.hnhn_layer.hnhnlayer method)": [[21, "topomodelx.nn.hypergraph.hnhn_layer.HNHNLayer.normalize_incidence_matrices"]], "reset_parameters() (topomodelx.nn.hypergraph.hnhn_layer.hnhnlayer method)": [[21, "topomodelx.nn.hypergraph.hnhn_layer.HNHNLayer.reset_parameters"]], "topomodelx.nn.hypergraph.hnhn_layer": [[21, "module-topomodelx.nn.hypergraph.hnhn_layer"]], "hnhnlayer (class in topomodelx.nn.hypergraph.hnhn_layer_bis)": [[22, "topomodelx.nn.hypergraph.hnhn_layer_bis.HNHNLayer"]], "forward() (topomodelx.nn.hypergraph.hnhn_layer_bis.hnhnlayer method)": [[22, "topomodelx.nn.hypergraph.hnhn_layer_bis.HNHNLayer.forward"]], "topomodelx.nn.hypergraph.hnhn_layer_bis": [[22, "module-topomodelx.nn.hypergraph.hnhn_layer_bis"]], "hypergat (class in topomodelx.nn.hypergraph.hypergat)": [[23, "topomodelx.nn.hypergraph.hypergat.HyperGAT"]], "forward() (topomodelx.nn.hypergraph.hypergat.hypergat method)": [[23, "topomodelx.nn.hypergraph.hypergat.HyperGAT.forward"]], "topomodelx.nn.hypergraph.hypergat": [[23, "module-topomodelx.nn.hypergraph.hypergat"]], "hypergatlayer (class in topomodelx.nn.hypergraph.hypergat_layer)": [[24, "topomodelx.nn.hypergraph.hypergat_layer.HyperGATLayer"]], "attention() (topomodelx.nn.hypergraph.hypergat_layer.hypergatlayer method)": [[24, "topomodelx.nn.hypergraph.hypergat_layer.HyperGATLayer.attention"]], "forward() (topomodelx.nn.hypergraph.hypergat_layer.hypergatlayer method)": [[24, "topomodelx.nn.hypergraph.hypergat_layer.HyperGATLayer.forward"]], "reset_parameters() (topomodelx.nn.hypergraph.hypergat_layer.hypergatlayer method)": [[24, "topomodelx.nn.hypergraph.hypergat_layer.HyperGATLayer.reset_parameters"]], "topomodelx.nn.hypergraph.hypergat_layer": [[24, "module-topomodelx.nn.hypergraph.hypergat_layer"]], "update() (topomodelx.nn.hypergraph.hypergat_layer.hypergatlayer method)": [[24, "topomodelx.nn.hypergraph.hypergat_layer.HyperGATLayer.update"]], "hypersage (class in topomodelx.nn.hypergraph.hypersage)": [[25, "topomodelx.nn.hypergraph.hypersage.HyperSAGE"]], "forward() (topomodelx.nn.hypergraph.hypersage.hypersage method)": [[25, "topomodelx.nn.hypergraph.hypersage.HyperSAGE.forward"]], "topomodelx.nn.hypergraph.hypersage": [[25, "module-topomodelx.nn.hypergraph.hypersage"]], "generalizedmean (class in topomodelx.nn.hypergraph.hypersage_layer)": [[26, "topomodelx.nn.hypergraph.hypersage_layer.GeneralizedMean"]], "hypersagelayer (class in topomodelx.nn.hypergraph.hypersage_layer)": [[26, "topomodelx.nn.hypergraph.hypersage_layer.HyperSAGELayer"]], "aggregate() (topomodelx.nn.hypergraph.hypersage_layer.hypersagelayer method)": [[26, "topomodelx.nn.hypergraph.hypersage_layer.HyperSAGELayer.aggregate"]], "forward() (topomodelx.nn.hypergraph.hypersage_layer.generalizedmean method)": [[26, "topomodelx.nn.hypergraph.hypersage_layer.GeneralizedMean.forward"]], "forward() (topomodelx.nn.hypergraph.hypersage_layer.hypersagelayer method)": [[26, "topomodelx.nn.hypergraph.hypersage_layer.HyperSAGELayer.forward"]], "topomodelx.nn.hypergraph.hypersage_layer": [[26, "module-topomodelx.nn.hypergraph.hypersage_layer"]], "update() (topomodelx.nn.hypergraph.hypersage_layer.hypersagelayer method)": [[26, "topomodelx.nn.hypergraph.hypersage_layer.HyperSAGELayer.update"]], "unigcn (class in topomodelx.nn.hypergraph.unigcn)": [[28, "topomodelx.nn.hypergraph.unigcn.UniGCN"]], "forward() (topomodelx.nn.hypergraph.unigcn.unigcn method)": [[28, "topomodelx.nn.hypergraph.unigcn.UniGCN.forward"]], "topomodelx.nn.hypergraph.unigcn": [[28, "module-topomodelx.nn.hypergraph.unigcn"]], "unigcnlayer (class in topomodelx.nn.hypergraph.unigcn_layer)": [[29, "topomodelx.nn.hypergraph.unigcn_layer.UniGCNLayer"]], "forward() (topomodelx.nn.hypergraph.unigcn_layer.unigcnlayer method)": [[29, "topomodelx.nn.hypergraph.unigcn_layer.UniGCNLayer.forward"]], "reset_parameters() (topomodelx.nn.hypergraph.unigcn_layer.unigcnlayer method)": [[29, "topomodelx.nn.hypergraph.unigcn_layer.UniGCNLayer.reset_parameters"]], "topomodelx.nn.hypergraph.unigcn_layer": [[29, "module-topomodelx.nn.hypergraph.unigcn_layer"]], "unigcnii (class in topomodelx.nn.hypergraph.unigcnii)": [[30, "topomodelx.nn.hypergraph.unigcnii.UniGCNII"]], "forward() (topomodelx.nn.hypergraph.unigcnii.unigcnii method)": [[30, "topomodelx.nn.hypergraph.unigcnii.UniGCNII.forward"]], "topomodelx.nn.hypergraph.unigcnii": [[30, "module-topomodelx.nn.hypergraph.unigcnii"]], "unigcniilayer (class in topomodelx.nn.hypergraph.unigcnii_layer)": [[31, "topomodelx.nn.hypergraph.unigcnii_layer.UniGCNIILayer"]], "forward() (topomodelx.nn.hypergraph.unigcnii_layer.unigcniilayer method)": [[31, "topomodelx.nn.hypergraph.unigcnii_layer.UniGCNIILayer.forward"]], "reset_parameters() (topomodelx.nn.hypergraph.unigcnii_layer.unigcniilayer method)": [[31, "topomodelx.nn.hypergraph.unigcnii_layer.UniGCNIILayer.reset_parameters"]], "topomodelx.nn.hypergraph.unigcnii_layer": [[31, "module-topomodelx.nn.hypergraph.unigcnii_layer"]], "unigin (class in topomodelx.nn.hypergraph.unigin)": [[32, "topomodelx.nn.hypergraph.unigin.UniGIN"]], "forward() (topomodelx.nn.hypergraph.unigin.unigin method)": [[32, "topomodelx.nn.hypergraph.unigin.UniGIN.forward"]], "topomodelx.nn.hypergraph.unigin": [[32, "module-topomodelx.nn.hypergraph.unigin"]], "uniginlayer (class in topomodelx.nn.hypergraph.unigin_layer)": [[33, "topomodelx.nn.hypergraph.unigin_layer.UniGINLayer"]], "forward() (topomodelx.nn.hypergraph.unigin_layer.uniginlayer method)": [[33, "topomodelx.nn.hypergraph.unigin_layer.UniGINLayer.forward"]], "topomodelx.nn.hypergraph.unigin_layer": [[33, "module-topomodelx.nn.hypergraph.unigin_layer"]], "unisage (class in topomodelx.nn.hypergraph.unisage)": [[34, "topomodelx.nn.hypergraph.unisage.UniSAGE"]], "forward() (topomodelx.nn.hypergraph.unisage.unisage method)": [[34, "topomodelx.nn.hypergraph.unisage.UniSAGE.forward"]], "topomodelx.nn.hypergraph.unisage": [[34, "module-topomodelx.nn.hypergraph.unisage"]], "unisagelayer (class in topomodelx.nn.hypergraph.unisage_layer)": [[35, "topomodelx.nn.hypergraph.unisage_layer.UniSAGELayer"]], "forward() (topomodelx.nn.hypergraph.unisage_layer.unisagelayer method)": [[35, "topomodelx.nn.hypergraph.unisage_layer.UniSAGELayer.forward"]], "reset_parameters() (topomodelx.nn.hypergraph.unisage_layer.unisagelayer method)": [[35, "topomodelx.nn.hypergraph.unisage_layer.UniSAGELayer.reset_parameters"]], "topomodelx.nn.hypergraph.unisage_layer": [[35, "module-topomodelx.nn.hypergraph.unisage_layer"]], "dist2cycle (class in topomodelx.nn.simplicial.dist2cycle)": [[37, "topomodelx.nn.simplicial.dist2cycle.Dist2Cycle"]], "forward() (topomodelx.nn.simplicial.dist2cycle.dist2cycle method)": [[37, "topomodelx.nn.simplicial.dist2cycle.Dist2Cycle.forward"]], "topomodelx.nn.simplicial.dist2cycle": [[37, "module-topomodelx.nn.simplicial.dist2cycle"]], "dist2cyclelayer (class in topomodelx.nn.simplicial.dist2cycle_layer)": [[38, "topomodelx.nn.simplicial.dist2cycle_layer.Dist2CycleLayer"]], "forward() (topomodelx.nn.simplicial.dist2cycle_layer.dist2cyclelayer method)": [[38, "topomodelx.nn.simplicial.dist2cycle_layer.Dist2CycleLayer.forward"]], "reset_parameters() (topomodelx.nn.simplicial.dist2cycle_layer.dist2cyclelayer method)": [[38, "topomodelx.nn.simplicial.dist2cycle_layer.Dist2CycleLayer.reset_parameters"]], "topomodelx.nn.simplicial.dist2cycle_layer": [[38, "module-topomodelx.nn.simplicial.dist2cycle_layer"]], "hsn (class in topomodelx.nn.simplicial.hsn)": [[39, "topomodelx.nn.simplicial.hsn.HSN"]], "forward() (topomodelx.nn.simplicial.hsn.hsn method)": [[39, "topomodelx.nn.simplicial.hsn.HSN.forward"]], "topomodelx.nn.simplicial.hsn": [[39, "module-topomodelx.nn.simplicial.hsn"]], "hsnlayer (class in topomodelx.nn.simplicial.hsn_layer)": [[40, "topomodelx.nn.simplicial.hsn_layer.HSNLayer"]], "forward() (topomodelx.nn.simplicial.hsn_layer.hsnlayer method)": [[40, "topomodelx.nn.simplicial.hsn_layer.HSNLayer.forward"]], "reset_parameters() (topomodelx.nn.simplicial.hsn_layer.hsnlayer method)": [[40, "topomodelx.nn.simplicial.hsn_layer.HSNLayer.reset_parameters"]], "topomodelx.nn.simplicial.hsn_layer": [[40, "module-topomodelx.nn.simplicial.hsn_layer"]], "san (class in topomodelx.nn.simplicial.san)": [[42, "topomodelx.nn.simplicial.san.SAN"]], "compute_projection_matrix() (topomodelx.nn.simplicial.san.san method)": [[42, "topomodelx.nn.simplicial.san.SAN.compute_projection_matrix"]], "forward() (topomodelx.nn.simplicial.san.san method)": [[42, "topomodelx.nn.simplicial.san.SAN.forward"]], "topomodelx.nn.simplicial.san": [[42, "module-topomodelx.nn.simplicial.san"]], "sanconv (class in topomodelx.nn.simplicial.san_layer)": [[43, "topomodelx.nn.simplicial.san_layer.SANConv"]], "sanlayer (class in topomodelx.nn.simplicial.san_layer)": [[43, "topomodelx.nn.simplicial.san_layer.SANLayer"]], "forward() (topomodelx.nn.simplicial.san_layer.sanconv method)": [[43, "topomodelx.nn.simplicial.san_layer.SANConv.forward"]], "forward() (topomodelx.nn.simplicial.san_layer.sanlayer method)": [[43, "topomodelx.nn.simplicial.san_layer.SANLayer.forward"]], "reset_parameters() (topomodelx.nn.simplicial.san_layer.sanlayer method)": [[43, "topomodelx.nn.simplicial.san_layer.SANLayer.reset_parameters"]], "topomodelx.nn.simplicial.san_layer": [[43, "module-topomodelx.nn.simplicial.san_layer"]], "scacmps (class in topomodelx.nn.simplicial.sca_cmps)": [[44, "topomodelx.nn.simplicial.sca_cmps.SCACMPS"]], "forward() (topomodelx.nn.simplicial.sca_cmps.scacmps method)": [[44, "topomodelx.nn.simplicial.sca_cmps.SCACMPS.forward"]], "topomodelx.nn.simplicial.sca_cmps": [[44, "module-topomodelx.nn.simplicial.sca_cmps"]], "scacmpslayer (class in topomodelx.nn.simplicial.sca_cmps_layer)": [[45, "topomodelx.nn.simplicial.sca_cmps_layer.SCACMPSLayer"]], "forward() (topomodelx.nn.simplicial.sca_cmps_layer.scacmpslayer method)": [[45, "topomodelx.nn.simplicial.sca_cmps_layer.SCACMPSLayer.forward"]], "intra_aggr() (topomodelx.nn.simplicial.sca_cmps_layer.scacmpslayer method)": [[45, "topomodelx.nn.simplicial.sca_cmps_layer.SCACMPSLayer.intra_aggr"]], "reset_parameters() (topomodelx.nn.simplicial.sca_cmps_layer.scacmpslayer method)": [[45, "topomodelx.nn.simplicial.sca_cmps_layer.SCACMPSLayer.reset_parameters"]], "topomodelx.nn.simplicial.sca_cmps_layer": [[45, "module-topomodelx.nn.simplicial.sca_cmps_layer"]], "weight_func() (topomodelx.nn.simplicial.sca_cmps_layer.scacmpslayer method)": [[45, "topomodelx.nn.simplicial.sca_cmps_layer.SCACMPSLayer.weight_func"]], "sccn (class in topomodelx.nn.simplicial.sccn)": [[46, "topomodelx.nn.simplicial.sccn.SCCN"]], "forward() (topomodelx.nn.simplicial.sccn.sccn method)": [[46, "topomodelx.nn.simplicial.sccn.SCCN.forward"]], "topomodelx.nn.simplicial.sccn": [[46, "module-topomodelx.nn.simplicial.sccn"]], "sccnlayer (class in topomodelx.nn.simplicial.sccn_layer)": [[47, "topomodelx.nn.simplicial.sccn_layer.SCCNLayer"]], "forward() (topomodelx.nn.simplicial.sccn_layer.sccnlayer method)": [[47, "topomodelx.nn.simplicial.sccn_layer.SCCNLayer.forward"]], "reset_parameters() (topomodelx.nn.simplicial.sccn_layer.sccnlayer method)": [[47, "topomodelx.nn.simplicial.sccn_layer.SCCNLayer.reset_parameters"]], "topomodelx.nn.simplicial.sccn_layer": [[47, "module-topomodelx.nn.simplicial.sccn_layer"]], "sccnn (class in topomodelx.nn.simplicial.sccnn)": [[48, "topomodelx.nn.simplicial.sccnn.SCCNN"]], "sccnncomplex (class in topomodelx.nn.simplicial.sccnn)": [[48, "topomodelx.nn.simplicial.sccnn.SCCNNComplex"]], "forward() (topomodelx.nn.simplicial.sccnn.sccnn method)": [[48, "topomodelx.nn.simplicial.sccnn.SCCNN.forward"]], "forward() (topomodelx.nn.simplicial.sccnn.sccnncomplex method)": [[48, "topomodelx.nn.simplicial.sccnn.SCCNNComplex.forward"]], "topomodelx.nn.simplicial.sccnn": [[48, "module-topomodelx.nn.simplicial.sccnn"]], "sccnnlayer (class in topomodelx.nn.simplicial.sccnn_layer)": [[49, "topomodelx.nn.simplicial.sccnn_layer.SCCNNLayer"]], "aggr_norm_func() (topomodelx.nn.simplicial.sccnn_layer.sccnnlayer method)": [[49, "topomodelx.nn.simplicial.sccnn_layer.SCCNNLayer.aggr_norm_func"]], "chebyshev_conv() (topomodelx.nn.simplicial.sccnn_layer.sccnnlayer method)": [[49, "topomodelx.nn.simplicial.sccnn_layer.SCCNNLayer.chebyshev_conv"]], "forward() (topomodelx.nn.simplicial.sccnn_layer.sccnnlayer method)": [[49, "topomodelx.nn.simplicial.sccnn_layer.SCCNNLayer.forward"]], "reset_parameters() (topomodelx.nn.simplicial.sccnn_layer.sccnnlayer method)": [[49, "topomodelx.nn.simplicial.sccnn_layer.SCCNNLayer.reset_parameters"]], "topomodelx.nn.simplicial.sccnn_layer": [[49, "module-topomodelx.nn.simplicial.sccnn_layer"]], "update() (topomodelx.nn.simplicial.sccnn_layer.sccnnlayer method)": [[49, "topomodelx.nn.simplicial.sccnn_layer.SCCNNLayer.update"]], "scconv (class in topomodelx.nn.simplicial.scconv)": [[50, "topomodelx.nn.simplicial.scconv.SCConv"]], "forward() (topomodelx.nn.simplicial.scconv.scconv method)": [[50, "topomodelx.nn.simplicial.scconv.SCConv.forward"]], "topomodelx.nn.simplicial.scconv": [[50, "module-topomodelx.nn.simplicial.scconv"]], "scconvlayer (class in topomodelx.nn.simplicial.scconv_layer)": [[51, "topomodelx.nn.simplicial.scconv_layer.SCConvLayer"]], "forward() (topomodelx.nn.simplicial.scconv_layer.scconvlayer method)": [[51, "topomodelx.nn.simplicial.scconv_layer.SCConvLayer.forward"]], "reset_parameters() (topomodelx.nn.simplicial.scconv_layer.scconvlayer method)": [[51, "topomodelx.nn.simplicial.scconv_layer.SCConvLayer.reset_parameters"]], "topomodelx.nn.simplicial.scconv_layer": [[51, "module-topomodelx.nn.simplicial.scconv_layer"]], "scn2 (class in topomodelx.nn.simplicial.scn2)": [[52, "topomodelx.nn.simplicial.scn2.SCN2"]], "forward() (topomodelx.nn.simplicial.scn2.scn2 method)": [[52, "topomodelx.nn.simplicial.scn2.SCN2.forward"]], "topomodelx.nn.simplicial.scn2": [[52, "module-topomodelx.nn.simplicial.scn2"]], "scn2layer (class in topomodelx.nn.simplicial.scn2_layer)": [[53, "topomodelx.nn.simplicial.scn2_layer.SCN2Layer"]], "forward() (topomodelx.nn.simplicial.scn2_layer.scn2layer method)": [[53, "topomodelx.nn.simplicial.scn2_layer.SCN2Layer.forward"]], "reset_parameters() (topomodelx.nn.simplicial.scn2_layer.scn2layer method)": [[53, "topomodelx.nn.simplicial.scn2_layer.SCN2Layer.reset_parameters"]], "topomodelx.nn.simplicial.scn2_layer": [[53, "module-topomodelx.nn.simplicial.scn2_layer"]], "scnn (class in topomodelx.nn.simplicial.scnn)": [[54, "topomodelx.nn.simplicial.scnn.SCNN"]], "forward() (topomodelx.nn.simplicial.scnn.scnn method)": [[54, "topomodelx.nn.simplicial.scnn.SCNN.forward"]], "topomodelx.nn.simplicial.scnn": [[54, "module-topomodelx.nn.simplicial.scnn"]], "scnnlayer (class in topomodelx.nn.simplicial.scnn_layer)": [[55, "topomodelx.nn.simplicial.scnn_layer.SCNNLayer"]], "aggr_norm_func() (topomodelx.nn.simplicial.scnn_layer.scnnlayer method)": [[55, "topomodelx.nn.simplicial.scnn_layer.SCNNLayer.aggr_norm_func"]], "chebyshev_conv() (topomodelx.nn.simplicial.scnn_layer.scnnlayer method)": [[55, "topomodelx.nn.simplicial.scnn_layer.SCNNLayer.chebyshev_conv"]], "forward() (topomodelx.nn.simplicial.scnn_layer.scnnlayer method)": [[55, "topomodelx.nn.simplicial.scnn_layer.SCNNLayer.forward"]], "reset_parameters() (topomodelx.nn.simplicial.scnn_layer.scnnlayer method)": [[55, "topomodelx.nn.simplicial.scnn_layer.SCNNLayer.reset_parameters"]], "topomodelx.nn.simplicial.scnn_layer": [[55, "module-topomodelx.nn.simplicial.scnn_layer"]], "update() (topomodelx.nn.simplicial.scnn_layer.scnnlayer method)": [[55, "topomodelx.nn.simplicial.scnn_layer.SCNNLayer.update"]], "scone (class in topomodelx.nn.simplicial.scone)": [[56, "topomodelx.nn.simplicial.scone.SCoNe"]], "trajectoriesdataset (class in topomodelx.nn.simplicial.scone)": [[56, "topomodelx.nn.simplicial.scone.TrajectoriesDataset"]], "forward() (topomodelx.nn.simplicial.scone.scone method)": [[56, "topomodelx.nn.simplicial.scone.SCoNe.forward"]], "generate_complex() (in module topomodelx.nn.simplicial.scone)": [[56, "topomodelx.nn.simplicial.scone.generate_complex"]], "generate_trajectories() (in module topomodelx.nn.simplicial.scone)": [[56, "topomodelx.nn.simplicial.scone.generate_trajectories"]], "topomodelx.nn.simplicial.scone": [[56, "module-topomodelx.nn.simplicial.scone"]], "vectorize_path() (topomodelx.nn.simplicial.scone.trajectoriesdataset method)": [[56, "topomodelx.nn.simplicial.scone.TrajectoriesDataset.vectorize_path"]], "sconelayer (class in topomodelx.nn.simplicial.scone_layer)": [[57, "topomodelx.nn.simplicial.scone_layer.SCoNeLayer"]], "forward() (topomodelx.nn.simplicial.scone_layer.sconelayer method)": [[57, "topomodelx.nn.simplicial.scone_layer.SCoNeLayer.forward"]], "reset_parameters() (topomodelx.nn.simplicial.scone_layer.sconelayer method)": [[57, "topomodelx.nn.simplicial.scone_layer.SCoNeLayer.reset_parameters"]], "topomodelx.nn.simplicial.scone_layer": [[57, "module-topomodelx.nn.simplicial.scone_layer"]], "broadcast() (in module topomodelx.utils.scatter)": [[58, "topomodelx.utils.scatter.broadcast"]], "scatter() (in module topomodelx.utils.scatter)": [[58, "topomodelx.utils.scatter.scatter"]], "scatter_add() (in module topomodelx.utils.scatter)": [[58, "topomodelx.utils.scatter.scatter_add"]], "scatter_mean() (in module topomodelx.utils.scatter)": [[58, "topomodelx.utils.scatter.scatter_mean"]], "scatter_sum() (in module topomodelx.utils.scatter)": [[58, "topomodelx.utils.scatter.scatter_sum"]], "topomodelx.utils.scatter": [[58, "module-topomodelx.utils.scatter"]]}})