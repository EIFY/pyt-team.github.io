Search.setIndex({"docnames": ["api/base/aggregation", "api/base/conv", "api/base/index", "api/base/message_passing", "api/index", "api/nn/cell/can", "api/nn/cell/can_layer", "api/nn/cell/ccxn", "api/nn/cell/ccxn_layer", "api/nn/cell/cwn", "api/nn/cell/cwn_layer", "api/nn/cell/index", "api/nn/hypergraph/allset", "api/nn/hypergraph/allset_layer", "api/nn/hypergraph/allset_transformer", "api/nn/hypergraph/allset_transformer_layer", "api/nn/hypergraph/dhgcn", "api/nn/hypergraph/dhgcn_layer", "api/nn/hypergraph/hmpnn", "api/nn/hypergraph/hmpnn_layer", "api/nn/hypergraph/hnhn", "api/nn/hypergraph/hnhn_layer", "api/nn/hypergraph/hnhn_layer_bis", "api/nn/hypergraph/hypergat", "api/nn/hypergraph/hypergat_layer", "api/nn/hypergraph/hypersage", "api/nn/hypergraph/hypersage_layer", "api/nn/hypergraph/index", "api/nn/hypergraph/unigcn", "api/nn/hypergraph/unigcn_layer", "api/nn/hypergraph/unigcnii", "api/nn/hypergraph/unigcnii_layer", "api/nn/hypergraph/unigin", "api/nn/hypergraph/unigin_layer", "api/nn/hypergraph/unisage", "api/nn/hypergraph/unisage_layer", "api/nn/index", "api/nn/simplicial/dist2cycle", "api/nn/simplicial/dist2cycle_layer", "api/nn/simplicial/hsn", "api/nn/simplicial/hsn_layer", "api/nn/simplicial/index", "api/nn/simplicial/san", "api/nn/simplicial/san_layer", "api/nn/simplicial/sca_cmps", "api/nn/simplicial/sca_cmps_layer", "api/nn/simplicial/sccn", "api/nn/simplicial/sccn_layer", "api/nn/simplicial/sccnn", "api/nn/simplicial/sccnn_layer", "api/nn/simplicial/scconv", "api/nn/simplicial/scconv_layer", "api/nn/simplicial/scn2", "api/nn/simplicial/scn2_layer", "api/nn/simplicial/scnn", "api/nn/simplicial/scnn_layer", "api/nn/simplicial/scone", "api/nn/simplicial/scone_layer", "api/utils/index", "challenge/index", "contributing/index", "index", "notebooks/cell/can_train", "notebooks/cell/ccxn_train", "notebooks/cell/cwn_train", "notebooks/combinatorial/hmc_train", "notebooks/hypergraph/allset_train", "notebooks/hypergraph/allset_transformer_train", "notebooks/hypergraph/dhgcn_train", "notebooks/hypergraph/hmpnn_train", "notebooks/hypergraph/hnhn_train", "notebooks/hypergraph/hnhn_train_bis", "notebooks/hypergraph/hypergat_train", "notebooks/hypergraph/hypersage_train", "notebooks/hypergraph/unigcn_train", "notebooks/hypergraph/unigcnii_train", "notebooks/hypergraph/unigin_train", "notebooks/hypergraph/unisage_train", "notebooks/simplicial/dist2cycle_train", "notebooks/simplicial/hsn_train", "notebooks/simplicial/san_train", "notebooks/simplicial/sca_cmps_train", "notebooks/simplicial/sccn_train", "notebooks/simplicial/sccnn_train", "notebooks/simplicial/scconv_train", "notebooks/simplicial/scn2_train", "notebooks/simplicial/scnn_train", "notebooks/simplicial/scone_train", "tutorials/index"], "filenames": ["api/base/aggregation.rst", "api/base/conv.rst", "api/base/index.rst", "api/base/message_passing.rst", "api/index.rst", "api/nn/cell/can.rst", "api/nn/cell/can_layer.rst", "api/nn/cell/ccxn.rst", "api/nn/cell/ccxn_layer.rst", "api/nn/cell/cwn.rst", "api/nn/cell/cwn_layer.rst", "api/nn/cell/index.rst", "api/nn/hypergraph/allset.rst", "api/nn/hypergraph/allset_layer.rst", "api/nn/hypergraph/allset_transformer.rst", "api/nn/hypergraph/allset_transformer_layer.rst", "api/nn/hypergraph/dhgcn.rst", "api/nn/hypergraph/dhgcn_layer.rst", "api/nn/hypergraph/hmpnn.rst", "api/nn/hypergraph/hmpnn_layer.rst", "api/nn/hypergraph/hnhn.rst", "api/nn/hypergraph/hnhn_layer.rst", "api/nn/hypergraph/hnhn_layer_bis.rst", "api/nn/hypergraph/hypergat.rst", "api/nn/hypergraph/hypergat_layer.rst", "api/nn/hypergraph/hypersage.rst", "api/nn/hypergraph/hypersage_layer.rst", "api/nn/hypergraph/index.rst", "api/nn/hypergraph/unigcn.rst", "api/nn/hypergraph/unigcn_layer.rst", "api/nn/hypergraph/unigcnii.rst", "api/nn/hypergraph/unigcnii_layer.rst", "api/nn/hypergraph/unigin.rst", "api/nn/hypergraph/unigin_layer.rst", "api/nn/hypergraph/unisage.rst", "api/nn/hypergraph/unisage_layer.rst", "api/nn/index.rst", "api/nn/simplicial/dist2cycle.rst", "api/nn/simplicial/dist2cycle_layer.rst", "api/nn/simplicial/hsn.rst", "api/nn/simplicial/hsn_layer.rst", "api/nn/simplicial/index.rst", "api/nn/simplicial/san.rst", "api/nn/simplicial/san_layer.rst", "api/nn/simplicial/sca_cmps.rst", "api/nn/simplicial/sca_cmps_layer.rst", "api/nn/simplicial/sccn.rst", "api/nn/simplicial/sccn_layer.rst", "api/nn/simplicial/sccnn.rst", "api/nn/simplicial/sccnn_layer.rst", "api/nn/simplicial/scconv.rst", "api/nn/simplicial/scconv_layer.rst", "api/nn/simplicial/scn2.rst", "api/nn/simplicial/scn2_layer.rst", "api/nn/simplicial/scnn.rst", "api/nn/simplicial/scnn_layer.rst", "api/nn/simplicial/scone.rst", "api/nn/simplicial/scone_layer.rst", "api/utils/index.rst", "challenge/index.rst", "contributing/index.rst", "index.rst", "notebooks/cell/can_train.ipynb", "notebooks/cell/ccxn_train.ipynb", "notebooks/cell/cwn_train.ipynb", "notebooks/combinatorial/hmc_train.ipynb", "notebooks/hypergraph/allset_train.ipynb", "notebooks/hypergraph/allset_transformer_train.ipynb", "notebooks/hypergraph/dhgcn_train.ipynb", "notebooks/hypergraph/hmpnn_train.ipynb", "notebooks/hypergraph/hnhn_train.ipynb", "notebooks/hypergraph/hnhn_train_bis.ipynb", "notebooks/hypergraph/hypergat_train.ipynb", "notebooks/hypergraph/hypersage_train.ipynb", "notebooks/hypergraph/unigcn_train.ipynb", "notebooks/hypergraph/unigcnii_train.ipynb", "notebooks/hypergraph/unigin_train.ipynb", "notebooks/hypergraph/unisage_train.ipynb", "notebooks/simplicial/dist2cycle_train.ipynb", "notebooks/simplicial/hsn_train.ipynb", "notebooks/simplicial/san_train.ipynb", "notebooks/simplicial/sca_cmps_train.ipynb", "notebooks/simplicial/sccn_train.ipynb", "notebooks/simplicial/sccnn_train.ipynb", "notebooks/simplicial/scconv_train.ipynb", "notebooks/simplicial/scn2_train.ipynb", "notebooks/simplicial/scnn_train.ipynb", "notebooks/simplicial/scone_train.ipynb", "tutorials/index.rst"], "titles": ["Aggregation", "Conv", "Base", "Message Passing", "API Reference", "CAN", "Can_Layer", "CCXN", "CCXN_Layer", "CWN", "Cwn_Layer", "Cell", "AllSet", "AllSet_Layer", "AllSet_Transformer", "AllSet_Transformer_Layer", "DHGCN", "DHGCN_Layer", "HMPNN", "HMPNN_Layer", "HNHN", "HNHN_Layer", "HNHN_Layer_Bis", "Hypergat", "Hypergat_Layer", "Hypersage", "Hypersage_Layer", "Hypergraph", "Unigcn", "Unigcn_Layer", "Unigcnii", "Unigcnii_Layer", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "Neural Networks", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "Simplicial", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "Utils", "ICML 2023 Topological Deep Learning Challenge", "Contributing", "\ud83c\udf10 TopoModelX (TMX) \ud83c\udf69", "Train a Cell Attention Network (CAN)", "Train a Convolutional Cell Complex Network (CCXN)", "Train a CW Network (CWN)", "Train a Combinatorial Complex Attention Neural Network for Mesh Classification.", "Train an All-Set TNN", "Train an All-Set-Transformer TNN", "Train a DHGCN TNN", "Train a Hypergraph Message Passing Neural Network (HMPNN)", "Train a Hypergraph Networks with Hyperedge Neurons (HNHN)", "Train a Hypergraph Network with Hyperedge Neurons (HNHN)", "Train a Hypergraph Neural Network", "Train a Hypersage TNN", "Train a UNIGCN TNN", "Train a hypergraph neural network using UniGCNII layers", "Train a UNIGIN TNN", "Train a Uni-sage TNN", "Train a Simplicial Neural Network for Homology Localization (Dist2Cycle)", "Train a Simplicial High-Skip Network (HSN)", "Train a Simplicial Attention Network (SAN)", "Train a Simplicial Complex Autoencoder (SCA) with Coadjacency Message Passing Scheme (CMPS)", "Train a Simplicial Complex Convolutional Network (SCCN)", "Train a SCCNN", "Train a Simplicial 2-complex convolutional neural network (SCConv)", "Train a Simplex Convolutional Network (SCN) of Rank 2", "Train a Simplicial Convolutional Neural Network (SCNN)", "Train a Simplicial Complex Net (SCoNe)", "Tutorials"], "terms": {"modul": [0, 3, 5, 6, 10, 12, 13, 14, 15, 19, 33, 36, 59, 60, 65, 66, 67, 71, 83, 86], "class": [0, 1, 2, 3, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 62, 63, 64, 65, 66, 68, 70, 71, 74, 75, 76, 77, 78, 79, 80, 81, 82, 87], "topomodelx": [0, 1, 3, 4, 5, 6, 7, 8, 9, 10, 12, 13, 14, 15, 18, 19, 20, 21, 22, 23, 24, 25, 26, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87], "base": [0, 1, 3, 4, 10, 11, 15, 27, 41, 45, 59, 65, 66, 67, 83, 84, 86, 87], "aggr_func": [0, 3, 6, 19, 47], "liter": [0, 1, 3, 6, 15, 19, 21, 24, 26, 35, 43, 47, 57], "mean": [0, 3, 6, 19, 26, 35, 47, 49, 54, 58, 60, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 75, 78, 79, 80, 82, 83, 84, 85, 86, 87], "sum": [0, 3, 6, 15, 19, 29, 31, 33, 35, 47, 49, 62, 65, 70, 72, 80, 82, 84, 87], "update_func": [0, 1, 6, 15, 24, 26, 46, 47, 48, 49, 50, 54, 55, 57, 82, 87], "relu": [0, 1, 6, 13, 15, 22, 24, 26, 47, 57, 65, 72, 87], "sigmoid": [0, 1, 6, 19, 26, 46, 47, 57, 62, 82, 87], "tanh": [0, 6, 47, 57], "none": [0, 1, 3, 6, 8, 10, 12, 13, 15, 19, 21, 24, 26, 29, 31, 35, 38, 40, 43, 45, 47, 48, 49, 51, 53, 54, 55, 57, 58, 60, 65, 86, 87], "sourc": [0, 1, 3, 5, 6, 7, 8, 9, 10, 12, 13, 14, 15, 18, 19, 20, 21, 22, 23, 24, 25, 26, 28, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 40, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59], "messag": [0, 1, 2, 4, 6, 8, 10, 15, 18, 19, 20, 21, 23, 24, 25, 26, 28, 29, 30, 31, 32, 33, 34, 35, 37, 39, 42, 43, 45, 46, 47, 50, 52, 53, 59, 62, 63, 64, 65, 66, 67, 68, 70, 72, 73, 80, 82], "pass": [0, 1, 2, 4, 5, 6, 8, 10, 15, 18, 19, 20, 21, 23, 24, 25, 26, 28, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 40, 42, 43, 45, 46, 47, 50, 51, 52, 53, 54, 56, 57, 59, 62, 63, 65, 66, 67, 68, 72, 73, 80, 82, 86, 87], "layer": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 12, 13, 14, 15, 18, 19, 20, 21, 22, 23, 24, 25, 26, 28, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 40, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 57, 59, 62, 63, 64, 65, 66, 67, 68, 70, 71, 72, 73, 74, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87], "paramet": [0, 1, 3, 5, 6, 7, 8, 9, 10, 12, 13, 14, 15, 18, 19, 20, 21, 22, 23, 24, 25, 26, 28, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 40, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 57, 60, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87], "default": [0, 1, 3, 5, 6, 8, 10, 12, 13, 14, 15, 18, 19, 21, 23, 24, 25, 26, 28, 30, 32, 34, 35, 42, 43, 47, 54, 60, 71], "method": [0, 1, 3, 6, 15, 21, 24, 26, 43, 49, 55, 59, 60, 65, 69, 71], "inter": [0, 26, 73, 83], "neighborhood": [0, 1, 2, 3, 6, 8, 10, 15, 29, 43, 59, 62, 63, 64, 65, 66, 67, 73, 80, 81, 87], "updat": [0, 1, 2, 3, 6, 10, 15, 24, 26, 29, 31, 33, 35, 41, 43, 49, 55, 57, 62, 65, 66, 67, 80, 87], "appli": [0, 1, 3, 5, 6, 10, 12, 14, 15, 18, 19, 24, 26, 47, 57, 67, 72, 80, 84, 87], "merg": 0, "forward": [0, 1, 2, 3, 5, 6, 7, 8, 9, 10, 12, 13, 14, 15, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 65, 71, 87], "x": [0, 1, 3, 6, 8, 10, 12, 13, 14, 15, 19, 21, 24, 25, 26, 29, 31, 33, 35, 38, 40, 42, 43, 45, 47, 49, 51, 53, 54, 55, 56, 57, 60, 62, 63, 64, 65, 66, 67, 69, 70, 71, 72, 73, 74, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87], "list": [0, 12, 13, 14, 15, 44, 45, 56, 60, 65, 68, 75, 87], "A": [0, 3, 8, 10, 12, 13, 14, 15, 18, 21, 26, 33, 38, 51, 59, 60, 61, 62, 63, 64, 65, 70, 78, 79, 80, 81, 82, 83, 84, 86, 87], "each": [0, 1, 3, 6, 24, 26, 40, 44, 45, 46, 47, 48, 49, 55, 57, 59, 60, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 82, 83, 87], "ha": [0, 1, 6, 60, 63, 64, 66, 67, 68, 70, 72, 73, 75, 78, 79, 80, 81, 82, 83, 84, 85, 86], "shape": [0, 1, 3, 5, 6, 7, 8, 9, 10, 13, 15, 18, 19, 20, 21, 22, 23, 24, 25, 26, 28, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 40, 42, 43, 44, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 57, 58, 60, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86], "n_skeleton_in": 0, "channel": [0, 5, 6, 8, 13, 15, 31, 37, 38, 39, 40, 43, 46, 47, 48, 50, 51, 52, 53, 54, 55, 65, 75, 78, 79, 80, 82], "len": [0, 65, 70, 71, 74, 75, 76, 77, 78, 79, 80, 82, 83, 84, 86, 87], "n_messages_to_merg": 0, "input": [0, 1, 3, 5, 6, 7, 8, 9, 10, 12, 13, 14, 15, 18, 19, 21, 22, 24, 26, 29, 30, 31, 33, 35, 38, 40, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 57, 60, 62, 63, 64, 65, 66, 67, 68, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 82, 83, 86], "step": [0, 1, 3, 6, 8, 10, 21, 24, 26, 29, 33, 35, 49, 55, 59, 60, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87], "4": [0, 1, 10, 14, 15, 24, 26, 49, 55, 60, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 82, 83, 84, 85, 86, 87], "h": [0, 3, 24, 31, 40, 47, 51, 62, 67, 72, 73, 75, 79, 80, 82, 83, 84, 86], "arrai": [0, 60, 63, 64, 66, 67, 68, 70, 72, 73, 75, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87], "like": [0, 49, 55, 57, 60, 62, 80, 82, 83, 86], "n_skeleton_out": 0, "out_channel": [0, 1, 3, 5, 6, 10, 12, 14, 15, 23, 24, 25, 26, 29, 32, 33, 35, 42, 43, 49, 54, 55, 57, 66, 67, 72, 73, 76, 80, 86], "featur": [0, 1, 3, 5, 6, 7, 8, 9, 10, 12, 13, 14, 15, 18, 19, 20, 21, 22, 23, 24, 25, 26, 28, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 40, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 57, 59, 60, 62, 63, 64, 65, 66, 67, 68, 70, 71, 72, 73, 75, 79, 80, 81, 82, 83, 84, 85, 86, 87], "skeleton": [0, 45, 62, 87], "out": [0, 6, 58, 59, 60, 61, 62, 68, 71, 80, 81, 82, 87], "return": [0, 1, 3, 5, 6, 7, 8, 9, 10, 12, 13, 14, 15, 18, 19, 20, 21, 22, 23, 24, 25, 26, 28, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 40, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 57, 58, 60, 65, 71, 82, 83, 84, 86, 87], "convolut": [1, 6, 8, 9, 10, 21, 42, 43, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 59, 80], "in_channel": [1, 3, 6, 12, 13, 14, 15, 23, 24, 25, 26, 29, 30, 31, 33, 35, 42, 43, 49, 54, 55, 57, 65, 66, 67, 72, 73, 78, 79, 80, 82, 83, 86], "aggr_norm": [1, 15, 29, 48, 49, 54, 55], "bool": [1, 3, 5, 6, 7, 8, 12, 13, 14, 15, 21, 29, 33, 35, 44, 45, 49, 54, 55, 60], "fals": [1, 3, 6, 7, 8, 12, 13, 14, 15, 29, 33, 35, 44, 45, 48, 49, 54, 55, 62, 63, 64, 66, 67, 68, 70, 72, 73, 74, 76, 77, 81, 83, 84, 85, 86], "att": [1, 3, 7, 8, 24, 43, 44, 45, 49, 63, 65, 72, 81], "initi": [1, 3, 6, 8, 10, 15, 21, 24, 26, 29, 33, 35, 40, 43, 45, 47, 49, 55, 57, 62, 63, 66, 67, 69, 70, 71, 73, 80, 83, 84], "xavier_uniform": [1, 3, 6, 15, 21, 24, 26, 43, 55, 73], "xavier_norm": [1, 3, 6, 15, 21, 24, 26, 43, 49], "initialization_gain": [1, 3, 15, 24], "float": [1, 3, 5, 6, 12, 13, 14, 15, 19, 21, 22, 24, 30, 31, 33, 42, 49, 55, 57, 62, 63, 64, 65, 66, 67, 68, 70, 71, 72, 73, 74, 75, 77, 78, 79, 80, 82, 83, 84, 85, 86, 87], "1": [1, 3, 5, 6, 7, 8, 9, 10, 12, 13, 14, 15, 18, 19, 20, 21, 22, 23, 24, 25, 26, 28, 29, 30, 31, 32, 33, 34, 35, 38, 39, 40, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 53, 54, 55, 57, 58, 59, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 84, 85, 87], "414": [1, 3, 15, 21, 24, 49, 55, 70, 74], "with_linear_transform": 1, "true": [1, 5, 6, 21, 62, 63, 64, 65, 66, 67, 68, 70, 72, 73, 74, 76, 77, 78, 79, 80, 82, 83, 84, 85, 86, 87], "2": [1, 3, 5, 6, 7, 8, 9, 10, 12, 13, 14, 18, 20, 21, 23, 25, 26, 28, 29, 30, 32, 33, 34, 35, 37, 39, 40, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 59, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 87], "3": [1, 5, 6, 8, 10, 21, 26, 29, 33, 35, 40, 45, 47, 51, 53, 55, 57, 60, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87], "build": [1, 2, 22, 56, 59, 60, 87], "rout": 1, "given": [1, 3, 8, 10, 19, 21, 22, 29, 33, 35, 40, 45, 47, 57, 59, 62, 63, 64, 65, 66, 67, 70, 72, 73, 75, 78, 79, 80, 82, 83, 84, 86, 87], "one": [1, 3, 15, 21, 24, 47, 53, 54, 57, 59, 60, 63, 65, 70, 75, 78, 79, 80, 81, 82, 83, 84, 87], "matrix": [1, 3, 5, 6, 7, 8, 9, 10, 13, 15, 18, 19, 20, 21, 22, 23, 24, 25, 26, 28, 29, 30, 31, 32, 33, 34, 35, 38, 39, 40, 42, 43, 44, 49, 50, 51, 53, 55, 57, 60, 63, 64, 66, 67, 68, 69, 70, 71, 72, 73, 75, 78, 79, 80, 81, 82, 83, 85, 86, 87], "includ": [1, 59, 60, 65, 87], "an": [1, 4, 6, 8, 10, 49, 55, 59, 60, 62, 63, 64, 65, 68, 70, 72, 73, 74, 76, 77, 78, 79, 80, 82, 83, 84, 85, 86, 87], "option": [1, 3, 5, 6, 7, 8, 10, 13, 14, 15, 31, 59, 60, 67], "specif": [1, 9, 59, 60, 64, 66, 70, 72, 75], "function": [1, 3, 5, 6, 12, 13, 14, 15, 19, 22, 26, 31, 35, 45, 46, 47, 49, 50, 51, 55, 57, 58, 59, 60, 62, 64, 65, 66, 67, 73, 75, 78, 79, 80, 82, 83, 86, 87], "int": [1, 5, 6, 7, 8, 9, 10, 12, 13, 14, 15, 18, 19, 20, 21, 22, 23, 24, 25, 26, 28, 29, 30, 31, 32, 33, 34, 35, 37, 39, 40, 42, 43, 44, 45, 46, 47, 48, 49, 50, 52, 53, 54, 55, 56, 57, 58, 62, 65, 70, 71, 74, 76, 77, 81, 87], "dimens": [1, 6, 7, 8, 9, 10, 12, 13, 14, 15, 18, 19, 20, 21, 22, 23, 24, 25, 26, 28, 29, 32, 33, 34, 35, 37, 39, 40, 42, 44, 45, 46, 47, 48, 49, 50, 52, 53, 54, 55, 56, 57, 60, 62, 63, 64, 65, 66, 67, 68, 70, 71, 72, 73, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87], "output": [1, 3, 5, 6, 8, 10, 12, 13, 14, 15, 19, 21, 22, 24, 26, 29, 31, 33, 35, 38, 40, 42, 43, 45, 47, 48, 49, 51, 53, 54, 55, 57, 59, 60, 62, 65, 72, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 86, 87], "whether": [1, 3, 5, 6, 7, 8, 12, 13, 14, 15, 21, 29, 33, 35, 44, 45, 54], "normal": [1, 6, 12, 13, 14, 15, 21, 22, 29, 31, 47, 49, 50, 51, 53, 55, 65, 67, 82, 85], "aggreg": [1, 2, 3, 4, 6, 10, 15, 19, 26, 29, 35, 45, 46, 47, 49, 50, 54, 55, 59, 62, 65, 72, 73, 80, 84, 86], "size": [1, 12, 14, 15, 29, 62, 63, 64, 66, 67, 68, 70, 72, 73, 75, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87], "us": [1, 3, 5, 6, 7, 8, 10, 19, 21, 22, 29, 30, 31, 33, 35, 42, 44, 45, 46, 47, 50, 51, 56, 57, 59, 60, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 76, 77, 78, 79, 80, 81, 82, 83, 85, 86, 87], "attent": [1, 2, 3, 5, 6, 7, 8, 15, 24, 42, 43, 44, 45, 59, 67, 72], "learnabl": [1, 3, 6, 13, 15, 21, 29, 35, 38, 40, 43, 47, 49, 53, 55, 57, 62, 65, 67, 80, 86], "linear": [1, 6, 7, 9, 19, 23, 25, 28, 29, 31, 32, 34, 35, 44, 54, 59, 62, 71, 78, 79, 80, 82, 83, 86, 87], "transform": [1, 6, 14, 15, 29, 31, 35, 60, 65, 71, 75, 80, 86], "nb": 1, "equal": [1, 60, 72, 87], "x_sourc": [1, 3, 6, 15, 24, 43], "x_target": [1, 3, 6, 24, 26], "tensor": [1, 3, 4, 5, 6, 7, 8, 9, 10, 12, 13, 14, 15, 18, 19, 20, 21, 22, 23, 24, 25, 26, 28, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 40, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 62, 63, 64, 65, 66, 67, 68, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 82, 83, 84, 85, 86, 87], "thi": [1, 2, 3, 6, 8, 10, 19, 21, 22, 26, 29, 33, 35, 40, 43, 45, 47, 48, 49, 53, 54, 55, 57, 58, 59, 60, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 75, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87], "implement": [1, 3, 4, 6, 8, 9, 10, 15, 18, 20, 21, 23, 24, 25, 26, 28, 29, 31, 32, 33, 34, 35, 37, 39, 40, 42, 43, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 59, 62, 65, 66, 67, 71, 80, 81, 82, 87], "from": [1, 3, 5, 6, 8, 10, 15, 18, 19, 21, 26, 29, 33, 35, 43, 45, 56, 57, 58, 59, 60, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87], "cell": [1, 3, 5, 6, 7, 8, 9, 10, 15, 24, 26, 36, 40, 43, 46, 47, 49, 50, 53, 54, 55, 59, 64, 65, 66, 67, 68, 73, 74, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 87], "via": [1, 3, 38, 40, 43, 46, 47], "defin": [1, 2, 3, 33, 43, 56, 59, 60, 62, 63, 64, 65, 66, 75, 80, 81, 87], "where": [1, 3, 43, 45, 49, 55, 59, 60, 62, 63, 64, 65, 67, 70, 72, 73, 78, 79, 80, 81, 82, 83, 84, 86, 87], "can": [1, 3, 6, 8, 11, 21, 43, 47, 48, 55, 57, 59, 60, 61, 65, 72, 73, 75, 78, 79, 80, 82, 84, 86, 87], "target": [1, 3, 6, 15, 24, 26, 43, 49, 55, 83, 86], "In": [1, 3, 19, 22, 43, 48, 59, 60, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87], "practic": [1, 3, 43, 66, 67], "If": [1, 3, 6, 10, 19, 24, 26, 31, 46, 60, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 75, 80, 81], "provid": [1, 3, 6, 31, 49, 55, 59, 60, 81, 83, 86], "i": [1, 2, 3, 5, 6, 8, 10, 11, 19, 21, 26, 27, 29, 30, 31, 33, 35, 38, 40, 41, 42, 45, 47, 53, 55, 57, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87], "assum": [1, 3, 15, 24, 26, 43], "e": [1, 3, 6, 12, 14, 19, 33, 49, 54, 55, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 70, 72, 73, 75, 78, 79, 80, 83, 85, 86, 87], "send": [1, 3, 8, 10, 21, 59, 62, 63, 64, 66, 67, 68, 70, 72, 73, 78, 79, 80, 85], "themselv": [1, 3, 82], "n_source_cel": [1, 3, 15, 24, 43], "all": [1, 3, 6, 15, 24, 26, 30, 31, 43, 48, 58, 59, 60, 65, 68, 72, 73, 78, 79, 80, 82, 83, 86, 87], "have": [1, 3, 15, 19, 24, 26, 43, 49, 55, 59, 60, 65, 66, 67, 68, 70, 72, 73, 78, 79, 82, 83, 86, 87], "same": [1, 3, 6, 15, 24, 26, 43, 47, 48, 49, 53, 59, 60, 62, 65, 80, 81, 83, 86, 87], "rank": [1, 3, 7, 9, 15, 20, 23, 24, 25, 26, 28, 31, 32, 34, 39, 42, 43, 46, 47, 50, 51, 53, 59, 62, 63, 64, 65, 66, 67, 68, 70, 72, 73, 78, 79, 80, 81, 82, 83, 84, 86], "r": [1, 3, 6, 8, 10, 15, 24, 42, 43, 45, 46, 47, 53, 60, 62, 63, 64, 65, 66, 67, 72, 80, 81, 82, 87], "torch": [1, 3, 5, 6, 7, 8, 9, 10, 12, 13, 14, 15, 18, 19, 20, 21, 22, 23, 24, 25, 26, 28, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 40, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 57, 59, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87], "spars": [1, 3, 4, 6, 8, 10, 13, 15, 18, 19, 20, 21, 22, 24, 29, 33, 35, 38, 40, 43, 46, 47, 49, 53, 55, 57, 60, 65, 66, 67, 70, 71, 72, 73, 75, 78, 80, 82, 83, 84], "n_target_cel": [1, 3, 15, 24, 26, 43, 49, 55], "": [1, 3, 15, 18, 19, 26, 43, 59, 60, 62, 63, 64, 65, 66, 67, 68, 69, 71, 75, 78, 79, 81, 82, 83, 87], "x_message_on_target": [1, 15, 24, 26], "embed": [1, 24, 26, 44, 49, 55, 65], "The": [2, 3, 4, 6, 8, 10, 11, 19, 20, 21, 22, 26, 27, 29, 31, 33, 35, 36, 40, 41, 42, 45, 47, 49, 53, 55, 57, 59, 66, 67, 68, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 81, 82, 83, 84, 85, 86, 87], "compos": [2, 8, 10, 11, 21, 27, 29, 33, 35, 41, 65], "primarili": [2, 11, 27, 36, 41], "three": [2, 29, 33, 35, 36, 41, 59, 60, 75], "conv": [2, 4, 21], "structur": [2, 62, 63, 64, 66, 67, 73, 80, 83, 86], "messagepass": [2, 3, 49, 55], "reset_paramet": [2, 3, 6, 13, 15, 21, 24, 27, 29, 31, 35, 38, 40, 41, 43, 45, 47, 49, 51, 53, 55, 57], "message_pass": 3, "add": [3, 6, 13, 15, 19, 58, 60, 75, 86], "uniform": [3, 6, 26, 87], "through": [3, 7, 9, 10, 18, 23, 25, 28, 30, 32, 34, 44, 56, 59, 71, 86, 87], "singl": [3, 26, 59, 64], "n": [3, 6, 10, 43, 45, 56, 59, 60, 61, 62, 64, 65, 72, 73, 80, 81, 87], "decompos": 3, "creat": [3, 10, 56, 59, 60, 67, 69, 72, 73], "go": [3, 9, 10, 61, 62, 64, 65, 80, 87], "come": 3, "differ": [3, 22, 47, 53, 60, 65, 70, 78, 79, 80, 82, 83, 86, 87], "onto": [3, 80], "should": [3, 6, 59, 60, 87], "instanti": [3, 64], "directli": [3, 75], "rather": [3, 59, 60], "inherit": [3, 59], "subclass": [3, 49, 55], "effect": [3, 19, 65], "doe": [3, 59, 60, 83, 86, 87], "trainabl": [3, 33, 49, 55, 72], "weight": [3, 6, 15, 22, 24, 43, 45, 49, 55, 60, 65, 67, 72, 80, 82, 83, 87], "its": [3, 19, 29, 31, 33, 35, 59, 60, 65, 69, 71, 72], "gain": [3, 15, 21, 49, 55, 57], "refer": [3, 5, 6, 7, 8, 9, 10, 12, 13, 14, 15, 18, 19, 20, 21, 22, 23, 24, 25, 26, 28, 29, 30, 31, 32, 33, 34, 35, 38, 40, 43, 45, 47, 49, 51, 53, 55, 57, 60, 62, 73, 80], "hajij": [3, 7, 8, 10, 21, 26, 29, 33, 35, 38, 40, 43, 45, 47, 49, 51, 53, 55, 57, 59, 61, 62, 63, 65, 79, 80], "zamzmi": [3, 7, 8, 40, 45, 61], "papamark": [3, 45, 59, 61], "miolan": [3, 8, 10, 21, 26, 29, 33, 35, 38, 40, 43, 45, 47, 49, 51, 53, 55, 57, 59, 61], "guzm\u00e1n": [3, 40, 59, 61], "s\u00e1enz": [3, 40, 59, 61], "ramamurthi": [3, 40, 59, 61], "birdal": [3, 59, 61], "dei": [3, 59, 61], "mukherje": [3, 59, 61], "samaga": [3, 59, 61], "livesai": [3, 59, 61], "walter": [3, 59, 61], "rosen": [3, 59, 61], "schaub": [3, 59, 61], "topolog": [3, 4, 7, 8, 10, 21, 26, 29, 33, 35, 38, 40, 43, 45, 47, 49, 51, 53, 55, 57, 61, 62, 63, 64, 65, 70, 78, 79, 80, 81, 82, 84, 86, 87], "deep": [3, 8, 10, 21, 26, 29, 33, 35, 40, 45, 47, 51, 53, 55, 57, 61, 62, 63, 64, 65, 67, 70, 78, 79, 80, 81, 82, 84, 86, 87], "learn": [3, 8, 10, 20, 21, 22, 25, 26, 29, 31, 33, 35, 40, 45, 47, 51, 53, 55, 57, 60, 61, 62, 63, 64, 65, 70, 71, 72, 73, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87], "beyond": [3, 7, 8, 20, 21, 22, 51, 61, 62, 65, 80], "graph": [3, 5, 6, 13, 15, 20, 21, 22, 28, 29, 30, 31, 32, 33, 34, 35, 48, 49, 59, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 85, 86, 87], "data": [3, 7, 8, 47, 53, 60, 61, 62, 69, 71, 80, 82, 85], "2023": [3, 8, 10, 21, 26, 29, 33, 35, 38, 40, 43, 45, 47, 49, 51, 53, 55, 57, 61, 62, 63, 64, 65, 70, 75, 78, 79, 80, 81, 82, 83, 84, 86, 87], "http": [3, 5, 6, 7, 8, 9, 10, 12, 13, 14, 15, 18, 19, 20, 21, 22, 23, 24, 25, 26, 28, 29, 30, 31, 32, 33, 34, 35, 40, 43, 45, 47, 51, 53, 55, 57, 62, 65, 69, 70, 71, 75, 78, 79, 80, 82, 83, 85], "arxiv": [3, 5, 6, 7, 8, 9, 10, 12, 13, 14, 15, 18, 19, 21, 25, 26, 28, 29, 30, 31, 32, 33, 34, 35, 40, 43, 45, 47, 51, 53, 55, 57, 61], "org": [3, 5, 6, 7, 8, 9, 10, 12, 13, 14, 15, 18, 19, 21, 23, 24, 25, 26, 28, 29, 30, 31, 32, 33, 34, 35, 40, 43, 45, 47, 51, 53, 55, 57, 70, 78, 79, 80, 82, 83], "ab": [3, 8, 9, 10, 12, 13, 14, 15, 18, 19, 21, 25, 26, 29, 33, 35, 40, 43, 45, 47, 51, 53, 55, 57, 84], "2206": [3, 61, 78], "00606": [3, 61], "papillon": [3, 8, 10, 21, 26, 29, 33, 35, 38, 40, 43, 45, 47, 49, 51, 53, 55, 57, 59, 61, 62, 63, 64, 65, 70, 78, 79, 80, 81, 82, 84, 86, 87], "sanborn": [3, 8, 10, 21, 26, 29, 33, 35, 38, 40, 43, 45, 47, 49, 51, 53, 55, 57, 59, 61], "architectur": [3, 8, 10, 15, 21, 26, 29, 33, 35, 40, 43, 45, 47, 51, 53, 55, 57, 59, 61, 62, 63, 64, 65, 66, 67, 68, 70, 72, 73, 78, 79, 80, 81, 82, 84, 86, 87], "survei": [3, 8, 10, 21, 26, 29, 33, 35, 40, 45, 47, 51, 53, 55, 57, 59, 61, 62, 63, 64, 65, 70, 78, 79, 80, 81, 82, 84, 86, 87], "neural": [3, 4, 7, 8, 10, 12, 13, 14, 15, 18, 19, 21, 23, 25, 26, 28, 29, 30, 31, 32, 33, 34, 35, 38, 40, 43, 45, 47, 49, 51, 53, 54, 55, 56, 57, 59, 61], "network": [3, 4, 5, 6, 7, 8, 9, 10, 12, 13, 14, 15, 18, 19, 20, 21, 22, 23, 25, 26, 28, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 40, 42, 43, 45, 46, 47, 49, 50, 51, 52, 53, 54, 55, 56, 57, 59, 61], "2304": [3, 8, 10, 21, 26, 29, 33, 35, 40, 45, 47, 51, 53, 55, 57, 61], "10031": [3, 8, 10, 21, 26, 29, 33, 35, 40, 45, 47, 51, 53, 55, 57, 61], "x_messag": [3, 26], "receiv": [3, 10, 19, 26, 59], "sever": [3, 4, 26, 59], "per": [3, 15, 24, 26, 59, 60, 65], "correspond": [3, 10, 26, 47, 53, 55, 62, 65, 67, 69, 71, 75], "within": [3, 60, 62, 65, 69, 71, 80], "n_messag": [3, 24, 26], "associ": [3, 26, 60, 62, 63, 64, 65, 66, 67, 68, 72, 73, 74, 76, 77, 78, 79, 80, 81, 82, 83], "One": [3, 26, 37, 39, 42, 48, 52, 59, 82, 86, 87], "sent": [3, 26, 75], "comput": [3, 4, 6, 7, 9, 12, 13, 14, 15, 18, 19, 20, 21, 22, 23, 24, 25, 26, 28, 31, 32, 34, 37, 39, 42, 44, 46, 48, 49, 50, 52, 54, 55, 58, 59, 60, 62, 65, 71, 72, 75, 80, 86, 87], "scheme": [3, 6, 8, 45, 59, 63, 65, 80], "altern": [3, 59], "user": [3, 78, 82, 83, 86], "overwrit": 3, "order": [3, 40, 42, 43, 47, 48, 49, 53, 54, 55, 57, 59, 62, 65, 79, 80, 82, 83, 85, 86, 87], "replac": 3, "own": 3, "mechan": [3, 5, 6, 7, 8, 15, 24, 62, 63, 65, 67, 72, 80], "follow": [3, 6, 10, 56, 59, 60, 61, 62, 65, 66, 67, 68, 73, 74, 75, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87], "scalar": [3, 15, 21, 24, 63, 64, 81, 87], "between": [3, 6, 7, 8, 10, 13, 15, 19, 24, 26, 47, 53, 62, 65, 80, 82], "two": [3, 8, 10, 19, 21, 31, 49, 62, 65, 66, 67, 68, 70, 72, 73, 78, 79, 80, 82, 83, 86, 87], "m_": [3, 6, 8, 10, 13, 15, 19, 21, 24, 26, 29, 31, 33, 35, 38, 40, 43, 45, 47, 49, 51, 53, 55, 57, 62, 63, 64, 66, 67, 70, 72, 73, 78, 79, 80, 81, 82, 84, 86, 87], "y": [3, 6, 8, 10, 13, 15, 19, 21, 24, 26, 29, 31, 33, 35, 38, 40, 43, 45, 47, 49, 51, 53, 55, 57, 60, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87], "rightarrow": [3, 6, 8, 10, 13, 15, 19, 21, 24, 26, 29, 31, 33, 35, 38, 40, 43, 45, 47, 49, 51, 53, 55, 57, 62, 63, 64, 65, 66, 67, 70, 72, 73, 78, 79, 80, 81, 82, 84, 86, 87], "left": [3, 10, 26, 45, 56, 60, 64, 65, 72, 73, 87], "right": [3, 10, 26, 56, 60, 64, 65, 72, 73, 87], "travel": 3, "denot": [3, 66, 67, 72, 73, 86, 87], "mathcal": [3, 6, 8, 10, 13, 15, 19, 21, 24, 26, 29, 31, 33, 35, 38, 40, 43, 45, 47, 49, 51, 53, 55, 57, 62, 63, 64, 66, 67, 70, 72, 73, 78, 79, 80, 81, 82, 84, 86, 87], "mathbf": [3, 83, 86], "_x": [3, 38, 51, 53, 62, 78, 80, 84], "_y": [3, 31, 40, 47, 79, 82], "theta": [3, 6, 8, 21, 24, 26, 29, 33, 38, 40, 43, 45, 47, 49, 51, 53, 57, 62, 63, 65, 67, 70, 72, 73, 78, 79, 81, 82, 83, 84, 86, 87], "ar": [3, 6, 8, 10, 12, 13, 14, 15, 19, 21, 22, 29, 33, 35, 40, 43, 45, 47, 49, 57, 59, 60, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 75, 78, 79, 80, 81, 82, 83, 84, 86, 87], "call": [3, 19, 22, 49, 55, 60, 67, 87], "leftarrow": [3, 73], "across": [3, 22], "belong": [3, 60, 65, 70, 78, 79, 80, 82, 83], "m_x": [3, 6, 8, 10, 19, 21, 24, 26, 29, 31, 33, 35, 38, 40, 43, 45, 47, 49, 51, 53, 55, 57, 63, 64, 65, 70, 72, 73, 78, 79, 81, 82, 84, 86, 87], "text": [3, 8, 10, 15, 45, 49, 60, 65, 66, 67, 68, 69, 70, 71, 72, 73, 78, 79, 80, 81, 82, 83], "agg": [3, 8, 10, 19, 45, 81], "_": [3, 6, 8, 10, 21, 24, 26, 31, 35, 38, 40, 43, 45, 47, 51, 53, 55, 57, 63, 64, 65, 66, 67, 68, 70, 71, 72, 73, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87], "result": [3, 60, 62, 67, 70, 83, 86], "detail": [3, 10, 59, 60, 62, 67, 80], "found": [3, 65, 75], "construct": [3, 6, 69, 71, 75, 87], "reset": [3, 6, 13, 15, 21, 24, 29, 31, 35, 38, 40, 43, 45, 47, 49, 51, 53, 55, 57], "note": [3, 6, 8, 10, 21, 40, 43, 45, 48, 49, 51, 53, 54, 55, 57, 59, 60, 66, 67, 68, 72, 73, 78, 79, 83, 87], "give": [4, 62, 65, 67, 80], "overview": 4, "which": [4, 6, 19, 26, 43, 44, 48, 59, 60, 65, 66, 67, 69, 71, 72, 75, 78, 79, 80, 82, 83, 86, 87], "consist": [4, 31, 36, 58, 59, 60, 75, 87], "core": 4, "mathemat": 4, "concept": 4, "nn": [4, 5, 6, 7, 8, 9, 10, 12, 13, 14, 15, 18, 19, 20, 21, 22, 23, 24, 25, 26, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 59, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87], "organ": [4, 59, 82], "domain": [4, 59, 61, 62, 63, 64, 66, 67, 69, 70, 71, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87], "util": [4, 6, 30, 62, 65, 66, 67, 74, 76, 77, 80, 87], "broadcast": [4, 58, 83, 86], "scatter": [4, 58, 71, 87], "scatter_add": [4, 58], "scatter_mean": [4, 58], "scatter_sum": [4, 58], "in_channels_0": [5, 6, 7, 8, 9, 10, 49, 52, 53, 62, 63, 64, 83, 85, 86], "in_channels_1": [5, 6, 7, 8, 9, 10, 49, 52, 53, 62, 63, 64, 83, 85, 86], "num_class": [5, 7, 8, 9, 18, 30, 48, 52, 62, 63, 64, 65, 69, 71, 75, 83, 85], "dropout": [5, 6, 12, 13, 14, 15, 18, 19, 62, 71], "0": [5, 6, 7, 8, 9, 12, 13, 14, 15, 18, 19, 21, 22, 24, 26, 29, 30, 31, 33, 35, 39, 40, 42, 43, 45, 49, 50, 51, 53, 55, 57, 59, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87], "5": [5, 18, 19, 21, 30, 42, 60, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 82, 83, 84, 85, 86, 87], "head": [5, 6, 14, 15, 62, 67], "concat": [5, 6, 49, 55], "skip_connect": [5, 6], "att_activ": [5, 6, 62], "leakyrelu": [5, 6, 62, 65, 72, 80], "negative_slop": [5, 6, 62, 65], "n_layer": [5, 7, 9, 12, 14, 18, 20, 23, 25, 28, 30, 32, 34, 37, 39, 42, 44, 46, 48, 50, 52, 54, 62, 63, 64, 66, 67, 68, 69, 70, 71, 72, 73, 74, 76, 77, 78, 79, 80, 81, 82, 83, 84, 86], "att_lift": [5, 62], "classif": [5, 8, 10, 20, 21, 23, 25, 28, 30, 32, 34, 37, 39, 40, 42, 43, 45, 46, 48, 50, 52, 53, 54, 56, 59, 62, 63, 64, 66, 67, 68, 69, 70, 71, 72, 73, 74, 76, 77, 78, 79, 80, 81, 82, 84, 85], "number": [5, 6, 7, 9, 12, 13, 14, 15, 18, 19, 20, 30, 31, 42, 43, 46, 48, 50, 52, 54, 59, 62, 65, 71, 78, 79, 80, 81, 82, 84, 85, 87], "node": [5, 6, 7, 8, 9, 10, 13, 15, 18, 19, 20, 21, 22, 23, 24, 25, 26, 28, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 40, 42, 44, 45, 46, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 59, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 75, 76, 78, 79, 80, 81, 82, 84, 85, 87], "level": [5, 22, 24, 30, 45, 59, 62, 65, 66, 67, 68, 70, 72, 73, 78, 79, 80, 81, 82, 83, 86], "edg": [5, 6, 7, 8, 9, 10, 12, 13, 14, 15, 20, 21, 23, 24, 25, 28, 29, 31, 32, 33, 34, 35, 38, 40, 42, 44, 48, 49, 50, 51, 52, 53, 54, 55, 57, 59, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87], "num_classest": 5, "probabl": [5, 6, 12, 13, 14, 15, 65], "concaten": [5, 6, 19, 62, 67, 72, 87], "skip": [5, 6, 31, 37, 39, 40, 87], "connect": [5, 6, 31, 40, 56, 65, 72, 75, 79, 83, 87], "activ": [5, 6, 13, 15, 46, 47, 50, 65, 67], "lift": [5, 6, 59, 62, 63, 64, 65, 66, 67, 70, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 86], "signal": [5, 6, 62, 63, 64, 65, 66, 67, 68, 72, 73, 74, 76, 77, 80, 81], "giusti": [5, 6, 43, 59, 62, 80], "battiloro": [5, 6, 43, 59, 80], "testa": [5, 6], "di": [5, 6, 43], "lorenzo": [5, 6, 43], "sardellitti": [5, 6, 43], "barbarossa": [5, 6, 43], "2022": [5, 6, 12, 13, 14, 15, 18, 19, 40, 43, 45, 47, 53, 59, 62, 69, 78, 79, 80, 82, 86], "paper": [5, 6, 15, 19, 20, 21, 22, 59, 62, 65, 69, 70, 71, 78, 79, 80, 83, 84, 85, 86, 87], "pdf": [5, 6, 7, 8, 20, 21, 22, 23, 24, 28, 29, 30, 31, 32, 33, 34, 35, 40, 45, 55, 60], "2209": [5, 6], "08179": [5, 6], "repositori": [5, 6, 59, 60], "lrnzgiusti": [5, 6], "x_0": [5, 6, 7, 8, 9, 10, 12, 14, 18, 19, 20, 21, 22, 29, 30, 31, 32, 33, 35, 39, 40, 49, 50, 51, 52, 53, 62, 63, 64, 65, 66, 67, 68, 70, 71, 72, 73, 75, 78, 79, 80, 81, 82, 83, 84, 85, 86], "x_1": [5, 6, 7, 8, 9, 10, 18, 19, 20, 21, 22, 23, 28, 34, 49, 50, 51, 52, 53, 62, 63, 64, 65, 66, 67, 68, 70, 71, 72, 73, 74, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86], "neighborhood_0_to_0": [5, 6, 7, 8], "lower_neighborhood": [5, 6, 62], "upper_neighborhood": [5, 6, 62], "n_node": [5, 7, 9, 13, 15, 18, 19, 20, 21, 22, 23, 25, 28, 29, 32, 33, 34, 35, 37, 38, 39, 40, 42, 46, 48, 49, 50, 51, 52, 53, 54, 57, 70, 71], "n_edg": [5, 7, 9, 19, 20, 21, 22, 23, 25, 28, 29, 32, 33, 34, 35, 38, 39, 40, 42, 48, 49, 50, 51, 53, 54, 55, 57, 70], "lower": [5, 6, 43, 46, 47, 49, 54, 55, 56, 62, 80, 81, 83, 86, 87], "neighbourhood": [5, 6], "upper": [5, 6, 9, 10, 19, 42, 43, 46, 47, 48, 49, 50, 51, 54, 55, 56, 62, 64, 80, 83, 86, 87], "canlay": [6, 11, 62], "01": [6, 69, 75, 78, 83, 86, 87], "add_self_loop": [6, 11], "version": [6, 8, 9, 21, 63, 64, 65, 83, 86], "v1": 6, "v2": 6, "share_weight": 6, "kwarg": [6, 25, 26], "model": [6, 30, 37, 39, 59, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 82, 84, 85], "consid": [6, 42, 48, 49, 54, 55, 62, 80, 83], "though": 6, "addition": [6, 66, 67, 82], "ad": [6, 19, 59, 60, 65], "coeffici": [6, 62, 72], "otherwis": [6, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 75, 81, 87], "averag": [6, 9, 54, 87], "self": [6, 30, 31, 60, 62, 65, 71, 75, 78, 80, 82, 83, 86, 87], "loop": [6, 30, 31, 59, 66, 67, 68, 69, 71, 73, 74, 75, 77, 78, 79, 80, 82, 84, 85, 87], "callabl": [6, 13, 15, 19, 22], "origin": [6, 31, 59, 60, 62, 63, 64, 80, 82, 83, 85, 86, 87], "while": [6, 59, 60, 80], "attet": 6, "gatv2": [6, 62], "valid": [6, 60, 65, 74, 75, 76, 77, 87], "onli": [6, 8, 19, 53, 59, 60, 62, 65, 75, 80, 83, 85, 87], "share": [6, 19, 59, 62, 65], "prefer": [6, 59, 60], "necessari": [6, 58, 59, 62, 65, 80], "preprocess": 6, "n_k_cell": 6, "complex": [6, 7, 8, 9, 10, 23, 25, 28, 32, 34, 38, 40, 44, 45, 46, 47, 48, 49, 50, 51, 53, 54, 55, 56, 57, 59, 60, 62, 64, 66, 67, 68, 72, 73, 78, 79, 80, 85], "map": [6, 8, 10, 13, 15, 19, 20, 21, 22, 29, 33, 35, 38, 40, 46, 47, 57, 65, 84, 87], "a_k_low": 6, "a_k_up": 6, "n_1": [6, 43, 62, 80], "n_2": [6, 43, 62, 80], "a_": [6, 10, 40, 43, 62, 63, 64, 65, 72, 78, 79, 80], "uparrow": [6, 8, 10, 38, 40, 43, 47, 51, 53, 55, 57, 62, 63, 64, 65, 78, 79, 80, 82, 83, 84, 86, 87], "downarrow": [6, 38, 43, 45, 47, 51, 53, 55, 57, 62, 65, 78, 80, 81, 82, 83, 84, 86, 87], "begin": [6, 8, 10, 13, 15, 19, 21, 24, 26, 29, 31, 33, 35, 38, 40, 43, 45, 47, 49, 51, 53, 55, 57, 60, 62, 80], "align": [6, 8, 10, 13, 15, 19, 21, 24, 26, 29, 31, 33, 35, 38, 40, 43, 45, 47, 49, 51, 53, 55, 57, 62, 80, 86], "quad": [6, 8, 10, 13, 15, 19, 21, 24, 26, 29, 31, 33, 35, 38, 40, 43, 45, 47, 49, 51, 53, 55, 57, 62, 63, 64, 65, 66, 67, 70, 72, 73, 78, 79, 80, 81, 82, 84, 86, 87], "k": [6, 10, 19, 43, 45, 49, 55, 61, 62, 64, 67, 70, 72, 78, 79, 80, 81, 82, 84, 86, 87], "alpha_k": [6, 43, 62, 80], "h_x": [6, 8, 10, 13, 15, 19, 21, 24, 26, 29, 31, 33, 35, 38, 40, 43, 45, 47, 49, 53, 55, 57, 62, 63, 64, 65, 66, 67, 70, 72, 73, 78, 79, 80, 81, 82, 86, 87], "t": [6, 8, 10, 13, 15, 19, 21, 24, 26, 29, 31, 33, 35, 38, 40, 43, 45, 47, 49, 51, 53, 55, 57, 61, 62, 63, 64, 65, 66, 67, 70, 71, 72, 73, 75, 78, 79, 80, 81, 82, 83, 84, 86, 87], "h_y": [6, 8, 10, 13, 15, 19, 21, 26, 29, 33, 35, 40, 43, 49, 51, 55, 57, 62, 63, 64, 65, 66, 67, 70, 73, 79, 80, 84, 86, 87], "a_k": [6, 43, 62, 80, 87], "cdot": [6, 21, 24, 26, 29, 31, 33, 35, 38, 40, 43, 47, 49, 51, 53, 55, 57, 62, 65, 70, 72, 73, 78, 79, 80, 82, 84, 86, 87], "psi_k": [6, 43, 62, 80], "foral": [6, 43, 62, 72, 80], "n_k": [6, 43, 62, 80], "bigoplus_": [6, 43, 62, 80], "_k": [6, 10, 43, 45, 62, 64, 80, 81], "m": [6, 8, 38, 43, 45, 51, 53, 55, 57, 60, 63, 65, 72, 78, 81, 84, 86, 87], "bigotimes_": [6, 43, 62, 80], "phi": [6, 43, 62, 65, 80], "end": [6, 8, 10, 13, 15, 19, 21, 24, 26, 29, 31, 33, 35, 38, 40, 43, 45, 47, 49, 51, 53, 55, 57, 60, 62, 78, 79, 80, 82, 87], "liftlay": [6, 11, 62], "signal_lift_activ": 6, "signal_lift_dropout": 6, "adapt": [6, 58], "offici": [6, 71], "rate": [6, 18, 19, 65], "num_nod": [6, 30, 31], "num_edg": [6, 30, 31], "reiniti": 6, "xavier": 6, "multiheadcellattent": [6, 11, 62], "propos": [6, 8, 10, 15, 21, 24, 26, 29, 33, 35, 40, 43, 45, 47, 51, 53, 57, 62, 63, 64, 70, 73, 78, 79, 80, 83, 84, 85, 86, 87], "gat": [6, 62, 80], "adjac": [6, 7, 8, 9, 10, 18, 19, 37, 38, 39, 40, 46, 47, 49, 50, 51, 55, 62, 63, 64, 65, 69, 78, 79, 80, 82, 83, 87], "non": [6, 62, 65, 80], "zero": [6, 62, 66, 67, 75, 78, 79, 80, 83, 86, 87], "valu": [6, 42, 58, 63, 64, 65, 66, 67, 68, 69, 71, 72, 73, 75, 78, 81, 83, 84, 85, 86, 87], "empti": 6, "veli\u010dkovi\u0107": 6, "cucurul": 6, "casanova": 6, "romero": 6, "li\u00f2": 6, "bengio": [6, 20, 21, 22], "2017": [6, 67, 78], "1710": [6, 70], "10903": 6, "up": [6, 7, 10, 29, 31, 33, 35, 39, 43, 54, 55, 60, 80, 82, 87], "down": [6, 42, 43, 44, 45, 48, 50, 51, 54, 55, 80], "multiheadcellattention_v2": [6, 11], "brodi": 6, "alon": 6, "yahav": 6, "how": [6, 60, 67, 78, 79, 80], "2105": [6, 28, 29, 30, 31, 32, 33, 34, 35], "14491": 6, "alpha": [6, 21, 30, 31, 55, 62, 73, 80, 87], "multiheadliftlay": [6, 11, 62], "type": [6, 13, 15, 60, 65, 66], "built": [6, 59, 60], "object": [6, 19, 60], "signal_lift_readout": 6, "str": [6, 13, 15, 24, 26, 46, 49, 50, 55, 58, 60], "cat": 6, "multi": [6, 13, 15, 60, 67], "readout": [6, 48], "index": [6, 58, 60, 65, 69, 82, 87], "z": [6, 8, 10, 13, 15, 19, 24, 26, 29, 31, 33, 35, 40, 43, 49, 55, 57, 62, 63, 64, 66, 67, 72, 73, 79, 86, 87], "h_z": [6, 10, 13, 15, 19, 43, 62, 64, 66, 67], "poollay": [6, 11, 62], "k_pool": 6, "signal_pool_activ": [6, 62], "pool": [6, 7, 9, 23, 25, 28, 32, 34, 44, 62, 65], "ratio": [6, 65], "fraction": [6, 70], "keep": [6, 60, 62, 63, 64, 65, 66, 67, 68, 69, 71, 73, 74, 77, 78, 79, 80, 82, 83], "after": [6, 29, 35, 60, 62, 71, 76], "oper": [6, 13, 15, 49, 55, 57, 62, 65, 67, 72, 80, 82, 83, 87], "tupl": [6, 18, 43, 48, 49, 51, 56, 65, 87], "num_pooled_nod": 6, "gamma": [6, 62, 71], "tau": [6, 62], "c_r": [6, 62], "file": [6, 59, 60, 65], "sparse_coo_tensor": [6, 69, 71], "softmax": [6, 11, 57, 65, 80, 86, 87], "src": [6, 58, 75], "num_cel": 6, "There": [6, 59, 60, 69, 70, 71, 78, 79, 80, 82, 86], "subtract": 6, "maximum": [6, 46, 47, 82, 87], "element": [6, 60, 65, 67], "avoid": [6, 49, 55], "overflow": 6, "underflow": 6, "indic": [6, 43, 58, 71, 78, 79, 80, 82, 83, 87], "batch": [6, 87], "in_channels_2": [7, 8, 9, 10, 49, 52, 53, 62, 63, 64, 83, 85, 86], "face": [7, 8, 9, 10, 44, 48, 49, 50, 51, 52, 53, 54, 59, 62, 63, 64, 65, 66, 67, 68, 72, 73, 78, 79, 80, 81, 82, 83, 84, 85, 86], "istvan": [7, 8], "analysi": [7, 8], "workshop": [7, 8, 20, 21, 22, 40, 51, 59], "neurip": [7, 8, 9, 10, 51], "2020": [7, 8, 20, 21, 22, 23, 24, 25, 26, 51, 59, 63, 69, 70, 72, 73, 84], "2010": [7, 8, 25, 26], "00743": [7, 8], "neighborhood_1_to_2": [7, 8], "avg": [7, 44], "n_face": [7, 9, 48, 49, 50, 51, 53], "transpos": [7, 44, 45, 81, 84], "boundari": [7, 9, 10, 20, 23, 25, 28, 32, 34, 39, 57, 64, 66, 67, 68, 70, 72, 73, 78, 79, 82, 87], "x_2": [7, 8, 9, 10, 49, 50, 51, 52, 53, 63, 64, 65, 66, 67, 68, 72, 73, 78, 79, 80, 81, 82, 83, 84, 85, 86], "label": [7, 9, 23, 25, 28, 32, 34, 37, 39, 42, 44, 47, 48, 50, 52, 53, 60, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 80, 81, 84, 85, 87], "assign": [7, 9, 23, 25, 28, 32, 34, 37, 39, 42, 44, 46, 48, 50, 52, 60, 75, 78, 79, 80, 82, 83, 87], "whole": [7, 9, 23, 25, 28, 32, 34, 44, 48, 50, 54], "simplifi": [8, 21, 63], "ccxn": [8, 11], "et": [8, 9, 10, 19, 22, 29, 33, 35, 53, 59, 61, 62, 63, 64, 65, 66, 67, 69, 70, 71, 72, 73, 78, 79, 80, 81, 82, 83, 84, 86, 87], "al": [8, 9, 10, 22, 29, 33, 35, 53, 59, 61, 62, 63, 64, 65, 66, 67, 70, 71, 72, 73, 78, 79, 80, 81, 82, 83, 84, 86, 87], "ccxnlayer": [8, 11, 63], "entir": [8, 10, 62, 63, 64, 65], "equat": [8, 10, 21, 26, 29, 33, 35, 38, 40, 43, 45, 47, 49, 51, 53, 55, 57, 60, 61, 62, 63, 64, 70, 78, 79, 80, 82, 84, 87], "awesom": [8, 10, 21, 26, 29, 33, 35, 38, 40, 43, 45, 47, 49, 51, 53, 55, 57, 61, 67, 73], "tnn": [8, 10, 21, 26, 29, 33, 35, 38, 40, 43, 45, 47, 49, 51, 53, 55, 57, 59, 61], "wa": [8, 10, 21, 40, 45, 47, 57, 65, 81], "Its": [8, 10, 21, 29, 33, 35, 40, 45, 47, 57], "graphic": [8, 10, 21, 29, 33, 35, 40, 45, 47, 57, 61], "illustr": [8, 10, 21, 29, 33, 35, 40, 45, 47, 57, 85], "amp": [8, 63], "l": [8, 10, 18, 19, 38, 40, 42, 43, 45, 47, 51, 53, 55, 57, 62, 63, 64, 65, 72, 73, 78, 79, 80, 81, 82, 83, 84, 86, 87], "u": [8, 10, 19, 43, 45, 55, 59, 63, 64, 66, 67, 72, 81, 87], "cohomologi": [8, 63], "coboundari": [8, 9, 63, 64], "t_": [8, 26, 63, 65, 73], "c": [8, 13, 15, 19, 21, 24, 26, 29, 31, 33, 35, 40, 47, 49, 51, 61, 62, 63, 65, 66, 67, 70, 71, 72, 73, 78, 79, 82, 84, 87], "h_": [8, 24, 38, 45, 47, 53, 63, 72, 78, 81, 82], "n_0_cell": 8, "n_1_cell": 8, "a_0_up": 8, "n_2_cell": 8, "b_2": [8, 49, 50, 51, 57, 63, 80, 84], "requir": [8, 60, 62, 81], "predict": [8, 12, 14, 18, 20, 57, 60, 62, 65, 71, 87], "hid_channel": [9, 64], "cw": [9, 10, 59], "hidden": [9, 12, 13, 14, 15, 18, 42, 66, 67, 71, 72, 80, 87], "bodnar": [9, 10, 59, 64], "weisfeil": [9, 10, 64], "lehman": [9, 10, 64], "cellular": [9, 10, 59, 64], "2021": [9, 10, 28, 29, 30, 31, 32, 33, 34, 35, 55, 57, 59, 64, 66, 67, 70, 87], "2106": [9, 10, 12, 13, 14, 15], "12575": [9, 10], "neighborhood_1_to_1": [9, 10], "neighborhood_2_to_1": [9, 10], "neighborhood_0_to_1": [9, 10], "project": [9, 42, 44, 60, 80], "cwn": [10, 11, 59], "cwnlayer": [10, 11, 64], "conv_1_to_1": 10, "conv_0_to_1": 10, "aggregate_fn": 10, "update_fn": 10, "represent": [10, 18, 19, 20, 21, 22, 25, 26, 40, 47, 53, 56, 59, 62, 65, 66, 67, 69, 70, 71, 72, 82, 85, 87], "case": [10, 59, 62, 63, 64, 66, 67, 68, 70, 72, 73, 78, 79, 80, 85, 87], "convolv": 10, "neighbor": [10, 19, 22, 62, 73, 86, 87], "co": [10, 59, 75], "check": [10, 59, 60, 61, 70, 78, 79], "docstr": [10, 59], "_cwndefaultfirstconv": 10, "more": [10, 19, 59, 60, 61, 65, 78, 82, 83, 87], "_cwndefaultsecondconv": 10, "obtain": [10, 54, 73, 75, 80, 83, 86, 87], "_cwndefaultaggreg": 10, "_cwndefaultupd": 10, "final": [10, 19, 22, 54, 57, 59, 65, 69, 70, 71, 75, 80, 86, 87], "first": [10, 29, 31, 35, 60, 62, 63, 64, 65, 66, 67, 68, 70, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87], "exploit": [10, 62], "second": [10, 29, 31, 35, 59, 65, 66, 67, 81], "b": [10, 13, 15, 19, 21, 24, 26, 29, 31, 33, 35, 45, 47, 49, 50, 51, 60, 64, 65, 66, 67, 70, 72, 73, 81, 82, 83, 84], "Then": [10, 60, 63, 64], "agg_": [10, 13, 15, 19, 45, 63, 64, 66, 67, 81], "n_": [10, 43, 66, 67, 68, 69, 70, 71, 72, 73, 78, 79, 80, 82, 83, 86], "_cell": 10, "in_channels_": 10, "b_": [10, 47, 64, 65, 81, 82], "t_r": 10, "six": 11, "can_lay": 11, "ccxn_layer": 11, "cwn_layer": 11, "hypergraph": [12, 13, 14, 15, 18, 19, 20, 21, 22, 23, 24, 25, 26, 28, 29, 30, 31, 32, 33, 34, 35, 36, 59, 66, 67, 73, 74, 76, 77], "hidden_channel": [12, 13, 14, 15, 42, 66, 67, 80], "mlp_num_lay": [12, 13, 14, 15, 66, 67], "mlp_activ": [12, 13, 15], "mlp_dropout": [12, 13, 14, 15], "mlp_norm": [12, 13, 14, 15], "combin": [12, 14, 31, 62], "multipl": [12, 14, 59, 60, 87], "form": [12, 14, 56, 59, 78, 79, 82, 83, 87], "in_dim": [12, 14], "hid_dim": [12, 14, 67], "out_dim": [12, 14, 67, 72, 76], "input_dropout": [12, 14], "mlp": [12, 13, 14, 15, 27, 67], "chien": [12, 13, 14, 15, 59, 66, 67], "pan": [12, 13, 14, 15], "peng": [12, 13, 14, 15], "milenkov": [12, 13, 14, 15], "you": [12, 13, 14, 15, 51, 59, 60, 75], "multiset": [12, 13, 14, 15, 66, 67], "framework": [12, 13, 14, 15, 28, 29, 30, 31, 32, 33, 34, 35, 66, 67], "iclr": [12, 13, 14, 15, 40], "13264": [12, 13, 14, 15], "incidence_1": [12, 13, 14, 15, 18, 19, 20, 21, 22, 23, 28, 29, 30, 31, 32, 33, 34, 35, 38, 39, 40, 50, 51, 57, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 76, 77, 78, 79, 83, 84, 86], "edge_index": [12, 14, 62, 66, 67, 69, 72, 73], "allset": [13, 14, 15, 27, 66, 67], "allsetblock": [13, 27], "block": [13, 15, 59], "bipartit": [13, 15], "incid": [13, 15, 18, 19, 20, 21, 22, 24, 25, 26, 29, 30, 31, 33, 35, 38, 40, 44, 45, 46, 47, 48, 49, 50, 51, 57, 65, 66, 67, 68, 69, 70, 71, 72, 73, 75, 78, 79, 80, 81, 82, 83, 86], "hyperedg": [13, 15, 18, 19, 20, 21, 22, 24, 26, 35, 59, 66, 67, 68, 72, 73], "allsetlay": [13, 27], "vertex": [13, 15, 66, 67, 87], "sigma": [13, 21, 24, 26, 38, 40, 43, 47, 51, 53, 55, 57, 66, 70, 72, 73, 78, 79, 82, 83, 84, 86, 87], "n_hyperedg": [13, 15, 18, 22, 71], "b_1": [13, 15, 19, 20, 21, 22, 24, 26, 29, 31, 33, 35, 38, 40, 49, 50, 51, 57, 66, 67, 68, 69, 70, 71, 72, 73, 78, 79, 80, 84], "norm_lay": [13, 15], "activation_lay": [13, 15], "inplac": [13, 15], "bia": [13, 15, 21, 62, 83, 86], "perceptron": [13, 15, 67], "do": [13, 15, 59, 69, 71, 78, 79, 82, 87], "place": [13, 15, 59, 60, 66, 67], "allsettransform": [14, 15, 27, 59, 67], "allsettransformerblock": [15, 27], "number_queri": 15, "queri": 15, "over": [15, 59, 62, 65, 66, 67, 68, 69, 71, 73, 74, 77, 78, 79, 80, 82, 84, 85, 87], "allsettransformerlay": [15, 27], "ln": [15, 67], "multiheadattent": [15, 27], "qk": 15, "v": [15, 60, 65, 66, 67, 70, 72, 73, 75, 78, 79, 80, 82, 83, 86], "mh": [15, 67], "eq": [15, 78, 79, 80, 83, 86], "7": [15, 18, 19, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87], "factor": 15, "in_featur": [18, 19, 22, 30, 62, 69, 71, 75, 83, 86], "hidden_featur": [18, 69, 71], "adjacency_dropout_r": 18, "regular_dropout_r": 18, "gradual": 18, "reduc": [18, 19, 83], "last": [18, 54, 57, 65, 82, 87], "item": [18, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87], "hmpnnlayer": [18, 19, 27], "regular": [18, 19], "heydari": [18, 19, 59, 69], "livi": [18, 19, 69], "icann": [18, 19], "2203": [18, 19, 43], "16995": [18, 19], "b1": [18, 48, 49, 65, 70, 78, 79, 82, 83, 84, 86], "y_pred": [18, 69, 71, 75, 78, 79, 80, 82, 83, 86], "logit": [18, 20, 30, 46, 71], "hmpnn": [19, 27, 59], "introduc": [19, 22, 62, 65, 69, 71, 72, 80], "node_to_hyperedge_messaging_func": 19, "hyperedge_to_node_messaging_func": 19, "adjacency_dropout": 19, "updating_dropout": 19, "updating_func": 19, "compris": 19, "make": [19, 22, 33, 59, 62, 63, 64, 65, 66, 67, 68, 69, 71, 75, 81, 87], "new": [19, 22, 59, 60, 63], "reprsent": 19, "them": [19, 49, 62, 63, 64, 65, 66, 67, 68, 69, 71, 72, 75, 80, 81, 82], "also": [19, 55, 60, 62, 63, 64, 65, 66, 67, 68, 70, 71, 72, 73, 74, 76, 77, 78, 79, 80, 81, 85], "reciev": 19, "beforehand": [19, 22], "wai": [19, 60, 80], "could": [19, 62, 65, 80, 82], "explicit": 19, "rightarrow1": [19, 31, 49, 51, 84], "rightarrow0": [19, 24, 29, 31, 33, 35, 49, 51, 72, 84], "m_z": [19, 24, 26, 29, 31, 33, 35, 49, 72, 73], "plu": [19, 80], "accord": [19, 45, 71, 85], "It": [19, 30, 31, 59, 69, 71, 75], "get": [19, 57, 59, 80, 83, 86, 87], "back": 19, "retriev": [19, 62, 63, 64, 66, 67, 68, 70, 72, 73, 74, 76, 77, 78, 79, 80, 81, 82, 83, 85], "apply_regular_dropout": 19, "unmask": 19, "vector": [19, 56, 60, 65, 67, 69, 71, 72, 87], "scale": [19, 87], "d": [19, 43, 55, 60, 72, 78, 84], "mask": [19, 56, 62, 69, 80, 87], "total": [19, 80], "node_in_featur": 19, "hyperedge_in_featur": 19, "channels_nod": [20, 21, 23, 25, 28, 34, 66, 68, 70, 74, 77, 78, 79, 80, 82, 83, 86], "channels_edg": [20, 21, 23, 25, 28, 34, 66, 68, 70, 74, 77], "n_class": [20, 44, 46, 50, 70, 81, 84], "neuron": [20, 21, 22, 59], "multiclass": 20, "dong": [20, 21, 22, 59, 70, 71], "sawin": [20, 21, 22], "icml": [20, 21, 22, 57], "grlplu": [20, 21, 22], "github": [20, 21, 22, 59, 67, 69, 71, 73, 75], "io": [20, 21, 22], "40": [20, 21, 22, 65, 68, 69, 70, 71, 74, 75, 76, 77, 78, 82], "hypernod": [20, 21], "hnhnnetwork": [20, 27, 70], "templat": [21, 23, 60], "hnhnlayer": [21, 22, 27, 70, 71], "use_bia": 21, "use_normalized_incid": 21, "beta": [21, 30, 31, 75, 87], "bias_gain": 21, "bias_init": 21, "hnhn": [21, 22, 27, 59], "matric": [21, 44, 45, 46, 47, 48, 51, 59, 62, 63, 64, 65, 66, 67, 68, 70, 72, 73, 78, 79, 80, 82, 83, 85, 86], "usign": 21, "cardin": 21, "hyperparamet": [21, 31, 69, 71, 75], "control": 21, "strenght": 21, "support": [21, 75, 80, 83, 86], "train": [21, 26, 44, 59], "term": [21, 59, 80], "flag": 21, "import": [21, 22, 31, 59, 60, 62, 63, 64, 66, 67, 69, 71, 73, 75, 80, 87], "compute_normalization_matric": 21, "w": [21, 29, 31, 70, 72, 80], "xy": [21, 38, 40, 43, 47, 51, 53, 55, 57, 65, 70, 78, 79, 82, 84, 86, 87], "sum_": [21, 24, 26, 29, 31, 33, 35, 38, 40, 43, 47, 49, 51, 53, 55, 57, 65, 70, 72, 73, 78, 79, 82, 83, 84, 86, 87], "init_bias": 21, "normalize_incidence_matric": 21, "activation_func": 22, "normalization_param_alpha": [22, 71], "normalization_param_beta": [22, 71], "relai": 22, "other": [22, 58, 60, 87], "word": [22, 69, 71], "intermediari": 22, "those": [22, 72, 80], "dure": 22, "multipli": [22, 80], "reflect": 22, "param": 22, "power": [22, 26, 84], "amount": [23, 25, 28, 32, 34, 37, 39, 52, 59, 63, 64, 65, 66, 67, 68, 69, 71, 73, 74, 77, 83], "ding": [23, 24, 72], "wang": [23, 24], "li": [23, 24], "huan": [23, 24], "liu": [23, 24], "emnlp": [23, 24], "aclanthologi": [23, 24], "main": [23, 24, 60, 75], "399": [23, 24, 70, 74], "global": [23, 25, 28, 32, 34], "max": [23, 25, 28, 32, 34, 82], "hypergat": [24, 27, 72], "hypergatlay": [24, 27], "string": [24, 26, 60], "set": [24, 26, 68, 70, 73, 74, 75, 76, 77, 80, 82, 87], "see": [24, 26, 49, 51, 53, 59, 60, 61, 65, 87], "t_1": [24, 31, 72], "odot": [24, 38, 43, 49, 72, 78], "zy": [24, 26, 31, 40, 72, 73, 79], "xz": [24, 26, 40, 72, 73, 79], "arya": [25, 26, 73], "gupta": [25, 26], "rudinac": [25, 26], "wor": [25, 26], "gener": [25, 26, 40, 56, 60, 62, 66, 67, 75, 79, 80], "induct": [25, 26], "04558": [25, 26], "features_nod": 25, "hypersag": [26, 27], "generalizedmean": [26, 27], "hypersagelay": [26, 27], "aggr_func_intra": 26, "aggr_func_int": 26, "devic": [26, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 75, 80, 81, 84, 85], "cpu": [26, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 75, 80, 81, 84, 85], "p": [26, 43, 55, 72, 73, 80], "name": [26, 59, 60, 62, 66, 67, 69, 74, 76, 77], "mode": [26, 75], "intra": [26, 45, 73], "either": [26, 59, 60, 80, 87], "w_y": [26, 73], "frac": [26, 31, 49, 72, 73], "vert": [26, 67, 73, 87], "w_z": [26, 73], "lvert": [26, 73], "rvert": [26, 73], "n_target_nod": 26, "allset_lay": 27, "allset_transformer_lay": 27, "allset_transform": [27, 67], "dhgcn_layer": 27, "dhgcn": 27, "hmpnn_layer": 27, "hnhn_layer_bi": [27, 71], "hnhn_layer": [27, 70], "hypergat_lay": 27, "hypersage_lay": 27, "unigcn_lay": 27, "unigcnlay": [27, 29, 74], "unigcn": [27, 29, 59], "unigcnii_lay": 27, "unigcniilay": [27, 31], "unigcnii": [27, 31, 32], "uniginlay": [27, 33, 76], "unigin": [27, 32, 33], "unisagelay": [27, 35, 77], "unisag": [27, 34, 35, 77], "huang": [28, 29, 30, 31, 32, 33, 34, 35, 59], "yang": [28, 29, 30, 31, 32, 33, 34, 35, 47, 53, 55, 59, 82, 83, 85, 86], "unignn": [28, 29, 30, 31, 32, 33, 34, 35], "unifi": [28, 29, 30, 31, 32, 33, 34, 35], "ijcai": [28, 29, 30, 31, 32, 33, 34, 35], "00956": [28, 29, 30, 31, 32, 33, 34, 35], "use_bn": [29, 35], "boolean": [29, 35, 60], "bathnorm": [29, 35], "everi": [29, 30, 31, 33, 35, 59, 75, 76], "hyper": [29, 31, 33, 35, 75, 87], "constitu": [29, 31, 33, 35], "third": [29, 31, 35], "num_lay": [30, 75, 83, 86], "expect": [30, 31, 75, 82, 84], "contain": [30, 31, 56, 59, 62, 65, 66, 67, 69, 71, 75, 83, 87], "y_hat": [30, 62, 63, 64, 65, 66, 67, 68, 70, 72, 73, 75, 78, 79, 80, 82, 83, 84, 85, 86], "determin": [31, 87], "theta_2": 31, "theta_1": 31, "x_skip": 31, "degre": 31, "sqrt": 31, "d_x": 31, "d_z": 31, "in_channels_nod": [32, 76], "intermediate_channel": [32, 54, 65, 76, 86], "unigin_lay": 33, "ep": 33, "train_ep": 33, "g": [33, 49, 54, 55, 60, 66, 67, 72, 80, 87], "sequenti": [33, 71], "constant": 33, "gin": 33, "unisage_lay": 35, "e_aggr": 35, "amax": 35, "amin": 35, "v_aggr": 35, "operatornam": [35, 43, 72], "sage": 35, "submodul": 36, "simplici": [36, 37, 38, 39, 40, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 59, 63, 64, 65, 66, 67, 68, 70, 72, 73, 85], "dist2cycl": [37, 38, 41], "binari": [37, 39, 42, 46, 50, 52, 62, 69, 70, 71, 74, 76, 77, 80], "high": [37, 39, 40, 65], "x_1e": [37, 78], "linv": [37, 38, 78], "hot": [37, 39, 42, 48, 52, 70, 78, 79, 80, 83], "dist2cycle_lay": 38, "dist2cyclelay": [38, 41], "x_e": 38, "adjacency_0": [38, 39, 40, 62, 63, 65, 78, 79], "a_0": [38, 40], "hsn": [39, 40, 41, 59], "hsn_layer": [40, 59], "hsnlayer": [40, 41, 59, 79, 82], "complic": [40, 43], "higher": [40, 47, 53, 59, 62, 65, 79, 82, 85, 86], "geometr": [40, 59], "openreview": [40, 51], "net": [40, 51, 57, 59], "id": [40, 51, 60], "sc8glb": 40, "k6e9": 40, "sanconv": [41, 43], "sanlay": [41, 43], "san": [41, 42, 43, 59, 62], "compute_projection_matrix": [41, 42], "scacmpslay": [41, 45, 81], "intra_aggr": [41, 45], "weight_func": [41, 45], "scacmp": [41, 44, 81], "sccnlayer": [41, 47, 53, 82], "sccn": [41, 46, 47, 53, 59], "sccnnlayer": [41, 49, 83], "aggr_norm_func": [41, 49, 55], "chebyshev_conv": [41, 49, 55], "sccnn": [41, 48, 49], "sccnncomplex": [41, 48, 83], "scconvlay": [41, 51], "scconv": [41, 50, 51], "scn2layer": [41, 47, 53], "scn2": [41, 47, 52, 85], "scnnlayer": [41, 55, 86], "scnn": [41, 54, 55, 83], "sconelay": [41, 57, 87], "scone": [41, 56, 57, 59], "trajectoriesdataset": [41, 56, 87], "vectorize_path": [41, 56, 87], "generate_complex": [41, 56, 87], "generate_trajectori": [41, 56, 87], "n_filter": [42, 43], "order_harmon": 42, "epsilon_harmon": 42, "simplex_order_k": [42, 80], "simplic": [42, 45, 48, 55, 80, 82, 83, 86, 87], "approxim": [42, 43, 87], "filter": [42, 43, 49, 83], "harmon": 42, "1e": [42, 70, 74, 77, 87], "epsilon": 42, "laplacian": [42, 43, 44, 45, 48, 49, 53, 54, 55, 80, 81, 82, 83, 85], "calcul": [42, 60], "compon": [42, 60, 80], "hodg": [42, 49, 53, 55, 80, 82, 83], "laplacian_up": [42, 43, 54, 55, 80, 86], "laplacian_down": [42, 43, 54, 55, 80, 86], "channels_in": 42, "ld": [42, 78], "san_lay": 43, "07485": 43, "l_": [43, 55, 57, 80, 81, 86, 87], "wh_1": 43, "simplex": [43, 44, 52, 53, 63, 64, 66, 67, 68, 72, 73, 78, 79, 80, 82, 83, 84, 86, 87], "projection_mat": 43, "2p": [43, 80], "q_r": [43, 80], "n_cell": 43, "down_indic": 43, "n_cells_down": 43, "n_neighbor": 43, "up_indic": 43, "n_cells_up": 43, "sca": [44, 45, 59], "cmp": [44, 45], "sca_cmp": [44, 81], "channels_list": [44, 45, 81], "complex_dim": [44, 45, 81], "tetahedron": 44, "respect": [44, 59, 64, 65, 66, 67, 72, 73, 80, 83, 86, 87], "complex_dimens": 44, "highest": [44, 45, 87], "being": [44, 59, 80], "x_list": [44, 45], "laplacian_down_list": 44, "incidence_t_list": 44, "etc": [44, 55, 60], "start": [44, 59, 60, 75, 87], "autoencod": [45, 59], "sca_cmps_lay": 45, "coadjac": [45, 65], "chain": [45, 57, 87], "maroula": 45, "cai": 45, "2103": 45, "04046": 45, "down_lap_list": 45, "incidencet_list": 45, "qquad": [45, 49, 81], "hold": [45, 60], "untouch": 45, "max_rank": [46, 47, 82, 83, 86], "dict": [46, 47], "length": [46, 47, 60, 69, 71, 75], "n_rank_r_cel": [46, 47], "n_rank_r_minus_1_cel": [46, 47], "b_r": [46, 47, 64, 82], "h_r": [46, 47, 53, 82], "sccn_layer": [47, 53], "ani": [47, 48, 60, 62, 63, 73, 87], "leftmost": 47, "diagram": [47, 53, 59, 65], "yang22c": [47, 53, 85], "figur": [47, 53, 65, 70], "11": [47, 53, 59, 62, 65, 68, 69, 70, 71, 73, 74, 75, 76, 77, 78, 79, 80, 82, 83, 84, 86, 87], "scn2_layer": [47, 53], "scn": [47, 53], "abov": [47, 53, 59, 60, 86, 87], "below": [47, 53, 69, 76, 87], "sala": [47, 53, 85], "bogdan": [47, 53, 85], "effici": [47, 53, 78, 82, 83, 85], "proceed": [47, 53, 57, 85], "mlr": [47, 53, 57, 85], "press": [47, 53, 57, 85], "v198": [47, 53, 85], "yang22a": [47, 53, 85], "html": [47, 53, 57, 85], "describ": [47, 56, 60, 67], "unnorm": 47, "bigcup": [47, 82], "out_featur": [47, 62, 83, 86], "in_channels_al": [48, 83], "intermediate_channels_al": [48, 83], "out_channels_al": [48, 83], "conv_ord": [48, 49, 55, 83], "sc_order": [48, 49, 83], "task": [48, 54, 59, 66, 67, 69, 70, 71, 75, 78, 79, 82, 83, 87], "we": [48, 49, 54, 55, 57, 59, 60, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 84, 85, 87], "cours": [48, 60], "amend": 48, "intermedi": [48, 54, 65], "sc": [48, 49, 56, 81, 83, 86, 87], "x_all": [48, 49, 83], "laplacian_al": [48, 49, 83], "incidence_al": [48, 49, 83], "entri": [48, 80], "n_simplic": [48, 49, 54, 55], "l0": 48, "l1_d": 48, "l1_u": 48, "l2": 48, "b2": [48, 49, 65, 82, 83, 84, 86], "sccnn_layer": 49, "triangl": [49, 55, 56, 57, 65, 80, 83, 87], "To": [49, 54, 61, 65, 70, 78, 79, 86, 87], "too": 49, "mani": [49, 60, 62], "exampl": [49, 55, 59, 80, 83, 86, 87], "here": [49, 51, 55, 59, 60, 62, 69, 71, 75, 78, 79, 82, 83, 87], "pseudocod": [49, 55], "l_0": 49, "lap_down": [49, 55], "l_1_down": 49, "lap_up": [49, 55], "l_1_up": 49, "lap": 49, "l_2": 49, "y_0": 49, "y_1": 49, "y_2": 49, "look": [49, 55, 60, 87], "einsum": [49, 55], "weight_0": 49, "weight_1": 49, "weight_2": 49, "total_order_0": 49, "total_order_1": 49, "total_order_2": 49, "chebyshev": [49, 55], "conv_oper": [49, 55], "perform": [49, 54, 55, 59, 60, 62, 63, 64, 65, 66, 67, 68, 70, 72, 73, 74, 75, 77, 78, 79, 80, 82, 84, 85, 87], "num_channel": [49, 55], "repres": [49, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 75, 78, 79, 80, 85, 87], "n_triangl": [49, 57], "laplacian_0": [49, 52, 53, 83, 85, 86], "laplacian_down_1": [49, 81, 83, 86], "laplacian_up_1": [49, 83, 86], "laplacian_2": [49, 52, 53, 83, 85, 86], "part": [49, 55, 62, 78, 79, 80, 82, 83], "node_channel": [50, 51, 84], "edge_channel": [50, 51, 84], "face_channel": [50, 51, 84], "incidence_1_norm": [50, 51, 84], "incidence_2": [50, 51, 57, 64, 65, 83, 84, 86], "incidence_2_norm": [50, 51, 84], "adjacency_up_0_norm": [50, 51, 84], "adjacency_up_1_norm": [50, 51, 84], "adjacency_down_1_norm": [50, 51, 84], "adjacency_down_2_norm": [50, 51, 84], "_1": [50, 51, 83, 84, 86], "_2": [50, 51, 83], "scconv_lay": 51, "bunch": [51, 84], "fung": 51, "singh": [51, 59], "tda": 51, "forum": 51, "tlbnskrt6j": 51, "tild": [51, 84], "x0_out": 51, "x1_out": 51, "x2_out": 51, "For": [51, 54, 57, 60, 61, 65, 70, 73, 78, 79, 83, 86, 87], "mai": [51, 59], "helper": 51, "pyt": 51, "team": [51, 59], "laplacian_1": [52, 53, 85], "log": [53, 60, 82, 87], "rightmost": 53, "pshm23": 53, "2i": [53, 82], "node_featur": 53, "edge_featur": 53, "face_featur": 53, "l_upper": 53, "l_lower": 53, "conv_order_down": [54, 55, 86], "conv_order_up": [54, 55, 86], "aggr": [54, 86], "At": [54, 83, 86], "simplci": 54, "challeng": 54, "dimension": [54, 65, 67, 87], "scnn_layer": 55, "total_ord": 55, "isufi": 55, "leu": 55, "2110": 55, "02585": 55, "n_simplex": 55, "simplicialcomplex": [56, 74, 76, 77, 87], "hidden_dim": [56, 87], "trajectori": [56, 57], "dataset": [56, 59, 62, 63, 64, 65, 66, 67, 68, 69, 71, 72, 73, 74, 75, 76, 77, 80, 81], "path": [56, 87], "100": [56, 63, 68, 69, 70, 71, 74, 75, 76, 77, 81, 82, 84, 85, 87], "ndarrai": [56, 60, 87], "uniformli": [56, 87], "sampl": [56, 60, 62, 65, 70, 87], "random": [56, 62, 87], "point": [56, 87], "unit": [56, 59, 60, 87], "squar": [56, 60, 65, 87], "delaunai": [56, 87], "triangul": [56, 87], "delet": [56, 87], "some": [56, 60, 82, 87], "pre": [56, 59, 87], "disk": [56, 87], "coord": [56, 87], "n_max": [56, 87], "1000": [56, 70, 87], "corner": [56, 87], "middl": [56, 87], "scone_lay": 57, "when": [57, 59, 60, 62, 80, 87], "stack": [57, 62, 63, 64, 68, 70, 71, 74, 76, 77, 78, 79, 80, 82, 83, 86, 87], "befor": [57, 59, 60, 75, 87], "neighbour": [57, 87], "next": [57, 60, 70, 72, 75, 87], "roddenberri": [57, 59, 87], "mitchel": 57, "glaze": 57, "principl": [57, 87], "v139": 57, "roddenberry21a": 57, "variou": 58, "librari": 58, "torch_scatt": 58, "py": [58, 59, 60, 75, 78, 82, 83, 84, 86], "rusty1": 58, "pytorch_scatt": 58, "dim": [58, 60, 69, 71, 75, 78, 79, 80, 83, 86, 87], "dim_siz": 58, "welcom": [59, 60], "host": 59, "annual": 59, "topologi": [59, 80], "geometri": 59, "tag": 59, "machin": [59, 85], "review": [59, 60, 61], "contributor": [59, 60], "mathild": [59, 61], "mustafa": [59, 61], "nina": [59, 61], "florian": 59, "frantzen": 59, "ghada": [59, 61], "alzamzmi": 59, "theodor": [59, 61], "michael": [59, 61], "scholkemp": 59, "josef": 59, "hopp": 59, "karthikeyan": [59, 61], "natesan": [59, 61], "johan": 59, "math": [59, 60, 62, 67, 72, 73, 86, 87], "audun": 59, "myer": 59, "helen": 59, "jenn": 59, "tim": 59, "doster": 59, "tegan": 59, "emerson": 59, "henri": 59, "kving": 59, "bastian": [59, 85], "rieck": [59, 85], "sophia": [59, 61], "jan": 59, "meissner": 59, "paul": [59, 61, 85], "tolga": [59, 61], "vincent": 59, "grand": 59, "aldo": [59, 61], "tamal": [59, 61], "soham": [59, 61], "shreya": [59, 61], "neal": [59, 61], "robin": [59, 61], "edit": [59, 60], "now": [59, 62, 63, 66, 67, 68, 69, 70, 71, 72, 73, 75, 78, 79, 80, 85, 87], "thank": 59, "stellar": 59, "contirbut": 59, "foster": 59, "reproduc": [59, 61], "open": [59, 75], "research": [59, 85], "winner": 59, "announc": 59, "luca": 59, "scofano": 59, "claudio": 59, "guillermo": 59, "bernardez": 59, "simon": 59, "fiorellino": 59, "indro": 59, "spinelli": 59, "scardapan": 59, "lev": 59, "telyatninkov": 59, "olga": 59, "zaghen": 59, "sadrodin": 59, "barikbin": 59, "odin": 59, "hoff": 59, "gardaa": 59, "dmitrii": 59, "gavrilev": 59, "gleb": 59, "bazhenov": 59, "suraj": 59, "combinatori": 59, "rub\u00e9n": 59, "ballest": 59, "manuel": 59, "lecha": 59, "sergio": 59, "escalera": 59, "hoan": [59, 65], "aiden": 59, "brent": 59, "honor": 59, "mention": 59, "jen": 59, "agerberg": 59, "georg": 59, "b\u00f6kman": 59, "pavlo": 59, "melnyk": 59, "alessandro": 59, "salatiello": 59, "alexand": 59, "nikitin": 59, "purpos": [59, 60, 63, 64, 65, 66, 67, 68, 69, 71, 73, 74, 77, 78, 79, 80, 82, 83], "crowdsourc": 59, "ask": 59, "contribut": [59, 72, 83], "code": [59, 60, 87], "previous": 59, "exist": 59, "benchmark": [59, 62, 63, 64, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 81, 82, 83, 84, 85, 86], "python": [59, 60, 61], "packag": [59, 61, 75, 78, 82, 83, 84, 86], "take": [59, 62, 80, 87], "pull": [59, 60], "request": [59, 60, 75, 87], "literatur": [59, 61, 75], "leverag": [59, 80], "infrastructur": 59, "invit": 59, "regularli": 59, "white": 59, "summar": 59, "find": [59, 62, 87], "publish": 59, "qualifi": 59, "opportun": 59, "author": [59, 61, 66, 71, 87], "top": [59, 62, 83], "8": [59, 62, 64, 65, 66, 67, 68, 69, 70, 71, 72, 74, 75, 76, 77, 78, 79, 80, 82, 83, 85, 86, 87], "best": [59, 67], "addit": [59, 65], "softwar": [59, 60], "journal": 59, "special": 59, "recognit": 59, "date": 59, "time": [59, 62, 66, 67, 68, 69, 70, 71, 72, 73, 78, 79, 80, 82, 83, 86, 87], "must": [59, 66, 67, 68, 70, 72, 73, 78, 79, 80, 82, 83, 86], "juli": 59, "13": [59, 63, 65, 69, 70, 71, 74, 76, 77, 78, 79, 81, 82, 83, 84, 85, 86, 87], "16": [59, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 80, 81, 82, 83, 84, 85, 86, 87], "59": [59, 69, 70, 74, 78, 82], "pacif": 59, "standard": [59, 60, 75, 80], "modifi": [59, 60, 81], "until": 59, "everyon": [59, 60], "free": [59, 87], "suffici": 59, "accept": 59, "automat": [59, 80], "subscrib": 59, "encourag": 59, "earli": 59, "help": [59, 60], "debug": 59, "fail": 59, "test": [59, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 73, 74, 75, 76, 77, 78, 79, 80, 82, 83], "address": [59, 65], "potenti": 59, "issu": [59, 75], "similar": [59, 82], "qualiti": 59, "earlier": [59, 82], "prioriti": 59, "consider": 59, "restrict": 59, "member": 59, "than": [59, 86, 87], "princip": 59, "develop": [59, 60], "allow": [59, 82], "fig": [59, 87], "compli": 59, "action": 59, "workflow": 59, "successfulli": 59, "lint": 59, "format": [59, 60, 75, 83], "black": [59, 87], "isort": 59, "flake8": 59, "_layer": 59, "ex": 59, "store": [59, 70], "directori": [59, 60], "primit": 59, "equival": [59, 60, 65], "depict": 59, "_train": 59, "ipynb": 59, "hsn_train": 59, "tutori": [59, 61, 75], "process": [59, 60, 75], "well": [59, 60], "load": [59, 63, 64, 65, 66, 67, 68, 71, 72, 73, 78, 79, 81, 82, 83, 84, 85, 86], "toponetx": [59, 62, 63, 64, 65, 66, 67, 68, 70, 72, 73, 74, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87], "shrec16": [59, 63, 64, 65, 66, 67, 68, 72, 73, 81, 83, 86], "suitabl": [59, 70], "template_lay": 59, "karat": [59, 70, 78, 79, 80, 82, 83], "club": [59, 70, 78, 79, 80, 82, 83], "choic": [59, 62, 63, 64, 66, 67, 68, 72, 73, 74, 76, 77, 81], "along": [59, 62, 80], "simpl": [59, 86], "depend": 59, "accuraci": [59, 65, 70, 71, 74, 75, 76, 77, 78, 79, 80, 82, 83, 86, 87], "test_": [59, 60], "name_of_model": 59, "test_hsn_lay": 59, "testhsnlay": 59, "pleas": [59, 60, 62, 75, 80, 83, 86], "pytest": [59, 60], "unittest": 59, "further": [59, 62, 80], "manipul": 59, "modif": 59, "accompani": 59, "appropri": [59, 60], "locat": [59, 60, 75], "With": [59, 72], "said": 59, "highli": 59, "most": [59, 60, 80, 82], "resort": 59, "absolut": 59, "condorcet": 59, "decid": [59, 80], "criteria": 59, "chosen": [59, 82], "correctli": 59, "need": [59, 62, 63, 64, 66, 67, 68, 70, 72, 73, 78, 79, 80, 85, 87], "match": 59, "readabl": [59, 60], "clean": 59, "api": [59, 60], "written": 59, "clearli": 59, "explain": 59, "robust": 59, "reward": 59, "nor": 59, "goal": 59, "accur": 59, "our": [59, 60, 61, 62, 63, 64, 66, 67, 68, 70, 72, 73, 74, 76, 77, 78, 79, 80, 81, 82, 83, 85, 86, 87], "field": 59, "select": [59, 82, 86], "maintain": 59, "collabor": 59, "whose": [59, 60], "vote": 59, "onc": [59, 66, 67, 68, 72, 73], "googl": [59, 60], "express": [59, 72], "even": 59, "link": [59, 60], "record": [59, 60, 66, 67, 68, 72, 73], "email": 59, "identifi": 59, "voter": 59, "ident": [59, 65, 67, 78, 79, 83], "remain": [59, 87], "secret": 59, "feel": [59, 87], "contact": 59, "slack": 59, "ucsb": 59, "edu": [59, 65], "guid": 60, "aim": [60, 62, 80], "eas": 60, "both": [60, 62, 65, 80, 87], "novic": 60, "experienc": 60, "commun": 60, "effort": 60, "fork": 60, "upstream": 60, "submit": [60, 75], "pr": 60, "synchron": 60, "your": [60, 66], "branch": 60, "git": 60, "checkout": 60, "sure": 60, "section": [60, 65, 87], "re": [60, 78, 87], "done": [60, 63, 64, 65, 66, 67, 68, 69, 72, 73, 81, 83, 84, 85, 86, 87], "commit": 60, "modified_fil": 60, "my": [60, 81], "push": 60, "toponextx": 60, "instruct": 60, "repeat": 60, "folder": 60, "filenam": 60, "test_add": 60, "def": [60, 65, 71, 82, 83, 84, 86, 87], "test_capital_cas": 60, "assert": [60, 87], "9": [60, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 82, 83, 85, 86, 87], "statement": 60, "under": 60, "correct": [60, 62, 65, 74, 76, 77, 87], "instal": 60, "tool": 60, "pip": 60, "dev": 60, "verifi": 60, "break": 60, "doc": 60, "descript": [60, 65, 87], "usag": 60, "inform": [60, 65, 66, 67, 69, 72, 73, 86], "markdown": 60, "languag": 60, "common": [60, 62, 65], "restructuredtext": 60, "numpi": [60, 62, 63, 64, 65, 66, 67, 68, 70, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87], "style": 60, "understand": 60, "role": 60, "syntax": 60, "autom": 60, "pars": 60, "inclus": 60, "print": [60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87], "__doc__": 60, "attribut": 60, "try": [60, 62, 63, 80, 87], "np": [60, 62, 63, 64, 65, 66, 67, 68, 70, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87], "good": [60, 85], "These": 60, "ones": [60, 69, 71], "summari": 60, "line": 60, "79": [60, 69, 70, 74, 82, 86], "char": 60, "immedi": 60, "capit": 60, "letter": 60, "period": 60, "verb": 60, "imper": 60, "mood": 60, "possibl": [60, 75], "uncertain": 60, "oppos": 60, "evalu": [60, 62, 69, 70, 71, 75, 76], "separ": 60, "blank": 60, "argument": [60, 66, 67, 80], "On": 60, "state": [60, 73, 75], "rest": 60, "space": [60, 65, 83, 87], "side": 60, "default_valu": 60, "indent": 60, "esp": 60, "would": [60, 78, 79], "want": [60, 75, 87], "veri": [60, 75], "rais": [60, 83, 86], "latex": 60, "cite": [60, 75], "my_method": 60, "my_param_1": 60, "my_param_2": 60, "big": 60, "o": [60, 67, 70, 80], "short": 60, "my_result": 60, "relev": 60, "snippet": 60, "show": [60, 70, 78, 79, 87], "script": 60, "wikipedia": 60, "page": [60, 85], "And": 60, "fill": 60, "scikit": 60, "fit_predict": 60, "sample_weight": 60, "cluster": [60, 81], "center": [60, 87], "conveni": 60, "fit": 60, "sparse_matrix": 60, "n_featur": 60, "ignor": [60, 74, 77], "Not": 60, "present": [60, 62, 65], "convent": [60, 62], "observ": 60, "labels_": 60, "mind": 60, "instead": [60, 75, 83], "vari": 60, "notat": [60, 62, 63, 64, 65, 70, 78, 79, 80, 81, 82, 84, 86, 87], "axi": [60, 87], "bracket": 60, "multinomi": 60, "1d": 60, "2d": 60, "subset": [60, 62, 69, 71], "datafram": 60, "explicitli": 60, "relat": [60, 69], "colon": 60, "explan": 60, "_weight_boost": 60, "adaboost": 60, "great": 60, "ve": 60, "discuss": 60, "Of": 60, "verbos": 60, "thei": [60, 62, 66, 67, 78, 79, 80, 82, 83, 86], "rst": 60, "80": [60, 69, 70, 71, 74, 81, 82], "charact": 60, "except": [60, 62, 80], "tabl": 60, "tdl": 61, "blue": 61, "laid": 61, "extend": [61, 83], "avail": [61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 75, 80, 81], "about": [61, 65], "blueprint": 61, "misc": 61, "hajij2023topolog": 61, "titl": [61, 70], "year": 61, "eprint": 61, "archiveprefix": 61, "primaryclass": 61, "lg": 61, "papillon2023architectur": 61, "notebook": [62, 65, 66, 67, 68, 69, 70, 71, 72, 73, 78, 79, 80, 83, 84, 85, 86, 87], "didact": [62, 80], "clear": [62, 80], "technic": [62, 80], "document": [62, 69, 71, 75, 80], "sinc": [62, 78, 79, 82, 83, 87], "introduct": 62, "achiev": [62, 65, 67, 82, 87], "outstand": [62, 65], "howev": [62, 75, 83, 86], "pairwis": [62, 66, 67, 68, 72, 73, 87], "relationship": 62, "among": 62, "abl": [62, 80], "fulli": [62, 65], "interact": 62, "real": [62, 87], "world": [62, 87], "vertic": [62, 65, 87], "captur": 62, "particular": 62, "encod": [62, 70, 78, 79, 80, 83, 87], "design": 62, "independ": [62, 67], "thu": [62, 80, 85], "strategi": [62, 73], "approach": 62, "hierarch": [62, 65], "incorpor": 62, "algorithm": 62, "ii": 62, "optim": [62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87], "iii": 62, "extract": [62, 66, 67, 72, 73], "compact": 62, "meaning": 62, "remark": [62, 80], "custom": [62, 80], "symbol": [62, 80], "involv": [62, 80], "made": [62, 70, 78, 79, 80, 82, 83, 85, 86], "stage": 62, "nbsphinx": [62, 67, 72, 73, 86], "textrm": [62, 80], "parameter": 62, "mathbb": [62, 65, 66, 67, 72, 87], "2f_0": 62, "f_0": 62, "textbf": [62, 66, 67, 73, 80], "bigg": [62, 80, 83, 86], "f": [62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87], "respons": [62, 75], "reciproc": 62, "round": [62, 84], "tcdot": 62, "xin": 62, "score": [62, 87], "_r": 62, "coars": 62, "mutag": [62, 74, 76, 77], "tudataset": [62, 74, 76, 77], "paperswithcod": 62, "com": [62, 69, 71, 75], "__": [62, 65], "188": [62, 70, 74, 82], "chemic": 62, "compound": 62, "discret": 62, "mutagen": 62, "salmonella": 62, "typhimurium": 62, "sklearn": [62, 63, 64, 66, 67, 68, 69, 70, 71, 72, 73, 74, 76, 77, 81, 83, 85, 86], "model_select": [62, 63, 64, 66, 67, 68, 70, 72, 73, 74, 76, 77, 81, 83, 85, 86], "train_test_split": [62, 63, 64, 66, 67, 68, 70, 72, 73, 74, 76, 77, 81, 83, 85, 86], "cell_complex": [62, 63, 64], "cellcomplex": 62, "torch_geometr": [62, 69, 71, 74, 76, 77, 80], "convert": [62, 65, 70, 74, 76, 77, 78, 79, 80, 83, 85], "to_networkx": [62, 74, 76, 77, 80], "gpu": [62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 75, 81], "run": [62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 75, 82, 87], "cuda": [62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 75, 80, 81, 84, 85], "is_avail": [62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 75, 80, 81, 84, 85], "els": [62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 75, 80, 81, 83, 84, 85, 86], "root": [62, 74, 76, 77], "tmp": [62, 74, 75, 76, 77], "use_edge_attr": [62, 74, 76, 77], "use_node_attr": 62, "cc_list": [62, 63, 64], "x_0_list": 62, "x_1_list": [62, 74, 76, 77], "y_list": [62, 74, 76, 77], "append": [62, 63, 64, 65, 66, 67, 68, 70, 72, 73, 74, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87], "edge_attr": 62, "i_cc": 62, "th": [62, 63, 64, 65, 66, 67, 68, 72, 73, 80, 81, 84, 85], "36": [62, 65, 69, 70, 71, 74, 76, 77, 78, 82], "0th": [62, 84], "17": [62, 63, 65, 69, 70, 71, 74, 76, 77, 78, 81, 82, 83, 86], "38": [62, 65, 69, 70, 71, 74, 76, 77, 78, 82], "lower_neighborhood_list": 62, "upper_neighborhood_list": 62, "adjacency_0_list": [62, 63], "adjacency_matrix": [62, 63, 64, 65, 78, 79, 82, 83, 84, 87], "from_numpi": [62, 63, 64, 65, 66, 67, 68, 70, 71, 72, 73, 74, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86], "todens": [62, 63, 64, 65, 66, 67, 68, 70, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86], "to_spars": [62, 63, 64, 65, 66, 67, 68, 70, 72, 73, 76, 78, 79, 80, 81, 82, 83, 84, 85, 86], "lower_neighborhood_t": 62, "down_laplacian_matrix": [62, 78, 80, 81, 83, 84, 86], "upper_neighborhood_t": 62, "up_laplacian_matrix": [62, 80, 83, 84, 86], "32": [62, 65, 69, 70, 71, 72, 74, 76, 77, 78, 82, 87], "specifi": [62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 82, 83, 85, 86], "loss": [62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 82, 83, 84, 85, 86, 87], "without": [62, 63, 86], "6": [62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87], "crit": [62, 63, 65, 74, 76, 77], "crossentropyloss": [62, 63, 65, 69, 70, 71, 75, 76], "opt": [62, 63, 65, 66, 67, 68, 72, 73, 81, 82, 83, 84, 86], "adam": [62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87], "lr": [62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87], "001": [62, 65, 76, 85], "lift_lay": 62, "modulelist": [62, 83, 86], "lower_att": 62, "lin": 62, "64": [62, 66, 67, 69, 70, 74, 78, 82], "upper_att": 62, "lin_0": 62, "128": [62, 70, 74, 82], "lin_1": 62, "split": [62, 63, 64, 66, 67, 68, 70, 71, 73, 74, 75, 76, 77, 84, 87], "test_siz": [62, 63, 64, 66, 67, 68, 70, 72, 73, 74, 76, 77, 81, 83, 85, 86, 87], "x_1_train": [62, 63, 64, 74, 76, 77, 81, 83, 84], "x_1_test": [62, 63, 64, 74, 76, 77, 81, 83], "shuffl": [62, 63, 64, 65, 66, 67, 68, 72, 73, 74, 76, 77, 81, 83, 85, 86, 87], "x_0_train": [62, 63, 64, 66, 67, 68, 71, 72, 73, 81, 83, 84], "x_0_test": [62, 63, 64, 66, 67, 68, 71, 72, 73, 81, 83], "lower_neighborhood_train": 62, "lower_neighborhood_test": 62, "upper_neighborhood_train": 62, "upper_neighborhood_test": 62, "adjacency_0_train": [62, 63], "adjacency_0_test": [62, 63], "y_train": [62, 63, 64, 66, 67, 68, 70, 72, 73, 74, 76, 77, 78, 79, 80, 81, 82, 83, 85, 86], "y_test": [62, 63, 64, 66, 67, 68, 70, 72, 73, 74, 76, 77, 78, 79, 80, 81, 82, 83, 85, 86], "test_interv": [62, 63, 64, 65, 66, 67, 68, 70, 72, 73, 75, 78, 79, 80, 82, 83, 84, 85, 86], "num_epoch": [62, 63, 64, 65, 66, 67, 68, 70, 72, 73, 74, 75, 76, 77, 78, 79, 80, 82, 83, 84, 85, 86], "epoch_i": [62, 63, 64, 65, 66, 67, 68, 70, 72, 73, 78, 79, 80, 82, 83, 84, 85, 86], "rang": [62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 82, 83, 84, 85, 86, 87], "epoch_loss": [62, 63, 64, 65, 66, 67, 68, 72, 73, 78, 79, 80, 82, 83, 84, 85, 86], "num_sampl": 62, "zip": [62, 63, 64, 66, 67, 68, 72, 73, 74, 76, 77, 83, 84, 85, 86], "dtype": [62, 66, 67, 68, 69, 70, 71, 72, 73, 75, 83, 86], "long": [62, 65, 69, 71, 82], "zero_grad": [62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 82, 83, 84, 85, 86, 87], "argmax": [62, 65, 69, 71, 75, 76, 86, 87], "backward": [62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 82, 83, 84, 85, 86, 87], "train_acc": [62, 65, 69, 70, 75, 78, 79, 80, 82, 83, 86, 87], "epoch": [62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 82, 83, 84, 85, 86, 87], "4f": [62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 78, 79, 80, 82, 83, 85, 86], "flush": [62, 63, 64, 65, 66, 67, 68, 70, 72, 73, 78, 79, 80, 82, 83, 84, 85, 86], "no_grad": [62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 82, 83, 84, 85, 86], "test_acc": [62, 65, 69, 70, 75, 78, 79, 80, 82, 83, 86], "6251": 62, "6794": [62, 70], "5965": 62, "6007": 62, "6947": [62, 70], "6094": 62, "5881": 62, "5885": [62, 69], "7099": 62, "6316": 62, "5685": 62, "7252": 62, "5792": 62, "7176": 62, "6491": 62, "5614": 62, "7405": 62, "7368": 62, "small": [63, 64, 66, 67, 68, 72, 73, 81, 82, 83, 84, 85, 86], "14": [63, 65, 69, 70, 71, 74, 75, 76, 77, 78, 81, 82, 83, 86, 87], "3d": [63, 64, 65, 66, 67, 68, 72, 73, 81, 84, 85], "mesh": [63, 64, 66, 67, 68, 72, 73, 81, 83, 84, 85, 86], "15": [63, 65, 69, 70, 71, 74, 75, 76, 77, 78, 81, 82, 83, 86, 87], "shrec": [63, 64, 65, 66, 67, 68, 72, 73, 81, 84, 85, 86], "shrec_16": [63, 64, 65, 66, 67, 68, 72, 73, 81, 83, 84, 85, 86], "kei": [63, 64, 66, 67, 68, 72, 73, 81, 83, 84, 85, 86], "node_feat": [63, 64, 65, 66, 67, 68, 70, 72, 73, 78, 79, 80, 81, 82, 83, 84, 85, 86], "edge_feat": [63, 64, 65, 66, 67, 68, 70, 72, 73, 78, 79, 80, 81, 82, 83, 84, 85, 86], "face_feat": [63, 64, 65, 66, 67, 68, 72, 73, 78, 79, 80, 81, 82, 83, 84, 85, 86], "i_complex": [63, 64, 66, 67, 68, 72, 73, 81, 84, 85], "6th": [63, 64, 66, 67, 68, 72, 73, 81, 85], "252": [63, 64, 66, 67, 68, 70, 72, 73, 74, 81, 84, 85], "750": [63, 64, 66, 67, 68, 70, 72, 73, 81, 84, 85], "10": [63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87], "500": [63, 64, 65, 66, 67, 68, 70, 72, 73, 74, 81, 84, 85], "messg": [63, 66, 67, 72, 73, 78, 79, 85], "incidence_2_t_list": 63, "to_cell_complex": [63, 64], "incidence_2_t": [63, 81], "incidence_matrix": [63, 64, 65, 66, 67, 68, 70, 72, 73, 74, 76, 77, 78, 79, 80, 81, 82, 83, 84, 86], "18": [63, 69, 70, 71, 74, 76, 77, 78, 81, 82, 86], "19": [63, 69, 70, 71, 74, 76, 77, 78, 81, 82, 84, 87], "20": [63, 69, 70, 71, 74, 75, 76, 77, 78, 81, 82, 87], "loss_fn": [63, 66, 67, 68, 69, 71, 72, 73, 75, 81, 83, 84, 85, 86], "mseloss": [63, 64, 66, 67, 68, 72, 73, 81, 83, 84, 85, 86], "21": [63, 69, 70, 71, 74, 76, 77, 78, 81, 82, 85], "incidence_2_t_train": 63, "incidence_2_t_test": 63, "low": [63, 64, 65, 66, 67, 68, 69, 71, 73, 74, 77, 78, 79, 80, 82, 83, 84, 85], "minim": [63, 64, 65, 66, 67, 68, 69, 71, 73, 74, 77, 83], "rapid": [63, 64, 65, 66, 67, 68, 69, 71, 73, 74, 77, 83], "22": [63, 69, 70, 71, 74, 75, 76, 77, 78, 82], "test_loss": [63, 64, 66, 67, 68, 69, 72, 73, 83, 84, 85, 86], "93": [63, 69, 70, 71, 74, 82, 84], "0369": [63, 70], "83": [63, 64, 69, 70, 71, 74, 82], "5211": 63, "45": [63, 69, 70, 71, 74, 75, 76, 77, 78, 79, 80, 82, 83, 84, 86], "6080": 63, "85": [63, 68, 69, 70, 71, 74, 82, 86], "5098": 63, "81": [63, 69, 70, 74, 82], "9403": 63, "34": [63, 65, 69, 70, 71, 74, 76, 77, 78, 79, 80, 82, 83, 84, 86], "5233": 63, "23": [63, 69, 70, 71, 74, 76, 77, 78, 82], "24": [63, 69, 70, 71, 74, 76, 77, 78, 82], "7646": 63, "0686": [63, 70], "65": [63, 69, 70, 74, 78, 82], "3428": 63, "82": [63, 69, 70, 74, 82], "4051": 63, "0310": 63, "35": [63, 65, 69, 70, 71, 74, 75, 76, 77, 78, 82], "6043": 63, "interc": 64, "incidence_2_list": [64, 83, 84, 86], "adjacency_1_list": 64, "incidence_1_t_list": 64, "adjacency_1": [64, 65, 84], "incidence_1_t": [64, 81], "05": [64, 87], "criterion": [64, 70], "x_2_train": [64, 81, 83, 84], "x_2_test": [64, 81, 83], "adjacency_1_train": 64, "adjacency_1_test": 64, "incidence_2_train": [64, 83], "incidence_2_test": [64, 83], "incidence_1_t_train": 64, "incidence_1_t_test": 64, "107": [64, 70, 74, 82], "6411": 64, "84": [64, 69, 70, 71, 74, 82, 86], "9619": 64, "51": [64, 69, 70, 71, 74, 78, 82], "5892": 64, "2476": [64, 70], "7038": 64, "50": [64, 65, 69, 70, 71, 74, 75, 76, 77, 78, 82], "7303": 64, "sequenc": 65, "definit": [65, 86], "31": [65, 69, 70, 71, 74, 75, 76, 77, 78, 82, 83], "33": [65, 69, 70, 71, 74, 76, 77, 78, 82], "phi_u": 65, "phi_a": 65, "i_x": 65, "i_i": 65, "previou": 65, "actual": 65, "column": 65, "row": [65, 67], "2016": [65, 67, 69], "shapenet": 65, "stanford": 65, "480": [65, 70, 74], "30": [65, 69, 70, 71, 74, 75, 76, 77, 78, 79, 80, 82, 83, 84, 86, 87], "distinct": 65, "latter": 65, "entiti": 65, "direct": 65, "posit": 65, "p_v": 65, "coordin": [65, 87], "n_v": 65, "dihedr": 65, "angl": [65, 87], "span": 65, "theta_": 65, "area": 65, "n_f": 65, "theta_f": 65, "certain": [65, 66, 86], "dataload": 65, "hmc_layer": 65, "hmclayer": 65, "hmc": 65, "shrecdataset": 65, "npz": 65, "__init__": [65, 71, 84, 87], "cc": 65, "to_combinatorial_complex": 65, "a0": [65, 78, 79], "a1": 65, "coa2": 65, "_get_neighborhood_matrix": 65, "floattensor": [65, 75], "to_rank": 65, "setdiag": 65, "uniqu": [65, 69, 70, 71, 84], "channels_dim": 65, "__len__": [65, 87], "__getitem__": [65, 87], "idx": [65, 87], "shrec_train": 65, "shrec_test": 65, "full": [65, 70], "training_dataset": 65, "training_dataload": 65, "batch_siz": [65, 87], "37": [65, 69, 70, 71, 74, 76, 77, 78, 82], "testing_dataset": 65, "testing_dataload": 65, "classifi": [65, 70, 71, 75], "emploi": 65, "outlin": 65, "articl": 65, "integr": 65, "nodal": 65, "euclidean": [65, 87], "trainer": 65, "cross": 65, "entropi": 65, "hoanmeshclassifi": 65, "learning_r": 65, "_to_devic": 65, "move": 65, "el": 65, "25": [65, 69, 70, 71, 74, 75, 76, 77, 78, 82, 87], "interv": 65, "training_accuraci": 65, "_train_epoch": 65, "test_accuraci": [65, 78, 79, 80, 82, 83, 86], "training_sampl": 65, "total_loss": 65, "coadjacency_2": 65, "_compute_loss_and_upd": 65, "backpropag": 65, "ground": [65, 87], "truth": [65, 87], "eval": [65, 69, 71, 74, 75, 76, 77, 87], "test_sampl": 65, "moreov": 65, "neg": [65, 87], "slope": 65, "leaki": 65, "alreadi": 65, "almost": 65, "perfect": 65, "although": 65, "39": [65, 69, 70, 71, 74, 76, 77, 78, 82, 83, 84], "60": [65, 69, 70, 74, 78, 82, 87], "final_channel": 65, "channels_per_lay": 65, "defub": 65, "5354": 65, "0521": [65, 70], "1654": [65, 70, 78], "0688": [65, 70], "9049": 65, "1229": [65, 70], "6443": 65, "1917": [65, 70], "3331": 65, "2604": 65, "let": [65, 66, 67, 72, 73, 87], "longer": 65, "0427": [65, 70], "3438": 65, "7749": 65, "4167": 65, "5524": 65, "4958": 65, "4013": 65, "5333": [65, 78], "2587": 65, "6042": 65, "1615": [65, 70], "6208": 65, "0356": [65, 70], "6292": 65, "0127": [65, 71], "6604": 65, "9019": 65, "6937": [65, 70], "8633": 65, "7083": 65, "6750": [65, 70], "7822": 65, "7354": 65, "7564": 65, "7479": 65, "12": [65, 68, 69, 70, 71, 73, 74, 75, 76, 77, 78, 79, 80, 82, 83, 85, 86, 87], "7253": 65, "7500": [65, 70, 78, 82, 83], "6240": 65, "8063": 65, "6154": 65, "7979": 65, "6222": 65, "8021": 65, "5450": 65, "8042": 65, "5201": [65, 82], "8396": 65, "collect": [66, 67, 68, 69, 71, 72, 73, 86], "v_": [66, 67, 72], "e_": [66, 67], "rule": [66, 67], "put": [66, 67, 82], "f_": [66, 67], "permut": [66, 67], "invari": [66, 67], "parametr": [66, 67], "learnt": [66, 67], "solv": 66, "problem": [66, 73, 75], "what": [66, 67, 68, 72, 73, 74, 76, 77, 82], "feed": [66, 67, 68, 72, 73, 74, 76, 77, 87], "amtric": [66, 67, 68, 72, 73], "unsign": [66, 67, 68, 72, 73], "becom": [66, 67, 68, 72, 73, 80], "simplciial": [66, 67, 68, 72, 73], "wise": [66, 67, 68, 72, 73], "hg_list": [66, 67, 68, 72, 73, 74, 76, 77], "incidence_1_list": [66, 67, 68, 72, 73, 74, 76, 77, 83, 84, 86], "sign": [66, 67, 68, 70, 72, 73, 84, 87], "hg": [66, 67, 68, 72, 73, 74, 76, 77], "to_hypergraph": [66, 67, 68, 70, 72, 73, 74, 76, 77], "1250": [66, 67, 70, 72, 73], "incidence_1_train": [66, 67, 72, 73, 74, 76, 77, 83], "incidence_1_test": [66, 67, 72, 73, 74, 76, 77, 83], "to_edge_index": [66, 67, 72, 73], "274": [66, 67, 70, 72, 73, 74], "8176": 66, "529": [66, 67, 70, 72, 73], "0000": [66, 67, 70, 72, 73, 78, 79, 80, 82, 83, 86], "6125": [66, 67, 72, 73], "repo": [67, 71, 73], "rise": 67, "so": [67, 78, 79, 80, 82, 83, 86, 87], "dive": 67, "iter": 67, "Their": 67, "omega": 67, "overset": [67, 80], "delta": 67, "mathbin": 67, "ba": 67, "hf_": 67, "multihead": 67, "vaswani": 67, "load_ext": [67, 74, 77, 80], "autoreload": [67, 74, 77, 80], "q_n": 67, "9063": 67, "dir": 68, "templatelay": 68, "4791557": 68, "1252": [68, 70], "3105": 68, "2295": 68, "2015": 68, "3119": 68, "2987": 68, "2244": [68, 70], "4548": 68, "263": [68, 70, 74], "3355": 68, "280": [68, 70, 74], "4737": [68, 70], "110": [68, 70, 74, 75, 82], "2089": 68, "183": [68, 70, 74, 82], "3875": [68, 69], "26": [68, 69, 70, 71, 74, 76, 77, 78, 82], "0619": [68, 70], "cora": [69, 75], "2708": 69, "academ": [69, 71], "5429": 69, "citat": [69, 75], "categori": [69, 71], "case_bas": 69, "genetic_algorithm": 69, "neural_network": 69, "probabilistic_method": 69, "reinforcement_learn": 69, "rule_learn": 69, "theori": 69, "1433": [69, 70], "stand": [69, 71], "presenc": [69, 71], "planetoid": 69, "metric": [69, 70, 71], "accuracy_scor": [69, 71], "download": [69, 71, 75], "val": [69, 74, 76, 77, 87], "kimiyoung": 69, "raw": [69, 71, 75], "master": [69, 71, 75], "ind": 69, "tx": 69, "allx": 69, "ty": 69, "alli": 69, "manual_se": [69, 87], "41": [69, 70, 71, 74, 76, 77, 78, 82], "256": [69, 70, 74], "train_y_tru": [69, 71], "train_mask": [69, 71], "val_y_tru": 69, "val_mask": 69, "initial_x_1": 69, "zeros_lik": 69, "y_pred_logit": [69, 71], "train_loss": [69, 87], "val_loss": [69, 87], "val_acc": [69, 87], "acc": [69, 71], "2f": [69, 71], "1079": [69, 70], "1436": [69, 70], "0234": 69, "1016": [69, 70], "9800": 69, "0681": [69, 70], "9504": 69, "0389": [69, 70], "9194": 69, "0137": 69, "9241": 69, "9917": 69, "8917": 69, "9729": 69, "8710": 69, "9556": 69, "8574": 69, "29": [69, 70, 71, 74, 76, 77, 78, 82], "9402": 69, "8646": 69, "9265": 69, "8540": 69, "9137": 69, "8430": 69, "9012": 69, "8336": 69, "8886": 69, "8405": 69, "8775": 69, "8264": 69, "8668": 69, "8065": 69, "8562": 69, "8158": 69, "8456": 69, "7957": 69, "44": [69, 70, 71, 74, 76, 77, 78, 82], "8346": 69, "8028": 69, "8249": 69, "7882": 69, "8156": 69, "42": [69, 70, 71, 74, 76, 77, 78, 82, 83], "7912": 69, "8070": 69, "7610": 69, "46": [69, 70, 71, 74, 75, 76, 77, 78, 82], "7985": 69, "7617": 69, "47": [69, 70, 71, 74, 76, 77, 78, 82], "7903": 69, "7596": 69, "7827": 69, "7391": 69, "7737": 69, "7316": 69, "7650": 69, "27": [69, 70, 71, 74, 76, 77, 78, 82, 84], "7364": 69, "7559": 69, "43": [69, 70, 71, 74, 76, 77, 78, 82, 84], "28": [69, 70, 71, 74, 76, 77, 78, 82], "7184": 69, "48": [69, 70, 71, 74, 76, 77, 78, 82], "7454": 69, "7086": [69, 70], "7362": 69, "6815": 69, "7276": 69, "6673": 69, "7178": 69, "6846": [69, 70], "7080": 69, "6483": 69, "7007": 69, "6435": [69, 70], "54": [69, 70, 74, 75, 78, 82], "6982": 69, "6354": 69, "6998": 69, "6336": 69, "5937": 69, "6972": [69, 70], "56": [69, 70, 74, 78, 82], "6962": 69, "5970": 69, "55": [69, 70, 74, 78, 82], "6861": [69, 70], "5597": 69, "52": [69, 70, 74, 78, 82], "5444": 69, "6510": 69, "5500": 69, "6320": 69, "5396": 69, "6160": 69, "5096": 69, "6032": 69, "4991": 69, "5924": 69, "5020": 69, "58": [69, 70, 74, 78, 82], "5830": 69, "49": [69, 70, 71, 74, 76, 77, 78, 82], "4710": 69, "5741": 69, "4607": 69, "67": [69, 70, 74, 78, 82], "5691": [69, 70], "4339": 69, "62": [69, 70, 71, 74, 78, 82], "5616": 69, "4425": 69, "66": [69, 70, 74, 78, 82], "5615": 69, "4206": 69, "5477": 69, "4142": 69, "5281": [69, 70], "53": [69, 70, 74, 78, 82], "4085": 69, "5042": 69, "4017": 69, "4884": 69, "3852": 69, "4838": 69, "3908": 69, "61": [69, 70, 74, 78, 82], "4847": 69, "57": [69, 70, 74, 78, 82], "3434": 69, "71": [69, 70, 74, 78, 82], "4878": [69, 70], "3259": 69, "69": [69, 70, 74, 78, 82], "4851": 69, "3378": 69, "4907": 69, "2930": 69, "4966": 69, "3125": 69, "5017": 69, "3093": 69, "4984": 69, "63": [69, 70, 74, 82], "2770": 69, "4915": 69, "2753": 69, "4854": 69, "2741": 69, "4831": [69, 70], "2784": 69, "4728": 69, "2445": 69, "4712": 69, "68": [69, 70, 71, 74, 78, 82], "4743": 69, "1976": [69, 70], "4705": 69, "70": [69, 70, 74, 78, 82], "1895": [69, 70], "4638": 69, "4672": 69, "72": [69, 70, 74, 78, 82], "1565": [69, 70], "74": [69, 70, 74, 78, 82], "4450": 69, "73": [69, 70, 74, 78, 82], "1744": [69, 70], "4279": 69, "1863": [69, 70], "4286": 69, "75": [69, 70, 74, 76, 78, 82], "1765": [69, 70], "4333": [69, 78], "76": [69, 70, 74, 78, 82], "1773": [69, 70], "4499": 69, "77": [69, 70, 74, 78, 82, 85], "1232": [69, 70], "4701": 69, "78": [69, 70, 74, 78, 79, 80, 82, 83, 86], "0667": [69, 70], "4692": 69, "0716": [69, 70], "4593": 69, "0567": [69, 70], "0626": [69, 70], "4309": 69, "0453": [69, 70], "4244": 69, "0300": 69, "4135": 69, "0505": [69, 70], "4152": 69, "0212": 69, "4340": 69, "86": [69, 70, 74, 82], "0401": [69, 70], "4580": 69, "87": [69, 70, 74, 82, 85], "9888": 69, "4668": 69, "88": [69, 70, 74, 82, 86], "0185": 69, "4771": 69, "89": [69, 70, 74, 82], "9739": 69, "90": [69, 70, 74, 82, 86], "9517": 69, "4661": 69, "91": [69, 70, 71, 74, 82], "9341": 69, "4519": 69, "92": [69, 70, 71, 74, 82], "9642": 69, "4457": 69, "9191": 69, "4126": 69, "94": [69, 70, 71, 74, 82], "8985": 69, "4001": 69, "95": [69, 70, 74, 82], "8884": 69, "3864": 69, "96": [69, 70, 71, 74, 82], "9020": 69, "97": [69, 70, 71, 74, 82, 84, 85], "8673": 69, "3893": 69, "98": [69, 70, 71, 74, 82], "9479": 69, "3814": [69, 70], "99": [69, 70, 74, 75, 82, 84, 87], "8873": 69, "3855": 69, "8565": 69, "3936": 69, "against": [69, 71], "test_y_tru": [69, 71], "test_mask": [69, 71], "3152": 69, "karateclub": [70, 78, 79, 80, 82, 83], "matplotlib": [70, 71, 87], "pyplot": [70, 71, 87], "plt": [70, 71, 87], "www": [70, 78, 79, 80, 82, 83], "jstor": [70, 78, 79, 80, 82, 83], "stabl": [70, 78, 79, 80, 82, 83], "3629752": [70, 78, 79, 80, 82, 83], "singular": [70, 78, 79, 80, 82, 83], "social": [70, 78, 79, 80, 82, 83], "group": [70, 78, 79, 80, 82, 83], "dataset_sim": 70, "karate_club": [70, 78, 79, 80, 82, 83, 86], "complex_typ": [70, 78, 79, 80, 82, 83, 86], "dataset_hyp": 70, "santii": [70, 78, 79], "channels_": 70, "get_simplex_attribut": [70, 78, 79, 80, 82, 83, 86], "y_1h": 70, "ey": [70, 75, 78, 82], "astyp": 70, "stratifi": 70, "ind_train": 70, "ind_test": 70, "arang": 70, "random_st": 70, "float32": [70, 75], "int32": 70, "2000": 70, "get_accuraci": 70, "lambda": 70, "yhat": 70, "ytrue": 70, "nan": [70, 84], "y_hat_cl": 70, "nloss": 70, "ntrain_acc": 70, "7254": 70, "5000": [70, 82], "7232": 70, "7211": 70, "7190": 70, "7170": 70, "7152": 70, "7134": 70, "7117": 70, "7101": 70, "7072": 70, "7058": 70, "7046": 70, "7034": 70, "7024": 70, "7014": 70, "7005": 70, "6997": 70, "6990": 70, "6983": 70, "6977": 70, "6967": 70, "6963": 70, "6960": 70, "6957": 70, "6954": 70, "6952": 70, "6951": 70, "6949": 70, "6948": 70, "6946": 70, "1429": [70, 78], "3571": 70, "6945": 70, "6944": 70, "6943": 70, "6942": 70, "6941": 70, "3214": 70, "0714": 70, "1071": 70, "6940": 70, "6939": 70, "6938": 70, "6936": 70, "6935": 70, "101": [70, 74, 82], "102": [70, 74, 82], "103": [70, 74, 82, 84, 86], "104": [70, 74, 82, 85], "105": [70, 74, 82, 84], "106": [70, 74, 82], "108": [70, 74, 75, 82], "109": [70, 74, 75, 82], "6934": 70, "111": [70, 74, 75, 82], "112": [70, 74, 82], "113": [70, 74, 75, 82, 84], "114": [70, 74, 82], "115": [70, 74, 82], "116": [70, 74, 82], "117": [70, 74, 82], "118": [70, 74, 82], "119": [70, 74, 82], "6933": 70, "120": [70, 74, 82], "121": [70, 74, 82, 86], "122": [70, 74, 82], "123": [70, 74, 82], "124": [70, 74, 82], "125": [70, 74, 82], "126": [70, 74, 82], "127": [70, 74, 82], "129": [70, 74, 82], "130": [70, 74, 82], "6932": 70, "131": [70, 74, 82], "132": [70, 74, 82], "133": [70, 74, 75, 82], "134": [70, 74, 82], "135": [70, 74, 82], "136": [70, 74, 82], "137": [70, 74, 82], "138": [70, 74, 82], "139": [70, 74, 82], "140": [70, 74, 82], "141": [70, 74, 82], "6931": 70, "142": [70, 74, 82], "143": [70, 74, 78, 82, 83], "144": [70, 74, 82], "145": [70, 74, 82], "146": [70, 74, 82], "147": [70, 74, 82, 85], "148": [70, 74, 82], "149": [70, 74, 82], "150": [70, 74, 82, 87], "151": [70, 74, 82], "152": [70, 74, 82], "6930": 70, "153": [70, 74, 82], "154": [70, 74, 82], "155": [70, 74, 82], "156": [70, 74, 82], "157": [70, 74, 82], "158": [70, 74, 82], "159": [70, 74, 82], "160": [70, 74, 82], "161": [70, 74, 82], "162": [70, 74, 82], "163": [70, 74, 82], "6929": 70, "164": [70, 74, 82], "165": [70, 74, 82], "166": [70, 74, 82], "167": [70, 74, 82], "168": [70, 74, 82], "169": [70, 74, 82], "170": [70, 74, 82], "171": [70, 74, 82], "172": [70, 74, 82], "173": [70, 74, 82], "6928": 70, "174": [70, 74, 82], "175": [70, 74, 82], "176": [70, 74, 82], "177": [70, 74, 82, 86], "178": [70, 74, 82], "179": [70, 74, 82], "180": [70, 74, 82], "181": [70, 74, 82], "182": [70, 74, 82], "6927": 70, "184": [70, 74, 82], "185": [70, 74, 75, 82], "186": [70, 74, 82], "187": [70, 74, 82], "189": [70, 74, 82], "190": [70, 74, 82], "6926": 70, "191": [70, 74, 82], "192": [70, 74, 75, 82], "193": [70, 74, 82], "194": [70, 74, 82], "195": [70, 74, 82], "196": [70, 74, 82], "197": [70, 74, 82], "6925": 70, "198": [70, 74, 82, 85, 86], "199": [70, 74, 75, 82], "200": [70, 74, 75, 82], "201": [70, 74], "202": [70, 74], "203": [70, 74], "6924": 70, "204": [70, 74], "205": [70, 74], "206": [70, 74], "207": [70, 74], "208": [70, 74], "6923": 70, "209": [70, 74], "210": [70, 74], "211": [70, 74], "212": [70, 74], "213": [70, 74], "6922": 70, "214": [70, 74, 83], "215": [70, 74], "216": [70, 74], "217": [70, 74], "6921": 70, "218": [70, 74], "219": [70, 74], "220": [70, 74], "221": [70, 74], "6920": 70, "222": [70, 74], "223": [70, 74], "224": [70, 74], "225": [70, 74], "6919": 70, "226": [70, 74], "227": [70, 74, 85], "228": [70, 74], "229": [70, 74], "6918": [70, 80], "230": [70, 74], "231": [70, 74], "232": [70, 74, 85], "6917": 70, "233": [70, 74], "234": [70, 74], "235": [70, 74], "6916": 70, "236": [70, 74], "237": [70, 74], "6915": 70, "238": [70, 74], "239": [70, 74], "240": [70, 74], "6914": 70, "241": [70, 74], "242": [70, 74], "6913": 70, "243": [70, 74], "244": [70, 74], "245": [70, 74], "6912": 70, "246": [70, 74], "247": [70, 74], "6911": 70, "248": [70, 74], "249": [70, 74], "6910": 70, "250": [70, 74], "251": [70, 74], "6909": 70, "253": [70, 74], "6908": 70, "254": [70, 74], "6907": 70, "255": [70, 74, 75], "6906": 70, "257": [70, 74], "258": [70, 74], "6905": 70, "259": [70, 74], "6904": 70, "260": [70, 74], "261": [70, 74], "6903": 70, "262": [70, 74], "6902": 70, "264": [70, 74], "6901": 70, "265": [70, 74], "6900": 70, "266": [70, 74], "6899": 70, "267": [70, 74], "268": [70, 74], "6898": 70, "269": [70, 74], "6897": 70, "270": [70, 74, 78], "6896": 70, "271": [70, 74], "6895": 70, "272": [70, 74], "6894": 70, "273": [70, 74], "6893": 70, "275": [70, 73, 74], "6892": 70, "276": [70, 74], "6891": [70, 82], "277": [70, 74], "6890": 70, "278": [70, 74], "6889": 70, "279": [70, 74], "6888": 70, "6886": 70, "281": [70, 74], "6885": 70, "282": [70, 74, 85], "6884": 70, "283": [70, 74], "6883": 70, "284": [70, 74], "6882": 70, "285": [70, 74], "6881": 70, "286": [70, 74], "6879": [70, 78], "287": [70, 74], "6878": 70, "288": [70, 74], "6877": 70, "289": [70, 74], "6876": 70, "290": [70, 74], "6874": 70, "5357": 70, "291": [70, 74], "6873": 70, "292": [70, 74], "6871": 70, "293": [70, 74], "6870": 70, "294": [70, 74], "6868": 70, "295": [70, 74], "6867": 70, "296": [70, 74], "6865": 70, "297": [70, 74], "6864": 70, "298": [70, 74], "6862": 70, "299": [70, 74], "300": [70, 74], "6859": 70, "5714": 70, "6667": [70, 82, 83], "301": [70, 74], "6857": 70, "302": [70, 74, 75], "6855": 70, "303": [70, 74], "6854": 70, "304": [70, 74, 85], "6852": 70, "305": [70, 74], "6850": 70, "306": [70, 74], "6848": 70, "307": [70, 74], "308": [70, 74], "6844": 70, "6429": 70, "309": [70, 74], "6842": 70, "310": [70, 74], "6840": 70, "311": [70, 74], "6838": 70, "312": [70, 74], "6835": 70, "313": [70, 74], "6833": 70, "314": [70, 74], "6831": 70, "315": [70, 74], "6829": 70, "316": [70, 74], "6826": 70, "317": [70, 74], "6824": 70, "318": [70, 74], "6821": 70, "319": [70, 74], "6819": 70, "320": [70, 74], "6816": 70, "321": [70, 74], "6814": [70, 80], "322": [70, 74], "6811": 70, "323": [70, 74], "6808": 70, "324": [70, 74], "6806": 70, "325": [70, 74], "6803": 70, "326": [70, 74], "6800": 70, "327": [70, 74], "6797": 70, "328": [70, 74], "329": [70, 74], "6791": 70, "330": [70, 74], "6788": 70, "331": [70, 74], "6785": 70, "332": [70, 74], "6782": 70, "333": [70, 74], "6778": 70, "334": [70, 74], "6775": 70, "335": [70, 74], "6772": 70, "336": [70, 74], "6768": 70, "337": [70, 74], "6765": 70, "338": [70, 74], "6761": 70, "339": [70, 74], "6758": 70, "340": [70, 74], "6754": 70, "341": [70, 74], "342": [70, 74], "6746": 70, "343": [70, 74], "6742": 70, "344": [70, 74], "6738": 70, "345": [70, 74], "6734": 70, "346": [70, 74], "6730": 70, "347": [70, 74], "6726": 70, "6786": 70, "348": [70, 74], "6722": 70, "349": [70, 74], "6717": [70, 80], "350": [70, 74], "6713": 70, "351": [70, 74], "6708": 70, "352": [70, 74], "6704": 70, "353": [70, 74], "6699": 70, "354": [70, 74], "6694": 70, "355": [70, 74], "6690": 70, "356": [70, 74], "6685": 70, "357": [70, 74], "6680": 70, "358": [70, 74], "6675": 70, "359": [70, 74], "6670": 70, "360": [70, 74], "6664": 70, "7143": 70, "361": [70, 74], "6659": 70, "362": [70, 74], "6654": 70, "363": [70, 74], "6648": 70, "364": [70, 74], "6642": 70, "365": [70, 74], "6637": 70, "366": [70, 74], "6631": 70, "367": [70, 74], "6625": 70, "8214": 70, "368": [70, 74], "6619": 70, "369": [70, 74], "6613": 70, "370": [70, 74], "6607": 70, "371": [70, 74], "6600": 70, "372": [70, 74], "6594": 70, "373": [70, 74], "6588": 70, "374": [70, 74], "6581": 70, "375": [70, 74], "6574": 70, "376": [70, 74], "6568": 70, "377": [70, 74], "6561": 70, "378": [70, 74], "6554": 70, "379": [70, 74], "6547": 70, "380": [70, 74], "6539": 70, "381": [70, 74], "6532": 70, "382": [70, 74], "6525": 70, "383": [70, 74], "6517": 70, "384": [70, 74], "6509": 70, "385": [70, 74], "6502": 70, "386": [70, 74], "6494": 70, "387": [70, 74], "6486": 70, "8571": 70, "388": [70, 74], "6478": 70, "389": [70, 74], "6469": 70, "390": [70, 74], "6461": 70, "391": [70, 74], "6453": 70, "392": [70, 74], "6444": 70, "393": [70, 74], "394": [70, 74], "6427": 70, "395": [70, 74, 75], "6418": 70, "396": [70, 74], "6409": 70, "397": [70, 74], "6399": 70, "398": [70, 74], "6390": 70, "6381": 70, "400": [70, 71, 74], "6371": 70, "8333": [70, 82], "401": [70, 74], "6361": [70, 83], "402": [70, 74], "6351": 70, "403": [70, 74], "6342": 70, "404": [70, 74], "6331": 70, "405": [70, 74], "6321": 70, "406": [70, 74], "6311": 70, "407": [70, 74], "6300": 70, "408": [70, 74, 83], "6290": [70, 78], "409": [70, 74], "6279": 70, "410": [70, 74], "6268": 70, "411": [70, 74], "6257": 70, "412": [70, 74], "6246": 70, "413": [70, 74], "6235": 70, "6223": 70, "415": [70, 74], "6212": 70, "416": [70, 74], "6200": 70, "417": [70, 74], "6188": 70, "418": [70, 74], "6176": 70, "419": [70, 74], "6164": 70, "420": [70, 74], "6152": 70, "421": [70, 74], "6139": 70, "422": [70, 74], "6127": 70, "423": [70, 74], "6114": 70, "424": [70, 74], "6101": 70, "425": [70, 74], "6088": 70, "426": [70, 74], "6075": 70, "427": [70, 74], "6061": 70, "428": [70, 74], "6048": 70, "429": [70, 74], "6034": 70, "430": [70, 74], "6020": 70, "431": [70, 74], "6006": 70, "432": [70, 74], "5992": 70, "433": [70, 74], "5978": 70, "434": [70, 74], "5963": 70, "435": [70, 74], "5949": 70, "436": [70, 74], "5934": 70, "8929": 70, "437": [70, 74], "5919": 70, "438": [70, 74], "5904": 70, "439": [70, 74], "5888": 70, "440": [70, 74], "5873": 70, "441": [70, 74], "5857": 70, "442": [70, 74], "5841": 70, "443": [70, 74, 75], "5825": 70, "9286": 70, "444": [70, 74], "5809": 70, "445": [70, 74], "5793": 70, "446": [70, 74], "5776": 70, "447": [70, 74], "5759": 70, "448": [70, 74], "5742": 70, "449": [70, 74], "5725": 70, "450": [70, 74], "5708": 70, "451": [70, 74], "452": [70, 74], "5673": 70, "453": [70, 74], "5655": 70, "454": [70, 74], "5637": 70, "455": [70, 74], "5619": 70, "456": [70, 74], "5600": 70, "457": [70, 74], "5582": 70, "458": [70, 74], "5563": 70, "459": [70, 74], "5544": [70, 78], "460": [70, 74], "5525": 70, "461": [70, 74], "5506": 70, "462": [70, 74], "5486": 70, "463": [70, 74], "5466": 70, "464": [70, 74], "5447": 70, "465": [70, 74], "5427": 70, "466": [70, 74], "5406": 70, "467": [70, 74], "5386": 70, "468": [70, 74], "5365": 70, "469": [70, 74], "5345": 70, "470": [70, 74], "5324": 70, "471": [70, 74], "5303": 70, "472": [70, 74], "473": [70, 74], "5260": 70, "474": [70, 74], "5239": 70, "475": [70, 74], "5217": 70, "476": [70, 74], "5195": 70, "477": [70, 74], "5173": 70, "478": [70, 74], "5151": 70, "479": [70, 74], "5129": 70, "5107": 70, "481": [70, 74], "5084": 70, "482": [70, 74], "5062": 70, "483": [70, 74], "5039": 70, "484": [70, 74], "5016": [70, 82], "485": [70, 74], "4993": 70, "486": [70, 74], "4970": 70, "487": [70, 74], "4947": 70, "488": [70, 74], "4924": 70, "489": [70, 74], "4901": 70, "490": [70, 74], "491": [70, 74], "4855": 70, "492": [70, 74], "493": [70, 74], "4808": 70, "494": [70, 74], "4784": 70, "495": [70, 74], "4761": 70, "496": [70, 74], "497": [70, 74], "4714": 70, "498": [70, 74], "4690": 70, "499": [70, 74], "4667": 70, "4643": 70, "501": 70, "4619": 70, "502": 70, "4596": 70, "503": 70, "4572": 70, "504": 70, "4549": 70, "505": 70, "4525": 70, "506": 70, "4501": 70, "507": 70, "4478": 70, "508": 70, "4454": 70, "509": 70, "4431": 70, "510": 70, "4407": 70, "511": 70, "4384": 70, "512": 70, "4360": 70, "513": 70, "4337": 70, "514": [70, 85], "4313": 70, "515": 70, "4290": 70, "516": 70, "4267": 70, "517": 70, "4243": 70, "518": 70, "4220": 70, "519": 70, "4197": 70, "520": 70, "4174": 70, "521": 70, "4151": 70, "522": 70, "4128": 70, "523": 70, "4105": 70, "524": 70, "4082": 70, "525": 70, "4059": 70, "526": 70, "4036": 70, "527": 70, "4014": 70, "528": 70, "3991": 70, "3969": 70, "530": 70, "3946": 70, "531": 70, "3924": 70, "532": 70, "3902": 70, "533": 70, "3880": 70, "534": 70, "3858": 70, "535": 70, "3836": 70, "536": [70, 83, 86], "537": 70, "3792": 70, "538": 70, "3770": 70, "539": 70, "3749": 70, "540": 70, "3727": 70, "541": 70, "3706": 70, "542": 70, "3685": 70, "543": 70, "3664": 70, "544": 70, "3643": 70, "545": 70, "3622": 70, "546": [70, 84], "3601": 70, "547": 70, "3580": 70, "548": 70, "3560": 70, "549": 70, "3539": 70, "550": 70, "3519": 70, "551": 70, "3499": 70, "552": 70, "3479": 70, "553": 70, "3459": 70, "554": 70, "3439": 70, "555": 70, "3419": 70, "556": 70, "3400": 70, "557": 70, "3380": 70, "558": 70, "3361": 70, "559": 70, "3342": 70, "560": 70, "3323": 70, "561": 70, "3304": 70, "562": 70, "3285": 70, "563": 70, "3266": 70, "564": 70, "3248": 70, "565": 70, "3229": 70, "9643": 70, "566": 70, "3211": 70, "567": 70, "3193": 70, "568": 70, "3175": 70, "569": 70, "3157": 70, "570": 70, "3139": 70, "571": 70, "3121": 70, "572": 70, "3103": 70, "573": 70, "3086": 70, "574": 70, "3069": 70, "575": 70, "3052": [70, 71], "576": 70, "3034": 70, "577": 70, "3018": [70, 78], "578": 70, "3001": 70, "579": 70, "2984": 70, "580": 70, "2967": 70, "581": 70, "2951": 70, "582": 70, "2935": 70, "583": 70, "2919": 70, "584": 70, "2902": 70, "585": [70, 74], "2887": 70, "586": 70, "2871": 70, "587": 70, "2855": 70, "588": 70, "2839": 70, "589": 70, "2824": 70, "590": 70, "2809": 70, "591": 70, "2794": 70, "592": 70, "2778": 70, "593": 70, "2763": 70, "594": 70, "2749": 70, "595": 70, "2734": 70, "596": 70, "2719": 70, "597": 70, "2705": 70, "598": 70, "2690": 70, "599": 70, "2676": 70, "600": 70, "2662": 70, "601": 70, "2648": 70, "602": 70, "2634": 70, "603": 70, "2620": 70, "604": 70, "2607": 70, "605": 70, "2593": 70, "606": 70, "2579": 70, "607": 70, "2566": 70, "608": 70, "2553": 70, "609": 70, "2540": 70, "610": 70, "2527": 70, "611": 70, "2514": 70, "612": 70, "2501": 70, "613": [70, 87], "2488": 70, "614": 70, "615": 70, "2463": 70, "616": 70, "2451": 70, "617": 70, "2438": 70, "618": 70, "2426": [70, 71], "619": 70, "2414": 70, "620": 70, "2402": 70, "621": 70, "2390": 70, "622": 70, "2378": 70, "623": 70, "2367": 70, "624": 70, "2355": 70, "625": [70, 74], "2344": 70, "626": 70, "2332": 70, "627": 70, "2321": 70, "628": 70, "2310": 70, "629": 70, "2299": 70, "630": 70, "2287": 70, "631": 70, "2277": 70, "632": 70, "2266": 70, "633": 70, "2255": 70, "634": 70, "635": [70, 83], "2234": 70, "636": 70, "2223": 70, "637": 70, "2213": 70, "638": 70, "2202": 70, "639": 70, "2192": 70, "640": 70, "2182": 70, "641": 70, "2172": 70, "642": 70, "2162": 70, "643": 70, "2152": 70, "644": 70, "2142": 70, "645": 70, "2132": 70, "646": 70, "2123": 70, "647": 70, "2113": 70, "648": 70, "2104": 70, "649": 70, "2094": 70, "650": 70, "2085": 70, "651": 70, "2076": 70, "652": 70, "2066": 70, "653": 70, "2057": 70, "654": 70, "2048": 70, "655": 70, "2039": 70, "656": 70, "2030": 70, "657": 70, "658": 70, "2013": 70, "659": 70, "2004": 70, "660": 70, "1995": 70, "661": 70, "1987": 70, "662": 70, "1978": 70, "663": 70, "1970": 70, "664": 70, "1962": [70, 78], "665": 70, "1953": 70, "666": 70, "1945": 70, "667": 70, "1937": 70, "668": 70, "1929": 70, "669": 70, "1921": 70, "670": 70, "1913": [70, 82], "671": 70, "1905": 70, "672": 70, "1897": 70, "673": 70, "1889": 70, "674": 70, "1882": 70, "675": 70, "1874": 70, "676": 70, "1866": 70, "677": 70, "1859": 70, "678": 70, "1851": [70, 78], "679": 70, "1844": 70, "680": 70, "1837": 70, "681": 70, "1829": 70, "682": 70, "1822": 70, "683": 70, "1815": 70, "684": 70, "1808": 70, "685": 70, "1801": 70, "686": 70, "1794": 70, "687": 70, "1787": 70, "688": 70, "1780": [70, 78], "689": 70, "690": 70, "1766": 70, "691": 70, "1759": 70, "692": 70, "1753": 70, "693": 70, "1746": 70, "694": 70, "1740": 70, "695": 70, "1733": 70, "696": 70, "1726": 70, "697": 70, "1720": 70, "698": 70, "1714": 70, "699": 70, "1707": 70, "700": 70, "1701": 70, "701": 70, "1695": 70, "702": 70, "1688": 70, "703": 70, "1682": 70, "704": 70, "1676": 70, "705": 70, "1670": 70, "706": 70, "1664": 70, "707": 70, "1658": 70, "708": 70, "1652": 70, "709": 70, "1646": 70, "710": 70, "1640": 70, "711": 70, "1635": 70, "712": 70, "1629": 70, "713": 70, "1623": 70, "714": 70, "1617": 70, "715": 70, "1612": 70, "716": 70, "1606": 70, "717": 70, "1601": 70, "718": 70, "1595": 70, "719": 70, "1590": 70, "720": 70, "1584": 70, "721": 70, "1579": 70, "722": 70, "1573": 70, "723": 70, "1568": 70, "724": 70, "1563": 70, "725": 70, "1557": 70, "726": 70, "1552": 70, "727": 70, "1547": 70, "728": 70, "1542": 70, "729": 70, "1537": 70, "730": 70, "1532": 70, "731": 70, "1527": 70, "732": 70, "1522": 70, "733": 70, "1517": 70, "734": 70, "1512": 70, "735": 70, "1507": 70, "736": 70, "1502": 70, "737": 70, "1497": 70, "738": 70, "1492": 70, "739": 70, "1488": 70, "740": 70, "1483": 70, "741": 70, "1478": 70, "742": [70, 74], "1473": [70, 78], "743": 70, "1469": 70, "744": 70, "1464": 70, "745": 70, "1460": 70, "746": 70, "1455": 70, "747": 70, "1451": 70, "748": 70, "1446": 70, "749": 70, "1442": 70, "1437": 70, "751": 70, "752": 70, "1428": 70, "753": 70, "1424": 70, "754": 70, "1420": 70, "755": 70, "1415": 70, "756": 70, "1411": 70, "757": 70, "1407": 70, "758": 70, "1403": 70, "759": 70, "1399": 70, "760": 70, "1394": 70, "761": 70, "1390": 70, "762": 70, "1386": 70, "763": 70, "1382": 70, "764": 70, "1378": 70, "765": 70, "1374": 70, "766": 70, "1370": 70, "767": 70, "1366": 70, "768": 70, "1362": 70, "769": 70, "1358": 70, "770": 70, "1354": 70, "771": 70, "1351": 70, "772": 70, "1347": 70, "773": 70, "1343": [70, 78], "774": 70, "1339": 70, "775": 70, "1335": 70, "776": 70, "1332": [70, 78], "777": 70, "1328": 70, "778": 70, "1324": 70, "779": 70, "1320": 70, "780": 70, "1317": 70, "781": 70, "1313": 70, "782": 70, "1310": 70, "783": 70, "1306": 70, "784": 70, "1302": 70, "785": 70, "1299": 70, "786": 70, "1295": 70, "787": 70, "1292": 70, "788": 70, "1288": [70, 78], "789": 70, "1285": 70, "790": 70, "1281": 70, "791": 70, "1278": 70, "792": 70, "1275": 70, "793": 70, "1271": 70, "794": 70, "1268": 70, "795": 70, "1265": 70, "796": 70, "1261": 70, "797": 70, "1258": 70, "798": 70, "1255": [70, 78], "799": 70, "1251": 70, "800": 70, "1248": 70, "801": 70, "1245": 70, "802": 70, "1242": 70, "803": 70, "1239": 70, "804": 70, "1235": 70, "805": 70, "806": 70, "807": 70, "1226": 70, "808": 70, "1223": 70, "809": 70, "1220": 70, "810": 70, "1217": 70, "811": 70, "1214": 70, "812": 70, "1211": 70, "813": 70, "1208": 70, "814": 70, "1205": 70, "815": 70, "1202": 70, "816": 70, "1199": 70, "817": 70, "1196": 70, "818": 70, "1193": 70, "819": 70, "1190": 70, "820": 70, "1187": 70, "821": 70, "1184": 70, "822": 70, "1181": 70, "823": 70, "1178": 70, "824": 70, "1176": 70, "825": 70, "1173": 70, "826": 70, "1170": 70, "827": 70, "1167": 70, "828": 70, "1164": 70, "829": 70, "1162": 70, "830": 70, "1159": 70, "831": 70, "1156": 70, "832": 70, "1153": 70, "833": 70, "1151": 70, "834": 70, "1148": 70, "835": 70, "1145": 70, "836": 70, "1143": 70, "837": 70, "1140": 70, "838": 70, "1137": 70, "839": 70, "1135": 70, "840": 70, "1132": 70, "841": 70, "1130": 70, "842": 70, "1127": 70, "843": 70, "1125": 70, "844": 70, "1122": 70, "845": 70, "1119": 70, "846": 70, "1117": 70, "847": 70, "1114": 70, "848": 70, "1112": 70, "849": 70, "1109": 70, "850": 70, "1107": [70, 71], "851": 70, "1105": 70, "852": 70, "1102": 70, "853": 70, "1100": 70, "854": 70, "1097": 70, "855": 70, "1095": 70, "856": 70, "1092": 70, "857": 70, "1090": 70, "858": 70, "1088": 70, "859": 70, "1085": [70, 71], "860": 70, "1083": 70, "861": 70, "1081": 70, "862": 70, "1078": 70, "863": 70, "1076": 70, "864": 70, "1074": 70, "865": 70, "866": 70, "1069": 70, "867": 70, "1067": 70, "868": 70, "1065": 70, "869": 70, "1062": 70, "870": 70, "1060": [70, 83], "871": 70, "1058": 70, "872": 70, "1056": 70, "873": 70, "1054": 70, "874": 70, "1051": 70, "875": 70, "1049": 70, "876": 70, "1047": 70, "877": 70, "1045": 70, "878": 70, "1043": 70, "879": 70, "1041": 70, "880": 70, "1038": 70, "881": 70, "1036": 70, "882": 70, "1034": 70, "883": 70, "1032": 70, "884": 70, "1030": 70, "885": 70, "1028": 70, "886": 70, "1026": 70, "887": 70, "1024": 70, "888": 70, "1022": 70, "889": 70, "1020": 70, "890": 70, "1018": 70, "891": 70, "892": 70, "1014": 70, "893": 70, "1012": [70, 78], "894": 70, "1010": 70, "895": 70, "1008": 70, "896": 70, "1006": [70, 78], "897": 70, "1004": 70, "898": 70, "1002": 70, "899": 70, "900": 70, "0998": [70, 78], "901": 70, "0996": [70, 71, 78], "902": [70, 74], "0994": 70, "903": 70, "0992": [70, 78], "904": 70, "0990": 70, "905": 70, "0988": 70, "906": 70, "0986": 70, "907": 70, "0985": 70, "908": 70, "0983": 70, "909": 70, "0981": 70, "910": 70, "0979": 70, "911": 70, "0977": 70, "912": 70, "0975": 70, "913": 70, "0973": 70, "914": 70, "0972": 70, "915": 70, "0970": 70, "916": 70, "0968": [70, 78], "917": 70, "0966": 70, "918": 70, "0964": 70, "919": 70, "0963": 70, "920": 70, "0961": 70, "921": 70, "0959": [70, 78], "922": 70, "0957": 70, "923": 70, "0956": 70, "924": 70, "0954": [70, 78], "925": 70, "0952": 70, "926": 70, "0950": 70, "927": 70, "0949": 70, "928": 70, "0947": 70, "929": 70, "0945": 70, "930": 70, "0944": 70, "931": 70, "0942": 70, "932": 70, "0940": 70, "933": 70, "0939": 70, "934": 70, "0937": [70, 78], "935": 70, "0935": 70, "936": 70, "0934": 70, "937": 70, "0932": 70, "938": 70, "0930": [70, 78], "939": 70, "0929": 70, "940": 70, "0927": [70, 78], "941": 70, "0925": [70, 71], "942": 70, "0924": 70, "943": 70, "0922": 70, "944": 70, "0920": [70, 78], "945": 70, "0919": 70, "946": 70, "0917": 70, "947": 70, "0916": 70, "948": 70, "0914": 70, "949": 70, "0913": 70, "950": 70, "0911": 70, "951": 70, "0909": 70, "952": 70, "0908": 70, "953": 70, "0906": 70, "954": 70, "0905": 70, "955": 70, "0903": 70, "956": 70, "0902": 70, "957": 70, "0900": 70, "958": 70, "0899": 70, "959": 70, "0897": [70, 78], "960": 70, "0896": 70, "961": 70, "0894": 70, "962": 70, "0893": 70, "963": 70, "0891": [70, 78], "964": 70, "0890": 70, "965": 70, "0888": 70, "966": 70, "0887": 70, "967": 70, "0885": 70, "968": 70, "0884": 70, "969": 70, "0882": 70, "970": 70, "0881": 70, "971": 70, "0880": 70, "972": 70, "0878": 70, "973": 70, "0877": 70, "974": 70, "0875": 70, "975": 70, "0874": 70, "976": 70, "0872": [70, 85], "977": 70, "0871": [70, 85], "978": 70, "0870": 70, "979": 70, "0868": 70, "980": 70, "0867": 70, "981": 70, "0865": 70, "982": 70, "0864": [70, 78], "983": 70, "0863": 70, "984": 70, "0861": 70, "985": 70, "0860": 70, "986": 70, "0859": 70, "987": 70, "0857": 70, "988": 70, "0856": 70, "989": 70, "0855": 70, "990": 70, "0853": 70, "991": 70, "0852": 70, "992": 70, "0851": [70, 82], "993": 70, "0849": 70, "994": 70, "0848": 70, "995": 70, "0847": 70, "996": 70, "0845": 70, "997": 70, "0844": 70, "998": 70, "0843": 70, "999": 70, "0841": 70, "0840": 70, "1001": 70, "0839": 70, "0838": 70, "1003": 70, "0836": 70, "0835": 70, "1005": 70, "0834": 70, "0832": 70, "1007": 70, "0831": 70, "0830": 70, "1009": [70, 78], "0829": 70, "0827": 70, "1011": 70, "0826": 70, "0825": [70, 71], "1013": 70, "0824": 70, "0823": 70, "1015": 70, "0821": 70, "0820": 70, "1017": 70, "0819": [70, 78], "0818": 70, "1019": [70, 78], "0816": 70, "0815": 70, "1021": 70, "0814": 70, "0813": 70, "1023": [70, 78], "0812": 70, "0810": 70, "1025": 70, "0809": 70, "0808": 70, "1027": 70, "0807": 70, "0806": 70, "1029": 70, "0805": 70, "0803": [70, 78], "1031": 70, "0802": 70, "0801": 70, "1033": 70, "0800": 70, "0799": 70, "1035": 70, "0798": 70, "0797": [70, 71], "1037": 70, "0795": 70, "0794": 70, "1039": 70, "0793": 70, "1040": 70, "0792": 70, "0791": 70, "1042": 70, "0790": 70, "0789": [70, 78], "1044": 70, "0788": 70, "0787": 70, "1046": [70, 78], "0785": 70, "0784": 70, "1048": 70, "0783": 70, "0782": 70, "1050": 70, "0781": 70, "0780": 70, "1052": 70, "0779": 70, "1053": 70, "0778": 70, "0777": 70, "1055": 70, "0776": 70, "0775": 70, "1057": 70, "0774": 70, "0772": 70, "1059": 70, "0771": 70, "0770": 70, "1061": 70, "0769": 70, "0768": 70, "1063": 70, "0767": 70, "1064": 70, "0766": 70, "0765": 70, "1066": 70, "0764": 70, "0763": 70, "1068": 70, "0762": [70, 78], "0761": 70, "1070": 70, "0760": 70, "0759": [70, 78], "1072": 70, "0758": 70, "1073": 70, "0757": 70, "0756": 70, "1075": 70, "0755": 70, "0754": [70, 78], "1077": 70, "0753": 70, "0752": 70, "0751": 70, "1080": [70, 78], "0750": 70, "0749": [70, 78], "1082": 70, "0748": 70, "0747": 70, "1084": 70, "0746": [70, 78], "0745": 70, "1086": 70, "0744": 70, "1087": 70, "0743": 70, "0742": 70, "1089": 70, "0741": 70, "0740": 70, "1091": 70, "0739": 70, "0738": 70, "1093": 70, "0737": 70, "1094": [70, 86], "0736": 70, "1096": 70, "0735": 70, "0734": 70, "1098": 70, "0733": 70, "1099": 70, "0732": 70, "0731": [70, 78], "1101": [70, 82], "0730": 70, "0729": 70, "1103": 70, "0728": 70, "1104": 70, "0727": 70, "0726": 70, "1106": [70, 78], "0725": [70, 78], "0724": [70, 78], "1108": 70, "0723": 70, "1110": 70, "0722": 70, "1111": 70, "0721": [70, 78], "0720": [70, 78], "1113": 70, "0719": 70, "0718": 70, "1115": 70, "0717": 70, "1116": 70, "1118": 70, "0715": 70, "1120": 70, "0713": 70, "1121": 70, "0712": 70, "0711": 70, "1123": 70, "0710": 70, "1124": 70, "0709": 70, "1126": 70, "0708": 70, "0707": 70, "1128": 70, "0706": 70, "1129": 70, "0705": 70, "0704": 70, "1131": 70, "0703": 70, "1133": 70, "0702": 70, "1134": 70, "0701": [70, 78], "0700": 70, "1136": [70, 82], "0699": [70, 78], "0698": 70, "1138": 70, "1139": 70, "0697": 70, "0696": 70, "1141": 70, "0695": 70, "1142": 70, "0694": 70, "0693": [70, 78], "1144": 70, "0692": 70, "1146": 70, "0691": 70, "1147": 70, "0690": 70, "0689": 70, "1149": 70, "1150": 70, "0687": [70, 78], "1152": 70, "0685": [70, 78], "1154": 70, "1155": 70, "0684": 70, "0683": 70, "1157": 70, "0682": 70, "1158": 70, "1160": 70, "0680": 70, "1161": 70, "0679": 70, "0678": 70, "1163": 70, "0677": 70, "1165": 70, "0676": 70, "1166": 70, "0675": 70, "0674": [70, 82], "1168": 70, "1169": 70, "0673": 70, "0672": 70, "1171": 70, "0671": 70, "1172": 70, "0670": 70, "1174": 70, "0669": 70, "1175": 70, "0668": 70, "1177": 70, "0666": 70, "1179": [70, 71, 78], "0665": 70, "1180": 70, "0664": 70, "1182": 70, "0663": [70, 78], "1183": 70, "0662": 70, "1185": 70, "0661": 70, "1186": 70, "0660": 70, "0659": 70, "1188": 70, "1189": 70, "0658": 70, "0657": 70, "1191": 70, "0656": 70, "1192": 70, "0655": 70, "1194": [70, 71], "0654": 70, "1195": 70, "0653": 70, "1197": 70, "0652": 70, "1198": 70, "0651": 70, "1200": [70, 87], "0650": [70, 78], "1201": 70, "0649": 70, "1203": 70, "0648": 70, "1204": 70, "0647": 70, "0646": 70, "1206": 70, "1207": 70, "0645": 70, "0644": 70, "1209": 70, "1210": 70, "0643": 70, "0642": 70, "1212": 70, "1213": 70, "0641": 70, "0640": 70, "1215": 70, "1216": 70, "0639": 70, "0638": 70, "1218": 70, "1219": 70, "0637": 70, "0636": [70, 71], "1221": 70, "0635": 70, "1222": 70, "0634": 70, "1224": 70, "0633": 70, "1225": 70, "0632": 70, "1227": 70, "0631": 70, "1228": 70, "0630": 70, "1230": 70, "0629": 70, "1231": 70, "0628": 70, "1233": 70, "1234": 70, "0627": 70, "1236": 70, "1237": 70, "0625": 70, "1238": 70, "0624": 70, "1240": 70, "0623": [70, 71], "1241": 70, "0622": 70, "1243": 70, "0621": 70, "1244": 70, "0620": 70, "1246": 70, "1247": 70, "0618": 70, "1249": 70, "0617": 70, "0616": 70, "0615": 70, "1253": 70, "1254": 70, "0614": 70, "0613": 70, "1256": 70, "1257": [70, 82], "0612": 70, "1259": 70, "0611": 70, "1260": 70, "0610": 70, "1262": 70, "0609": 70, "1263": 70, "1264": 70, "0608": 70, "0607": [70, 71, 78], "1266": 70, "1267": 70, "0606": 70, "1269": 70, "0605": 70, "1270": 70, "0604": [70, 71], "1272": 70, "0603": [70, 71], "1273": 70, "1274": 70, "0602": 70, "0601": 70, "1276": 70, "1277": 70, "0600": 70, "1279": 70, "0599": 70, "1280": 70, "0598": 70, "1282": 70, "0597": [70, 71], "1283": [70, 82], "1284": 70, "0596": 70, "0595": 70, "1286": 70, "1287": 70, "0594": 70, "1289": 70, "0593": 70, "1290": 70, "1291": 70, "0592": 70, "0591": 70, "1293": [70, 78], "1294": [70, 78], "0590": 70, "1296": 70, "0589": 70, "1297": 70, "1298": 70, "0588": 70, "0587": 70, "1300": 70, "1301": 70, "0586": 70, "1303": 70, "0585": [70, 71], "1304": 70, "1305": 70, "0584": [70, 71], "0583": 70, "1307": 70, "1308": 70, "0582": [70, 71], "1309": 70, "0581": 70, "1311": 70, "1312": 70, "0580": [70, 78], "1314": 70, "0579": 70, "1315": 70, "0578": 70, "1316": [70, 78], "0577": 70, "1318": 70, "1319": 70, "0576": [70, 78], "1321": 70, "0575": 70, "1322": 70, "1323": 70, "0574": 70, "1325": 70, "0573": 70, "1326": 70, "1327": 70, "0572": 70, "0571": 70, "1329": 70, "1330": 70, "0570": 70, "1331": 70, "0569": 70, "1333": 70, "1334": 70, "0568": 70, "1336": 70, "1337": 70, "1338": 70, "0566": 70, "1340": 70, "0565": [70, 71], "1341": 70, "1342": 70, "0564": 70, "1344": 70, "0563": 70, "1345": 70, "1346": 70, "0562": [70, 82], "1348": [70, 82], "0561": 70, "1349": 70, "0560": 70, "1350": 70, "0559": 70, "1352": 70, "1353": 70, "0558": 70, "1355": 70, "0557": 70, "1356": [70, 78], "1357": 70, "0556": 70, "1359": 70, "0555": 70, "1360": 70, "1361": 70, "0554": [70, 71], "1363": 70, "0553": [70, 78], "1364": 70, "1365": 70, "0552": 70, "1367": 70, "0551": 70, "1368": 70, "1369": 70, "0550": 70, "1371": 70, "0549": 70, "1372": 70, "1373": 70, "0548": 70, "1375": 70, "1376": 70, "0547": 70, "1377": 70, "0546": [70, 78], "1379": 70, "1380": [70, 78], "0545": 70, "1381": 70, "0544": 70, "1383": 70, "1384": 70, "0543": [70, 71], "1385": 70, "0542": 70, "1387": 70, "1388": 70, "0541": 70, "1389": 70, "0540": 70, "1391": 70, "1392": 70, "0539": 70, "1393": 70, "0538": 70, "1395": 70, "1396": 70, "0537": [70, 78], "1397": 70, "1398": 70, "0536": [70, 71], "1400": 70, "1401": 70, "0535": 70, "1402": 70, "0534": 70, "1404": 70, "1405": 70, "0533": 70, "1406": 70, "0532": 70, "1408": 70, "1409": 70, "0531": 70, "1410": 70, "0530": 70, "1412": 70, "1413": 70, "1414": 70, "0529": [70, 82], "1416": 70, "0528": 70, "1417": 70, "1418": 70, "0527": 70, "1419": 70, "0526": 70, "1421": 70, "1422": 70, "1423": 70, "0525": 70, "1425": 70, "0524": 70, "1426": [70, 78], "1427": 70, "0523": 70, "0522": 70, "1430": 70, "1431": 70, "1432": 70, "1434": 70, "0520": 70, "1435": 70, "0519": 70, "1438": 70, "0518": 70, "1439": 70, "1440": 70, "1441": 70, "0517": 70, "1443": 70, "0516": 70, "1444": 70, "1445": 70, "0515": 70, "1447": 70, "1448": 70, "0514": 70, "1449": 70, "1450": 70, "0513": 70, "1452": 70, "0512": [70, 82], "1453": 70, "1454": 70, "0511": [70, 71], "1456": 70, "1457": [70, 78], "0510": 70, "1458": 70, "1459": 70, "0509": 70, "1461": 70, "1462": 70, "0508": 70, "1463": 70, "0507": 70, "1465": 70, "1466": 70, "0506": 70, "1467": 70, "1468": 70, "1470": 70, "1471": 70, "0504": 70, "1472": 70, "1474": [70, 78], "0503": 70, "1475": [70, 78], "1476": 70, "0502": 70, "1477": 70, "0501": 70, "1479": 70, "1480": 70, "1481": 70, "0500": 70, "1482": 70, "0499": 70, "1484": [70, 78], "1485": 70, "1486": 70, "0498": 70, "1487": [70, 78], "0497": 70, "1489": 70, "1490": 70, "1491": 70, "0496": 70, "1493": 70, "0495": 70, "1494": 70, "1495": [70, 78], "1496": 70, "0494": 70, "1498": [70, 71], "0493": 70, "1499": 70, "1500": 70, "1501": 70, "0492": 70, "1503": 70, "0491": 70, "1504": 70, "1505": 70, "1506": 70, "0490": 70, "1508": 70, "0489": 70, "1509": 70, "1510": 70, "1511": [70, 78], "0488": 70, "1513": 70, "1514": 70, "0487": 70, "1515": 70, "1516": [70, 82], "0486": 70, "1518": [70, 78], "1519": [70, 78], "0485": 70, "1520": 70, "1521": 70, "0484": 70, "1523": 70, "1524": 70, "0483": 70, "1525": 70, "1526": 70, "0482": [70, 71], "1528": 70, "1529": 70, "0481": 70, "1530": 70, "1531": 70, "0480": 70, "1533": 70, "1534": 70, "0479": 70, "1535": 70, "1536": 70, "0478": [70, 71], "1538": 70, "1539": 70, "1540": 70, "0477": 70, "1541": 70, "0476": 70, "1543": 70, "1544": 70, "1545": [70, 78], "0475": 70, "1546": 70, "1548": 70, "0474": 70, "1549": 70, "1550": 70, "0473": 70, "1551": 70, "1553": 70, "0472": 70, "1554": 70, "1555": 70, "1556": 70, "0471": 70, "1558": 70, "1559": 70, "0470": 70, "1560": [70, 78], "1561": 70, "0469": [70, 82], "1562": 70, "1564": 70, "0468": 70, "1566": 70, "1567": 70, "0467": 70, "1569": 70, "1570": 70, "0466": 70, "1571": 70, "1572": 70, "0465": 70, "1574": 70, "1575": [70, 78], "0464": 70, "1576": 70, "1577": 70, "1578": 70, "0463": 70, "1580": 70, "1581": 70, "0462": 70, "1582": 70, "1583": 70, "0461": 70, "1585": 70, "1586": 70, "1587": 70, "0460": 70, "1588": 70, "1589": 70, "0459": 70, "1591": 70, "1592": 70, "0458": 70, "1593": 70, "1594": 70, "0457": 70, "1596": 70, "1597": 70, "1598": 70, "0456": 70, "1599": 70, "1600": 70, "0455": 70, "1602": 70, "1603": 70, "1604": 70, "0454": 70, "1605": 70, "1607": 70, "1608": 70, "1609": 70, "1610": 70, "0452": 70, "1611": 70, "1613": 70, "0451": 70, "1614": 70, "1616": 70, "0450": 70, "1618": [70, 71], "1619": 70, "0449": 70, "1620": 70, "1621": 70, "0448": 70, "1622": 70, "1624": 70, "0447": 70, "1625": 70, "1626": 70, "1627": 70, "0446": 70, "1628": [70, 78], "1630": 70, "1631": 70, "0445": 70, "1632": 70, "1633": 70, "1634": 70, "0444": 70, "1636": 70, "0443": 70, "1637": 70, "1638": 70, "1639": 70, "0442": [70, 71], "1641": 70, "1642": 70, "1643": 70, "0441": 70, "1644": 70, "1645": 70, "0440": 70, "1647": 70, "1648": 70, "1649": 70, "0439": 70, "1650": 70, "1651": 70, "0438": 70, "1653": 70, "1655": 70, "0437": 70, "1656": 70, "1657": 70, "0436": 70, "1659": 70, "1660": 70, "1661": 70, "0435": 70, "1662": 70, "1663": 70, "0434": 70, "1665": 70, "1666": 70, "1667": 70, "0433": 70, "1668": 70, "1669": 70, "1671": 70, "0432": 70, "1672": 70, "1673": 70, "1674": 70, "0431": 70, "1675": 70, "1677": 70, "0430": 70, "1678": 70, "1679": 70, "1680": 70, "0429": 70, "1681": 70, "1683": 70, "0428": 70, "1684": 70, "1685": 70, "1686": 70, "1687": 70, "1689": 70, "1690": 70, "0426": 70, "1691": 70, "1692": 70, "1693": 70, "0425": 70, "1694": 70, "1696": 70, "0424": 70, "1697": 70, "1698": 70, "1699": 70, "0423": 70, "1700": 70, "1702": 70, "1703": 70, "0422": 70, "1704": 70, "1705": 70, "1706": 70, "0421": 70, "1708": 70, "1709": 70, "0420": 70, "1711": 70, "1712": 70, "1713": 70, "0419": 70, "1715": 70, "1716": 70, "0418": [70, 71], "1717": 70, "1718": 70, "1719": 70, "0417": 70, "1721": 70, "1722": 70, "0416": 70, "1723": 70, "1724": [70, 78], "1725": [70, 78], "0415": 70, "1727": 70, "1728": 70, "1729": 70, "0414": 70, "1730": [70, 78], "1731": 70, "1732": 70, "0413": 70, "1734": [70, 78], "1735": 70, "1736": 70, "0412": 70, "1737": 70, "1738": [70, 78], "1739": [70, 71], "0411": [70, 71], "1741": 70, "1742": 70, "1743": 70, "0410": 70, "1745": 70, "0409": 70, "1747": 70, "1748": 70, "1749": 70, "1750": 70, "0408": 70, "1751": 70, "1752": 70, "0407": 70, "1754": 70, "1755": 70, "1756": 70, "1757": 70, "0406": [70, 71], "1758": 70, "1760": 70, "0405": 70, "1761": 70, "1762": 70, "1763": [70, 78], "0404": 70, "1764": 70, "1767": 70, "0403": 70, "1768": 70, "1769": 70, "1770": 70, "0402": 70, "1771": 70, "1772": 70, "1774": 70, "1775": 70, "1776": 70, "1777": 70, "1778": 70, "0400": [70, 71], "1779": 70, "1781": 70, "0399": 70, "1782": 70, "1783": 70, "1784": 70, "1785": 70, "0398": 70, "1786": 70, "1788": 70, "0397": 70, "1789": 70, "1790": 70, "1791": 70, "1792": 70, "0396": 70, "1793": 70, "1795": 70, "0395": 70, "1796": 70, "1797": 70, "1798": 70, "1799": 70, "0394": 70, "1800": 70, "1802": 70, "1803": 70, "0393": 70, "1804": [70, 78], "1805": 70, "1806": 70, "0392": 70, "1807": 70, "1809": 70, "1810": 70, "0391": [70, 85], "1811": 70, "1812": [70, 78], "1813": 70, "1814": 70, "0390": 70, "1816": 70, "1817": 70, "1818": 70, "1819": 70, "1820": 70, "1821": [70, 78], "0388": 70, "1823": 70, "1824": 70, "1825": 70, "0387": 70, "1826": 70, "1827": 70, "1828": 70, "0386": 70, "1830": 70, "1831": [70, 78], "1832": 70, "0385": 70, "1833": 70, "1834": 70, "1835": 70, "1836": [70, 78], "0384": 70, "1838": 70, "1839": 70, "1840": 70, "0383": 70, "1841": 70, "1842": 70, "1843": 70, "0382": 70, "1845": 70, "1846": 70, "1847": 70, "0381": 70, "1848": 70, "1849": 70, "1850": 70, "0380": 70, "1852": 70, "1853": 70, "1854": 70, "1855": 70, "0379": [70, 71], "1856": 70, "1857": 70, "1858": 70, "0378": 70, "1860": 70, "1861": 70, "1862": 70, "0377": 70, "1864": 70, "1865": 70, "0376": 70, "1867": 70, "1868": 70, "1869": 70, "1870": 70, "0375": [70, 71], "1871": 70, "1872": 70, "1873": 70, "0374": 70, "1875": 70, "1876": 70, "1877": 70, "1878": 70, "0373": 70, "1879": 70, "1880": 70, "1881": 70, "0372": 70, "1883": 70, "1884": 70, "1885": 70, "1886": 70, "0371": 70, "1887": 70, "1888": 70, "0370": 70, "1890": 70, "1891": 70, "1892": 70, "1893": 70, "1894": 70, "1896": 70, "0368": 70, "1898": 70, "1899": 70, "1900": 70, "1901": [70, 71], "0367": 70, "1902": 70, "1903": 70, "1904": 70, "0366": 70, "1906": 70, "1907": 70, "1908": 70, "1909": 70, "0365": 70, "1910": 70, "1911": 70, "1912": 70, "0364": 70, "1914": 70, "1915": 70, "1916": 70, "0363": [70, 82], "1918": 70, "1919": 70, "1920": 70, "0362": 70, "1922": 70, "1923": 70, "1924": 70, "1925": 70, "0361": 70, "1926": [70, 86], "1927": 70, "1928": 70, "0360": 70, "1930": 70, "1931": 70, "1932": 70, "1933": 70, "0359": 70, "1934": 70, "1935": 70, "1936": 70, "1938": 70, "0358": 70, "1939": 70, "1940": 70, "1941": 70, "1942": 70, "0357": 70, "1943": 70, "1944": 70, "1946": 70, "1947": 70, "1948": 70, "1949": 70, "1950": 70, "0355": 70, "1951": 70, "1952": 70, "1954": 70, "0354": 70, "1955": 70, "1956": 70, "1957": 70, "1958": 70, "0353": 70, "1959": 70, "1960": 70, "1961": 70, "0352": 70, "1963": 70, "1964": 70, "1965": 70, "1966": 70, "0351": 70, "1967": 70, "1968": 70, "1969": 70, "1971": 70, "0350": 70, "1972": 70, "1973": 70, "1974": 70, "1975": 70, "0349": 70, "1977": 70, "1979": 70, "0348": 70, "1980": 70, "1981": 70, "1982": 70, "1983": 70, "0347": 70, "1984": 70, "1985": 70, "1986": 70, "1988": 70, "0346": 70, "1989": 70, "1990": 70, "1991": 70, "1992": 70, "0345": 70, "1993": 70, "1994": 70, "1996": 70, "0344": 70, "1997": 70, "1998": 70, "1999": 70, "plot": [70, 87], "dpi": 70, "test_epoch": 70, "isfinit": 70, "linestyl": 70, "marker": 70, "legend": [70, 87], "xlabel": 70, "cites": 71, "3703": 71, "manifold": 71, "tsne": 71, "randomnodesplit": 71, "wget": [71, 75], "twistedcub": 71, "citeseer6cls3703": 71, "pt": [71, 87], "paper_x": 71, "longtensor": [71, 75], "paper_author": 71, "train_test_splitt": 71, "num_test": 71, "num_val": 71, "dropout_r": 71, "super": 71, "to_hidden_linear": 71, "to_categories_linear": 71, "enumer": [71, 75, 87], "schedul": 71, "initial_lr": 71, "04": [71, 78, 87], "lr_schedul": 71, "steplr": 71, "7841": 71, "6729": 71, "2133": 71, "5536": 71, "4931": 71, "3316": 71, "5507": 71, "6564": [71, 78], "6441": 71, "2435": 71, "5326": 71, "3037": 71, "5990": 71, "2278": 71, "2163": 71, "worth": 71, "visual": [71, 87], "n_compon": 71, "fit_transform": 71, "ax1": 71, "ax2": 71, "subplot": [71, 87], "suptitl": 71, "set_titl": [71, 87], "As": 72, "j": [72, 73, 78, 82, 83, 84, 87], "highlight": 72, "formal": [72, 80], "alpha_": 72, "jk": 72, "nonlinear": [72, 86], "exp": 72, "u_": 72, "limits_": 72, "context": 72, "again": [72, 80, 81], "vi": [72, 73], "beta_": 72, "ij": 72, "anoth": 72, "measur": 72, "interpret": 73, "propag": 73, "divid": 73, "2170": 73, "simplicial_complex": [74, 76, 77, 87], "warn": [74, 77, 83, 84], "filterwarn": [74, 77], "to_sparse_csr": [74, 75, 77], "bceloss": [74, 77], "x_1_val": [74, 76, 77], "incidence_1_v": [74, 76, 77], "y_val": [74, 76, 77], "pred": [74, 76, 77, 87], "4901733398438": 74, "4375": 74, "79052734375": 74, "759521484375": 74, "1170654296875": 74, "8601379394531": 74, "66136169433594": 74, "54958724975586": 74, "5625": [74, 76, 77], "33615493774414": 74, "13643646240234": 74, "25325775146484": 74, "162841796875": 74, "53228759765625": 74, "69290924072266": 74, "98519897460938": 74, "16600036621094": 74, "80245208740234": 74, "98604583740234": 74, "02069854736328": 74, "27203369140625": 74, "05550384521484": 74, "7593765258789": 74, "24275970458984": 74, "24573516845703": 74, "25870132446289": 74, "992156982421875": 74, "94447326660156": 74, "74713897705078": 74, "584598541259766": 74, "6875": 74, "302337646484375": 74, "985782623291016": 74, "713340759277344": 74, "53618240356445": 74, "84739303588867": 74, "595306396484375": 74, "71818161010742": 74, "3809814453125": 74, "186588287353516": 74, "00263214111328": 74, "166587829589844": 74, "39109420776367": 74, "600830078125": 74, "973331451416016": 74, "825225830078125": 74, "596601486206055": 74, "79454231262207": 74, "8666934967041": 74, "986522674560547": 74, "880859375": 74, "91712188720703": 74, "44558334350586": 74, "152557373046875": 74, "227420806884766": 74, "160226821899414": 74, "39475440979004": 74, "113832473754883": 74, "215381622314453": 74, "52114486694336": 74, "805360794067383": 74, "915090560913086": 74, "797382354736328": 74, "480539321899414": 74, "068044662475586": 74, "685386657714844": 74, "43670654296875": 74, "364776611328125": 74, "432109832763672": 74, "538002014160156": 74, "5771427154541": 74, "4938907623291": 74, "30146026611328": 74, "066486358642578": 74, "864112854003906": 74, "738616943359375": 74, "689714431762695": 74, "684139251708984": 74, "6783390045166": 74, "640270233154297": 74, "568225860595703": 74, "463821411132812": 74, "352237701416016": 74, "259632110595703": 74, "200979232788086": 74, "173927307128906": 74, "162473678588867": 74, "145278930664062": 74, "108654022216797": 74, "052175521850586": 74, "986543655395508": 74, "926197052001953": 74, "880918502807617": 74, "85179901123047": 74, "83198356628418": 74, "812116622924805": 74, "78550148010254": 74, "751976013183594": 74, "7146053314209": 74, "67841148376465": 74, "64767074584961": 74, "623760223388672": 74, "604145050048828": 74, "58517074584961": 74, "56340789794922": 74, "537738800048828": 74, "5096435546875": 74, "481836318969727": 74, "456588745117188": 74, "435287475585938": 74, "416616439819336": 74, "397968292236328": 74, "377914428710938": 74, "355894088745117": 74, "332746505737305": 74, "310028076171875": 74, "288677215576172": 74, "26887321472168": 74, "249961853027344": 74, "23108673095703": 74, "212047576904297": 74, "192527770996094": 74, "173789978027344": 74, "15878677368164": 74, "14487648010254": 74, "13152313232422": 74, "117942810058594": 74, "103792190551758": 74, "088790893554688": 74, "0731201171875": 74, "057111740112305": 74, "041547775268555": 74, "026466369628906": 74, "011743545532227": 74, "996620178222656": 74, "98100471496582": 74, "965646743774414": 74, "950223922729492": 74, "93490982055664": 74, "919763565063477": 74, "90479850769043": 74, "890039443969727": 74, "875774383544922": 74, "861370086669922": 74, "847206115722656": 74, "833080291748047": 74, "819049835205078": 74, "805124282836914": 74, "79128074645996": 74, "777509689331055": 74, "76376724243164": 74, "74854278564453": 74, "73321533203125": 74, "717744827270508": 74, "702198028564453": 74, "686790466308594": 74, "671096801757812": 74, "655136108398438": 74, "63947296142578": 74, "623905181884766": 74, "608251571655273": 74, "592540740966797": 74, "576845169067383": 74, "56026840209961": 74, "543073654174805": 74, "525781631469727": 74, "50815773010254": 74, "49064064025879": 74, "472421646118164": 74, "450693130493164": 74, "427898406982422": 74, "402362823486328": 74, "37494659423828": 74, "34657859802246": 74, "317203521728516": 74, "286909103393555": 74, "2688045501709": 74, "2530574798584": 74, "237436294555664": 74, "222070693969727": 74, "206933975219727": 74, "19208335876465": 74, "177217483520508": 74, "16236114501953": 74, "147443771362305": 74, "132776260375977": 74, "118249893188477": 74, "103797912597656": 74, "089706420898438": 74, "075761795043945": 74, "06186294555664": 74, "048063278198242": 74, "034448623657227": 74, "020954132080078": 74, "006834030151367": 74, "993017196655273": 74, "979351043701172": 74, "965789794921875": 74, "952112197875977": 74, "938526153564453": 74, "924880981445312": 74, "911245346069336": 74, "897811889648438": 74, "88424301147461": 74, "870620727539062": 74, "856914520263672": 74, "843372344970703": 74, "82991600036621": 74, "81630516052246": 74, "802886962890625": 74, "78949737548828": 74, "776121139526367": 74, "76279640197754": 74, "74949836730957": 74, "737751007080078": 74, "729055404663086": 74, "715679168701172": 74, "70354461669922": 74, "692848205566406": 74, "682186126708984": 74, "671817779541016": 74, "66116714477539": 74, "65018081665039": 74, "638872146606445": 74, "627206802368164": 74, "615779876708984": 74, "6042423248291": 74, "59211540222168": 74, "58187484741211": 74, "570674896240234": 74, "558454513549805": 74, "547941207885742": 74, "537683486938477": 74, "526987075805664": 74, "515642166137695": 74, "504106521606445": 74, "49180030822754": 74, "479785919189453": 74, "46455955505371": 74, "45136070251465": 74, "43777847290039": 74, "424055099487305": 74, "409717559814453": 74, "400564193725586": 74, "39628028869629": 74, "390756607055664": 74, "38397979736328": 74, "37428855895996": 74, "362167358398438": 74, "34809112548828": 74, "3326473236084": 74, "31566047668457": 74, "30012321472168": 74, "2796630859375": 74, "261516571044922": 74, "250732421875": 74, "238964080810547": 74, "224985122680664": 74, "208303451538086": 74, "189075469970703": 74, "167694091796875": 74, "145658493041992": 74, "13551902770996": 74, "12187385559082": 74, "102636337280273": 74, "088523864746094": 74, "074373245239258": 74, "05896759033203": 74, "041915893554688": 74, "023332595825195": 74, "00411033630371": 74, "985628128051758": 74, "970544815063477": 74, "95485496520996": 74, "941598892211914": 74, "92700958251953": 74, "912050247192383": 74, "8923282623291": 74, "879552841186523": 74, "867490768432617": 74, "854406356811523": 74, "84088706970215": 74, "826339721679688": 74, "81657600402832": 74, "806161880493164": 74, "795644760131836": 74, "78535270690918": 74, "775672912597656": 74, "760944366455078": 74, "752826690673828": 74, "74382209777832": 74, "729705810546875": 74, "711393356323242": 74, "697967529296875": 74, "6846923828125": 74, "672142028808594": 74, "657779693603516": 74, "64270782470703": 74, "630765914916992": 74, "6149845123291": 74, "603057861328125": 74, "590776443481445": 74, "57485008239746": 74, "556278228759766": 74, "543123245239258": 74, "52927017211914": 74, "51650047302246": 74, "50031280517578": 74, "48053550720215": 74, "46886444091797": 74, "45595359802246": 74, "437623977661133": 74, "424482345581055": 74, "410137176513672": 74, "396631240844727": 74, "381338119506836": 74, "362808227539062": 74, "34630012512207": 74, "32933235168457": 74, "31389808654785": 74, "300844192504883": 74, "28453826904297": 74, "27028465270996": 74, "258142471313477": 74, "245216369628906": 74, "230785369873047": 74, "215072631835938": 74, "198575973510742": 74, "183231353759766": 74, "1727294921875": 74, "159303665161133": 74, "14360237121582": 74, "128843307495117": 74, "117029190063477": 74, "105663299560547": 74, "09357452392578": 74, "080169677734375": 74, "066062927246094": 74, "051239013671875": 74, "04073715209961": 74, "02910804748535": 74, "01323127746582": 74, "00203514099121": 74, "9904842376709": 74, "977466583251953": 74, "96651840209961": 74, "952547073364258": 74, "943500518798828": 74, "931215286254883": 74, "920156478881836": 74, "90941047668457": 74, "897315979003906": 74, "884029388427734": 74, "870525360107422": 74, "857534408569336": 74, "847549438476562": 74, "83431053161621": 74, "82513999938965": 74, "813926696777344": 74, "80405044555664": 74, "79069709777832": 74, "777841567993164": 74, "76871681213379": 74, "757659912109375": 74, "74506950378418": 74, "73027992248535": 74, "721105575561523": 74, "70802116394043": 74, "69561767578125": 74, "686477661132812": 74, "67528533935547": 74, "6632022857666": 74, "654359817504883": 74, "6440486907959": 74, "63262367248535": 74, "62179183959961": 74, "61004066467285": 74, "599712371826172": 74, "58791160583496": 74, "574491500854492": 74, "56350326538086": 74, "55362319946289": 74, "542652130126953": 74, "531946182250977": 74, "520584106445312": 74, "508346557617188": 74, "49889373779297": 74, "488155364990234": 74, "476438522338867": 74, "46401596069336": 74, "453584671020508": 74, "442052841186523": 74, "431604385375977": 74, "420881271362305": 74, "410736083984375": 74, "401206970214844": 74, "389665603637695": 74, "37841796875": 74, "36739730834961": 74, "357166290283203": 74, "347293853759766": 74, "335630416870117": 74, "327388763427734": 74, "315521240234375": 74, "30586051940918": 74, "29657745361328": 74, "28609848022461": 74, "276351928710938": 74, "265724182128906": 74, "25368309020996": 74, "241600036621094": 74, "229310989379883": 74, "2224178314209": 74, "2132511138916": 74, "199750900268555": 74, "18743133544922": 74, "1822509765625": 74, "17019271850586": 74, "155643463134766": 74, "146547317504883": 74, "136232376098633": 74, "123960494995117": 74, "11368751525879": 74, "101808547973633": 74, "09125518798828": 74, "083723068237305": 74, "07281494140625": 74, "058805465698242": 74, "049646377563477": 74, "04451560974121": 74, "0327091217041": 74, "01663589477539": 74, "00825309753418": 74, "000246047973633": 74, "990230560302734": 74, "977436065673828": 74, "96634864807129": 74, "95425796508789": 74, "939908981323242": 74, "92905616760254": 74, "918004989624023": 74, "902795791625977": 74, "884109497070312": 74, "871601104736328": 74, "85983657836914": 74, "84302520751953": 74, "82725715637207": 74, "81039047241211": 74, "79295539855957": 74, "775209426879883": 74, "761594772338867": 74, "746509552001953": 74, "734050750732422": 74, "72390365600586": 74, "70878028869629": 74, "69400405883789": 74, "678071975708008": 74, "666906356811523": 74, "6538143157959": 74, "6412410736084": 74, "629528045654297": 74, "61863899230957": 74, "606908798217773": 74, "595272064208984": 74, "58217430114746": 74, "568185806274414": 74, "558835983276367": 74, "548227310180664": 74, "534801483154297": 74, "52486228942871": 74, "514476776123047": 74, "506332397460938": 74, "492887496948242": 74, "481121063232422": 74, "469911575317383": 74, "46491813659668": 74, "452470779418945": 74, "435522079467773": 74, "428197860717773": 74, "420866012573242": 74, "40703773498535": 74, "395797729492188": 74, "38579559326172": 74, "372974395751953": 74, "36663246154785": 74, "354814529418945": 74, "3371524810791": 74, "334217071533203": 74, "32594871520996": 74, "31154441833496": 74, "300630569458008": 74, "291522979736328": 74, "277509689331055": 74, "264963150024414": 74, "253454208374023": 74, "240015029907227": 74, "22855567932129": 74, "216880798339844": 74, "206361770629883": 74, "196760177612305": 74, "184263229370117": 74, "171443939208984": 74, "160396575927734": 74, "cicitationcora": 75, "hgnn": 75, "utlil": 75, "neccessari": 75, "pickl": 75, "scipi": [75, 78, 82, 83, 84, 87], "sp": 75, "computation": 75, "expens": [75, 78, 82, 83], "malllabiisc": 75, "hypergcn": 75, "cocit": 75, "09": [75, 78, 85, 87], "07": [75, 78, 87], "resolv": 75, "await": 75, "githubusercont": 75, "08": [75, 78, 87], "ok": 75, "404937": 75, "395k": 75, "applic": 75, "octet": 75, "stream": 75, "save": 75, "gt": [75, 87], "45k": 75, "kb": 75, "mb": 75, "101905": 75, "100k": 75, "52k": 75, "006": 75, "5436": 75, "3k": 75, "31k": 75, "51582": 75, "50k": 75, "37k": 75, "003": 75, "rb": 75, "handl": 75, "ipykernel_1201318": 75, "2369537045": 75, "deprecationwarn": 75, "csr_matrix": [75, 78, 82, 83], "namespac": 75, "csr": 75, "deprec": [75, 83], "pytorch": 75, "num": 75, "gcnii": 75, "h2": [75, 82], "hstack": 75, "2226475299": 75, "userwarn": [75, 83, 84, 86], "miss": 75, "trigger": 75, "intern": 75, "aten": 75, "sparsecsrtensorimpl": 75, "cpp": 75, "predefin": 75, "train_idx": 75, "test_idx": 75, "current": [75, 80, 83, 86], "readi": [75, 87], "8357142806053162": 75, "47507786750793457": 75, "7642857432365417": 75, "5183022022247314": 75, "9214285612106323": 75, "5611370801925659": 75, "9428571462631226": 75, "597741425037384": 75, "9642857313156128": 75, "5915108919143677": 75, "9857142567634583": 75, "5837227702140808": 75, "9571428298950195": 75, "579828679561615": 75, "5654205679893494": 75, "5735981464385986": 75, "5813862681388855": 75, "node_dim": 76, "seper": 76, "unsqueez": [76, 87], "388511657714844": 76, "726402282714844": 76, "228939056396484": 76, "930335998535156": 76, "89554214477539": 76, "096839904785156": 76, "53312301635742": 76, "17842483520508": 76, "982330322265625": 76, "8914680480957": 76, "861602783203125": 76, "86212921142578": 76, "87543869018555": 76, "89324951171875": 76, "91067886352539": 76, "92604446411133": 76, "93891525268555": 76, "949344635009766": 76, "9575080871582": 76, "963714599609375": 76, "96826934814453": 76, "97142791748047": 76, "97331619262695": 76, "974063873291016": 76, "97380828857422": 76, "97255325317383": 76, "97031784057617": 76, "96706008911133": 76, "96271896362305": 76, "957176208496094": 76, "95022964477539": 76, "941707611083984": 76, "93131637573242": 76, "918678283691406": 76, "90337371826172": 76, "884979248046875": 76, "863189697265625": 76, "837684631347656": 76, "808353424072266": 76, "776126861572266": 76, "743309020996094": 76, "71380615234375": 76, "691314697265625": 76, "67386245727539": 76, "649879455566406": 76, "607601165771484": 76, "54794692993164": 76, "47976303100586": 76, "408973693847656": 76, "33806228637695": 76, "11848068237305": 77, "48760986328125": 77, "41103744506836": 77, "46265411376953": 77, "654991149902344": 77, "48741149902344": 77, "493499755859375": 77, "90041732788086": 77, "03223419189453": 77, "20775604248047": 77, "63990020751953": 77, "34232711791992": 77, "30369567871094": 77, "4339485168457": 77, "60327911376953": 77, "69844055175781": 77, "66060256958008": 77, "48517990112305": 77, "20755386352539": 77, "90876007080078": 77, "64041519165039": 77, "4536018371582": 77, "356746673583984": 77, "32301712036133": 77, "31113052368164": 77, "28101348876953": 77, "19704818725586": 77, "05533218383789": 77, "8785400390625": 77, "683712005615234": 77, "50434875488281": 77, "371429443359375": 77, "28118133544922": 77, "21925735473633": 77, "157997131347656": 77, "07044219970703": 77, "944950103759766": 77, "793039321899414": 77, "64575958251953": 77, "5272216796875": 77, "440631866455078": 77, "375879287719727": 77, "311176300048828": 77, "22810173034668": 77, "12361717224121": 77, "010883331298828": 77, "924745559692383": 77, "86925506591797": 77, "828832626342773": 77, "78775405883789": 77, "alexandro": 78, "kero": 78, "linalg": 78, "npla": 78, "becaus": [78, 79, 81, 83, 86], "serv": [78, 79], "simpli": [78, 79, 87], "demonstr": [78, 79], "similarli": [78, 79, 82], "emerg": [78, 79, 80, 82, 83], "four": [78, 79, 80, 82, 83], "y_true": [78, 79, 80, 83, 86], "l_tilde_pinv": 78, "pinv": 78, "invers": 78, "0971": 78, "0923": 78, "0892": 78, "2140": 78, "2069": 78, "2927": 78, "2309": 78, "0943": 78, "0948": 78, "2678": 78, "0804": 78, "3090": 78, "0960": 78, "2077": 78, "2056": 78, "2813": 78, "nnz": 78, "layout": 78, "sparse_coo": 78, "56771909e": 78, "02": [78, 87], "49643084e": 78, "13434650e": 78, "60154799e": 78, "03": [78, 87], "73820292e": 78, "65885226e": 78, "04038181e": 78, "51925802e": 78, "73643677e": 78, "95577741e": 78, "09312067e": 78, "39698386e": 78, "11006736e": 78, "25540316e": 78, "87149896e": 78, "65674657e": 78, "43987098e": 78, "79396772e": 78, "00662204e": 78, "45058060e": 78, "36910174e": 78, "82942520e": 78, "24798042e": 78, "85055751e": 78, "78386103e": 78, "24821486e": 78, "81510593e": 78, "07917011e": 78, "30485535e": 78, "19925834e": 78, "56662779e": 78, "25658545e": 78, "29514395e": 78, "73054542e": 78, "57650283e": 78, "87089108e": 78, "31973699e": 78, "45874534e": 78, "78385898e": 78, "24821523e": 78, "38282800e": 78, "29527006e": 78, "24821542e": 78, "45585343e": 78, "20149602e": 78, "39614227e": 78, "52603984e": 78, "02427802e": 78, "38569428e": 78, "20058507e": 78, "89658767e": 78, "67997003e": 78, "90682733e": 78, "88636552e": 78, "61071175e": 78, "75768661e": 78, "22418800e": 78, "07488209e": 78, "26928225e": 78, "52925774e": 78, "50903371e": 78, "71863856e": 78, "40345353e": 78, "36909867e": 78, "82943824e": 78, "90223058e": 78, "08467136e": 78, "43380561e": 78, "27135092e": 78, "31898531e": 78, "01219751e": 78, "78963115e": 78, "97890193e": 78, "49229891e": 78, "67953214e": 78, "75078206e": 78, "75904313e": 78, "03583546e": 78, "12457962e": 78, "10897127e": 78, "18870673e": 78, "28672193e": 78, "61245163e": 78, "48166016e": 78, "75217551e": 78, "67996958e": 78, "90682673e": 78, "44834775e": 78, "90006804e": 78, "59747154e": 78, "69860917e": 78, "59747209e": 78, "69862127e": 78, "59747284e": 78, "69861429e": 78, "59747191e": 78, "59747247e": 78, "69860823e": 78, "59747135e": 78, "11979373e": 78, "90869734e": 78, "59747228e": 78, "69860637e": 78, "59747303e": 78, "69861010e": 78, "59747116e": 78, "17587730e": 78, "43268425e": 78, "43105909e": 78, "32787512e": 78, "03376685e": 78, "44168448e": 78, "62169540e": 78, "41996737e": 78, "73246880e": 78, "97727704e": 78, "03496753e": 78, "71378374e": 78, "92902595e": 78, "15740368e": 78, "94057676e": 78, "48602486e": 78, "40909785e": 78, "14646482e": 78, "38315065e": 78, "76777497e": 78, "38311899e": 78, "76780128e": 78, "37373477e": 78, "49392605e": 78, "30545244e": 78, "10224779e": 78, "69429579e": 78, "59057510e": 78, "11831834e": 78, "86165255e": 78, "07662510e": 78, "53556532e": 78, "82225195e": 78, "76254632e": 78, "62731223e": 78, "63466549e": 78, "16528196e": 78, "62805045e": 78, "36022410e": 78, "48832843e": 78, "19494419e": 78, "13972221e": 78, "zia003": 78, "anaconda3": [78, 82, 83, 84, 86], "env": [78, 82, 83, 84, 86], "topox2": 78, "lib": [78, 82, 83, 84, 86], "site": [78, 82, 83, 84, 86], "_index": [78, 82, 83], "sparseefficiencywarn": [78, 82, 83], "chang": [78, 82, 83, 87], "sparsiti": [78, 82, 83], "lil_matrix": [78, 82, 83], "_set_arrayxarrai": [78, 82, 83], "produc": [78, 79, 80, 82, 86], "compar": [78, 79, 80, 82, 86], "binary_cross_entropy_with_logit": [78, 79, 80, 82, 86], "y_hat_test": [78, 79, 80, 82, 83, 86], "y_pred_test": [78, 79, 80, 82, 83, 86], "7231": 78, "6000": [78, 83], "6989": 78, "5667": [78, 79, 82, 83], "2500": 78, "6737": [78, 83], "6434": 78, "6362": 78, "6199": 78, "6117": 78, "6057": 78, "6011": 78, "5964": 78, "5911": 78, "5855": 78, "5805": 78, "5764": 78, "4000": 78, "5730": 78, "5696": 78, "5660": 78, "5624": [78, 85], "5593": 78, "5567": 78, "5520": 78, "5496": 78, "5472": 78, "5451": 78, "5432": 78, "5414": 78, "5397": 78, "5379": 78, "5362": 78, "5346": 78, "5320": 78, "5308": 78, "5295": 78, "5284": 78, "5273": 78, "5264": 78, "5255": 78, "5246": 78, "5238": 78, "5230": 78, "5223": 78, "5216": 78, "5210": 78, "5204": 78, "5198": 78, "5193": 78, "5188": 78, "5183": 78, "5178": 78, "5174": 78, "5170": 78, "5166": 78, "5163": 78, "5159": 78, "5156": 78, "7216": 79, "7169": 79, "7151": 79, "7109": 79, "7039": 79, "work": 80, "novel": 80, "hing": 80, "proper": 80, "orient": 80, "fashion": 80, "kernel": 80, "l_r": 80, "widetild": 80, "hy": 80, "neq": [80, 87], "affin": 80, "therefor": 80, "hop": [80, 86], "suppos": 80, "_j": 80, "underset": 80, "w_": 80, "q_": 80, "tb_1": 80, "b_2b_2": 80, "notic": 80, "pattern": 80, "just": [80, 82, 87], "maxium": 80, "valueerror": [80, 83, 86], "gradient": 80, "tx_0": 80, "estim": 80, "diverg": 80, "deriv": 80, "seen": 80, "incidence_0_1": 80, "accordingli": 80, "mm": 80, "henc": 80, "y_hat_edg": 80, "fn": 80, "y_hat_edge_test": 80, "_pred_test": 80, "ge": 80, "7322": 80, "3667": 80, "7208": 80, "7333": [80, 82], "7070": 80, "6753": 80, "6695": 80, "6682": 80, "6674": 80, "laplacian_down_1_list": [81, 83, 86], "laplacian_down_2_list": 81, "incidence1_t_list": 81, "incidence2_t_list": 81, "laplacian_down_2": [81, 83, 86], "laplacian_down_1_train": [81, 83], "laplacian_down_1_test": [81, 83], "laplacian_down_2_train": 81, "laplacian_down_2_test": 81, "incidence1_t_train": 81, "incidence1_t_test": 81, "incidence2_t_train": 81, "incidence2_t_test": 81, "hzpmc22": 81, "did": 81, "la": 82, "r_": 82, "mathrm": 82, "leq": [82, 87], "feat_dim": 82, "arbitrari": 82, "choos": [82, 87], "dictionari": 82, "tha": 82, "arbitrarili": 82, "formul": 82, "quit": 82, "close": 82, "usual": 82, "suggest": 82, "refrain": 82, "tetrahedron": 82, "sparse_to_torch": 82, "rank_": 82, "rank_0": 82, "coadjacency_matrix": [82, 83], "h0": 82, "h1": 82, "h3": 82, "b3": 82, "ninamiolan": [82, 83, 86], "tmx": [82, 83, 86], "python3": [82, 83, 84, 86], "x_3": 82, "tetrahedron_feat": 82, "track": 82, "rank_1": 82, "rank_2": 82, "rank_3": 82, "typic": 82, "due": [82, 83, 86], "6721": 82, "6333": [82, 83], "6284": 82, "6173": 82, "6110": 82, "7000": [82, 83], "5831": 82, "5695": 82, "5638": 82, "5493": 82, "5384": 82, "7667": 82, "5141": 82, "5038": 82, "4906": 82, "4763": 82, "4545": 82, "4483": 82, "4153": 82, "8000": 82, "4062": 82, "3790": 82, "3916": 82, "3529": 82, "8667": 82, "2900": 82, "2359": 82, "9333": 82, "2002": 82, "9667": 82, "2970": 82, "9000": 82, "2032": 82, "2329": 82, "2019": 82, "0873": 82, "0293": 82, "0272": 82, "0264": 82, "0245": 82, "0207": 82, "0165": 82, "0132": 82, "0114": 82, "0113": 82, "0117": 82, "0101": 82, "0081": 82, "0071": 82, "0065": 82, "0061": 82, "0057": 82, "0054": 82, "0050": 82, "0046": 82, "0043": 82, "0040": 82, "0038": 82, "0036": 82, "0034": 82, "0033": 82, "0032": 82, "0031": 82, "0030": 82, "0029": 82, "0028": 82, "0027": 82, "0026": 82, "0025": 82, "0024": 82, "0023": 82, "0022": 82, "0021": 82, "0020": 82, "0019": 82, "0018": 82, "0017": 82, "0016": 82, "0015": 82, "0014": 82, "0013": 82, "0012": 82, "0011": 82, "0010": 82, "0009": 82, "0008": 82, "account": 83, "_t": [83, 86], "p_d": [83, 86], "p_u": [83, 86], "likewis": 83, "essenti": 83, "_0": 83, "yet": [83, 86], "laplacian_0_list": [83, 86], "laplacian_up_1_list": [83, 86], "laplacian_2_list": [83, 86], "hodge_laplacian_matrix": [83, 86], "size_averag": 83, "reduct": [83, 86], "in_linear_0": 83, "in_linear_1": 83, "in_linear_2": 83, "out_linear_0": 83, "out_linear_1": 83, "out_linear_2": 83, "_reduct": 83, "arg": [83, 84], "ret": 83, "laplacian_0_train": 83, "laplacian_0_test": 83, "laplacian_up_1_train": 83, "laplacian_up_1_test": 83, "laplacian_2_train": 83, "laplacian_2_test": 83, "lead": [83, 86], "incorrect": [83, 86], "ensur": [83, 86], "mse_loss": [83, 86], "944857": 83, "9959": 83, "5353": 83, "2055": 83, "6187": 83, "7969": 83, "4612": 83, "5951": 83, "8349": 83, "8397": 83, "9482": 83, "laplacian_up_2": [83, 86], "get_simplicial_featur": [83, 86], "which_feat": [83, 86], "elif": [83, 86], "binary_cross_entropi": 83, "7911": 83, "coo_matrix": 84, "diag": 84, "return_count": 84, "normalize_higher_order_adj": 84, "a_opt": 84, "cochain": 84, "num_of_k_simplic": 84, "num_of_j_simplic": 84, "rowsum": 84, "r_inv_sqrt": 84, "flatten": 84, "isinf": 84, "r_mat_inv_sqrt": 84, "a_opt_to": 84, "dot": 84, "neigborood": 84, "ssconv": 84, "get_neighborhood": 84, "incidence_1_norm_list": 84, "incidence_2_norm_list": 84, "adjacency_up_0_norm_list": 84, "adjacency_up_1_norm_list": 84, "adjacency_down_1_norm_list": 84, "adjacency_down_2_norm_list": 84, "up_laplacian_1_list": 84, "up_laplacian_2_list": 84, "down_laplacian_1_list": 84, "down_laplacian_2_list": 84, "up_laplacian_1": 84, "up_laplacian_2": 84, "down_laplacian_1": 84, "down_laplacian_2": 84, "todo": 84, "home": 84, "kha053": 84, "nvml": 84, "incid1": 84, "incid1_norm": 84, "incid2": 84, "incid2_norm": 84, "adj0_up_norm": 84, "adj1_up_norm": 84, "adj1_down_norm": 84, "adj2_down_norm": 84, "correct_count": 84, "x_0t": 84, "x_1t": 84, "x_2t": 84, "incid1t": 84, "incid1_normt": 84, "incid2t": 84, "incid2_normt": 84, "adj0_up_normt": 84, "adj1_up_normt": 84, "adj1_down_normt": 84, "adj2_down_normt": 84, "yt": 84, "ysb22": 85, "ruochen": 85, "freder": 85, "razvan": 85, "pascanu": 85, "editor": 85, "confer": 85, "volum": 85, "pmlr": 85, "dec": 85, "2022a": 85, "chose": 85, "reshap": 85, "normalized_laplacian_matrix": 85, "x_0s_train": 85, "x_0s_test": 85, "x_1s_train": 85, "x_1s_test": 85, "x_2s_train": 85, "x_2s_test": 85, "laplacian_0s_train": 85, "laplacian_0s_test": 85, "laplacian_1s_train": 85, "laplacian_1s_test": 85, "laplacian_2s_train": 85, "laplacian_2s_test": 85, "6056": 85, "2707": 85, "9831": 85, "8605": 85, "0164": 85, "0106": 85, "9957": 85, "5802": 85, "itself": 86, "larger": 86, "x_train": 86, "x_test": 86, "laplacian_down_train": 86, "laplacian_down_test": 86, "laplacian_up_train": 86, "laplacian_up_test": 86, "simplex_order_select": 86, "7131": 86, "5218": 86, "6860": 86, "6109": 86, "7219": 86, "8411": 86, "7747": 86, "2382": 86, "0308": 86, "maxim": 86, "chennel_edg": 86, "channel_fac": 86, "classm": 86, "rm": 86, "channels_x": 86, "squeez": [86, 87], "8799": 86, "rgs21": 87, "spend": 87, "synthet": 87, "ahead": 87, "itertool": 87, "product": 87, "tnx": 87, "networkx": 87, "nx": 87, "random_split": 87, "tqdm": 87, "spatial": 87, "distanc": 87, "seed": 87, "lt": 87, "_c": 87, "0x1664f0f50": 87, "less": 87, "cloud": 87, "insid": 87, "remov": 87, "centroid": 87, "sort": 87, "argsort": 87, "tri": 87, "disk_cent": 87, "disk_radiu": 87, "indices_includ": 87, "cdist": 87, "idx_dict": 87, "instanc": 87, "shortest": 87, "plot_complex": 87, "plane": 87, "poli": 87, "polygon": 87, "color": 87, "green": 87, "gca": 87, "add_patch": 87, "vstack": 87, "i_1": 87, "i_2": 87, "ldot": 87, "i_m": 87, "i_j": 87, "i_": 87, "supervis": 87, "setup": 87, "subsect": 87, "randomli": 87, "pick": 87, "triplet": 87, "around": 87, "anti": 87, "diagon": 87, "mid": 87, "region": 87, "start_nod": 87, "mid_nod": 87, "end_nod": 87, "all_triplet": 87, "increas": 87, "underli": 87, "distance_matrix": 87, "squareform": 87, "pdist": 87, "toarrai": 87, "from_numpy_arrai": 87, "path_1": 87, "shortest_path": 87, "path_2": 87, "plot_path": 87, "red": 87, "arrow": 87, "quiver": 87, "scale_unit": 87, "yield": 87, "vectorized_trajectori": 87, "neigbors_mask": 87, "last_nod": 87, "turn": 87, "a_1": 87, "a_2": 87, "a_j": 87, "i_n": 87, "later": 87, "lookup": 87, "speed": 87, "edge_lookup_t": 87, "discard": 87, "neighbors_mask": 87, "c0": 87, "loader": 87, "val_siz": 87, "train_siz": 87, "train_d": 87, "val_d": 87, "test_d": 87, "train_dl": 87, "val_dl": 87, "test_dl": 87, "c_1": 87, "partial_1": 87, "c_0": 87, "That": 87, "hat": 87, "_m": 87, "likelihood": 87, "penal": 87, "weight_decai": 87, "5e": 87, "loss_funct": 87, "nllloss": 87, "nll": 87, "training_histori": 87, "training_loss": 87, "traj": 87, "06": 87, "quick": 87, "confirm": 87, "everyth": 87, "reason": 87, "ax": 87, "ncol": 87, "figsiz": 87, "better": 87, "guess": 87, "3f": 87, "constructor": 87, "affect": 87, "capabl": 87, "revers": 87, "Or": 87, "ocean": 87, "drifter": 87}, "objects": {"topomodelx.base": [[0, 0, 0, "-", "aggregation"], [1, 0, 0, "-", "conv"], [3, 0, 0, "-", "message_passing"]], "topomodelx.base.aggregation": [[0, 1, 1, "", "Aggregation"]], "topomodelx.base.aggregation.Aggregation": [[0, 2, 1, "", "forward"], [0, 2, 1, "", "update"]], "topomodelx.base.conv": [[1, 1, 1, "", "Conv"]], "topomodelx.base.conv.Conv": [[1, 2, 1, "", "forward"], [1, 2, 1, "", "update"]], "topomodelx.base.message_passing": [[3, 1, 1, "", "MessagePassing"]], "topomodelx.base.message_passing.MessagePassing": [[3, 2, 1, "", "aggregate"], [3, 2, 1, "", "attention"], [3, 2, 1, "", "forward"], [3, 2, 1, "", "message"], [3, 2, 1, "", "reset_parameters"]], "topomodelx.nn.cell": [[5, 0, 0, "-", "can"], [6, 0, 0, "-", "can_layer"], [7, 0, 0, "-", "ccxn"], [8, 0, 0, "-", "ccxn_layer"], [9, 0, 0, "-", "cwn"], [10, 0, 0, "-", "cwn_layer"]], "topomodelx.nn.cell.can": [[5, 1, 1, "", "CAN"]], "topomodelx.nn.cell.can.CAN": [[5, 2, 1, "", "forward"]], "topomodelx.nn.cell.can_layer": [[6, 1, 1, "", "CANLayer"], [6, 1, 1, "", "LiftLayer"], [6, 1, 1, "", "MultiHeadCellAttention"], [6, 1, 1, "", "MultiHeadCellAttention_v2"], [6, 1, 1, "", "MultiHeadLiftLayer"], [6, 1, 1, "", "PoolLayer"], [6, 3, 1, "", "add_self_loops"], [6, 3, 1, "", "softmax"]], "topomodelx.nn.cell.can_layer.CANLayer": [[6, 2, 1, "", "forward"], [6, 2, 1, "", "reset_parameters"]], "topomodelx.nn.cell.can_layer.LiftLayer": [[6, 2, 1, "", "forward"], [6, 2, 1, "", "message"], [6, 2, 1, "", "reset_parameters"]], "topomodelx.nn.cell.can_layer.MultiHeadCellAttention": [[6, 2, 1, "", "attention"], [6, 2, 1, "", "forward"], [6, 2, 1, "", "message"], [6, 2, 1, "", "reset_parameters"]], "topomodelx.nn.cell.can_layer.MultiHeadCellAttention_v2": [[6, 2, 1, "", "attention"], [6, 2, 1, "", "forward"], [6, 2, 1, "", "message"], [6, 2, 1, "", "reset_parameters"]], "topomodelx.nn.cell.can_layer.MultiHeadLiftLayer": [[6, 2, 1, "", "forward"], [6, 2, 1, "", "reset_parameters"]], "topomodelx.nn.cell.can_layer.PoolLayer": [[6, 2, 1, "", "forward"], [6, 2, 1, "", "reset_parameters"]], "topomodelx.nn.cell.ccxn": [[7, 1, 1, "", "CCXN"]], "topomodelx.nn.cell.ccxn.CCXN": [[7, 2, 1, "", "forward"]], "topomodelx.nn.cell.ccxn_layer": [[8, 1, 1, "", "CCXNLayer"]], "topomodelx.nn.cell.ccxn_layer.CCXNLayer": [[8, 2, 1, "", "forward"]], "topomodelx.nn.cell.cwn": [[9, 1, 1, "", "CWN"]], "topomodelx.nn.cell.cwn.CWN": [[9, 2, 1, "", "forward"]], "topomodelx.nn.cell.cwn_layer": [[10, 1, 1, "", "CWNLayer"]], "topomodelx.nn.cell.cwn_layer.CWNLayer": [[10, 2, 1, "", "forward"]], "topomodelx.nn.hypergraph": [[12, 0, 0, "-", "allset"], [13, 0, 0, "-", "allset_layer"], [14, 0, 0, "-", "allset_transformer"], [15, 0, 0, "-", "allset_transformer_layer"], [18, 0, 0, "-", "hmpnn"], [19, 0, 0, "-", "hmpnn_layer"], [20, 0, 0, "-", "hnhn"], [21, 0, 0, "-", "hnhn_layer"], [22, 0, 0, "-", "hnhn_layer_bis"], [23, 0, 0, "-", "hypergat"], [24, 0, 0, "-", "hypergat_layer"], [25, 0, 0, "-", "hypersage"], [26, 0, 0, "-", "hypersage_layer"], [28, 0, 0, "-", "unigcn"], [29, 0, 0, "-", "unigcn_layer"], [30, 0, 0, "-", "unigcnii"], [31, 0, 0, "-", "unigcnii_layer"], [32, 0, 0, "-", "unigin"], [33, 0, 0, "-", "unigin_layer"], [34, 0, 0, "-", "unisage"], [35, 0, 0, "-", "unisage_layer"]], "topomodelx.nn.hypergraph.allset": [[12, 1, 1, "", "AllSet"]], "topomodelx.nn.hypergraph.allset.AllSet": [[12, 2, 1, "", "forward"]], "topomodelx.nn.hypergraph.allset_layer": [[13, 1, 1, "", "AllSetBlock"], [13, 1, 1, "", "AllSetLayer"], [13, 1, 1, "", "MLP"]], "topomodelx.nn.hypergraph.allset_layer.AllSetBlock": [[13, 2, 1, "", "forward"], [13, 2, 1, "", "reset_parameters"]], "topomodelx.nn.hypergraph.allset_layer.AllSetLayer": [[13, 2, 1, "", "forward"], [13, 2, 1, "", "reset_parameters"]], "topomodelx.nn.hypergraph.allset_transformer": [[14, 1, 1, "", "AllSetTransformer"]], "topomodelx.nn.hypergraph.allset_transformer.AllSetTransformer": [[14, 2, 1, "", "forward"]], "topomodelx.nn.hypergraph.allset_transformer_layer": [[15, 1, 1, "", "AllSetTransformerBlock"], [15, 1, 1, "", "AllSetTransformerLayer"], [15, 1, 1, "", "MLP"], [15, 1, 1, "", "MultiHeadAttention"]], "topomodelx.nn.hypergraph.allset_transformer_layer.AllSetTransformerBlock": [[15, 2, 1, "", "forward"], [15, 2, 1, "", "reset_parameters"]], "topomodelx.nn.hypergraph.allset_transformer_layer.AllSetTransformerLayer": [[15, 2, 1, "", "forward"], [15, 2, 1, "", "reset_parameters"]], "topomodelx.nn.hypergraph.allset_transformer_layer.MultiHeadAttention": [[15, 2, 1, "", "attention"], [15, 2, 1, "", "forward"], [15, 2, 1, "", "reset_parameters"]], "topomodelx.nn.hypergraph.hmpnn": [[18, 1, 1, "", "HMPNN"]], "topomodelx.nn.hypergraph.hmpnn.HMPNN": [[18, 2, 1, "", "forward"]], "topomodelx.nn.hypergraph.hmpnn_layer": [[19, 1, 1, "", "HMPNNLayer"]], "topomodelx.nn.hypergraph.hmpnn_layer.HMPNNLayer": [[19, 2, 1, "", "apply_regular_dropout"], [19, 2, 1, "", "forward"]], "topomodelx.nn.hypergraph.hnhn": [[20, 1, 1, "", "HNHN"], [20, 1, 1, "", "HNHNNetwork"]], "topomodelx.nn.hypergraph.hnhn.HNHN": [[20, 2, 1, "", "forward"]], "topomodelx.nn.hypergraph.hnhn.HNHNNetwork": [[20, 2, 1, "", "forward"]], "topomodelx.nn.hypergraph.hnhn_layer": [[21, 1, 1, "", "HNHNLayer"]], "topomodelx.nn.hypergraph.hnhn_layer.HNHNLayer": [[21, 2, 1, "", "compute_normalization_matrices"], [21, 2, 1, "", "forward"], [21, 2, 1, "", "init_biases"], [21, 2, 1, "", "normalize_incidence_matrices"], [21, 2, 1, "", "reset_parameters"]], "topomodelx.nn.hypergraph.hnhn_layer_bis": [[22, 1, 1, "", "HNHNLayer"]], "topomodelx.nn.hypergraph.hnhn_layer_bis.HNHNLayer": [[22, 2, 1, "", "forward"]], "topomodelx.nn.hypergraph.hypergat": [[23, 1, 1, "", "HyperGAT"]], "topomodelx.nn.hypergraph.hypergat.HyperGAT": [[23, 2, 1, "", "forward"]], "topomodelx.nn.hypergraph.hypergat_layer": [[24, 1, 1, "", "HyperGATLayer"]], "topomodelx.nn.hypergraph.hypergat_layer.HyperGATLayer": [[24, 2, 1, "", "attention"], [24, 2, 1, "", "forward"], [24, 2, 1, "", "reset_parameters"], [24, 2, 1, "", "update"]], "topomodelx.nn.hypergraph.hypersage": [[25, 1, 1, "", "HyperSAGE"]], "topomodelx.nn.hypergraph.hypersage.HyperSAGE": [[25, 2, 1, "", "forward"]], "topomodelx.nn.hypergraph.hypersage_layer": [[26, 1, 1, "", "GeneralizedMean"], [26, 1, 1, "", "HyperSAGELayer"]], "topomodelx.nn.hypergraph.hypersage_layer.GeneralizedMean": [[26, 2, 1, "", "forward"]], "topomodelx.nn.hypergraph.hypersage_layer.HyperSAGELayer": [[26, 2, 1, "", "aggregate"], [26, 2, 1, "", "forward"], [26, 2, 1, "", "update"]], "topomodelx.nn.hypergraph.unigcn": [[28, 1, 1, "", "UniGCN"]], "topomodelx.nn.hypergraph.unigcn.UniGCN": [[28, 2, 1, "", "forward"]], "topomodelx.nn.hypergraph.unigcn_layer": [[29, 1, 1, "", "UniGCNLayer"]], "topomodelx.nn.hypergraph.unigcn_layer.UniGCNLayer": [[29, 2, 1, "", "forward"], [29, 2, 1, "", "reset_parameters"]], "topomodelx.nn.hypergraph.unigcnii": [[30, 1, 1, "", "UniGCNII"]], "topomodelx.nn.hypergraph.unigcnii.UniGCNII": [[30, 2, 1, "", "forward"]], "topomodelx.nn.hypergraph.unigcnii_layer": [[31, 1, 1, "", "UniGCNIILayer"]], "topomodelx.nn.hypergraph.unigcnii_layer.UniGCNIILayer": [[31, 2, 1, "", "forward"], [31, 2, 1, "", "reset_parameters"]], "topomodelx.nn.hypergraph.unigin": [[32, 1, 1, "", "UniGIN"]], "topomodelx.nn.hypergraph.unigin.UniGIN": [[32, 2, 1, "", "forward"]], "topomodelx.nn.hypergraph.unigin_layer": [[33, 1, 1, "", "UniGINLayer"]], "topomodelx.nn.hypergraph.unigin_layer.UniGINLayer": [[33, 2, 1, "", "forward"]], "topomodelx.nn.hypergraph.unisage": [[34, 1, 1, "", "UniSAGE"]], "topomodelx.nn.hypergraph.unisage.UniSAGE": [[34, 2, 1, "", "forward"]], "topomodelx.nn.hypergraph.unisage_layer": [[35, 1, 1, "", "UniSAGELayer"]], "topomodelx.nn.hypergraph.unisage_layer.UniSAGELayer": [[35, 2, 1, "", "forward"], [35, 2, 1, "", "reset_parameters"]], "topomodelx.nn.simplicial": [[37, 0, 0, "-", "dist2cycle"], [38, 0, 0, "-", "dist2cycle_layer"], [39, 0, 0, "-", "hsn"], [40, 0, 0, "-", "hsn_layer"], [42, 0, 0, "-", "san"], [43, 0, 0, "-", "san_layer"], [44, 0, 0, "-", "sca_cmps"], [45, 0, 0, "-", "sca_cmps_layer"], [46, 0, 0, "-", "sccn"], [47, 0, 0, "-", "sccn_layer"], [48, 0, 0, "-", "sccnn"], [49, 0, 0, "-", "sccnn_layer"], [50, 0, 0, "-", "scconv"], [51, 0, 0, "-", "scconv_layer"], [52, 0, 0, "-", "scn2"], [53, 0, 0, "-", "scn2_layer"], [54, 0, 0, "-", "scnn"], [55, 0, 0, "-", "scnn_layer"], [56, 0, 0, "-", "scone"], [57, 0, 0, "-", "scone_layer"]], "topomodelx.nn.simplicial.dist2cycle": [[37, 1, 1, "", "Dist2Cycle"]], "topomodelx.nn.simplicial.dist2cycle.Dist2Cycle": [[37, 2, 1, "", "forward"]], "topomodelx.nn.simplicial.dist2cycle_layer": [[38, 1, 1, "", "Dist2CycleLayer"]], "topomodelx.nn.simplicial.dist2cycle_layer.Dist2CycleLayer": [[38, 2, 1, "", "forward"], [38, 2, 1, "", "reset_parameters"]], "topomodelx.nn.simplicial.hsn": [[39, 1, 1, "", "HSN"]], "topomodelx.nn.simplicial.hsn.HSN": [[39, 2, 1, "", "forward"]], "topomodelx.nn.simplicial.hsn_layer": [[40, 1, 1, "", "HSNLayer"]], "topomodelx.nn.simplicial.hsn_layer.HSNLayer": [[40, 2, 1, "", "forward"], [40, 2, 1, "", "reset_parameters"]], "topomodelx.nn.simplicial.san": [[42, 1, 1, "", "SAN"]], "topomodelx.nn.simplicial.san.SAN": [[42, 2, 1, "", "compute_projection_matrix"], [42, 2, 1, "", "forward"]], "topomodelx.nn.simplicial.san_layer": [[43, 1, 1, "", "SANConv"], [43, 1, 1, "", "SANLayer"]], "topomodelx.nn.simplicial.san_layer.SANConv": [[43, 2, 1, "", "forward"]], "topomodelx.nn.simplicial.san_layer.SANLayer": [[43, 2, 1, "", "forward"], [43, 2, 1, "", "reset_parameters"]], "topomodelx.nn.simplicial.sca_cmps": [[44, 1, 1, "", "SCACMPS"]], "topomodelx.nn.simplicial.sca_cmps.SCACMPS": [[44, 2, 1, "", "forward"]], "topomodelx.nn.simplicial.sca_cmps_layer": [[45, 1, 1, "", "SCACMPSLayer"]], "topomodelx.nn.simplicial.sca_cmps_layer.SCACMPSLayer": [[45, 2, 1, "", "forward"], [45, 2, 1, "", "intra_aggr"], [45, 2, 1, "", "reset_parameters"], [45, 2, 1, "", "weight_func"]], "topomodelx.nn.simplicial.sccn": [[46, 1, 1, "", "SCCN"]], "topomodelx.nn.simplicial.sccn.SCCN": [[46, 2, 1, "", "forward"]], "topomodelx.nn.simplicial.sccn_layer": [[47, 1, 1, "", "SCCNLayer"]], "topomodelx.nn.simplicial.sccn_layer.SCCNLayer": [[47, 2, 1, "", "forward"], [47, 2, 1, "", "reset_parameters"]], "topomodelx.nn.simplicial.sccnn": [[48, 1, 1, "", "SCCNN"], [48, 1, 1, "", "SCCNNComplex"]], "topomodelx.nn.simplicial.sccnn.SCCNN": [[48, 2, 1, "", "forward"]], "topomodelx.nn.simplicial.sccnn.SCCNNComplex": [[48, 2, 1, "", "forward"]], "topomodelx.nn.simplicial.sccnn_layer": [[49, 1, 1, "", "SCCNNLayer"]], "topomodelx.nn.simplicial.sccnn_layer.SCCNNLayer": [[49, 2, 1, "", "aggr_norm_func"], [49, 2, 1, "", "chebyshev_conv"], [49, 2, 1, "", "forward"], [49, 2, 1, "", "reset_parameters"], [49, 2, 1, "", "update"]], "topomodelx.nn.simplicial.scconv": [[50, 1, 1, "", "SCConv"]], "topomodelx.nn.simplicial.scconv.SCConv": [[50, 2, 1, "", "forward"]], "topomodelx.nn.simplicial.scconv_layer": [[51, 1, 1, "", "SCConvLayer"]], "topomodelx.nn.simplicial.scconv_layer.SCConvLayer": [[51, 2, 1, "", "forward"], [51, 2, 1, "", "reset_parameters"]], "topomodelx.nn.simplicial.scn2": [[52, 1, 1, "", "SCN2"]], "topomodelx.nn.simplicial.scn2.SCN2": [[52, 2, 1, "", "forward"]], "topomodelx.nn.simplicial.scn2_layer": [[53, 1, 1, "", "SCN2Layer"]], "topomodelx.nn.simplicial.scn2_layer.SCN2Layer": [[53, 2, 1, "", "forward"], [53, 2, 1, "", "reset_parameters"]], "topomodelx.nn.simplicial.scnn": [[54, 1, 1, "", "SCNN"]], "topomodelx.nn.simplicial.scnn.SCNN": [[54, 2, 1, "", "forward"]], "topomodelx.nn.simplicial.scnn_layer": [[55, 1, 1, "", "SCNNLayer"]], "topomodelx.nn.simplicial.scnn_layer.SCNNLayer": [[55, 2, 1, "", "aggr_norm_func"], [55, 2, 1, "", "chebyshev_conv"], [55, 2, 1, "", "forward"], [55, 2, 1, "", "reset_parameters"], [55, 2, 1, "", "update"]], "topomodelx.nn.simplicial.scone": [[56, 1, 1, "", "SCoNe"], [56, 1, 1, "", "TrajectoriesDataset"], [56, 3, 1, "", "generate_complex"], [56, 3, 1, "", "generate_trajectories"]], "topomodelx.nn.simplicial.scone.SCoNe": [[56, 2, 1, "", "forward"]], "topomodelx.nn.simplicial.scone.TrajectoriesDataset": [[56, 2, 1, "", "vectorize_path"]], "topomodelx.nn.simplicial.scone_layer": [[57, 1, 1, "", "SCoNeLayer"]], "topomodelx.nn.simplicial.scone_layer.SCoNeLayer": [[57, 2, 1, "", "forward"], [57, 2, 1, "", "reset_parameters"]], "topomodelx.utils": [[58, 0, 0, "-", "scatter"]], "topomodelx.utils.scatter": [[58, 3, 1, "", "broadcast"], [58, 3, 1, "", "scatter"], [58, 3, 1, "", "scatter_add"], [58, 3, 1, "", "scatter_mean"], [58, 3, 1, "", "scatter_sum"]]}, "objtypes": {"0": "py:module", "1": "py:class", "2": "py:method", "3": "py:function"}, "objnames": {"0": ["py", "module", "Python module"], "1": ["py", "class", "Python class"], "2": ["py", "method", "Python method"], "3": ["py", "function", "Python function"]}, "titleterms": {"aggreg": 0, "conv": 1, "base": 2, "messag": [3, 69, 81], "pass": [3, 69, 81], "api": 4, "refer": [4, 61, 85], "packag": 4, "modul": 4, "can": [5, 62], "can_lay": 6, "ccxn": [7, 63], "ccxn_layer": 8, "cwn": [9, 64], "cwn_layer": 10, "cell": [11, 62, 63], "allset": 12, "allset_lay": 13, "allset_transform": 14, "allset_transformer_lay": 15, "dhgcn": [16, 68], "dhgcn_layer": 17, "hmpnn": [18, 69], "hmpnn_layer": 19, "hnhn": [20, 70, 71], "hnhn_layer": 21, "hnhn_layer_bi": 22, "hypergat": 23, "hypergat_lay": 24, "hypersag": [25, 73], "hypersage_lay": 26, "hypergraph": [27, 68, 69, 70, 71, 72, 75, 88], "unigcn": [28, 74], "unigcn_lay": 29, "unigcnii": [30, 75], "unigcnii_lay": 31, "neural": [36, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88], "network": [36, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87], "simplici": [41, 78, 79, 80, 81, 82, 83, 84, 86, 87, 88], "util": 58, "icml": 59, "2023": 59, "topolog": [59, 88], "deep": 59, "learn": 59, "challeng": 59, "descript": 59, "public": 59, "outcom": 59, "particip": 59, "deadlin": 59, "how": 59, "submit": 59, "guidelin": 59, "submiss": 59, "requir": 59, "evalu": [59, 87], "question": 59, "contribut": 60, "make": 60, "chang": 60, "write": 60, "test": [60, 81, 84, 86, 87], "run": 60, "document": 60, "intro": 60, "docstr": 60, "The": [60, 62, 63, 64, 65, 80], "anatomi": 60, "exampl": 60, "topomodelx": 61, "tmx": 61, "get": 61, "start": 61, "train": [62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87], "attent": [62, 63, 65, 80], "abstract": [62, 80], "task": [62, 63, 64, 65, 80], "set": [62, 63, 64, 65, 66, 67], "up": [62, 63, 64, 65], "pre": [62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86], "process": [62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86], "creat": [62, 63, 64, 65, 66, 68, 70, 71, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 86, 87], "convolut": [63, 82, 83, 84, 85, 86], "complex": [63, 65, 81, 82, 83, 84, 86, 87, 88], "cw": 64, "combinatori": 65, "mesh": 65, "classif": [65, 83, 86], "import": [65, 68, 70, 72, 74, 76, 77, 78, 79, 81, 82, 83, 84, 85, 86], "data": [65, 68, 72, 74, 75, 76, 77, 81, 84, 87], "an": [66, 67], "all": [66, 67], "tnn": [66, 67, 68, 73, 74, 76, 77], "addit": [66, 67, 73], "theoret": [66, 67, 73], "clarif": [66, 67, 73], "transform": 67, "defin": [67, 68, 70, 72, 73, 78, 79, 82, 83, 84, 85, 86], "neighborhood": [68, 70, 72, 78, 79, 82, 83, 85, 86], "structur": [68, 70, 72, 78, 79, 82, 84, 85], "lift": [68, 72], "domain": [68, 72], "hyperedg": [70, 71], "neuron": [70, 71], "dataset": [70, 78, 79, 82, 83, 84, 85, 86, 87], "signal": [70, 78, 79, 82, 83, 86], "us": 75, "layer": 75, "load": 75, "unigin": 76, "uni": 77, "sage": 77, "homologi": 78, "local": 78, "dist2cycl": 78, "binari": [78, 79, 82, 83, 86], "label": [78, 79, 82, 83, 86], "featur": 78, "high": 79, "skip": 79, "hsn": 79, "san": 80, "autoencod": 81, "sca": 81, "coadjac": 81, "scheme": 81, "cmp": 81, "prepar": [81, 84, 86], "input": 81, "each": 81, "split": [81, 86], "model": [81, 83, 86, 87], "sccn": 82, "sccnn": 83, "we": [83, 86], "perform": [83, 86], "1": [83, 86], "shrec": 83, "strcture": [83, 86], "2": [83, 84, 85, 86], "node": [83, 86], "scconv": 84, "helper": 84, "function": 84, "neighbourhood": 84, "simplex": 85, "scn": 85, "rank": 85, "scnn": 86, "karat": 86, "weight": 86, "hodg": 86, "laplacian": 86, "net": [87, 88], "scone": 87, "tabl": 87, "content": 87, "gener": 87, "trajectori": 87, "pytorch": 87, "dataload": 87, "suggest": 87, "further": 87, "experiment": 87, "tutori": 88, "cellular": 88}, "envversion": {"sphinx.domains.c": 3, "sphinx.domains.changeset": 1, "sphinx.domains.citation": 1, "sphinx.domains.cpp": 9, "sphinx.domains.index": 1, "sphinx.domains.javascript": 3, "sphinx.domains.math": 2, "sphinx.domains.python": 4, "sphinx.domains.rst": 2, "sphinx.domains.std": 2, "nbsphinx": 4, "sphinx.ext.viewcode": 1, "sphinx": 60}, "alltitles": {"Aggregation": [[0, "module-topomodelx.base.aggregation"]], "Conv": [[1, "module-topomodelx.base.conv"]], "Base": [[2, "base"]], "Message Passing": [[3, "module-topomodelx.base.message_passing"]], "API Reference": [[4, "api-reference"]], "Packages & Modules": [[4, null]], "CAN": [[5, "module-topomodelx.nn.cell.can"]], "Can_Layer": [[6, "module-topomodelx.nn.cell.can_layer"]], "CCXN": [[7, "module-topomodelx.nn.cell.ccxn"]], "CCXN_Layer": [[8, "module-topomodelx.nn.cell.ccxn_layer"]], "CWN": [[9, "module-topomodelx.nn.cell.cwn"]], "Cwn_Layer": [[10, "module-topomodelx.nn.cell.cwn_layer"]], "Cell": [[11, "cell"]], "AllSet": [[12, "module-topomodelx.nn.hypergraph.allset"]], "AllSet_Layer": [[13, "module-topomodelx.nn.hypergraph.allset_layer"]], "AllSet_Transformer": [[14, "module-topomodelx.nn.hypergraph.allset_transformer"]], "AllSet_Transformer_Layer": [[15, "module-topomodelx.nn.hypergraph.allset_transformer_layer"]], "DHGCN": [[16, "dhgcn"]], "DHGCN_Layer": [[17, "dhgcn-layer"]], "HMPNN": [[18, "module-topomodelx.nn.hypergraph.hmpnn"]], "HMPNN_Layer": [[19, "module-topomodelx.nn.hypergraph.hmpnn_layer"]], "HNHN": [[20, "module-topomodelx.nn.hypergraph.hnhn"]], "HNHN_Layer": [[21, "module-topomodelx.nn.hypergraph.hnhn_layer"]], "HNHN_Layer_Bis": [[22, "module-topomodelx.nn.hypergraph.hnhn_layer_bis"]], "Hypergat": [[23, "module-topomodelx.nn.hypergraph.hypergat"]], "Hypergat_Layer": [[24, "module-topomodelx.nn.hypergraph.hypergat_layer"]], "Hypersage": [[25, "module-topomodelx.nn.hypergraph.hypersage"]], "Hypersage_Layer": [[26, "module-topomodelx.nn.hypergraph.hypersage_layer"]], "Hypergraph": [[27, "hypergraph"]], "Unigcn": [[28, "module-topomodelx.nn.hypergraph.unigcn"]], "Unigcn_Layer": [[29, "module-topomodelx.nn.hypergraph.unigcn_layer"]], "Unigcnii": [[30, "module-topomodelx.nn.hypergraph.unigcnii"]], "Unigcnii_Layer": [[31, "module-topomodelx.nn.hypergraph.unigcnii_layer"]], "Neural Networks": [[36, "neural-networks"]], "Simplicial": [[41, "simplicial"]], "Utils": [[58, "utils"]], "ICML 2023 Topological Deep Learning Challenge": [[59, "icml-2023-topological-deep-learning-challenge"]], "Description of the Challenge": [[59, "description-of-the-challenge"]], "\u2b50\ufe0f Publication Outcomes for Participants \u2b50\ufe0f": [[59, "publication-outcomes-for-participants"]], "Deadline": [[59, "deadline"]], "How to Submit": [[59, "how-to-submit"]], "Guidelines": [[59, "guidelines"]], "Submission Requirements": [[59, "submission-requirements"]], "Evaluation": [[59, "evaluation"]], "Questions": [[59, "questions"]], "Contributing": [[60, "contributing"]], "Making Changes": [[60, "making-changes"]], "Write Tests": [[60, "write-tests"]], "Run Tests": [[60, "run-tests"]], "Write Documentation": [[60, "write-documentation"]], "Intro to Docstrings": [[60, "intro-to-docstrings"]], "The Anatomy of a Docstring": [[60, "the-anatomy-of-a-docstring"]], "Docstring Examples": [[60, "docstring-examples"]], "\ud83c\udf10 TopoModelX (TMX) \ud83c\udf69": [[61, "topomodelx-tmx"]], "\ud83d\udd0d References": [[61, "references"]], "\ud83e\uddbe Getting Started": [[61, "getting-started"]], "Train a Cell Attention Network (CAN)": [[62, "Train-a-Cell-Attention-Network-(CAN)"]], "Abstract:": [[62, "Abstract:"]], "The Neural Network:": [[62, "The-Neural-Network:"], [63, "The-Neural-Network:"], [64, "The-Neural-Network:"], [65, "The-Neural-Network:"]], "The Task:": [[62, "The-Task:"], [63, "The-Task:"], [64, "The-Task:"], [65, "The-Task:"], [80, "The-Task:"]], "Set-up": [[62, "Set-up"], [63, "Set-up"], [64, "Set-up"], [65, "Set-up"]], "Pre-processing": [[62, "Pre-processing"], [63, "Pre-processing"], [64, "Pre-processing"], [65, "Pre-processing"], [66, "Pre-processing"], [67, "Pre-processing"], [68, "Pre-processing"], [69, "Pre-processing"], [70, "Pre-processing"], [71, "Pre-processing"], [72, "Pre-processing"], [73, "Pre-processing"], [74, "Pre-processing"], [76, "Pre-processing"], [77, "Pre-processing"], [78, "Pre-processing"], [79, "Pre-processing"], [80, "Pre-processing"], [81, "Pre-processing"], [82, "Pre-processing"], [83, "Pre-processing"], [84, "Pre-processing"], [85, "Pre-processing"], [86, "Pre-processing"]], "Create the Neural Network": [[62, "Create-the-Neural-Network"], [63, "Create-the-Neural-Network"], [64, "Create-the-Neural-Network"], [65, "Create-the-Neural-Network"], [66, "Create-the-Neural-Network"], [68, "Create-the-Neural-Network"], [70, "Create-the-Neural-Network"], [71, "Create-the-Neural-Network"], [74, "Create-the-Neural-Network"], [76, "Create-the-Neural-Network"], [77, "Create-the-Neural-Network"], [78, "Create-the-Neural-Network"], [79, "Create-the-Neural-Network"], [80, "Create-the-Neural-Network"], [82, "Create-the-Neural-Network"]], "Train the Neural Network": [[62, "Train-the-Neural-Network"], [63, "Train-the-Neural-Network"], [64, "Train-the-Neural-Network"], [65, "Train-the-Neural-Network"], [66, "Train-the-Neural-Network"], [67, "Train-the-Neural-Network"], [68, "Train-the-Neural-Network"], [69, "Train-the-Neural-Network"], [70, "Train-the-Neural-Network"], [71, "Train-the-Neural-Network"], [72, "Train-the-Neural-Network"], [73, "Train-the-Neural-Network"], [74, "Train-the-Neural-Network"], [76, "Train-the-Neural-Network"], [77, "Train-the-Neural-Network"], [78, "Train-the-Neural-Network"], [79, "Train-the-Neural-Network"], [80, "Train-the-Neural-Network"], [82, "Train-the-Neural-Network"], [85, "Train-the-Neural-Network"], [86, "Train-the-Neural-Network"]], "Train a Convolutional Cell Complex Network (CCXN)": [[63, "Train-a-Convolutional-Cell-Complex-Network-(CCXN)"]], "Train the Neural Network with Attention": [[63, "Train-the-Neural-Network-with-Attention"]], "Train a CW Network (CWN)": [[64, "Train-a-CW-Network-(CWN)"]], "Train a Combinatorial Complex Attention Neural Network for Mesh Classification.": [[65, "Train-a-Combinatorial-Complex-Attention-Neural-Network-for-Mesh-Classification."]], "Import data": [[65, "Import-data"], [68, "Import-data"], [72, "Import-data"], [74, "Import-data"], [76, "Import-data"], [77, "Import-data"], [81, "Import-data"]], "Train an All-Set TNN": [[66, "Train-an-All-Set-TNN"]], "Additional theoretical clarifications": [[66, "Additional-theoretical-clarifications"], [67, "Additional-theoretical-clarifications"], [73, "Additional-theoretical-clarifications"]], "Train an All-Set-Transformer TNN": [[67, "Train-an-All-Set-Transformer-TNN"]], "Define the Neural Network": [[67, "Define-the-Neural-Network"], [73, "Define-the-Neural-Network"]], "Train a DHGCN TNN": [[68, "Train-a-DHGCN-TNN"]], "Define neighborhood structures and lift into hypergraph domain.": [[68, "Define-neighborhood-structures-and-lift-into-hypergraph-domain."], [72, "Define-neighborhood-structures-and-lift-into-hypergraph-domain."]], "Train a Hypergraph Message Passing Neural Network (HMPNN)": [[69, "Train-a-Hypergraph-Message-Passing-Neural-Network-(HMPNN)"]], "Train a Hypergraph Networks with Hyperedge Neurons (HNHN)": [[70, "Train-a-Hypergraph-Networks-with-Hyperedge-Neurons-(HNHN)"]], "Import dataset": [[70, "Import-dataset"], [78, "Import-dataset"], [79, "Import-dataset"], [82, "Import-dataset"], [84, "Import-dataset"], [85, "Import-dataset"]], "Define neighborhood structures.": [[70, "Define-neighborhood-structures."], [78, "Define-neighborhood-structures."], [79, "Define-neighborhood-structures."], [82, "Define-neighborhood-structures."], [85, "Define-neighborhood-structures."]], "Import signal": [[70, "Import-signal"], [78, "Import-signal"], [79, "Import-signal"], [82, "Import-signal"], [83, "Import-signal"]], "Train a Hypergraph Network with Hyperedge Neurons (HNHN)": [[71, "Train-a-Hypergraph-Network-with-Hyperedge-Neurons-(HNHN)"]], "Train a Hypergraph Neural Network": [[72, "Train-a-Hypergraph-Neural-Network"]], "Train a Hypersage TNN": [[73, "Train-a-Hypersage-TNN"]], "Train a UNIGCN TNN": [[74, "Train-a-UNIGCN-TNN"]], "Train a hypergraph neural network using UniGCNII layers": [[75, "Train-a-hypergraph-neural-network-using-UniGCNII-layers"]], "Loading the data": [[75, "Loading-the-data"]], "Creating a neural network": [[75, "Creating-a-neural-network"]], "Training the neural network": [[75, "Training-the-neural-network"]], "Train a UNIGIN TNN": [[76, "Train-a-UNIGIN-TNN"]], "Train a Uni-sage TNN": [[77, "Train-a-Uni-sage-TNN"]], "Train a Simplicial Neural Network for Homology Localization (Dist2Cycle)": [[78, "Train-a-Simplicial-Neural-Network-for-Homology-Localization-(Dist2Cycle)"]], "Define binary labels": [[78, "Define-binary-labels"], [79, "Define-binary-labels"], [82, "Define-binary-labels"], [83, "Define-binary-labels"]], "Create Features": [[78, "Create-Features"]], "Train a Simplicial High-Skip Network (HSN)": [[79, "Train-a-Simplicial-High-Skip-Network-(HSN)"]], "Train a Simplicial Attention Network (SAN)": [[80, "Train-a-Simplicial-Attention-Network-(SAN)"]], "Abstract": [[80, "Abstract"]], "The Neural Network": [[80, "The-Neural-Network"]], "Train a Simplicial Complex Autoencoder (SCA) with Coadjacency Message Passing Scheme (CMPS)": [[81, "Train-a-Simplicial-Complex-Autoencoder-(SCA)-with-Coadjacency-Message-Passing-Scheme-(CMPS)"]], "Preparing the inputs to test each message passing scheme:": [[81, "Preparing-the-inputs-to-test-each-message-passing-scheme:"]], "Coadjacency Message Passing Scheme (CMPS):": [[81, "Coadjacency-Message-Passing-Scheme-(CMPS):"]], "Create the Neural Networks": [[81, "Create-the-Neural-Networks"]], "Train and Test Split": [[81, "Train-and-Test-Split"]], "Training and Testing Model": [[81, "Training-and-Testing-Model"]], "Train a Simplicial Complex Convolutional Network (SCCN)": [[82, "Train-a-Simplicial-Complex-Convolutional-Network-(SCCN)"]], "Train a SCCNN": [[83, "Train-a-SCCNN"]], "We train the model to perform:": [[83, "We-train-the-model-to-perform:"], [86, "We-train-the-model-to-perform:"]], "Simplicial Complex Convolutional Neural Networks [SCCNN]": [[83, "Simplicial-Complex-Convolutional-Neural-Networks-[SCCNN]"]], "1. Complex Classification": [[83, "1.-Complex-Classification"], [86, "1.-Complex-Classification"]], "Import shrec dataset": [[83, "Import-shrec-dataset"]], "Define Neighborhood Strctures": [[83, "Define-Neighborhood-Strctures"], [83, "id1"], [86, "Define-Neighborhood-Strctures"], [86, "id1"]], "Create and Train the Neural Network": [[83, "Create-and-Train-the-Neural-Network"], [83, "id2"], [84, "Create-and-Train-the-Neural-Network"]], "2. Node Classification": [[83, "2.-Node-Classification"], [86, "2.-Node-Classification"]], "Train a Simplicial 2-complex convolutional neural network (SCConv)": [[84, "Train-a-Simplicial-2-complex-convolutional-neural-network-(SCConv)"]], "Helper functions": [[84, "Helper-functions"]], "Define Neighbourhood Structures": [[84, "Define-Neighbourhood-Structures"]], "prepare training and test data": [[84, "prepare-training-and-test-data"]], "Train a Simplex Convolutional Network (SCN) of Rank 2": [[85, "Train-a-Simplex-Convolutional-Network-(SCN)-of-Rank-2"]], "References": [[85, "References"]], "Train a Simplicial Convolutional Neural Network (SCNN)": [[86, "Train-a-Simplicial-Convolutional-Neural-Network-(SCNN)"]], "Simplicial Convolutional Neural Networks [SCNN]": [[86, "Simplicial-Convolutional-Neural-Networks-[SCNN]"]], "Import Karate dataset": [[86, "Import-Karate-dataset"]], "Weighted Hodge Laplacians": [[86, "Weighted-Hodge-Laplacians"]], "Import signals": [[86, "Import-signals"]], "Define binary labels and Prepare the training-testing split": [[86, "Define-binary-labels-and-Prepare-the-training-testing-split"]], "Create the SCNN for node classification": [[86, "Create-the-SCNN-for-node-classification"]], "Train the SCNN": [[86, "Train-the-SCNN"]], "Train a Simplicial Complex Net (SCoNe)": [[87, "Train-a-Simplicial-Complex-Net-(SCoNe)"]], "Table of contents": [[87, "Table-of-contents"]], "Dataset generation": [[87, "Dataset-generation"]], "Generating trajectories": [[87, "Generating-trajectories"]], "Creating PyTorch dataloaders": [[87, "Creating-PyTorch-dataloaders"]], "Creating the Neural Network": [[87, "Creating-the-Neural-Network"]], "Training the Neural Network": [[87, "Training-the-Neural-Network"]], "Evaluating the model on test data": [[87, "Evaluating-the-model-on-test-data"]], "Suggestions for further experimentation": [[87, "Suggestions-for-further-experimentation"]], "Tutorials": [[88, "tutorials"]], "Topological Neural Nets on Cellular Complexes": [[88, "topological-neural-nets-on-cellular-complexes"]], "Topological Neural Nets on Hypergraphs": [[88, "topological-neural-nets-on-hypergraphs"]], "Topological Neural Nets on Simplicial Complexes": [[88, "topological-neural-nets-on-simplicial-complexes"]]}, "indexentries": {"aggregation (class in topomodelx.base.aggregation)": [[0, "topomodelx.base.aggregation.Aggregation"]], "forward() (topomodelx.base.aggregation.aggregation method)": [[0, "topomodelx.base.aggregation.Aggregation.forward"]], "module": [[0, "module-topomodelx.base.aggregation"], [1, "module-topomodelx.base.conv"], [3, "module-topomodelx.base.message_passing"], [5, "module-topomodelx.nn.cell.can"], [6, "module-topomodelx.nn.cell.can_layer"], [7, "module-topomodelx.nn.cell.ccxn"], [8, "module-topomodelx.nn.cell.ccxn_layer"], [9, "module-topomodelx.nn.cell.cwn"], [10, "module-topomodelx.nn.cell.cwn_layer"], [12, "module-topomodelx.nn.hypergraph.allset"], [13, "module-topomodelx.nn.hypergraph.allset_layer"], [14, "module-topomodelx.nn.hypergraph.allset_transformer"], [15, "module-topomodelx.nn.hypergraph.allset_transformer_layer"], [18, "module-topomodelx.nn.hypergraph.hmpnn"], [19, "module-topomodelx.nn.hypergraph.hmpnn_layer"], [20, "module-topomodelx.nn.hypergraph.hnhn"], [21, "module-topomodelx.nn.hypergraph.hnhn_layer"], [22, "module-topomodelx.nn.hypergraph.hnhn_layer_bis"], [23, "module-topomodelx.nn.hypergraph.hypergat"], [24, "module-topomodelx.nn.hypergraph.hypergat_layer"], [25, "module-topomodelx.nn.hypergraph.hypersage"], [26, "module-topomodelx.nn.hypergraph.hypersage_layer"], [28, "module-topomodelx.nn.hypergraph.unigcn"], [29, "module-topomodelx.nn.hypergraph.unigcn_layer"], [30, "module-topomodelx.nn.hypergraph.unigcnii"], [31, "module-topomodelx.nn.hypergraph.unigcnii_layer"], [32, "module-topomodelx.nn.hypergraph.unigin"], [33, "module-topomodelx.nn.hypergraph.unigin_layer"], [34, "module-topomodelx.nn.hypergraph.unisage"], [35, "module-topomodelx.nn.hypergraph.unisage_layer"], [37, "module-topomodelx.nn.simplicial.dist2cycle"], [38, "module-topomodelx.nn.simplicial.dist2cycle_layer"], [39, "module-topomodelx.nn.simplicial.hsn"], [40, "module-topomodelx.nn.simplicial.hsn_layer"], [42, "module-topomodelx.nn.simplicial.san"], [43, "module-topomodelx.nn.simplicial.san_layer"], [44, "module-topomodelx.nn.simplicial.sca_cmps"], [45, "module-topomodelx.nn.simplicial.sca_cmps_layer"], [46, "module-topomodelx.nn.simplicial.sccn"], [47, "module-topomodelx.nn.simplicial.sccn_layer"], [48, "module-topomodelx.nn.simplicial.sccnn"], [49, "module-topomodelx.nn.simplicial.sccnn_layer"], [50, "module-topomodelx.nn.simplicial.scconv"], [51, "module-topomodelx.nn.simplicial.scconv_layer"], [52, "module-topomodelx.nn.simplicial.scn2"], [53, "module-topomodelx.nn.simplicial.scn2_layer"], [54, "module-topomodelx.nn.simplicial.scnn"], [55, "module-topomodelx.nn.simplicial.scnn_layer"], [56, "module-topomodelx.nn.simplicial.scone"], [57, "module-topomodelx.nn.simplicial.scone_layer"], [58, "module-topomodelx.utils.scatter"]], "topomodelx.base.aggregation": [[0, "module-topomodelx.base.aggregation"]], "update() (topomodelx.base.aggregation.aggregation method)": [[0, "topomodelx.base.aggregation.Aggregation.update"]], "conv (class in topomodelx.base.conv)": [[1, "topomodelx.base.conv.Conv"]], "forward() (topomodelx.base.conv.conv method)": [[1, "topomodelx.base.conv.Conv.forward"]], "topomodelx.base.conv": [[1, "module-topomodelx.base.conv"]], "update() (topomodelx.base.conv.conv method)": [[1, "topomodelx.base.conv.Conv.update"]], "messagepassing (class in topomodelx.base.message_passing)": [[3, "topomodelx.base.message_passing.MessagePassing"]], "aggregate() (topomodelx.base.message_passing.messagepassing method)": [[3, "topomodelx.base.message_passing.MessagePassing.aggregate"]], "attention() (topomodelx.base.message_passing.messagepassing method)": [[3, "topomodelx.base.message_passing.MessagePassing.attention"]], "forward() (topomodelx.base.message_passing.messagepassing method)": [[3, "topomodelx.base.message_passing.MessagePassing.forward"]], "message() (topomodelx.base.message_passing.messagepassing method)": [[3, "topomodelx.base.message_passing.MessagePassing.message"]], "reset_parameters() (topomodelx.base.message_passing.messagepassing method)": [[3, "topomodelx.base.message_passing.MessagePassing.reset_parameters"]], "topomodelx.base.message_passing": [[3, "module-topomodelx.base.message_passing"]], "can (class in topomodelx.nn.cell.can)": [[5, "topomodelx.nn.cell.can.CAN"]], "forward() (topomodelx.nn.cell.can.can method)": [[5, "topomodelx.nn.cell.can.CAN.forward"]], "topomodelx.nn.cell.can": [[5, "module-topomodelx.nn.cell.can"]], "canlayer (class in topomodelx.nn.cell.can_layer)": [[6, "topomodelx.nn.cell.can_layer.CANLayer"]], "liftlayer (class in topomodelx.nn.cell.can_layer)": [[6, "topomodelx.nn.cell.can_layer.LiftLayer"]], "multiheadcellattention (class in topomodelx.nn.cell.can_layer)": [[6, "topomodelx.nn.cell.can_layer.MultiHeadCellAttention"]], "multiheadcellattention_v2 (class in topomodelx.nn.cell.can_layer)": [[6, "topomodelx.nn.cell.can_layer.MultiHeadCellAttention_v2"]], "multiheadliftlayer (class in topomodelx.nn.cell.can_layer)": [[6, "topomodelx.nn.cell.can_layer.MultiHeadLiftLayer"]], "poollayer (class in topomodelx.nn.cell.can_layer)": [[6, "topomodelx.nn.cell.can_layer.PoolLayer"]], "add_self_loops() (in module topomodelx.nn.cell.can_layer)": [[6, "topomodelx.nn.cell.can_layer.add_self_loops"]], "attention() (topomodelx.nn.cell.can_layer.multiheadcellattention method)": [[6, "topomodelx.nn.cell.can_layer.MultiHeadCellAttention.attention"]], "attention() (topomodelx.nn.cell.can_layer.multiheadcellattention_v2 method)": [[6, "topomodelx.nn.cell.can_layer.MultiHeadCellAttention_v2.attention"]], "forward() (topomodelx.nn.cell.can_layer.canlayer method)": [[6, "topomodelx.nn.cell.can_layer.CANLayer.forward"]], "forward() (topomodelx.nn.cell.can_layer.liftlayer method)": [[6, "topomodelx.nn.cell.can_layer.LiftLayer.forward"]], "forward() (topomodelx.nn.cell.can_layer.multiheadcellattention method)": [[6, "topomodelx.nn.cell.can_layer.MultiHeadCellAttention.forward"]], "forward() (topomodelx.nn.cell.can_layer.multiheadcellattention_v2 method)": [[6, "topomodelx.nn.cell.can_layer.MultiHeadCellAttention_v2.forward"]], "forward() (topomodelx.nn.cell.can_layer.multiheadliftlayer method)": [[6, "topomodelx.nn.cell.can_layer.MultiHeadLiftLayer.forward"]], "forward() (topomodelx.nn.cell.can_layer.poollayer method)": [[6, "topomodelx.nn.cell.can_layer.PoolLayer.forward"]], "message() (topomodelx.nn.cell.can_layer.liftlayer method)": [[6, "topomodelx.nn.cell.can_layer.LiftLayer.message"]], "message() (topomodelx.nn.cell.can_layer.multiheadcellattention method)": [[6, "topomodelx.nn.cell.can_layer.MultiHeadCellAttention.message"]], "message() (topomodelx.nn.cell.can_layer.multiheadcellattention_v2 method)": [[6, "topomodelx.nn.cell.can_layer.MultiHeadCellAttention_v2.message"]], "reset_parameters() (topomodelx.nn.cell.can_layer.canlayer method)": [[6, "topomodelx.nn.cell.can_layer.CANLayer.reset_parameters"]], "reset_parameters() (topomodelx.nn.cell.can_layer.liftlayer method)": [[6, "topomodelx.nn.cell.can_layer.LiftLayer.reset_parameters"]], "reset_parameters() (topomodelx.nn.cell.can_layer.multiheadcellattention method)": [[6, "topomodelx.nn.cell.can_layer.MultiHeadCellAttention.reset_parameters"]], "reset_parameters() (topomodelx.nn.cell.can_layer.multiheadcellattention_v2 method)": [[6, "topomodelx.nn.cell.can_layer.MultiHeadCellAttention_v2.reset_parameters"]], "reset_parameters() (topomodelx.nn.cell.can_layer.multiheadliftlayer method)": [[6, "topomodelx.nn.cell.can_layer.MultiHeadLiftLayer.reset_parameters"]], "reset_parameters() (topomodelx.nn.cell.can_layer.poollayer method)": [[6, "topomodelx.nn.cell.can_layer.PoolLayer.reset_parameters"]], "softmax() (in module topomodelx.nn.cell.can_layer)": [[6, "topomodelx.nn.cell.can_layer.softmax"]], "topomodelx.nn.cell.can_layer": [[6, "module-topomodelx.nn.cell.can_layer"]], "ccxn (class in topomodelx.nn.cell.ccxn)": [[7, "topomodelx.nn.cell.ccxn.CCXN"]], "forward() (topomodelx.nn.cell.ccxn.ccxn method)": [[7, "topomodelx.nn.cell.ccxn.CCXN.forward"]], "topomodelx.nn.cell.ccxn": [[7, "module-topomodelx.nn.cell.ccxn"]], "ccxnlayer (class in topomodelx.nn.cell.ccxn_layer)": [[8, "topomodelx.nn.cell.ccxn_layer.CCXNLayer"]], "forward() (topomodelx.nn.cell.ccxn_layer.ccxnlayer method)": [[8, "topomodelx.nn.cell.ccxn_layer.CCXNLayer.forward"]], "topomodelx.nn.cell.ccxn_layer": [[8, "module-topomodelx.nn.cell.ccxn_layer"]], "cwn (class in topomodelx.nn.cell.cwn)": [[9, "topomodelx.nn.cell.cwn.CWN"]], "forward() (topomodelx.nn.cell.cwn.cwn method)": [[9, "topomodelx.nn.cell.cwn.CWN.forward"]], "topomodelx.nn.cell.cwn": [[9, "module-topomodelx.nn.cell.cwn"]], "cwnlayer (class in topomodelx.nn.cell.cwn_layer)": [[10, "topomodelx.nn.cell.cwn_layer.CWNLayer"]], "forward() (topomodelx.nn.cell.cwn_layer.cwnlayer method)": [[10, "topomodelx.nn.cell.cwn_layer.CWNLayer.forward"]], "topomodelx.nn.cell.cwn_layer": [[10, "module-topomodelx.nn.cell.cwn_layer"]], "allset (class in topomodelx.nn.hypergraph.allset)": [[12, "topomodelx.nn.hypergraph.allset.AllSet"]], "forward() (topomodelx.nn.hypergraph.allset.allset method)": [[12, "topomodelx.nn.hypergraph.allset.AllSet.forward"]], "topomodelx.nn.hypergraph.allset": [[12, "module-topomodelx.nn.hypergraph.allset"]], "allsetblock (class in topomodelx.nn.hypergraph.allset_layer)": [[13, "topomodelx.nn.hypergraph.allset_layer.AllSetBlock"]], "allsetlayer (class in topomodelx.nn.hypergraph.allset_layer)": [[13, "topomodelx.nn.hypergraph.allset_layer.AllSetLayer"]], "mlp (class in topomodelx.nn.hypergraph.allset_layer)": [[13, "topomodelx.nn.hypergraph.allset_layer.MLP"]], "forward() (topomodelx.nn.hypergraph.allset_layer.allsetblock method)": [[13, "topomodelx.nn.hypergraph.allset_layer.AllSetBlock.forward"]], "forward() (topomodelx.nn.hypergraph.allset_layer.allsetlayer method)": [[13, "topomodelx.nn.hypergraph.allset_layer.AllSetLayer.forward"]], "reset_parameters() (topomodelx.nn.hypergraph.allset_layer.allsetblock method)": [[13, "topomodelx.nn.hypergraph.allset_layer.AllSetBlock.reset_parameters"]], "reset_parameters() (topomodelx.nn.hypergraph.allset_layer.allsetlayer method)": [[13, "topomodelx.nn.hypergraph.allset_layer.AllSetLayer.reset_parameters"]], "topomodelx.nn.hypergraph.allset_layer": [[13, "module-topomodelx.nn.hypergraph.allset_layer"]], "allsettransformer (class in topomodelx.nn.hypergraph.allset_transformer)": [[14, "topomodelx.nn.hypergraph.allset_transformer.AllSetTransformer"]], "forward() (topomodelx.nn.hypergraph.allset_transformer.allsettransformer method)": [[14, "topomodelx.nn.hypergraph.allset_transformer.AllSetTransformer.forward"]], "topomodelx.nn.hypergraph.allset_transformer": [[14, "module-topomodelx.nn.hypergraph.allset_transformer"]], "allsettransformerblock (class in topomodelx.nn.hypergraph.allset_transformer_layer)": [[15, "topomodelx.nn.hypergraph.allset_transformer_layer.AllSetTransformerBlock"]], "allsettransformerlayer (class in topomodelx.nn.hypergraph.allset_transformer_layer)": [[15, "topomodelx.nn.hypergraph.allset_transformer_layer.AllSetTransformerLayer"]], "mlp (class in topomodelx.nn.hypergraph.allset_transformer_layer)": [[15, "topomodelx.nn.hypergraph.allset_transformer_layer.MLP"]], "multiheadattention (class in topomodelx.nn.hypergraph.allset_transformer_layer)": [[15, "topomodelx.nn.hypergraph.allset_transformer_layer.MultiHeadAttention"]], "attention() (topomodelx.nn.hypergraph.allset_transformer_layer.multiheadattention method)": [[15, "topomodelx.nn.hypergraph.allset_transformer_layer.MultiHeadAttention.attention"]], "forward() (topomodelx.nn.hypergraph.allset_transformer_layer.allsettransformerblock method)": [[15, "topomodelx.nn.hypergraph.allset_transformer_layer.AllSetTransformerBlock.forward"]], "forward() (topomodelx.nn.hypergraph.allset_transformer_layer.allsettransformerlayer method)": [[15, "topomodelx.nn.hypergraph.allset_transformer_layer.AllSetTransformerLayer.forward"]], "forward() (topomodelx.nn.hypergraph.allset_transformer_layer.multiheadattention method)": [[15, "topomodelx.nn.hypergraph.allset_transformer_layer.MultiHeadAttention.forward"]], "reset_parameters() (topomodelx.nn.hypergraph.allset_transformer_layer.allsettransformerblock method)": [[15, "topomodelx.nn.hypergraph.allset_transformer_layer.AllSetTransformerBlock.reset_parameters"]], "reset_parameters() (topomodelx.nn.hypergraph.allset_transformer_layer.allsettransformerlayer method)": [[15, "topomodelx.nn.hypergraph.allset_transformer_layer.AllSetTransformerLayer.reset_parameters"]], "reset_parameters() (topomodelx.nn.hypergraph.allset_transformer_layer.multiheadattention method)": [[15, "topomodelx.nn.hypergraph.allset_transformer_layer.MultiHeadAttention.reset_parameters"]], "topomodelx.nn.hypergraph.allset_transformer_layer": [[15, "module-topomodelx.nn.hypergraph.allset_transformer_layer"]], "hmpnn (class in topomodelx.nn.hypergraph.hmpnn)": [[18, "topomodelx.nn.hypergraph.hmpnn.HMPNN"]], "forward() (topomodelx.nn.hypergraph.hmpnn.hmpnn method)": [[18, "topomodelx.nn.hypergraph.hmpnn.HMPNN.forward"]], "topomodelx.nn.hypergraph.hmpnn": [[18, "module-topomodelx.nn.hypergraph.hmpnn"]], "hmpnnlayer (class in topomodelx.nn.hypergraph.hmpnn_layer)": [[19, "topomodelx.nn.hypergraph.hmpnn_layer.HMPNNLayer"]], "apply_regular_dropout() (topomodelx.nn.hypergraph.hmpnn_layer.hmpnnlayer method)": [[19, "topomodelx.nn.hypergraph.hmpnn_layer.HMPNNLayer.apply_regular_dropout"]], "forward() (topomodelx.nn.hypergraph.hmpnn_layer.hmpnnlayer method)": [[19, "topomodelx.nn.hypergraph.hmpnn_layer.HMPNNLayer.forward"]], "topomodelx.nn.hypergraph.hmpnn_layer": [[19, "module-topomodelx.nn.hypergraph.hmpnn_layer"]], "hnhn (class in topomodelx.nn.hypergraph.hnhn)": [[20, "topomodelx.nn.hypergraph.hnhn.HNHN"]], "hnhnnetwork (class in topomodelx.nn.hypergraph.hnhn)": [[20, "topomodelx.nn.hypergraph.hnhn.HNHNNetwork"]], "forward() (topomodelx.nn.hypergraph.hnhn.hnhn method)": [[20, "topomodelx.nn.hypergraph.hnhn.HNHN.forward"]], "forward() (topomodelx.nn.hypergraph.hnhn.hnhnnetwork method)": [[20, "topomodelx.nn.hypergraph.hnhn.HNHNNetwork.forward"]], "topomodelx.nn.hypergraph.hnhn": [[20, "module-topomodelx.nn.hypergraph.hnhn"]], "hnhnlayer (class in topomodelx.nn.hypergraph.hnhn_layer)": [[21, "topomodelx.nn.hypergraph.hnhn_layer.HNHNLayer"]], "compute_normalization_matrices() (topomodelx.nn.hypergraph.hnhn_layer.hnhnlayer method)": [[21, "topomodelx.nn.hypergraph.hnhn_layer.HNHNLayer.compute_normalization_matrices"]], "forward() (topomodelx.nn.hypergraph.hnhn_layer.hnhnlayer method)": [[21, "topomodelx.nn.hypergraph.hnhn_layer.HNHNLayer.forward"]], "init_biases() (topomodelx.nn.hypergraph.hnhn_layer.hnhnlayer method)": [[21, "topomodelx.nn.hypergraph.hnhn_layer.HNHNLayer.init_biases"]], "normalize_incidence_matrices() (topomodelx.nn.hypergraph.hnhn_layer.hnhnlayer method)": [[21, "topomodelx.nn.hypergraph.hnhn_layer.HNHNLayer.normalize_incidence_matrices"]], "reset_parameters() (topomodelx.nn.hypergraph.hnhn_layer.hnhnlayer method)": [[21, "topomodelx.nn.hypergraph.hnhn_layer.HNHNLayer.reset_parameters"]], "topomodelx.nn.hypergraph.hnhn_layer": [[21, "module-topomodelx.nn.hypergraph.hnhn_layer"]], "hnhnlayer (class in topomodelx.nn.hypergraph.hnhn_layer_bis)": [[22, "topomodelx.nn.hypergraph.hnhn_layer_bis.HNHNLayer"]], "forward() (topomodelx.nn.hypergraph.hnhn_layer_bis.hnhnlayer method)": [[22, "topomodelx.nn.hypergraph.hnhn_layer_bis.HNHNLayer.forward"]], "topomodelx.nn.hypergraph.hnhn_layer_bis": [[22, "module-topomodelx.nn.hypergraph.hnhn_layer_bis"]], "hypergat (class in topomodelx.nn.hypergraph.hypergat)": [[23, "topomodelx.nn.hypergraph.hypergat.HyperGAT"]], "forward() (topomodelx.nn.hypergraph.hypergat.hypergat method)": [[23, "topomodelx.nn.hypergraph.hypergat.HyperGAT.forward"]], "topomodelx.nn.hypergraph.hypergat": [[23, "module-topomodelx.nn.hypergraph.hypergat"]], "hypergatlayer (class in topomodelx.nn.hypergraph.hypergat_layer)": [[24, "topomodelx.nn.hypergraph.hypergat_layer.HyperGATLayer"]], "attention() (topomodelx.nn.hypergraph.hypergat_layer.hypergatlayer method)": [[24, "topomodelx.nn.hypergraph.hypergat_layer.HyperGATLayer.attention"]], "forward() (topomodelx.nn.hypergraph.hypergat_layer.hypergatlayer method)": [[24, "topomodelx.nn.hypergraph.hypergat_layer.HyperGATLayer.forward"]], "reset_parameters() (topomodelx.nn.hypergraph.hypergat_layer.hypergatlayer method)": [[24, "topomodelx.nn.hypergraph.hypergat_layer.HyperGATLayer.reset_parameters"]], "topomodelx.nn.hypergraph.hypergat_layer": [[24, "module-topomodelx.nn.hypergraph.hypergat_layer"]], "update() (topomodelx.nn.hypergraph.hypergat_layer.hypergatlayer method)": [[24, "topomodelx.nn.hypergraph.hypergat_layer.HyperGATLayer.update"]], "hypersage (class in topomodelx.nn.hypergraph.hypersage)": [[25, "topomodelx.nn.hypergraph.hypersage.HyperSAGE"]], "forward() (topomodelx.nn.hypergraph.hypersage.hypersage method)": [[25, "topomodelx.nn.hypergraph.hypersage.HyperSAGE.forward"]], "topomodelx.nn.hypergraph.hypersage": [[25, "module-topomodelx.nn.hypergraph.hypersage"]], "generalizedmean (class in topomodelx.nn.hypergraph.hypersage_layer)": [[26, "topomodelx.nn.hypergraph.hypersage_layer.GeneralizedMean"]], "hypersagelayer (class in topomodelx.nn.hypergraph.hypersage_layer)": [[26, "topomodelx.nn.hypergraph.hypersage_layer.HyperSAGELayer"]], "aggregate() (topomodelx.nn.hypergraph.hypersage_layer.hypersagelayer method)": [[26, "topomodelx.nn.hypergraph.hypersage_layer.HyperSAGELayer.aggregate"]], "forward() (topomodelx.nn.hypergraph.hypersage_layer.generalizedmean method)": [[26, "topomodelx.nn.hypergraph.hypersage_layer.GeneralizedMean.forward"]], "forward() (topomodelx.nn.hypergraph.hypersage_layer.hypersagelayer method)": [[26, "topomodelx.nn.hypergraph.hypersage_layer.HyperSAGELayer.forward"]], "topomodelx.nn.hypergraph.hypersage_layer": [[26, "module-topomodelx.nn.hypergraph.hypersage_layer"]], "update() (topomodelx.nn.hypergraph.hypersage_layer.hypersagelayer method)": [[26, "topomodelx.nn.hypergraph.hypersage_layer.HyperSAGELayer.update"]], "unigcn (class in topomodelx.nn.hypergraph.unigcn)": [[28, "topomodelx.nn.hypergraph.unigcn.UniGCN"]], "forward() (topomodelx.nn.hypergraph.unigcn.unigcn method)": [[28, "topomodelx.nn.hypergraph.unigcn.UniGCN.forward"]], "topomodelx.nn.hypergraph.unigcn": [[28, "module-topomodelx.nn.hypergraph.unigcn"]], "unigcnlayer (class in topomodelx.nn.hypergraph.unigcn_layer)": [[29, "topomodelx.nn.hypergraph.unigcn_layer.UniGCNLayer"]], "forward() (topomodelx.nn.hypergraph.unigcn_layer.unigcnlayer method)": [[29, "topomodelx.nn.hypergraph.unigcn_layer.UniGCNLayer.forward"]], "reset_parameters() (topomodelx.nn.hypergraph.unigcn_layer.unigcnlayer method)": [[29, "topomodelx.nn.hypergraph.unigcn_layer.UniGCNLayer.reset_parameters"]], "topomodelx.nn.hypergraph.unigcn_layer": [[29, "module-topomodelx.nn.hypergraph.unigcn_layer"]], "unigcnii (class in topomodelx.nn.hypergraph.unigcnii)": [[30, "topomodelx.nn.hypergraph.unigcnii.UniGCNII"]], "forward() (topomodelx.nn.hypergraph.unigcnii.unigcnii method)": [[30, "topomodelx.nn.hypergraph.unigcnii.UniGCNII.forward"]], "topomodelx.nn.hypergraph.unigcnii": [[30, "module-topomodelx.nn.hypergraph.unigcnii"]], "unigcniilayer (class in topomodelx.nn.hypergraph.unigcnii_layer)": [[31, "topomodelx.nn.hypergraph.unigcnii_layer.UniGCNIILayer"]], "forward() (topomodelx.nn.hypergraph.unigcnii_layer.unigcniilayer method)": [[31, "topomodelx.nn.hypergraph.unigcnii_layer.UniGCNIILayer.forward"]], "reset_parameters() (topomodelx.nn.hypergraph.unigcnii_layer.unigcniilayer method)": [[31, "topomodelx.nn.hypergraph.unigcnii_layer.UniGCNIILayer.reset_parameters"]], "topomodelx.nn.hypergraph.unigcnii_layer": [[31, "module-topomodelx.nn.hypergraph.unigcnii_layer"]], "unigin (class in topomodelx.nn.hypergraph.unigin)": [[32, "topomodelx.nn.hypergraph.unigin.UniGIN"]], "forward() (topomodelx.nn.hypergraph.unigin.unigin method)": [[32, "topomodelx.nn.hypergraph.unigin.UniGIN.forward"]], "topomodelx.nn.hypergraph.unigin": [[32, "module-topomodelx.nn.hypergraph.unigin"]], "uniginlayer (class in topomodelx.nn.hypergraph.unigin_layer)": [[33, "topomodelx.nn.hypergraph.unigin_layer.UniGINLayer"]], "forward() (topomodelx.nn.hypergraph.unigin_layer.uniginlayer method)": [[33, "topomodelx.nn.hypergraph.unigin_layer.UniGINLayer.forward"]], "topomodelx.nn.hypergraph.unigin_layer": [[33, "module-topomodelx.nn.hypergraph.unigin_layer"]], "unisage (class in topomodelx.nn.hypergraph.unisage)": [[34, "topomodelx.nn.hypergraph.unisage.UniSAGE"]], "forward() (topomodelx.nn.hypergraph.unisage.unisage method)": [[34, "topomodelx.nn.hypergraph.unisage.UniSAGE.forward"]], "topomodelx.nn.hypergraph.unisage": [[34, "module-topomodelx.nn.hypergraph.unisage"]], "unisagelayer (class in topomodelx.nn.hypergraph.unisage_layer)": [[35, "topomodelx.nn.hypergraph.unisage_layer.UniSAGELayer"]], "forward() (topomodelx.nn.hypergraph.unisage_layer.unisagelayer method)": [[35, "topomodelx.nn.hypergraph.unisage_layer.UniSAGELayer.forward"]], "reset_parameters() (topomodelx.nn.hypergraph.unisage_layer.unisagelayer method)": [[35, "topomodelx.nn.hypergraph.unisage_layer.UniSAGELayer.reset_parameters"]], "topomodelx.nn.hypergraph.unisage_layer": [[35, "module-topomodelx.nn.hypergraph.unisage_layer"]], "dist2cycle (class in topomodelx.nn.simplicial.dist2cycle)": [[37, "topomodelx.nn.simplicial.dist2cycle.Dist2Cycle"]], "forward() (topomodelx.nn.simplicial.dist2cycle.dist2cycle method)": [[37, "topomodelx.nn.simplicial.dist2cycle.Dist2Cycle.forward"]], "topomodelx.nn.simplicial.dist2cycle": [[37, "module-topomodelx.nn.simplicial.dist2cycle"]], "dist2cyclelayer (class in topomodelx.nn.simplicial.dist2cycle_layer)": [[38, "topomodelx.nn.simplicial.dist2cycle_layer.Dist2CycleLayer"]], "forward() (topomodelx.nn.simplicial.dist2cycle_layer.dist2cyclelayer method)": [[38, "topomodelx.nn.simplicial.dist2cycle_layer.Dist2CycleLayer.forward"]], "reset_parameters() (topomodelx.nn.simplicial.dist2cycle_layer.dist2cyclelayer method)": [[38, "topomodelx.nn.simplicial.dist2cycle_layer.Dist2CycleLayer.reset_parameters"]], "topomodelx.nn.simplicial.dist2cycle_layer": [[38, "module-topomodelx.nn.simplicial.dist2cycle_layer"]], "hsn (class in topomodelx.nn.simplicial.hsn)": [[39, "topomodelx.nn.simplicial.hsn.HSN"]], "forward() (topomodelx.nn.simplicial.hsn.hsn method)": [[39, "topomodelx.nn.simplicial.hsn.HSN.forward"]], "topomodelx.nn.simplicial.hsn": [[39, "module-topomodelx.nn.simplicial.hsn"]], "hsnlayer (class in topomodelx.nn.simplicial.hsn_layer)": [[40, "topomodelx.nn.simplicial.hsn_layer.HSNLayer"]], "forward() (topomodelx.nn.simplicial.hsn_layer.hsnlayer method)": [[40, "topomodelx.nn.simplicial.hsn_layer.HSNLayer.forward"]], "reset_parameters() (topomodelx.nn.simplicial.hsn_layer.hsnlayer method)": [[40, "topomodelx.nn.simplicial.hsn_layer.HSNLayer.reset_parameters"]], "topomodelx.nn.simplicial.hsn_layer": [[40, "module-topomodelx.nn.simplicial.hsn_layer"]], "san (class in topomodelx.nn.simplicial.san)": [[42, "topomodelx.nn.simplicial.san.SAN"]], "compute_projection_matrix() (topomodelx.nn.simplicial.san.san method)": [[42, "topomodelx.nn.simplicial.san.SAN.compute_projection_matrix"]], "forward() (topomodelx.nn.simplicial.san.san method)": [[42, "topomodelx.nn.simplicial.san.SAN.forward"]], "topomodelx.nn.simplicial.san": [[42, "module-topomodelx.nn.simplicial.san"]], "sanconv (class in topomodelx.nn.simplicial.san_layer)": [[43, "topomodelx.nn.simplicial.san_layer.SANConv"]], "sanlayer (class in topomodelx.nn.simplicial.san_layer)": [[43, "topomodelx.nn.simplicial.san_layer.SANLayer"]], "forward() (topomodelx.nn.simplicial.san_layer.sanconv method)": [[43, "topomodelx.nn.simplicial.san_layer.SANConv.forward"]], "forward() (topomodelx.nn.simplicial.san_layer.sanlayer method)": [[43, "topomodelx.nn.simplicial.san_layer.SANLayer.forward"]], "reset_parameters() (topomodelx.nn.simplicial.san_layer.sanlayer method)": [[43, "topomodelx.nn.simplicial.san_layer.SANLayer.reset_parameters"]], "topomodelx.nn.simplicial.san_layer": [[43, "module-topomodelx.nn.simplicial.san_layer"]], "scacmps (class in topomodelx.nn.simplicial.sca_cmps)": [[44, "topomodelx.nn.simplicial.sca_cmps.SCACMPS"]], "forward() (topomodelx.nn.simplicial.sca_cmps.scacmps method)": [[44, "topomodelx.nn.simplicial.sca_cmps.SCACMPS.forward"]], "topomodelx.nn.simplicial.sca_cmps": [[44, "module-topomodelx.nn.simplicial.sca_cmps"]], "scacmpslayer (class in topomodelx.nn.simplicial.sca_cmps_layer)": [[45, "topomodelx.nn.simplicial.sca_cmps_layer.SCACMPSLayer"]], "forward() (topomodelx.nn.simplicial.sca_cmps_layer.scacmpslayer method)": [[45, "topomodelx.nn.simplicial.sca_cmps_layer.SCACMPSLayer.forward"]], "intra_aggr() (topomodelx.nn.simplicial.sca_cmps_layer.scacmpslayer method)": [[45, "topomodelx.nn.simplicial.sca_cmps_layer.SCACMPSLayer.intra_aggr"]], "reset_parameters() (topomodelx.nn.simplicial.sca_cmps_layer.scacmpslayer method)": [[45, "topomodelx.nn.simplicial.sca_cmps_layer.SCACMPSLayer.reset_parameters"]], "topomodelx.nn.simplicial.sca_cmps_layer": [[45, "module-topomodelx.nn.simplicial.sca_cmps_layer"]], "weight_func() (topomodelx.nn.simplicial.sca_cmps_layer.scacmpslayer method)": [[45, "topomodelx.nn.simplicial.sca_cmps_layer.SCACMPSLayer.weight_func"]], "sccn (class in topomodelx.nn.simplicial.sccn)": [[46, "topomodelx.nn.simplicial.sccn.SCCN"]], "forward() (topomodelx.nn.simplicial.sccn.sccn method)": [[46, "topomodelx.nn.simplicial.sccn.SCCN.forward"]], "topomodelx.nn.simplicial.sccn": [[46, "module-topomodelx.nn.simplicial.sccn"]], "sccnlayer (class in topomodelx.nn.simplicial.sccn_layer)": [[47, "topomodelx.nn.simplicial.sccn_layer.SCCNLayer"]], "forward() (topomodelx.nn.simplicial.sccn_layer.sccnlayer method)": [[47, "topomodelx.nn.simplicial.sccn_layer.SCCNLayer.forward"]], "reset_parameters() (topomodelx.nn.simplicial.sccn_layer.sccnlayer method)": [[47, "topomodelx.nn.simplicial.sccn_layer.SCCNLayer.reset_parameters"]], "topomodelx.nn.simplicial.sccn_layer": [[47, "module-topomodelx.nn.simplicial.sccn_layer"]], "sccnn (class in topomodelx.nn.simplicial.sccnn)": [[48, "topomodelx.nn.simplicial.sccnn.SCCNN"]], "sccnncomplex (class in topomodelx.nn.simplicial.sccnn)": [[48, "topomodelx.nn.simplicial.sccnn.SCCNNComplex"]], "forward() (topomodelx.nn.simplicial.sccnn.sccnn method)": [[48, "topomodelx.nn.simplicial.sccnn.SCCNN.forward"]], "forward() (topomodelx.nn.simplicial.sccnn.sccnncomplex method)": [[48, "topomodelx.nn.simplicial.sccnn.SCCNNComplex.forward"]], "topomodelx.nn.simplicial.sccnn": [[48, "module-topomodelx.nn.simplicial.sccnn"]], "sccnnlayer (class in topomodelx.nn.simplicial.sccnn_layer)": [[49, "topomodelx.nn.simplicial.sccnn_layer.SCCNNLayer"]], "aggr_norm_func() (topomodelx.nn.simplicial.sccnn_layer.sccnnlayer method)": [[49, "topomodelx.nn.simplicial.sccnn_layer.SCCNNLayer.aggr_norm_func"]], "chebyshev_conv() (topomodelx.nn.simplicial.sccnn_layer.sccnnlayer method)": [[49, "topomodelx.nn.simplicial.sccnn_layer.SCCNNLayer.chebyshev_conv"]], "forward() (topomodelx.nn.simplicial.sccnn_layer.sccnnlayer method)": [[49, "topomodelx.nn.simplicial.sccnn_layer.SCCNNLayer.forward"]], "reset_parameters() (topomodelx.nn.simplicial.sccnn_layer.sccnnlayer method)": [[49, "topomodelx.nn.simplicial.sccnn_layer.SCCNNLayer.reset_parameters"]], "topomodelx.nn.simplicial.sccnn_layer": [[49, "module-topomodelx.nn.simplicial.sccnn_layer"]], "update() (topomodelx.nn.simplicial.sccnn_layer.sccnnlayer method)": [[49, "topomodelx.nn.simplicial.sccnn_layer.SCCNNLayer.update"]], "scconv (class in topomodelx.nn.simplicial.scconv)": [[50, "topomodelx.nn.simplicial.scconv.SCConv"]], "forward() (topomodelx.nn.simplicial.scconv.scconv method)": [[50, "topomodelx.nn.simplicial.scconv.SCConv.forward"]], "topomodelx.nn.simplicial.scconv": [[50, "module-topomodelx.nn.simplicial.scconv"]], "scconvlayer (class in topomodelx.nn.simplicial.scconv_layer)": [[51, "topomodelx.nn.simplicial.scconv_layer.SCConvLayer"]], "forward() (topomodelx.nn.simplicial.scconv_layer.scconvlayer method)": [[51, "topomodelx.nn.simplicial.scconv_layer.SCConvLayer.forward"]], "reset_parameters() (topomodelx.nn.simplicial.scconv_layer.scconvlayer method)": [[51, "topomodelx.nn.simplicial.scconv_layer.SCConvLayer.reset_parameters"]], "topomodelx.nn.simplicial.scconv_layer": [[51, "module-topomodelx.nn.simplicial.scconv_layer"]], "scn2 (class in topomodelx.nn.simplicial.scn2)": [[52, "topomodelx.nn.simplicial.scn2.SCN2"]], "forward() (topomodelx.nn.simplicial.scn2.scn2 method)": [[52, "topomodelx.nn.simplicial.scn2.SCN2.forward"]], "topomodelx.nn.simplicial.scn2": [[52, "module-topomodelx.nn.simplicial.scn2"]], "scn2layer (class in topomodelx.nn.simplicial.scn2_layer)": [[53, "topomodelx.nn.simplicial.scn2_layer.SCN2Layer"]], "forward() (topomodelx.nn.simplicial.scn2_layer.scn2layer method)": [[53, "topomodelx.nn.simplicial.scn2_layer.SCN2Layer.forward"]], "reset_parameters() (topomodelx.nn.simplicial.scn2_layer.scn2layer method)": [[53, "topomodelx.nn.simplicial.scn2_layer.SCN2Layer.reset_parameters"]], "topomodelx.nn.simplicial.scn2_layer": [[53, "module-topomodelx.nn.simplicial.scn2_layer"]], "scnn (class in topomodelx.nn.simplicial.scnn)": [[54, "topomodelx.nn.simplicial.scnn.SCNN"]], "forward() (topomodelx.nn.simplicial.scnn.scnn method)": [[54, "topomodelx.nn.simplicial.scnn.SCNN.forward"]], "topomodelx.nn.simplicial.scnn": [[54, "module-topomodelx.nn.simplicial.scnn"]], "scnnlayer (class in topomodelx.nn.simplicial.scnn_layer)": [[55, "topomodelx.nn.simplicial.scnn_layer.SCNNLayer"]], "aggr_norm_func() (topomodelx.nn.simplicial.scnn_layer.scnnlayer method)": [[55, "topomodelx.nn.simplicial.scnn_layer.SCNNLayer.aggr_norm_func"]], "chebyshev_conv() (topomodelx.nn.simplicial.scnn_layer.scnnlayer method)": [[55, "topomodelx.nn.simplicial.scnn_layer.SCNNLayer.chebyshev_conv"]], "forward() (topomodelx.nn.simplicial.scnn_layer.scnnlayer method)": [[55, "topomodelx.nn.simplicial.scnn_layer.SCNNLayer.forward"]], "reset_parameters() (topomodelx.nn.simplicial.scnn_layer.scnnlayer method)": [[55, "topomodelx.nn.simplicial.scnn_layer.SCNNLayer.reset_parameters"]], "topomodelx.nn.simplicial.scnn_layer": [[55, "module-topomodelx.nn.simplicial.scnn_layer"]], "update() (topomodelx.nn.simplicial.scnn_layer.scnnlayer method)": [[55, "topomodelx.nn.simplicial.scnn_layer.SCNNLayer.update"]], "scone (class in topomodelx.nn.simplicial.scone)": [[56, "topomodelx.nn.simplicial.scone.SCoNe"]], "trajectoriesdataset (class in topomodelx.nn.simplicial.scone)": [[56, "topomodelx.nn.simplicial.scone.TrajectoriesDataset"]], "forward() (topomodelx.nn.simplicial.scone.scone method)": [[56, "topomodelx.nn.simplicial.scone.SCoNe.forward"]], "generate_complex() (in module topomodelx.nn.simplicial.scone)": [[56, "topomodelx.nn.simplicial.scone.generate_complex"]], "generate_trajectories() (in module topomodelx.nn.simplicial.scone)": [[56, "topomodelx.nn.simplicial.scone.generate_trajectories"]], "topomodelx.nn.simplicial.scone": [[56, "module-topomodelx.nn.simplicial.scone"]], "vectorize_path() (topomodelx.nn.simplicial.scone.trajectoriesdataset method)": [[56, "topomodelx.nn.simplicial.scone.TrajectoriesDataset.vectorize_path"]], "sconelayer (class in topomodelx.nn.simplicial.scone_layer)": [[57, "topomodelx.nn.simplicial.scone_layer.SCoNeLayer"]], "forward() (topomodelx.nn.simplicial.scone_layer.sconelayer method)": [[57, "topomodelx.nn.simplicial.scone_layer.SCoNeLayer.forward"]], "reset_parameters() (topomodelx.nn.simplicial.scone_layer.sconelayer method)": [[57, "topomodelx.nn.simplicial.scone_layer.SCoNeLayer.reset_parameters"]], "topomodelx.nn.simplicial.scone_layer": [[57, "module-topomodelx.nn.simplicial.scone_layer"]], "broadcast() (in module topomodelx.utils.scatter)": [[58, "topomodelx.utils.scatter.broadcast"]], "scatter() (in module topomodelx.utils.scatter)": [[58, "topomodelx.utils.scatter.scatter"]], "scatter_add() (in module topomodelx.utils.scatter)": [[58, "topomodelx.utils.scatter.scatter_add"]], "scatter_mean() (in module topomodelx.utils.scatter)": [[58, "topomodelx.utils.scatter.scatter_mean"]], "scatter_sum() (in module topomodelx.utils.scatter)": [[58, "topomodelx.utils.scatter.scatter_sum"]], "topomodelx.utils.scatter": [[58, "module-topomodelx.utils.scatter"]]}})