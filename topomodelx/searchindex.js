Search.setIndex({"docnames": ["api/base/aggregation", "api/base/conv", "api/base/index", "api/base/message_passing", "api/index", "api/nn/cell/can", "api/nn/cell/can_layer", "api/nn/cell/ccxn", "api/nn/cell/ccxn_layer", "api/nn/cell/cwn", "api/nn/cell/cwn_layer", "api/nn/cell/index", "api/nn/hypergraph/allset", "api/nn/hypergraph/allset_layer", "api/nn/hypergraph/allset_transformer", "api/nn/hypergraph/allset_transformer_layer", "api/nn/hypergraph/dhgcn", "api/nn/hypergraph/dhgcn_layer", "api/nn/hypergraph/hmpnn", "api/nn/hypergraph/hmpnn_layer", "api/nn/hypergraph/hnhn", "api/nn/hypergraph/hnhn_layer", "api/nn/hypergraph/hnhn_layer_bis", "api/nn/hypergraph/hypergat", "api/nn/hypergraph/hypergat_layer", "api/nn/hypergraph/hypersage", "api/nn/hypergraph/hypersage_layer", "api/nn/hypergraph/index", "api/nn/hypergraph/unigcn", "api/nn/hypergraph/unigcn_layer", "api/nn/hypergraph/unigcnii", "api/nn/hypergraph/unigcnii_layer", "api/nn/hypergraph/unigin", "api/nn/hypergraph/unigin_layer", "api/nn/hypergraph/unisage", "api/nn/hypergraph/unisage_layer", "api/nn/index", "api/nn/simplicial/dist2cycle", "api/nn/simplicial/dist2cycle_layer", "api/nn/simplicial/hsn", "api/nn/simplicial/hsn_layer", "api/nn/simplicial/index", "api/nn/simplicial/san", "api/nn/simplicial/san_layer", "api/nn/simplicial/sca_cmps", "api/nn/simplicial/sca_cmps_layer", "api/nn/simplicial/sccn", "api/nn/simplicial/sccn_layer", "api/nn/simplicial/sccnn", "api/nn/simplicial/sccnn_layer", "api/nn/simplicial/scconv", "api/nn/simplicial/scconv_layer", "api/nn/simplicial/scn2", "api/nn/simplicial/scn2_layer", "api/nn/simplicial/scnn", "api/nn/simplicial/scnn_layer", "api/nn/simplicial/scone", "api/nn/simplicial/scone_layer", "api/utils/index", "challenge/index", "contributing/index", "index", "notebooks/cell/can_train", "notebooks/cell/ccxn_train", "notebooks/cell/cwn_train", "notebooks/hypergraph/allset_train", "notebooks/hypergraph/allset_transformer_train", "notebooks/hypergraph/dhgcn_train", "notebooks/hypergraph/hmpnn_train", "notebooks/hypergraph/hnhn_train", "notebooks/hypergraph/hnhn_train_bis", "notebooks/hypergraph/hypergat_train", "notebooks/hypergraph/hypersage_train", "notebooks/hypergraph/unigcn_train", "notebooks/hypergraph/unigcnii_train", "notebooks/hypergraph/unigin_train", "notebooks/hypergraph/unisage_train", "notebooks/simplicial/dist2cycle_train", "notebooks/simplicial/hsn_train", "notebooks/simplicial/san_train", "notebooks/simplicial/sca_cmps_train", "notebooks/simplicial/sccn_train", "notebooks/simplicial/sccnn_train", "notebooks/simplicial/scconv_train", "notebooks/simplicial/scn2_train", "notebooks/simplicial/scnn_train", "notebooks/simplicial/scone_train", "tutorials/index"], "filenames": ["api/base/aggregation.rst", "api/base/conv.rst", "api/base/index.rst", "api/base/message_passing.rst", "api/index.rst", "api/nn/cell/can.rst", "api/nn/cell/can_layer.rst", "api/nn/cell/ccxn.rst", "api/nn/cell/ccxn_layer.rst", "api/nn/cell/cwn.rst", "api/nn/cell/cwn_layer.rst", "api/nn/cell/index.rst", "api/nn/hypergraph/allset.rst", "api/nn/hypergraph/allset_layer.rst", "api/nn/hypergraph/allset_transformer.rst", "api/nn/hypergraph/allset_transformer_layer.rst", "api/nn/hypergraph/dhgcn.rst", "api/nn/hypergraph/dhgcn_layer.rst", "api/nn/hypergraph/hmpnn.rst", "api/nn/hypergraph/hmpnn_layer.rst", "api/nn/hypergraph/hnhn.rst", "api/nn/hypergraph/hnhn_layer.rst", "api/nn/hypergraph/hnhn_layer_bis.rst", "api/nn/hypergraph/hypergat.rst", "api/nn/hypergraph/hypergat_layer.rst", "api/nn/hypergraph/hypersage.rst", "api/nn/hypergraph/hypersage_layer.rst", "api/nn/hypergraph/index.rst", "api/nn/hypergraph/unigcn.rst", "api/nn/hypergraph/unigcn_layer.rst", "api/nn/hypergraph/unigcnii.rst", "api/nn/hypergraph/unigcnii_layer.rst", "api/nn/hypergraph/unigin.rst", "api/nn/hypergraph/unigin_layer.rst", "api/nn/hypergraph/unisage.rst", "api/nn/hypergraph/unisage_layer.rst", "api/nn/index.rst", "api/nn/simplicial/dist2cycle.rst", "api/nn/simplicial/dist2cycle_layer.rst", "api/nn/simplicial/hsn.rst", "api/nn/simplicial/hsn_layer.rst", "api/nn/simplicial/index.rst", "api/nn/simplicial/san.rst", "api/nn/simplicial/san_layer.rst", "api/nn/simplicial/sca_cmps.rst", "api/nn/simplicial/sca_cmps_layer.rst", "api/nn/simplicial/sccn.rst", "api/nn/simplicial/sccn_layer.rst", "api/nn/simplicial/sccnn.rst", "api/nn/simplicial/sccnn_layer.rst", "api/nn/simplicial/scconv.rst", "api/nn/simplicial/scconv_layer.rst", "api/nn/simplicial/scn2.rst", "api/nn/simplicial/scn2_layer.rst", "api/nn/simplicial/scnn.rst", "api/nn/simplicial/scnn_layer.rst", "api/nn/simplicial/scone.rst", "api/nn/simplicial/scone_layer.rst", "api/utils/index.rst", "challenge/index.rst", "contributing/index.rst", "index.rst", "notebooks/cell/can_train.ipynb", "notebooks/cell/ccxn_train.ipynb", "notebooks/cell/cwn_train.ipynb", "notebooks/hypergraph/allset_train.ipynb", "notebooks/hypergraph/allset_transformer_train.ipynb", "notebooks/hypergraph/dhgcn_train.ipynb", "notebooks/hypergraph/hmpnn_train.ipynb", "notebooks/hypergraph/hnhn_train.ipynb", "notebooks/hypergraph/hnhn_train_bis.ipynb", "notebooks/hypergraph/hypergat_train.ipynb", "notebooks/hypergraph/hypersage_train.ipynb", "notebooks/hypergraph/unigcn_train.ipynb", "notebooks/hypergraph/unigcnii_train.ipynb", "notebooks/hypergraph/unigin_train.ipynb", "notebooks/hypergraph/unisage_train.ipynb", "notebooks/simplicial/dist2cycle_train.ipynb", "notebooks/simplicial/hsn_train.ipynb", "notebooks/simplicial/san_train.ipynb", "notebooks/simplicial/sca_cmps_train.ipynb", "notebooks/simplicial/sccn_train.ipynb", "notebooks/simplicial/sccnn_train.ipynb", "notebooks/simplicial/scconv_train.ipynb", "notebooks/simplicial/scn2_train.ipynb", "notebooks/simplicial/scnn_train.ipynb", "notebooks/simplicial/scone_train.ipynb", "tutorials/index.rst"], "titles": ["Aggregation", "Conv", "Base", "Message Passing", "API Reference", "CAN", "Can_Layer", "CCXN", "CCXN_Layer", "CWN", "Cwn_Layer", "Cell", "AllSet", "AllSet_Layer", "AllSet_Transformer", "AllSet_Transformer_Layer", "DHGCN", "DHGCN_Layer", "HMPNN", "HMPNN_Layer", "HNHN", "HNHN_Layer", "HNHN_Layer_Bis", "Hypergat", "Hypergat_Layer", "Hypersage", "Hypersage_Layer", "Hypergraph", "Unigcn", "Unigcn_Layer", "Unigcnii", "Unigcnii_Layer", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "Neural Networks", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "Simplicial", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "&lt;no title&gt;", "Utils", "ICML 2023 Topological Deep Learning Challenge", "Contributing", "\ud83c\udf10 TopoModelX (TMX) \ud83c\udf69", "Train a Cell Attention Network (CAN)", "Train a Convolutional Cell Complex Network (CCXN)", "Train a CW Network (CWN)", "Train an All-Set TNN", "Train an All-Set-Transformer TNN", "Train a DHGCN TNN", "Train a Hypergraph Message Passing Neural Network (HMPNN)", "Train a Hypergraph Networks with Hyperedge Neurons (HNHN)", "Train a Hypergraph Network with Hyperedge Neurons (HNHN)", "Train a Hypergraph Neural Network", "Train a Hypersage TNN", "Train a UNIGCN TNN", "Train a hypergraph neural network using UniGCNII layers", "Train a UNIGIN TNN", "Train a Uni-sage TNN", "Train a Simplicial Neural Network for Homology Localization (Dist2Cycle)", "Train a Simplicial High-Skip Network (HSN)", "Train a Simplicial Attention Network (SAN)", "Train a Simplicial Complex Autoencoder (SCA) with Coadjacency Message Passing Scheme (CMPS)", "Train a Simplicial Complex Convolutional Network (SCCN)", "Train a SCCNN", "Train a Simplicial 2-complex convolutional neural network (SCConv)", "Train a Simplex Convolutional Network (SCN) of Rank 2", "Train a Simplicial Convolutional Neural Network (SCNN)", "Train a Simplicial Complex Net (SCoNe)", "Tutorials"], "terms": {"modul": [0, 3, 5, 6, 10, 12, 13, 14, 15, 19, 33, 36, 59, 60, 65, 66, 70, 82, 85], "class": [0, 1, 2, 3, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 62, 63, 64, 65, 67, 69, 70, 73, 74, 75, 76, 77, 78, 79, 80, 81, 86], "topomodelx": [0, 1, 3, 4, 5, 6, 7, 8, 9, 10, 12, 13, 14, 15, 18, 19, 20, 21, 22, 23, 24, 25, 26, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86], "base": [0, 1, 3, 4, 10, 11, 15, 27, 41, 45, 59, 65, 66, 82, 83, 85, 86], "aggr_func": [0, 3, 6, 19, 47], "liter": [0, 1, 3, 6, 15, 19, 21, 24, 26, 35, 43, 47, 57], "mean": [0, 3, 6, 19, 26, 35, 47, 49, 54, 58, 60, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 74, 77, 78, 79, 81, 82, 83, 84, 85, 86], "sum": [0, 3, 6, 19, 29, 31, 33, 35, 47, 49, 62, 69, 71, 79, 81, 83, 86], "update_func": [0, 1, 6, 15, 24, 26, 46, 47, 48, 49, 50, 54, 55, 57, 81, 86], "relu": [0, 1, 6, 13, 15, 22, 24, 26, 47, 57, 71, 86], "sigmoid": [0, 1, 6, 19, 26, 46, 47, 57, 62, 81, 86], "tanh": [0, 6, 47, 57], "none": [0, 1, 3, 6, 8, 10, 12, 13, 15, 19, 21, 24, 26, 29, 31, 35, 38, 40, 43, 45, 47, 48, 49, 51, 53, 54, 55, 57, 58, 60, 85, 86], "sourc": [0, 1, 3, 5, 6, 7, 8, 9, 10, 12, 13, 14, 15, 18, 19, 20, 21, 22, 23, 24, 25, 26, 28, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 40, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59], "messag": [0, 1, 2, 4, 6, 8, 10, 15, 18, 19, 20, 21, 23, 24, 25, 26, 28, 29, 30, 31, 32, 33, 34, 35, 37, 39, 42, 43, 45, 46, 47, 50, 52, 53, 59, 62, 63, 64, 65, 66, 67, 69, 71, 72, 79, 81], "pass": [0, 1, 2, 4, 6, 8, 10, 15, 18, 19, 20, 21, 23, 24, 25, 26, 28, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 40, 42, 43, 45, 46, 47, 50, 51, 52, 53, 54, 56, 57, 59, 62, 63, 65, 66, 67, 71, 72, 79, 81, 85, 86], "layer": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 12, 13, 14, 15, 18, 19, 20, 21, 22, 23, 24, 25, 26, 28, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 40, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 57, 59, 62, 63, 64, 65, 66, 67, 69, 70, 71, 72, 73, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86], "paramet": [0, 1, 3, 5, 6, 7, 8, 9, 10, 12, 13, 14, 15, 18, 19, 20, 21, 22, 23, 24, 25, 26, 28, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 40, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 57, 60, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86], "default": [0, 1, 3, 5, 6, 8, 10, 12, 13, 14, 15, 19, 21, 24, 25, 26, 30, 35, 42, 43, 47, 54, 60], "method": [0, 1, 3, 6, 15, 21, 24, 26, 43, 49, 55, 59, 60, 68, 70], "inter": [0, 26, 72, 82], "neighborhood": [0, 1, 2, 3, 6, 8, 10, 15, 29, 43, 59, 62, 63, 64, 65, 66, 72, 79, 80, 86], "updat": [0, 1, 2, 3, 6, 10, 15, 24, 26, 29, 31, 33, 35, 41, 43, 49, 55, 57, 62, 65, 66, 79, 86], "appli": [0, 1, 3, 5, 6, 10, 12, 14, 15, 18, 19, 24, 26, 47, 57, 66, 71, 79, 83, 86], "merg": 0, "forward": [0, 1, 2, 3, 5, 6, 7, 8, 9, 10, 12, 13, 14, 15, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 70, 86], "x": [0, 1, 3, 6, 8, 10, 12, 13, 14, 15, 19, 21, 24, 25, 26, 29, 31, 33, 35, 38, 40, 42, 43, 45, 47, 48, 49, 51, 53, 54, 55, 56, 57, 60, 62, 63, 64, 65, 66, 68, 69, 70, 71, 72, 73, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86], "list": [0, 12, 13, 14, 15, 44, 45, 56, 60, 67, 74, 86], "len": [0, 69, 70, 73, 74, 75, 76, 77, 78, 79, 81, 82, 83, 85, 86], "n_messages_to_merg": 0, "each": [0, 1, 3, 6, 24, 26, 40, 44, 45, 46, 47, 48, 49, 55, 57, 59, 60, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 81, 82, 86], "ha": [0, 1, 6, 60, 63, 64, 65, 66, 67, 69, 71, 72, 74, 77, 78, 79, 80, 81, 82, 83, 84, 85], "shape": [0, 1, 3, 5, 6, 7, 8, 9, 10, 13, 15, 18, 19, 20, 21, 22, 23, 24, 25, 26, 28, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 40, 42, 43, 44, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 57, 58, 60, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85], "n_skeleton_in": 0, "channel": [0, 5, 6, 8, 13, 15, 31, 37, 38, 39, 40, 43, 46, 47, 48, 50, 51, 52, 53, 54, 55, 74, 77, 78, 79, 81], "input": [0, 1, 3, 5, 6, 7, 8, 9, 10, 12, 13, 14, 15, 18, 19, 21, 22, 24, 26, 29, 30, 31, 33, 35, 38, 40, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 57, 60, 62, 63, 64, 65, 66, 67, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 81, 82, 85], "step": [0, 1, 3, 6, 8, 10, 21, 24, 26, 29, 33, 35, 49, 55, 59, 60, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86], "4": [0, 1, 10, 14, 15, 24, 26, 49, 55, 60, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 81, 82, 83, 84, 85, 86], "h": [0, 3, 24, 31, 40, 47, 51, 62, 66, 71, 72, 74, 78, 79, 81, 82, 83, 85], "arrai": [0, 60, 63, 64, 65, 66, 67, 69, 71, 72, 74, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86], "like": [0, 49, 55, 57, 60, 62, 79, 81, 82, 85], "n_skeleton_out": 0, "out_channel": [0, 1, 3, 5, 6, 10, 12, 14, 15, 23, 24, 25, 26, 29, 32, 33, 35, 42, 43, 49, 54, 55, 57, 65, 66, 71, 72, 75, 79, 85], "featur": [0, 1, 3, 5, 6, 7, 8, 9, 10, 12, 13, 14, 15, 18, 19, 20, 21, 22, 23, 24, 25, 26, 28, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 40, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 57, 59, 60, 62, 63, 64, 65, 66, 67, 69, 70, 71, 72, 74, 78, 79, 80, 81, 82, 83, 84, 85, 86], "skeleton": [0, 45, 62, 86], "out": [0, 6, 58, 59, 60, 61, 62, 67, 70, 79, 80, 81, 86], "return": [0, 1, 3, 5, 6, 7, 8, 9, 10, 12, 13, 14, 15, 18, 19, 20, 21, 22, 23, 24, 25, 26, 28, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 40, 42, 43, 44, 45, 46, 47, 48, 49, 50, 52, 53, 54, 55, 57, 58, 60, 70, 81, 82, 83, 85, 86], "convolut": [1, 6, 8, 9, 10, 21, 42, 43, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 59, 79], "in_channel": [1, 3, 6, 12, 13, 14, 15, 23, 24, 25, 26, 29, 30, 31, 33, 35, 42, 43, 49, 54, 55, 57, 65, 66, 71, 72, 77, 78, 79, 81, 82, 85], "aggr_norm": [1, 15, 29, 48, 49, 54, 55], "bool": [1, 3, 5, 6, 7, 8, 12, 13, 14, 15, 21, 29, 33, 35, 44, 45, 49, 54, 55, 60], "fals": [1, 3, 6, 7, 8, 12, 13, 14, 15, 29, 33, 35, 44, 45, 48, 49, 54, 55, 62, 63, 64, 65, 66, 67, 69, 71, 72, 73, 75, 76, 80, 82, 83, 84, 85], "att": [1, 3, 7, 8, 24, 43, 44, 45, 49, 63, 71, 80], "initi": [1, 3, 6, 8, 10, 15, 21, 24, 26, 29, 33, 35, 40, 43, 45, 47, 49, 55, 57, 62, 63, 65, 66, 68, 69, 70, 72, 79, 82, 83], "xavier_uniform": [1, 3, 6, 15, 21, 24, 26, 43, 55, 72], "xavier_norm": [1, 3, 6, 15, 21, 24, 26, 43, 49], "initialization_gain": [1, 3, 15, 24], "float": [1, 3, 5, 6, 12, 13, 14, 15, 19, 21, 22, 24, 30, 31, 33, 42, 49, 55, 57, 62, 63, 64, 65, 66, 67, 69, 70, 71, 72, 73, 74, 76, 77, 78, 79, 81, 82, 83, 84, 85, 86], "1": [1, 3, 5, 6, 7, 8, 9, 10, 12, 13, 14, 15, 18, 19, 20, 21, 22, 23, 24, 25, 26, 28, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 40, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 53, 54, 55, 57, 58, 59, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 83, 84, 86], "414": [1, 3, 15, 21, 24, 49, 55, 69, 73], "with_linear_transform": 1, "true": [1, 5, 6, 21, 62, 63, 64, 65, 66, 67, 69, 71, 72, 73, 75, 76, 77, 78, 79, 81, 82, 83, 84, 85, 86], "2": [1, 3, 5, 6, 7, 8, 9, 10, 12, 13, 14, 18, 20, 21, 23, 25, 26, 28, 29, 30, 32, 33, 34, 35, 37, 39, 40, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 59, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 86], "3": [1, 5, 6, 8, 10, 21, 26, 29, 33, 35, 40, 45, 47, 51, 53, 55, 57, 60, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86], "build": [1, 2, 22, 56, 59, 60, 86], "rout": 1, "given": [1, 3, 8, 10, 19, 21, 22, 29, 33, 35, 40, 45, 47, 57, 59, 62, 63, 64, 65, 66, 69, 71, 72, 74, 77, 78, 79, 81, 82, 83, 85, 86], "one": [1, 3, 15, 21, 24, 47, 53, 54, 57, 59, 60, 63, 69, 74, 77, 78, 79, 80, 81, 82, 83, 86], "matrix": [1, 3, 6, 7, 8, 9, 10, 13, 15, 18, 19, 20, 21, 22, 23, 24, 25, 26, 28, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 40, 42, 43, 44, 49, 50, 51, 53, 55, 57, 60, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 74, 77, 78, 79, 80, 81, 82, 84, 85, 86], "includ": [1, 59, 60, 86], "an": [1, 4, 6, 8, 10, 49, 55, 59, 60, 62, 63, 64, 67, 69, 71, 72, 73, 75, 76, 77, 78, 79, 81, 82, 83, 84, 85, 86], "option": [1, 3, 5, 6, 7, 8, 10, 12, 13, 14, 15, 31, 42, 43, 59, 60, 66], "specif": [1, 9, 59, 60, 64, 65, 69, 71, 74], "function": [1, 3, 5, 6, 12, 13, 14, 15, 19, 22, 26, 31, 35, 45, 46, 47, 49, 50, 51, 55, 57, 58, 59, 60, 62, 64, 65, 66, 72, 74, 77, 78, 79, 81, 82, 85, 86], "int": [1, 5, 6, 7, 8, 9, 10, 12, 13, 14, 15, 18, 19, 20, 21, 22, 23, 24, 25, 26, 28, 29, 30, 31, 32, 33, 34, 35, 37, 39, 40, 42, 43, 44, 45, 46, 47, 48, 49, 50, 52, 53, 54, 55, 56, 57, 58, 62, 69, 70, 73, 75, 76, 80, 86], "dimens": [1, 6, 7, 8, 9, 10, 12, 13, 14, 15, 18, 19, 20, 21, 22, 23, 24, 25, 26, 28, 29, 32, 33, 34, 35, 37, 39, 40, 42, 44, 45, 46, 47, 48, 49, 50, 52, 53, 54, 55, 56, 57, 60, 62, 63, 64, 65, 66, 67, 69, 70, 71, 72, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86], "output": [1, 3, 5, 6, 8, 10, 12, 13, 14, 15, 19, 21, 22, 24, 26, 29, 31, 33, 35, 38, 40, 42, 43, 45, 47, 48, 49, 53, 54, 55, 57, 59, 60, 62, 71, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 85, 86], "whether": [1, 3, 5, 6, 7, 8, 12, 13, 14, 15, 21, 29, 33, 35, 44, 45, 54], "normal": [1, 6, 12, 13, 14, 15, 21, 22, 29, 31, 47, 49, 50, 51, 53, 55, 66, 81, 84], "aggreg": [1, 2, 3, 4, 6, 10, 15, 19, 26, 29, 35, 45, 46, 47, 49, 50, 54, 55, 59, 62, 71, 72, 79, 83, 85], "size": [1, 12, 14, 15, 29, 62, 63, 64, 65, 66, 67, 69, 71, 72, 74, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86], "us": [1, 3, 5, 6, 7, 8, 10, 19, 21, 22, 29, 30, 31, 33, 35, 42, 44, 45, 46, 47, 50, 51, 56, 57, 59, 60, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 75, 76, 77, 78, 79, 80, 81, 82, 84, 85, 86], "attent": [1, 2, 3, 5, 6, 7, 8, 15, 24, 42, 43, 44, 45, 59, 66, 71], "learnabl": [1, 3, 6, 13, 15, 21, 29, 35, 38, 40, 43, 47, 49, 53, 55, 57, 62, 66, 79, 85], "linear": [1, 6, 7, 9, 19, 23, 25, 28, 29, 31, 32, 34, 35, 44, 54, 59, 62, 70, 77, 78, 79, 81, 82, 85, 86], "transform": [1, 6, 14, 15, 29, 31, 35, 60, 70, 74, 79, 85], "nb": 1, "equal": [1, 60, 71, 86], "x_sourc": [1, 3, 6, 15, 24, 43], "x_target": [1, 3, 6, 24, 26], "tensor": [1, 3, 4, 5, 6, 7, 8, 9, 10, 12, 13, 14, 15, 18, 19, 20, 21, 22, 23, 24, 25, 26, 28, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 40, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 62, 63, 64, 65, 66, 67, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 81, 82, 83, 84, 85, 86], "thi": [1, 2, 3, 6, 8, 10, 19, 21, 22, 26, 29, 33, 35, 40, 43, 45, 47, 48, 49, 53, 54, 55, 57, 58, 59, 60, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 74, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86], "implement": [1, 3, 4, 6, 8, 9, 10, 15, 18, 20, 21, 23, 24, 25, 26, 28, 29, 31, 32, 33, 34, 35, 37, 39, 40, 42, 43, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 59, 62, 65, 66, 70, 79, 80, 81, 86], "from": [1, 3, 5, 6, 8, 10, 15, 18, 19, 21, 26, 29, 33, 35, 43, 45, 56, 57, 58, 59, 60, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86], "cell": [1, 3, 5, 6, 7, 8, 9, 10, 15, 24, 26, 36, 40, 43, 46, 47, 49, 50, 53, 54, 55, 59, 64, 65, 66, 67, 72, 73, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 86], "via": [1, 3, 38, 40, 43, 46, 47], "defin": [1, 2, 3, 33, 43, 56, 59, 60, 62, 63, 64, 65, 74, 79, 80, 86], "where": [1, 3, 43, 45, 49, 55, 59, 60, 62, 63, 64, 66, 69, 71, 72, 77, 78, 79, 80, 81, 82, 83, 85, 86], "can": [1, 3, 6, 8, 11, 21, 43, 47, 48, 55, 57, 59, 60, 61, 71, 72, 74, 77, 78, 79, 81, 83, 85, 86], "target": [1, 3, 6, 15, 24, 26, 43, 49, 55, 82, 85], "In": [1, 3, 19, 22, 43, 48, 59, 60, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86], "practic": [1, 3, 43, 65, 66], "If": [1, 3, 6, 10, 19, 24, 26, 31, 46, 60, 62, 63, 64, 65, 66, 67, 68, 69, 70, 74, 79, 80], "provid": [1, 3, 6, 31, 49, 55, 59, 60, 80, 82, 85], "i": [1, 2, 3, 5, 6, 8, 10, 11, 19, 21, 24, 25, 26, 27, 29, 30, 31, 33, 35, 38, 40, 41, 42, 45, 47, 53, 55, 57, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86], "assum": [1, 3, 15, 24, 26, 43], "e": [1, 3, 6, 12, 14, 19, 33, 49, 54, 55, 59, 60, 61, 62, 63, 64, 65, 66, 67, 69, 71, 72, 74, 77, 78, 79, 82, 84, 85, 86], "send": [1, 3, 8, 10, 21, 59, 62, 63, 64, 65, 66, 67, 69, 71, 72, 77, 78, 79, 84], "themselv": [1, 3, 81], "n_source_cel": [1, 3, 15, 24, 43], "all": [1, 3, 6, 15, 24, 26, 30, 31, 43, 48, 58, 59, 60, 67, 71, 72, 77, 78, 79, 81, 82, 85, 86], "have": [1, 3, 15, 19, 24, 26, 43, 49, 55, 59, 60, 65, 66, 67, 69, 71, 72, 77, 78, 81, 82, 85, 86], "same": [1, 3, 6, 15, 24, 26, 43, 47, 48, 49, 53, 59, 60, 62, 79, 80, 82, 85, 86], "rank": [1, 3, 7, 9, 15, 20, 23, 24, 25, 26, 28, 31, 32, 34, 37, 39, 42, 43, 47, 50, 51, 53, 59, 62, 63, 64, 65, 66, 67, 69, 71, 72, 77, 78, 79, 80, 81, 82, 83, 85], "r": [1, 3, 6, 8, 10, 15, 24, 42, 43, 45, 46, 47, 53, 60, 62, 63, 64, 65, 66, 71, 79, 80, 81, 86], "torch": [1, 3, 5, 6, 7, 8, 9, 10, 12, 13, 14, 15, 18, 19, 20, 21, 22, 24, 26, 29, 30, 31, 33, 35, 38, 40, 43, 44, 45, 46, 47, 49, 50, 51, 53, 55, 57, 59, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86], "spars": [1, 3, 4, 6, 8, 10, 13, 15, 18, 19, 20, 21, 22, 24, 29, 33, 35, 38, 40, 43, 46, 47, 49, 53, 55, 57, 60, 65, 66, 69, 70, 71, 72, 74, 77, 79, 81, 82, 83], "n_target_cel": [1, 3, 15, 24, 26, 43, 49, 55], "": [1, 3, 15, 18, 19, 26, 43, 59, 60, 62, 63, 64, 65, 66, 67, 68, 70, 74, 77, 78, 80, 81, 82, 86], "x_message_on_target": [1, 24, 26, 49, 55], "embed": [1, 24, 26, 44, 49, 55], "The": [2, 3, 4, 6, 8, 10, 11, 19, 20, 21, 22, 26, 27, 29, 31, 33, 35, 36, 40, 41, 42, 45, 47, 53, 55, 57, 59, 65, 66, 67, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 80, 81, 82, 83, 84, 85, 86], "compos": [2, 8, 10, 11, 21, 27, 29, 33, 35, 41], "primarili": [2, 11, 27, 36, 41], "three": [2, 29, 33, 35, 36, 41, 59, 60, 74], "conv": [2, 4, 21], "structur": [2, 62, 63, 64, 65, 66, 72, 79, 82, 85], "messagepass": [2, 3, 49, 55], "reset_paramet": [2, 3, 6, 13, 15, 21, 24, 27, 29, 31, 35, 38, 40, 41, 43, 45, 47, 49, 51, 53, 55, 57], "message_pass": 3, "add": [3, 6, 13, 15, 19, 58, 60, 74, 85], "uniform": [3, 6, 26, 86], "through": [3, 5, 7, 9, 10, 18, 23, 25, 28, 30, 32, 34, 44, 56, 59, 70, 85, 86], "singl": [3, 26, 59, 64], "n": [3, 6, 10, 43, 45, 56, 59, 60, 61, 62, 64, 71, 72, 79, 80, 86], "decompos": 3, "creat": [3, 10, 56, 59, 60, 66, 68, 71, 72], "go": [3, 9, 10, 61, 62, 64, 79, 86], "come": 3, "differ": [3, 22, 47, 53, 60, 69, 77, 78, 79, 81, 82, 85, 86], "onto": [3, 79], "should": [3, 6, 59, 60, 86], "instanti": [3, 64], "directli": [3, 74], "rather": [3, 59, 60], "inherit": [3, 59], "subclass": [3, 49, 55], "effect": [3, 19], "doe": [3, 59, 60, 82, 85, 86], "trainabl": [3, 33, 49, 55, 71], "weight": [3, 6, 15, 22, 24, 43, 45, 49, 55, 60, 66, 71, 79, 81, 82, 86], "its": [3, 19, 29, 31, 33, 35, 59, 60, 68, 70, 71], "gain": [3, 15, 21, 49, 55, 57], "refer": [3, 5, 6, 7, 8, 9, 10, 12, 13, 14, 15, 18, 19, 20, 21, 22, 23, 24, 25, 26, 28, 29, 30, 31, 32, 33, 34, 35, 38, 40, 43, 45, 47, 49, 51, 53, 55, 57, 60, 62, 72, 79], "hajij": [3, 7, 8, 10, 21, 26, 29, 33, 35, 38, 40, 43, 45, 47, 49, 51, 53, 55, 57, 59, 61, 62, 63, 78, 79], "zamzmi": [3, 7, 8, 40, 45, 61], "papamark": [3, 45, 59, 61], "miolan": [3, 8, 10, 21, 26, 29, 33, 35, 38, 40, 43, 45, 47, 49, 51, 53, 55, 57, 59, 61], "guzm\u00e1n": [3, 40, 59, 61], "s\u00e1enz": [3, 40, 59, 61], "ramamurthi": [3, 40, 59, 61], "birdal": [3, 59, 61], "dei": [3, 59, 61], "mukherje": [3, 59, 61], "samaga": [3, 59, 61], "livesai": [3, 59, 61], "walter": [3, 59, 61], "rosen": [3, 59, 61], "schaub": [3, 59, 61], "topolog": [3, 4, 7, 8, 10, 21, 26, 29, 33, 35, 38, 40, 43, 45, 47, 49, 51, 53, 55, 57, 61, 62, 63, 64, 69, 77, 78, 79, 80, 81, 83, 85, 86], "deep": [3, 8, 10, 21, 26, 29, 33, 35, 40, 45, 47, 51, 53, 55, 57, 61, 62, 63, 64, 66, 69, 77, 78, 79, 80, 81, 83, 85, 86], "learn": [3, 8, 10, 20, 21, 22, 25, 26, 29, 31, 33, 35, 40, 45, 47, 51, 53, 55, 57, 60, 61, 62, 63, 64, 69, 70, 71, 72, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86], "beyond": [3, 7, 8, 20, 21, 22, 51, 61, 62, 79], "graph": [3, 5, 6, 13, 15, 20, 21, 22, 28, 29, 30, 31, 32, 33, 34, 35, 48, 49, 59, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 84, 85, 86], "data": [3, 7, 8, 47, 53, 60, 61, 62, 68, 70, 79, 81, 84], "2023": [3, 8, 10, 21, 26, 29, 33, 35, 38, 40, 43, 45, 47, 49, 51, 53, 55, 57, 61, 62, 63, 64, 69, 74, 77, 78, 79, 80, 81, 82, 83, 85, 86], "http": [3, 5, 6, 7, 8, 9, 10, 12, 13, 14, 15, 18, 19, 20, 21, 22, 23, 24, 25, 26, 28, 29, 30, 31, 32, 33, 34, 35, 40, 43, 45, 47, 51, 53, 55, 57, 62, 68, 69, 70, 74, 77, 78, 79, 81, 82, 84], "arxiv": [3, 5, 6, 7, 8, 9, 10, 12, 13, 14, 15, 18, 19, 21, 25, 26, 28, 29, 30, 31, 32, 33, 34, 35, 40, 43, 45, 47, 51, 53, 55, 57, 61], "org": [3, 5, 6, 7, 8, 9, 10, 12, 13, 14, 15, 18, 19, 21, 23, 24, 25, 26, 28, 29, 30, 31, 32, 33, 34, 35, 40, 43, 45, 47, 51, 53, 55, 57, 69, 77, 78, 79, 81, 82], "ab": [3, 8, 9, 10, 12, 13, 14, 15, 18, 19, 21, 25, 26, 29, 33, 35, 40, 43, 45, 47, 51, 53, 55, 57, 83], "2206": [3, 61, 77], "00606": [3, 61], "papillon": [3, 8, 10, 21, 26, 29, 33, 35, 38, 40, 43, 45, 47, 49, 51, 53, 55, 57, 59, 61, 62, 63, 64, 69, 77, 78, 79, 80, 81, 83, 85, 86], "sanborn": [3, 8, 10, 21, 26, 29, 33, 35, 38, 40, 43, 45, 47, 49, 51, 53, 55, 57, 59, 61], "architectur": [3, 8, 10, 15, 21, 26, 29, 33, 35, 40, 43, 45, 47, 51, 53, 55, 57, 59, 61, 62, 63, 64, 65, 66, 67, 69, 71, 72, 77, 78, 79, 80, 81, 83, 85, 86], "survei": [3, 8, 10, 21, 26, 29, 33, 35, 40, 45, 47, 51, 53, 55, 57, 59, 61, 62, 63, 64, 69, 77, 78, 79, 80, 81, 83, 85, 86], "neural": [3, 4, 7, 8, 10, 12, 13, 14, 15, 18, 19, 21, 23, 25, 26, 28, 29, 30, 31, 32, 33, 34, 35, 38, 40, 43, 45, 47, 49, 51, 53, 54, 55, 56, 57, 59, 61], "network": [3, 4, 5, 6, 7, 8, 9, 10, 12, 13, 14, 15, 18, 19, 20, 21, 22, 23, 25, 26, 28, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 40, 42, 43, 45, 46, 47, 49, 50, 51, 52, 53, 54, 55, 56, 57, 59, 61], "2304": [3, 8, 10, 21, 26, 29, 33, 35, 40, 45, 47, 51, 53, 55, 57, 61], "10031": [3, 8, 10, 21, 26, 29, 33, 35, 40, 45, 47, 51, 53, 55, 57, 61], "x_messag": [3, 26], "A": [3, 8, 10, 12, 13, 14, 15, 18, 21, 26, 33, 38, 51, 59, 60, 61, 62, 63, 64, 69, 77, 78, 79, 80, 81, 82, 83, 85, 86], "receiv": [3, 10, 19, 26, 59], "sever": [3, 4, 26, 59], "per": [3, 15, 24, 26, 59, 60], "correspond": [3, 10, 26, 47, 53, 55, 62, 66, 68, 70, 74], "within": [3, 60, 62, 68, 70, 79], "n_messag": [3, 24, 26], "associ": [3, 26, 60, 62, 63, 64, 65, 66, 67, 71, 72, 73, 75, 76, 77, 78, 79, 80, 81, 82], "One": [3, 26, 37, 39, 42, 48, 52, 59, 81, 85, 86], "sent": [3, 26, 74], "comput": [3, 4, 5, 6, 7, 9, 12, 13, 14, 15, 18, 19, 20, 21, 22, 23, 24, 25, 26, 28, 31, 32, 34, 37, 39, 42, 44, 46, 48, 49, 50, 52, 54, 55, 58, 59, 60, 62, 70, 71, 74, 79, 85, 86], "scheme": [3, 6, 8, 45, 59, 63, 79], "altern": [3, 59], "user": [3, 77, 81, 82, 85], "overwrit": 3, "order": [3, 40, 42, 43, 47, 48, 49, 53, 54, 55, 57, 59, 62, 78, 79, 81, 82, 84, 85, 86], "replac": 3, "own": 3, "mechan": [3, 5, 6, 7, 8, 15, 24, 62, 63, 66, 71, 79], "follow": [3, 6, 10, 56, 59, 60, 61, 62, 65, 66, 67, 72, 73, 74, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86], "scalar": [3, 15, 21, 24, 63, 64, 80, 86], "between": [3, 6, 7, 8, 10, 13, 15, 19, 24, 26, 47, 53, 62, 79, 81], "two": [3, 8, 10, 19, 21, 31, 49, 62, 65, 66, 67, 69, 71, 72, 77, 78, 79, 81, 82, 85, 86], "m_": [3, 6, 8, 10, 13, 15, 19, 21, 24, 26, 29, 31, 33, 35, 38, 40, 43, 45, 47, 49, 51, 53, 55, 57, 62, 63, 64, 65, 66, 69, 71, 72, 77, 78, 79, 80, 81, 83, 85, 86], "y": [3, 6, 8, 10, 13, 15, 19, 21, 24, 26, 29, 31, 33, 35, 38, 40, 43, 45, 47, 49, 51, 53, 55, 57, 60, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86], "rightarrow": [3, 6, 8, 10, 13, 15, 19, 21, 24, 26, 29, 31, 33, 35, 38, 40, 43, 45, 47, 49, 51, 53, 55, 57, 62, 63, 64, 65, 66, 69, 71, 72, 77, 78, 79, 80, 81, 83, 85, 86], "left": [3, 10, 26, 45, 56, 60, 64, 71, 72, 86], "right": [3, 10, 26, 56, 60, 64, 71, 72, 86], "travel": 3, "denot": [3, 65, 66, 71, 72, 85, 86], "mathcal": [3, 6, 8, 10, 13, 15, 19, 21, 24, 26, 29, 31, 33, 35, 38, 40, 43, 45, 47, 49, 51, 53, 55, 57, 62, 63, 64, 65, 66, 69, 71, 72, 77, 78, 79, 80, 81, 83, 85, 86], "mathbf": [3, 82, 85], "_x": [3, 38, 51, 53, 62, 77, 79, 83], "_y": [3, 31, 40, 47, 78, 81], "theta": [3, 6, 8, 21, 24, 26, 29, 33, 38, 40, 43, 45, 47, 49, 51, 53, 57, 62, 63, 66, 69, 71, 72, 77, 78, 80, 81, 82, 83, 85, 86], "ar": [3, 6, 8, 10, 12, 13, 14, 15, 19, 21, 22, 29, 33, 35, 40, 43, 45, 47, 49, 57, 59, 60, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 74, 77, 78, 79, 80, 81, 82, 83, 85, 86], "call": [3, 19, 22, 49, 55, 60, 66, 86], "leftarrow": [3, 72], "across": [3, 22], "belong": [3, 60, 69, 77, 78, 79, 81, 82], "m_x": [3, 6, 8, 10, 19, 21, 24, 26, 29, 31, 33, 35, 38, 40, 43, 45, 47, 49, 51, 53, 55, 57, 63, 64, 69, 71, 72, 77, 78, 80, 81, 83, 85, 86], "text": [3, 8, 10, 15, 45, 49, 60, 65, 66, 67, 68, 69, 70, 71, 72, 77, 78, 79, 80, 81, 82], "agg": [3, 8, 10, 19, 45, 80], "_": [3, 6, 7, 8, 9, 10, 21, 23, 24, 25, 26, 28, 31, 32, 34, 35, 37, 38, 39, 40, 42, 43, 44, 45, 46, 47, 48, 50, 51, 52, 53, 55, 57, 63, 64, 65, 66, 67, 69, 70, 71, 72, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86], "result": [3, 60, 62, 66, 69, 82, 85], "detail": [3, 10, 59, 60, 62, 66, 79], "found": [3, 74], "construct": [3, 6, 68, 70, 74, 86], "reset": [3, 6, 13, 15, 21, 24, 29, 31, 35, 38, 40, 43, 45, 47, 49, 51, 53, 55, 57], "note": [3, 6, 8, 10, 21, 40, 43, 45, 48, 49, 51, 53, 54, 55, 57, 59, 60, 65, 66, 67, 71, 72, 77, 78, 82, 86], "give": [4, 62, 66, 79], "overview": 4, "which": [4, 6, 19, 26, 43, 44, 48, 59, 60, 65, 66, 68, 70, 71, 74, 77, 78, 79, 81, 82, 85, 86], "consist": [4, 31, 36, 58, 59, 60, 74, 86], "core": 4, "mathemat": 4, "concept": 4, "nn": [4, 5, 6, 7, 8, 9, 10, 12, 13, 14, 15, 18, 19, 20, 21, 22, 23, 24, 25, 26, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 59, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86], "organ": [4, 59, 81], "domain": [4, 59, 61, 62, 63, 64, 65, 66, 68, 69, 70, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86], "util": [4, 6, 30, 62, 65, 66, 73, 75, 76, 79, 86], "broadcast": [4, 58, 82, 85], "scatter": [4, 58, 70, 86], "scatter_add": [4, 58], "scatter_mean": [4, 58], "scatter_sum": [4, 58], "in_channels_0": [5, 6, 7, 8, 9, 10, 49, 52, 53, 62, 63, 64, 82, 84, 85], "in_channels_1": [5, 6, 7, 8, 9, 10, 49, 52, 53, 62, 63, 64, 82, 84, 85], "num_class": [5, 7, 8, 9, 18, 30, 48, 52, 62, 63, 64, 68, 70, 74, 82, 84], "dropout": [5, 6, 12, 13, 14, 15, 18, 19, 62, 70], "0": [5, 6, 7, 8, 9, 12, 13, 14, 15, 18, 19, 21, 22, 24, 26, 29, 30, 31, 33, 35, 37, 39, 40, 42, 43, 45, 49, 50, 51, 53, 55, 57, 59, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86], "5": [5, 18, 19, 21, 30, 42, 60, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 81, 82, 83, 84, 85, 86], "head": [5, 6, 14, 15, 62, 66], "concat": [5, 6, 49, 55], "skip_connect": [5, 6], "att_activ": [5, 6, 62], "leakyrelu": [5, 6, 62, 71, 79], "negative_slop": [5, 6, 62], "n_layer": [5, 7, 9, 12, 14, 18, 20, 23, 25, 28, 30, 32, 34, 37, 39, 42, 44, 46, 48, 50, 52, 54, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 75, 76, 77, 78, 79, 80, 81, 82, 83, 85], "att_lift": [5, 62], "classif": [5, 8, 10, 20, 21, 23, 25, 28, 30, 32, 34, 37, 39, 40, 42, 43, 45, 46, 48, 50, 52, 53, 54, 56, 59, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 75, 76, 77, 78, 79, 80, 81, 83, 84], "number": [5, 6, 7, 9, 12, 13, 14, 15, 18, 19, 20, 30, 31, 42, 43, 46, 48, 50, 52, 54, 59, 62, 70, 77, 78, 79, 80, 81, 83, 84, 86], "node": [5, 6, 7, 8, 9, 10, 13, 15, 18, 19, 20, 21, 22, 23, 24, 25, 26, 28, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 40, 42, 44, 45, 46, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 59, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 74, 75, 77, 78, 79, 80, 81, 83, 84, 86], "level": [5, 22, 24, 30, 45, 59, 62, 65, 66, 67, 69, 71, 72, 77, 78, 79, 80, 81, 82, 85], "edg": [5, 6, 7, 8, 9, 10, 12, 13, 14, 15, 20, 21, 23, 24, 25, 28, 29, 31, 32, 33, 34, 35, 38, 40, 42, 44, 48, 49, 50, 51, 52, 53, 54, 55, 57, 59, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86], "num_classest": 5, "probabl": [5, 6, 12, 13, 14, 15], "concaten": [5, 6, 19, 62, 66, 71, 86], "skip": [5, 6, 31, 37, 39, 40, 86], "connect": [5, 6, 31, 40, 56, 71, 74, 78, 82, 86], "activ": [5, 6, 13, 15, 46, 47, 50, 66], "lift": [5, 6, 59, 62, 63, 64, 65, 66, 69, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 85], "signal": [5, 6, 62, 63, 64, 65, 66, 67, 71, 72, 73, 75, 76, 79, 80], "giusti": [5, 6, 43, 59, 62, 79], "battiloro": [5, 6, 43, 59, 79], "testa": [5, 6], "di": [5, 6, 43], "lorenzo": [5, 6, 43], "sardellitti": [5, 6, 43], "barbarossa": [5, 6, 43], "2022": [5, 6, 12, 13, 14, 15, 18, 19, 40, 43, 45, 47, 53, 59, 62, 68, 77, 78, 79, 81, 85], "paper": [5, 6, 15, 19, 20, 21, 22, 59, 62, 68, 69, 70, 77, 78, 79, 82, 83, 84, 85, 86], "pdf": [5, 6, 7, 8, 20, 21, 22, 23, 24, 28, 29, 30, 31, 32, 33, 34, 35, 40, 45, 55, 60], "2209": [5, 6], "08179": [5, 6], "repositori": [5, 6, 59, 60], "lrnzgiusti": [5, 6], "x_0": [5, 6, 7, 8, 9, 10, 12, 14, 18, 19, 20, 21, 22, 29, 30, 31, 32, 33, 35, 37, 39, 40, 49, 50, 51, 52, 53, 62, 63, 64, 65, 66, 67, 69, 70, 71, 72, 74, 77, 78, 79, 80, 81, 82, 83, 84, 85], "x_1": [5, 6, 7, 8, 9, 10, 18, 19, 20, 21, 22, 23, 28, 34, 49, 50, 51, 52, 53, 62, 63, 64, 65, 66, 67, 69, 70, 71, 72, 73, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85], "neighborhood_0_to_0": [5, 6, 7, 8], "lower_neighborhood": [5, 6, 62], "upper_neighborhood": [5, 6, 62], "n_node": [5, 7, 9, 13, 15, 18, 19, 20, 21, 22, 23, 25, 28, 29, 32, 33, 34, 35, 37, 38, 39, 40, 42, 46, 48, 49, 50, 51, 52, 53, 54, 57, 69, 70], "n_edg": [5, 7, 9, 19, 20, 21, 22, 23, 25, 28, 29, 32, 33, 34, 35, 37, 38, 39, 40, 42, 48, 49, 50, 51, 53, 54, 55, 57, 69], "canlay": [6, 11, 62], "01": [6, 68, 74, 77, 82, 85, 86], "add_self_loop": [6, 11], "version": [6, 8, 9, 21, 63, 64, 82, 85], "v1": 6, "v2": 6, "share_weight": 6, "kwarg": [6, 25, 26], "model": [6, 30, 37, 39, 59, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 81, 83, 84], "consid": [6, 42, 48, 49, 54, 55, 62, 79, 82], "though": 6, "upper": [6, 9, 10, 19, 42, 43, 46, 47, 48, 49, 50, 51, 54, 55, 56, 62, 64, 79, 82, 85, 86], "lower": [6, 43, 46, 47, 49, 54, 55, 56, 62, 79, 80, 82, 85, 86], "addition": [6, 65, 66, 81], "ad": [6, 19, 59, 60], "prefer": [6, 59, 60], "necessari": [6, 58, 59, 62, 79], "self": [6, 30, 31, 60, 62, 70, 74, 77, 79, 81, 82, 85, 86], "loop": [6, 30, 31, 59, 65, 66, 67, 68, 70, 72, 73, 74, 76, 77, 78, 79, 81, 83, 84, 86], "preprocess": 6, "coeffici": [6, 62, 71], "otherwis": [6, 62, 63, 64, 65, 66, 67, 68, 69, 70, 74, 80, 86], "averag": [6, 9, 54, 86], "callabl": [6, 13, 15, 19, 22], "origin": [6, 31, 59, 60, 62, 63, 64, 79, 81, 82, 84, 85, 86], "while": [6, 59, 60, 79], "attet": 6, "gatv2": [6, 62], "valid": [6, 60, 73, 74, 75, 76, 86], "onli": [6, 8, 19, 53, 59, 60, 62, 74, 79, 82, 84, 86], "share": [6, 19, 59, 62], "n_1": [6, 43, 62, 79], "n_2": [6, 43, 62, 79], "a_": [6, 10, 40, 43, 62, 63, 64, 71, 77, 78, 79], "uparrow": [6, 8, 10, 38, 40, 43, 47, 51, 53, 55, 57, 62, 63, 64, 77, 78, 79, 81, 82, 83, 85, 86], "downarrow": [6, 38, 43, 45, 47, 51, 53, 55, 57, 62, 77, 79, 80, 81, 82, 83, 85, 86], "begin": [6, 8, 10, 13, 15, 19, 21, 24, 26, 29, 31, 33, 35, 38, 40, 43, 45, 47, 49, 51, 53, 55, 57, 60, 62, 79], "align": [6, 8, 10, 13, 15, 19, 21, 24, 26, 29, 31, 33, 35, 38, 40, 43, 45, 47, 49, 51, 53, 55, 57, 62, 79, 85], "quad": [6, 8, 10, 13, 15, 19, 21, 24, 26, 29, 31, 33, 35, 38, 40, 43, 45, 47, 49, 51, 53, 55, 57, 62, 63, 64, 65, 66, 69, 71, 72, 77, 78, 79, 80, 81, 83, 85, 86], "k": [6, 10, 19, 43, 45, 49, 55, 61, 62, 64, 66, 69, 71, 77, 78, 79, 80, 81, 83, 85, 86], "alpha_k": [6, 43, 62, 79], "h_x": [6, 8, 10, 13, 15, 19, 21, 24, 26, 29, 31, 33, 35, 38, 40, 43, 45, 47, 49, 53, 55, 57, 62, 63, 64, 65, 66, 69, 71, 72, 77, 78, 79, 80, 81, 85, 86], "t": [6, 8, 10, 13, 15, 19, 21, 24, 26, 29, 31, 33, 35, 38, 40, 43, 45, 47, 49, 51, 53, 55, 57, 61, 62, 63, 64, 65, 66, 69, 70, 71, 72, 74, 77, 78, 79, 80, 81, 82, 83, 85, 86], "h_y": [6, 8, 10, 13, 15, 19, 21, 26, 29, 33, 35, 40, 43, 49, 51, 55, 57, 62, 63, 64, 65, 66, 69, 72, 78, 79, 83, 85, 86], "a_k": [6, 43, 62, 79, 86], "cdot": [6, 21, 24, 26, 29, 31, 33, 35, 38, 40, 43, 47, 49, 51, 53, 55, 57, 62, 69, 71, 72, 77, 78, 79, 81, 83, 85, 86], "psi_k": [6, 43, 62, 79], "foral": [6, 43, 62, 71, 79], "n_k": [6, 43, 62, 79], "bigoplus_": [6, 43, 62, 79], "_k": [6, 10, 43, 45, 62, 64, 79, 80], "m": [6, 8, 38, 43, 45, 51, 53, 55, 57, 60, 63, 71, 77, 80, 83, 85, 86], "bigotimes_": [6, 43, 62, 79], "phi": [6, 43, 62, 79], "end": [6, 8, 10, 13, 15, 19, 21, 24, 26, 29, 31, 33, 35, 38, 40, 43, 45, 47, 49, 51, 53, 55, 57, 60, 62, 77, 78, 79, 81, 86], "n_k_cell": 6, "complex": [6, 7, 8, 9, 10, 23, 25, 28, 32, 34, 38, 40, 44, 45, 46, 47, 48, 49, 50, 51, 53, 54, 55, 56, 57, 59, 60, 62, 64, 65, 66, 67, 71, 72, 77, 78, 79, 84], "map": [6, 8, 10, 13, 15, 19, 20, 21, 22, 29, 33, 35, 38, 40, 46, 47, 57, 83, 86], "a_k_low": 6, "a_k_up": 6, "liftlay": [6, 11, 62], "signal_lift_activ": 6, "signal_lift_dropout": 6, "adapt": [6, 58], "offici": [6, 70], "rate": [6, 18, 19], "num_nod": [6, 30, 31], "num_edg": [6, 30, 31], "reiniti": 6, "xavier": 6, "multiheadcellattent": [6, 11, 62], "propos": [6, 8, 10, 15, 21, 24, 26, 29, 33, 35, 40, 43, 45, 47, 51, 53, 57, 62, 63, 64, 69, 72, 77, 78, 79, 82, 83, 84, 85, 86], "gat": [6, 62, 79], "adjac": [6, 7, 8, 9, 10, 18, 19, 37, 38, 39, 40, 46, 47, 49, 50, 51, 55, 62, 63, 64, 68, 77, 78, 79, 81, 82, 86], "non": [6, 62, 79], "zero": [6, 62, 65, 66, 74, 77, 78, 79, 82, 85, 86], "valu": [6, 42, 58, 63, 64, 65, 66, 67, 68, 70, 71, 72, 74, 77, 80, 82, 83, 84, 85, 86], "empti": 6, "veli\u010dkovi\u0107": 6, "cucurul": 6, "casanova": 6, "romero": 6, "li\u00f2": 6, "bengio": [6, 20, 21, 22], "2017": [6, 66, 77], "1710": [6, 69], "10903": 6, "up": [6, 7, 10, 29, 31, 33, 35, 37, 39, 43, 54, 55, 60, 79, 81, 86], "down": [6, 42, 43, 44, 45, 48, 50, 51, 54, 55, 79], "multiheadcellattention_v2": [6, 11], "brodi": 6, "alon": 6, "yahav": 6, "how": [6, 60, 66, 77, 78, 79], "2105": [6, 28, 29, 30, 31, 32, 33, 34, 35], "14491": 6, "alpha": [6, 21, 30, 31, 55, 62, 72, 79, 86], "multiheadliftlay": [6, 11, 62], "type": [6, 13, 15, 60, 65], "built": [6, 59, 60], "object": [6, 19, 60], "signal_lift_readout": 6, "str": [6, 13, 15, 24, 26, 46, 49, 50, 55, 58, 60], "cat": 6, "multi": [6, 13, 15, 60, 66], "readout": [6, 48], "z": [6, 8, 10, 13, 15, 19, 24, 26, 29, 31, 33, 35, 40, 43, 49, 55, 57, 62, 63, 64, 65, 66, 71, 72, 78, 85, 86], "h_z": [6, 10, 13, 15, 19, 43, 62, 64, 65, 66], "index": [6, 58, 60, 68, 81, 86], "poollay": [6, 11, 62], "k_pool": 6, "signal_pool_activ": [6, 62], "pool": [6, 7, 9, 23, 25, 28, 32, 34, 44, 62], "ratio": 6, "fraction": [6, 69], "keep": [6, 60, 62, 63, 64, 65, 66, 67, 68, 70, 72, 73, 76, 77, 78, 79, 81, 82], "after": [6, 29, 35, 60, 62, 70, 75], "oper": [6, 13, 15, 57, 62, 66, 71, 79, 81, 82, 86], "tupl": [6, 18, 43, 48, 49, 56, 86], "gamma": [6, 62, 70], "tau": [6, 62], "c_r": [6, 62], "num_pooled_nod": 6, "file": [6, 59, 60], "sparse_coo_tensor": [6, 68, 70], "softmax": [6, 11, 57, 79, 85, 86], "src": [6, 58, 74], "num_cel": 6, "There": [6, 59, 60, 68, 69, 70, 77, 78, 79, 81, 85], "subtract": 6, "maximum": [6, 47, 81, 86], "element": [6, 60, 66], "avoid": [6, 49, 55], "overflow": 6, "underflow": 6, "indic": [6, 43, 58, 70, 77, 78, 79, 81, 82, 86], "batch": [6, 86], "in_channels_2": [7, 8, 9, 10, 49, 52, 53, 62, 63, 64, 82, 84, 85], "face": [7, 8, 9, 10, 44, 48, 49, 50, 51, 52, 53, 54, 59, 62, 63, 64, 65, 66, 67, 71, 72, 77, 78, 79, 80, 81, 82, 83, 84, 85], "istvan": [7, 8], "analysi": [7, 8], "workshop": [7, 8, 20, 21, 22, 40, 51, 59], "neurip": [7, 8, 9, 10, 51], "2020": [7, 8, 20, 21, 22, 23, 24, 25, 26, 51, 59, 63, 68, 69, 71, 72, 83], "2010": [7, 8, 25, 26], "00743": [7, 8], "neighborhood_1_to_2": [7, 8], "avg": [7, 44], "n_face": [7, 9, 48, 49, 50, 51, 53], "transpos": [7, 44, 45, 80, 83], "boundari": [7, 9, 10, 20, 23, 25, 28, 32, 34, 37, 39, 57, 64, 65, 66, 67, 69, 71, 72, 77, 78, 81, 86], "x_2": [7, 8, 9, 10, 49, 50, 51, 52, 53, 63, 64, 65, 66, 67, 71, 72, 77, 78, 79, 80, 81, 82, 83, 84, 85], "label": [7, 9, 23, 25, 28, 32, 34, 37, 39, 42, 44, 47, 48, 50, 52, 53, 60, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 79, 80, 83, 84, 86], "assign": [7, 9, 23, 25, 28, 32, 34, 37, 39, 42, 44, 46, 48, 50, 52, 60, 74, 77, 78, 79, 81, 82, 86], "whole": [7, 9, 23, 25, 28, 32, 34, 44, 48, 50, 54], "simplifi": [8, 21, 63], "ccxn": [8, 11], "et": [8, 9, 10, 19, 22, 29, 33, 35, 53, 59, 61, 62, 63, 64, 65, 66, 68, 69, 70, 71, 72, 77, 78, 79, 80, 81, 82, 83, 85, 86], "al": [8, 9, 10, 22, 29, 33, 35, 53, 59, 61, 62, 63, 64, 65, 66, 69, 70, 71, 72, 77, 78, 79, 80, 81, 82, 83, 85, 86], "ccxnlayer": [8, 11, 63], "entir": [8, 10, 62, 63, 64], "equat": [8, 10, 21, 26, 29, 33, 35, 38, 40, 43, 45, 47, 49, 51, 53, 55, 57, 60, 61, 62, 63, 64, 69, 77, 78, 79, 81, 83, 86], "awesom": [8, 10, 21, 26, 29, 33, 35, 38, 40, 43, 45, 47, 49, 51, 53, 55, 57, 61, 66, 72], "tnn": [8, 10, 21, 26, 29, 33, 35, 38, 40, 43, 45, 47, 49, 51, 53, 55, 57, 59, 61], "wa": [8, 10, 21, 40, 45, 47, 57, 80], "Its": [8, 10, 21, 29, 33, 35, 40, 45, 47, 57], "graphic": [8, 10, 21, 29, 33, 35, 40, 45, 47, 57, 61], "illustr": [8, 10, 21, 29, 33, 35, 40, 45, 47, 57, 84], "amp": [8, 63], "l": [8, 10, 18, 19, 38, 40, 42, 43, 45, 47, 51, 53, 55, 57, 62, 63, 64, 71, 72, 77, 78, 79, 80, 81, 82, 83, 85, 86], "u": [8, 10, 19, 43, 45, 55, 59, 63, 64, 65, 66, 71, 80, 86], "cohomologi": [8, 63], "coboundari": [8, 9, 63, 64], "t_": [8, 26, 63, 72], "c": [8, 13, 15, 19, 21, 24, 26, 29, 31, 33, 35, 40, 47, 49, 51, 61, 62, 63, 65, 66, 69, 70, 71, 72, 77, 78, 81, 83, 86], "h_": [8, 24, 38, 45, 47, 53, 63, 71, 77, 80, 81], "n_0_cell": 8, "n_1_cell": 8, "a_0_up": 8, "n_2_cell": 8, "b_2": [8, 49, 50, 51, 57, 63, 79, 83], "requir": [8, 60, 62, 80], "predict": [8, 12, 14, 18, 20, 57, 60, 62, 70, 86], "hid_channel": [9, 64], "cw": [9, 10, 59], "hidden": [9, 12, 13, 14, 15, 18, 42, 65, 66, 70, 71, 79, 86], "bodnar": [9, 10, 59, 64], "weisfeil": [9, 10, 64], "lehman": [9, 10, 64], "cellular": [9, 10, 59, 64], "2021": [9, 10, 28, 29, 30, 31, 32, 33, 34, 35, 55, 57, 59, 64, 65, 66, 69, 86], "2106": [9, 10, 12, 13, 14, 15], "12575": [9, 10], "neighborhood_1_to_1": [9, 10], "neighborhood_2_to_1": [9, 10], "neighborhood_0_to_1": [9, 10], "project": [9, 42, 44, 60, 79], "cwn": [10, 11, 59], "cwnlayer": [10, 11, 64], "conv_1_to_1": 10, "conv_0_to_1": 10, "aggregate_fn": 10, "update_fn": 10, "represent": [10, 18, 19, 20, 21, 22, 25, 26, 40, 47, 53, 56, 59, 62, 65, 66, 68, 69, 70, 71, 81, 84, 86], "case": [10, 59, 62, 63, 64, 65, 66, 67, 69, 71, 72, 77, 78, 79, 84, 86], "convolv": 10, "neighbor": [10, 19, 22, 62, 72, 85, 86], "co": [10, 59, 74], "check": [10, 59, 60, 61, 69, 77, 78], "docstr": [10, 59], "_cwndefaultfirstconv": 10, "more": [10, 19, 59, 60, 61, 77, 81, 82, 86], "_cwndefaultsecondconv": 10, "obtain": [10, 54, 72, 74, 79, 82, 85, 86], "_cwndefaultaggreg": 10, "_cwndefaultupd": 10, "final": [10, 19, 22, 54, 57, 59, 68, 69, 70, 74, 79, 85, 86], "first": [10, 29, 31, 35, 60, 62, 63, 64, 65, 66, 67, 69, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86], "exploit": [10, 62], "second": [10, 29, 31, 35, 59, 65, 66, 80], "b": [10, 13, 15, 19, 21, 24, 26, 29, 31, 33, 35, 45, 47, 49, 50, 51, 60, 64, 65, 66, 69, 71, 72, 80, 81, 82, 83], "Then": [10, 60, 63, 64], "agg_": [10, 13, 15, 19, 45, 63, 64, 65, 66, 80], "n_": [10, 43, 65, 66, 67, 68, 69, 70, 71, 72, 77, 78, 79, 81, 82, 85], "_cell": 10, "in_channels_": 10, "b_": [10, 47, 64, 80, 81], "t_r": 10, "six": 11, "can_lay": 11, "ccxn_layer": 11, "cwn_layer": 11, "hypergraph": [12, 13, 14, 15, 18, 19, 20, 21, 22, 23, 24, 25, 26, 28, 29, 30, 31, 32, 33, 34, 35, 36, 59, 65, 66, 72, 73, 75, 76], "hidden_channel": [12, 13, 14, 15, 42, 65, 66, 79], "mlp_num_lay": [12, 13, 14, 15, 65, 66], "mlp_activ": [12, 13, 15], "mlp_dropout": [12, 13, 14, 15], "mlp_norm": [12, 13, 14, 15], "combin": [12, 14, 31, 62], "multipl": [12, 14, 59, 60, 86], "form": [12, 14, 56, 59, 77, 78, 81, 82, 86], "in_dim": [12, 14], "hid_dim": [12, 14, 66], "out_dim": [12, 14, 66, 71, 75], "input_dropout": [12, 14], "mlp": [12, 13, 14, 15, 27, 66], "chien": [12, 13, 14, 15, 59, 65, 66], "pan": [12, 13, 14, 15], "peng": [12, 13, 14, 15], "milenkov": [12, 13, 14, 15], "you": [12, 13, 14, 15, 51, 59, 60, 74], "multiset": [12, 13, 14, 15, 65, 66], "framework": [12, 13, 14, 15, 28, 29, 30, 31, 32, 33, 34, 35, 65, 66], "iclr": [12, 13, 14, 15, 40], "13264": [12, 13, 14, 15], "incidence_1": [12, 13, 14, 15, 18, 19, 20, 21, 22, 23, 28, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 40, 48, 50, 51, 57, 65, 66, 67, 68, 69, 70, 71, 72, 73, 75, 76, 77, 78, 82, 83, 85], "edge_index": [12, 14, 62, 65, 66, 68, 71, 72], "allset": [13, 14, 15, 27, 65, 66], "allsetblock": [13, 27], "block": [13, 15, 59], "bipartit": [13, 15], "incid": [13, 15, 18, 19, 20, 21, 22, 24, 25, 26, 29, 30, 31, 33, 35, 38, 40, 44, 45, 46, 47, 48, 49, 50, 51, 57, 65, 66, 67, 68, 69, 70, 71, 72, 74, 77, 78, 79, 80, 81, 82, 85], "hyperedg": [13, 15, 18, 19, 20, 21, 22, 24, 26, 35, 59, 65, 66, 67, 71, 72], "allsetlay": [13, 27], "vertex": [13, 15, 65, 66, 86], "sigma": [13, 21, 24, 26, 38, 40, 43, 47, 51, 53, 55, 57, 65, 69, 71, 72, 77, 78, 81, 82, 83, 85, 86], "n_hyperedg": [13, 15, 18, 22, 70], "b_1": [13, 15, 19, 20, 21, 22, 24, 26, 29, 31, 33, 35, 38, 40, 49, 50, 51, 57, 65, 66, 67, 68, 69, 70, 71, 72, 77, 78, 79, 83], "norm_lay": [13, 15], "activation_lay": [13, 15], "inplac": [13, 15], "bia": [13, 15, 21, 62, 82, 85], "perceptron": [13, 15, 66], "do": [13, 15, 59, 68, 70, 77, 78, 81, 86], "place": [13, 15, 59, 60, 65, 66], "allsettransform": [14, 15, 27, 59, 66], "allsettransformerblock": [15, 27], "number_queri": 15, "queri": 15, "allsettransformerlay": [15, 27], "ln": [15, 66], "multiheadattent": [15, 27], "qk": 15, "v": [15, 60, 65, 66, 69, 71, 72, 74, 77, 78, 79, 81, 82, 85], "mh": [15, 66], "eq": [15, 77, 78, 79, 82, 85], "7": [15, 18, 19, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86], "factor": 15, "in_featur": [18, 19, 22, 30, 62, 68, 70, 74, 82, 85], "hidden_featur": [18, 68, 70], "adjacency_dropout_r": 18, "regular_dropout_r": 18, "gradual": 18, "reduc": [18, 19, 82], "last": [18, 54, 57, 81, 86], "item": [18, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86], "hmpnnlayer": [18, 19, 27], "regular": [18, 19], "heydari": [18, 19, 59, 68], "livi": [18, 19, 68], "icann": [18, 19], "2203": [18, 19, 43], "16995": [18, 19], "b1": [18, 48, 49, 69, 77, 78, 81, 82, 83, 85], "y_pred": [18, 68, 70, 74, 77, 78, 79, 81, 82, 85], "logit": [18, 20, 30, 46, 70], "hmpnn": [19, 27, 59], "introduc": [19, 22, 62, 68, 70, 71, 79], "node_to_hyperedge_messaging_func": 19, "hyperedge_to_node_messaging_func": 19, "adjacency_dropout": 19, "updating_dropout": 19, "updating_func": 19, "compris": 19, "make": [19, 22, 33, 59, 62, 63, 64, 65, 66, 67, 68, 70, 74, 80, 86], "new": [19, 22, 59, 60, 63], "reprsent": 19, "them": [19, 49, 62, 63, 64, 65, 66, 67, 68, 70, 71, 74, 79, 80, 81], "also": [19, 55, 60, 62, 63, 64, 65, 66, 67, 69, 70, 71, 72, 73, 75, 76, 77, 78, 79, 80, 84], "reciev": 19, "beforehand": [19, 22], "wai": [19, 60, 79], "could": [19, 62, 79, 81], "explicit": 19, "rightarrow1": [19, 31, 49, 51, 83], "rightarrow0": [19, 24, 29, 31, 33, 35, 49, 51, 71, 83], "m_z": [19, 24, 26, 29, 31, 33, 35, 49, 71, 72], "plu": [19, 79], "accord": [19, 45, 70, 84], "It": [19, 30, 31, 59, 68, 70, 74], "get": [19, 57, 59, 79, 82, 85, 86], "back": 19, "retriev": [19, 62, 63, 64, 65, 66, 67, 69, 71, 72, 73, 75, 76, 77, 78, 79, 80, 81, 82, 84], "apply_regular_dropout": 19, "unmask": 19, "vector": [19, 56, 60, 66, 68, 70, 71, 86], "scale": [19, 86], "d": [19, 43, 55, 60, 71, 77, 83], "mask": [19, 56, 62, 68, 79, 86], "total": [19, 79], "node_in_featur": 19, "hyperedge_in_featur": 19, "channels_nod": [20, 21, 23, 25, 28, 34, 65, 67, 69, 73, 76, 77, 78, 79, 81, 82, 85], "channels_edg": [20, 21, 23, 25, 28, 34, 65, 67, 69, 73, 76], "n_class": [20, 44, 46, 50, 69, 80, 83], "neuron": [20, 21, 22, 59], "multiclass": 20, "dong": [20, 21, 22, 59, 69, 70], "sawin": [20, 21, 22], "icml": [20, 21, 22, 57], "grlplu": [20, 21, 22], "github": [20, 21, 22, 59, 66, 68, 70, 72, 74], "io": [20, 21, 22], "40": [20, 21, 22, 67, 68, 69, 70, 73, 74, 75, 76, 77, 81], "hypernod": [20, 21], "hnhnnetwork": [20, 27, 69], "templat": [21, 23, 60], "hnhnlayer": [21, 22, 27, 69, 70], "use_bia": 21, "use_normalized_incid": 21, "beta": [21, 30, 31, 74, 86], "bias_gain": 21, "bias_init": 21, "hnhn": [21, 22, 27, 59], "matric": [21, 44, 45, 46, 47, 48, 51, 59, 62, 63, 64, 65, 66, 67, 69, 71, 72, 77, 78, 79, 81, 82, 84, 85], "usign": 21, "cardin": 21, "hyperparamet": [21, 31, 68, 70, 74], "control": 21, "strenght": 21, "support": [21, 74, 79, 82, 85], "train": [21, 26, 44, 59], "term": [21, 59, 79], "flag": 21, "import": [21, 22, 31, 59, 60, 62, 63, 64, 65, 66, 68, 70, 72, 74, 79, 86], "compute_normalization_matric": 21, "w": [21, 29, 31, 69, 71, 79], "xy": [21, 38, 40, 43, 47, 51, 53, 55, 57, 69, 77, 78, 81, 83, 85, 86], "sum_": [21, 24, 26, 29, 31, 33, 35, 38, 40, 43, 47, 49, 51, 53, 55, 57, 69, 71, 72, 77, 78, 81, 82, 83, 85, 86], "init_bias": 21, "normalize_incidence_matric": 21, "activation_func": 22, "normalization_param_alpha": [22, 70], "normalization_param_beta": [22, 70], "relai": 22, "other": [22, 58, 60, 86], "word": [22, 68, 70], "intermediari": 22, "those": [22, 71, 79], "dure": 22, "multipli": [22, 79], "reflect": 22, "param": 22, "power": [22, 26, 83], "amount": [23, 25, 28, 32, 34, 37, 39, 52, 59, 63, 64, 65, 66, 67, 68, 70, 72, 73, 76, 82], "ding": [23, 24, 71], "wang": [23, 24], "li": [23, 24], "huan": [23, 24], "liu": [23, 24], "emnlp": [23, 24], "aclanthologi": [23, 24], "main": [23, 24, 60, 74], "399": [23, 24, 69, 73], "global": [23, 25, 28, 32, 34], "max": [23, 25, 28, 32, 34, 81], "hypergat": [24, 27, 71], "hypergatlay": [24, 27], "string": [24, 26, 60], "set": [24, 26, 67, 69, 72, 73, 74, 75, 76, 79, 81, 86], "see": [24, 26, 49, 51, 53, 59, 60, 61, 86], "t_1": [24, 31, 71], "odot": [24, 38, 43, 49, 71, 77], "zy": [24, 26, 31, 40, 71, 72, 78], "xz": [24, 26, 40, 71, 72, 78], "arya": [25, 26, 72], "gupta": [25, 26], "rudinac": [25, 26], "wor": [25, 26], "gener": [25, 26, 40, 56, 60, 62, 65, 66, 74, 78, 79], "induct": [25, 26], "04558": [25, 26], "features_nod": 25, "hypersag": [26, 27], "generalizedmean": [26, 27], "hypersagelay": [26, 27], "aggr_func_intra": 26, "aggr_func_int": 26, "devic": [26, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 74, 79, 80, 83, 84], "cpu": [26, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 74, 79, 80, 83, 84], "p": [26, 43, 55, 71, 72, 79], "name": [26, 59, 60, 62, 65, 66, 68, 73, 75, 76], "mode": [26, 74], "intra": [26, 45, 72], "either": [26, 59, 60, 79, 86], "w_y": [26, 72], "frac": [26, 31, 49, 71, 72], "vert": [26, 66, 72, 86], "w_z": [26, 72], "lvert": [26, 72], "rvert": [26, 72], "n_target_nod": 26, "allset_lay": 27, "allset_transformer_lay": 27, "allset_transform": [27, 66], "dhgcn_layer": 27, "dhgcn": 27, "hmpnn_layer": 27, "hnhn_layer_bi": [27, 70], "hnhn_layer": [27, 69], "hypergat_lay": 27, "hypersage_lay": 27, "unigcn_lay": 27, "unigcnlay": [27, 29, 73], "unigcn": [27, 29, 59], "unigcnii_lay": 27, "unigcniilay": [27, 31], "unigcnii": [27, 31, 32], "uniginlay": [27, 33, 75], "unigin": [27, 32, 33], "unisagelay": [27, 35, 76], "unisag": [27, 34, 35, 76], "huang": [28, 29, 30, 31, 32, 33, 34, 35, 59], "yang": [28, 29, 30, 31, 32, 33, 34, 35, 47, 53, 55, 59, 81, 82, 84, 85], "unignn": [28, 29, 30, 31, 32, 33, 34, 35], "unifi": [28, 29, 30, 31, 32, 33, 34, 35], "ijcai": [28, 29, 30, 31, 32, 33, 34, 35], "00956": [28, 29, 30, 31, 32, 33, 34, 35], "use_bn": [29, 35], "boolean": [29, 35, 60], "bathnorm": [29, 35], "everi": [29, 30, 31, 33, 35, 59, 74, 75], "hyper": [29, 31, 33, 35, 74, 86], "constitu": [29, 31, 33, 35], "third": [29, 31, 35], "num_lay": [30, 74, 82, 85], "expect": [30, 31, 74, 81, 83], "contain": [30, 31, 56, 59, 62, 65, 66, 68, 70, 74, 82, 86], "y_hat": [30, 62, 63, 64, 65, 66, 67, 69, 71, 72, 74, 77, 78, 79, 81, 82, 83, 84, 85], "determin": [31, 86], "theta_2": 31, "theta_1": 31, "x_skip": 31, "degre": 31, "sqrt": 31, "d_x": 31, "d_z": 31, "in_channels_nod": [32, 75], "intermediate_channel": [32, 54, 75, 85], "unigin_lay": 33, "ep": 33, "train_ep": 33, "g": [33, 49, 54, 55, 60, 65, 66, 71, 79, 86], "sequenti": [33, 70], "constant": 33, "gin": 33, "unisage_lay": 35, "e_aggr": 35, "amax": 35, "amin": 35, "v_aggr": 35, "operatornam": [35, 43, 71], "sage": 35, "submodul": 36, "simplici": [36, 37, 38, 39, 40, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 59, 63, 64, 65, 66, 67, 69, 71, 72, 84], "dist2cycl": [37, 38, 41], "binari": [37, 39, 42, 46, 50, 52, 62, 68, 69, 70, 73, 75, 76, 79], "high": [37, 39, 40], "x_1e": [37, 77], "linv": [37, 38, 77], "adjacency_0": [37, 38, 39, 40, 62, 63, 77, 78], "hot": [37, 39, 42, 48, 52, 69, 77, 78, 79, 82], "dist2cycle_lay": 38, "dist2cyclelay": [38, 41], "x_e": 38, "a_0": [38, 40], "hsn": [39, 40, 41, 59], "hsn_layer": [40, 59], "hsnlayer": [40, 41, 59, 78, 81], "complic": [40, 43], "higher": [40, 47, 53, 59, 62, 78, 81, 84, 85], "geometr": [40, 59], "openreview": [40, 51], "net": [40, 51, 57, 59], "id": [40, 51, 60], "sc8glb": 40, "k6e9": 40, "sanconv": [41, 43], "sanlay": [41, 43], "san": [41, 42, 43, 59, 62], "compute_projection_matrix": [41, 42], "scacmpslay": [41, 45, 80], "intra_aggr": [41, 45], "weight_func": [41, 45], "scacmp": [41, 44, 80], "sccnlayer": [41, 47, 53, 81], "sccn": [41, 46, 47, 53, 59], "sccnnlayer": [41, 49, 82], "aggr_norm_func": [41, 49, 55], "chebyshev_conv": [41, 49, 55], "sccnn": [41, 48, 49], "sccnncomplex": [41, 48, 82], "scconvlay": [41, 51], "scconv": [41, 50, 51], "scn2layer": [41, 47, 53], "scn2": [41, 47, 52, 84], "scnnlayer": [41, 55, 85], "scnn": [41, 54, 55, 82], "sconelay": [41, 57, 86], "scone": [41, 56, 57, 59], "trajectoriesdataset": [41, 56, 86], "vectorize_path": [41, 56, 86], "generate_complex": [41, 56, 86], "generate_trajectori": [41, 56, 86], "n_filter": [42, 43], "order_harmon": 42, "epsilon_harmon": 42, "simplex_order_k": [42, 79], "simplic": [42, 45, 48, 55, 79, 81, 82, 85, 86], "approxim": [42, 43, 86], "filter": [42, 43, 49, 82], "harmon": 42, "epsilon": 42, "1e": [42, 69, 73, 76, 86], "laplacian": [42, 43, 44, 45, 48, 49, 53, 54, 55, 79, 80, 81, 82, 84], "calcul": [42, 60], "compon": [42, 60, 79], "hodg": [42, 49, 53, 55, 79, 81, 82], "laplacian_up": [42, 43, 54, 55, 79, 85], "laplacian_down": [42, 43, 54, 55, 79, 85], "channels_in": 42, "ld": [42, 77], "san_lay": 43, "07485": 43, "l_": [43, 55, 57, 79, 80, 85, 86], "wh_1": 43, "simplex": [43, 44, 52, 53, 63, 64, 65, 66, 67, 71, 72, 77, 78, 79, 81, 82, 83, 85, 86], "projection_mat": 43, "2p": [43, 79], "q_r": [43, 79], "n_cell": 43, "down_indic": 43, "n_cells_down": 43, "n_neighbor": 43, "up_indic": 43, "n_cells_up": 43, "sca": [44, 45, 59], "cmp": [44, 45], "sca_cmp": [44, 80], "channels_list": [44, 45, 80], "complex_dim": [44, 45, 80], "tetahedron": 44, "respect": [44, 59, 64, 65, 66, 71, 72, 79, 82, 85, 86], "complex_dimens": 44, "highest": [44, 45, 86], "being": [44, 59, 79], "x_list": [44, 45], "laplacian_down_list": 44, "incidence_t_list": 44, "etc": [44, 55, 60], "start": [44, 59, 60, 74, 86], "autoencod": [45, 59], "sca_cmps_lay": 45, "coadjac": 45, "chain": [45, 57, 86], "maroula": 45, "cai": 45, "2103": 45, "04046": 45, "down_lap_list": 45, "incidencet_list": 45, "qquad": [45, 49, 80], "hold": [45, 60], "untouch": 45, "max_rank": [46, 47, 81, 82, 85], "dict": [46, 47], "length": [46, 47, 60, 68, 70, 74], "n_rank_r_cel": [46, 47], "n_rank_r_minus_1_cel": [46, 47], "b_r": [46, 47, 64, 81], "h_r": [46, 47, 53, 81], "sccn_layer": [47, 53], "ani": [47, 48, 60, 62, 63, 72, 86], "leftmost": 47, "diagram": [47, 53, 59], "yang22c": [47, 53, 84], "figur": [47, 53, 69], "11": [47, 53, 59, 62, 67, 68, 69, 70, 72, 73, 74, 75, 76, 77, 78, 79, 81, 82, 83, 85, 86], "scn2_layer": [47, 53], "scn": [47, 53], "abov": [47, 53, 59, 60, 85, 86], "below": [47, 53, 68, 75, 86], "sala": [47, 53, 84], "bogdan": [47, 53, 84], "effici": [47, 53, 77, 81, 82, 84], "proceed": [47, 53, 57, 84], "mlr": [47, 53, 57, 84], "press": [47, 53, 57, 84], "v198": [47, 53, 84], "yang22a": [47, 53, 84], "html": [47, 53, 57, 84], "describ": [47, 60, 66], "unnorm": 47, "bigcup": [47, 81], "out_featur": [47, 62, 82, 85], "in_channels_al": [48, 82], "intermediate_channels_al": [48, 82], "out_channels_al": [48, 82], "conv_ord": [48, 49, 55, 82], "sc_order": [48, 49, 82], "task": [48, 54, 59, 65, 66, 68, 69, 70, 74, 77, 78, 81, 82, 86], "we": [48, 49, 54, 55, 57, 59, 60, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 83, 84, 86], "cours": [48, 60], "amend": 48, "intermedi": [48, 54], "sc": [48, 49, 56, 80, 82, 85, 86], "numer": 48, "x_all": [48, 49, 82], "laplacian_al": [48, 49, 82], "incidence_al": [48, 49, 82], "entri": [48, 79], "n_simplic": [48, 49, 54, 55], "l0": 48, "l1_d": 48, "l1_u": 48, "l2": 48, "pf": 48, "b2": [48, 49, 81, 82, 83, 85], "sccnn_layer": 49, "triangl": [49, 55, 56, 57, 79, 82, 86], "ndoe": 49, "too": 49, "mani": [49, 60, 62], "exampl": [49, 55, 59, 79, 82, 85, 86], "here": [49, 51, 55, 59, 60, 62, 68, 70, 74, 77, 78, 81, 82, 86], "pseudocod": [49, 55], "l_0": 49, "lap_down": [49, 55], "l_1_down": 49, "lap_up": [49, 55], "l_1_up": 49, "lap": 49, "l_2": 49, "y_0": 49, "y_1": 49, "y_2": 49, "look": [49, 55, 60, 86], "einsum": [49, 55], "weight_0": 49, "weight_1": 49, "weight_2": 49, "total_order_0": 49, "total_order_1": 49, "total_order_2": 49, "chebyshev": [49, 55], "conv_oper": [49, 55], "perform": [49, 54, 55, 59, 60, 62, 63, 64, 65, 66, 67, 69, 71, 72, 73, 74, 76, 77, 78, 79, 81, 83, 84, 86], "num_channel": [49, 55], "repres": [49, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 74, 77, 78, 79, 84, 86], "n_triangl": [49, 57], "laplacian_0": [49, 52, 53, 82, 84, 85], "laplacian_down_1": [49, 80, 82, 85], "laplacian_up_1": [49, 82, 85], "laplacian_2": [49, 52, 53, 82, 84, 85], "part": [49, 55, 62, 77, 78, 79, 81, 82], "node_channel": [50, 51, 83], "edge_channel": [50, 51, 83], "face_channel": [50, 51, 83], "incidence_1_norm": [50, 51, 83], "incidence_2": [50, 51, 57, 64, 82, 83, 85], "incidence_2_norm": [50, 51, 83], "adjacency_up_0_norm": [50, 51, 83], "adjacency_up_1_norm": [50, 51, 83], "adjacency_down_1_norm": [50, 51, 83], "adjacency_down_2_norm": [50, 51, 83], "_1": [50, 51, 82, 83, 85], "_2": [50, 51, 82], "scconv_lay": 51, "bunch": [51, 83], "fung": 51, "singh": [51, 59], "tda": 51, "forum": 51, "tlbnskrt6j": 51, "tild": [51, 83], "For": [51, 54, 57, 60, 61, 69, 72, 77, 78, 82, 85, 86], "mai": [51, 59], "helper": 51, "pyt": 51, "team": [51, 59], "laplacian_1": [52, 53, 84], "log": [53, 60, 81, 86], "rightmost": 53, "pshm23": 53, "2i": [53, 81], "node_featur": 53, "edge_featur": 53, "face_featur": 53, "l_upper": 53, "l_lower": 53, "conv_order_down": [54, 55, 85], "conv_order_up": [54, 55, 85], "aggr": [54, 85], "At": [54, 82, 85], "simplci": 54, "To": [54, 61, 69, 77, 78, 85, 86], "challeng": 54, "one_dimensional_cells_mean": 54, "dimension": [54, 66, 86], "scnn_layer": 55, "total_ord": 55, "isufi": 55, "leu": 55, "2110": 55, "02585": 55, "n_simplex": 55, "simplicialcomplex": [56, 73, 75, 76, 86], "hidden_dim": [56, 86], "trajectori": [56, 57], "dataset": [56, 59, 62, 63, 64, 65, 66, 67, 68, 70, 71, 72, 73, 74, 75, 76, 79, 80], "path": [56, 86], "100": [56, 63, 67, 68, 69, 70, 73, 74, 75, 76, 80, 81, 83, 84, 86], "ndarrai": [56, 60, 86], "uniformli": [56, 86], "sampl": [56, 60, 62, 69, 86], "random": [56, 62, 86], "point": [56, 86], "unit": [56, 59, 60, 86], "squar": [56, 60, 86], "delaunai": [56, 86], "triangul": [56, 86], "delet": [56, 86], "some": [56, 60, 81, 86], "pre": [56, 59, 86], "disk": [56, 86], "coord": [56, 86], "n_max": [56, 86], "1000": [56, 69, 86], "corner": [56, 86], "middl": [56, 86], "scone_lay": 57, "stack": [57, 62, 63, 64, 67, 69, 70, 73, 75, 76, 77, 78, 79, 81, 82, 85, 86], "befor": [57, 59, 60, 74, 86], "neighbour": [57, 86], "next": [57, 60, 69, 71, 74, 86], "when": [57, 59, 60, 62, 79, 86], "roddenberri": [57, 59, 86], "mitchel": 57, "glaze": 57, "principl": [57, 86], "v139": 57, "roddenberry21a": 57, "variou": 58, "librari": 58, "torch_scatt": 58, "py": [58, 59, 60, 74, 77, 81, 82, 83, 85], "rusty1": 58, "pytorch_scatt": 58, "dim": [58, 60, 68, 70, 74, 77, 78, 79, 82, 85, 86], "dim_siz": 58, "welcom": [59, 60], "host": 59, "annual": 59, "topologi": [59, 79], "geometri": 59, "tag": 59, "machin": [59, 84], "review": [59, 60, 61], "contributor": [59, 60], "mathild": [59, 61], "mustafa": [59, 61], "nina": [59, 61], "florian": 59, "frantzen": 59, "ghada": [59, 61], "alzamzmi": 59, "theodor": [59, 61], "michael": [59, 61], "scholkemp": 59, "josef": 59, "hopp": 59, "karthikeyan": [59, 61], "natesan": [59, 61], "johan": 59, "math": [59, 60, 62, 66, 71, 72, 85, 86], "audun": 59, "myer": 59, "helen": 59, "jenn": 59, "tim": 59, "doster": 59, "tegan": 59, "emerson": 59, "henri": 59, "kving": 59, "bastian": [59, 84], "rieck": [59, 84], "sophia": [59, 61], "jan": 59, "meissner": 59, "paul": [59, 61, 84], "tolga": [59, 61], "vincent": 59, "grand": 59, "aldo": [59, 61], "tamal": [59, 61], "soham": [59, 61], "shreya": [59, 61], "neal": [59, 61], "robin": [59, 61], "edit": [59, 60], "now": [59, 62, 63, 65, 66, 67, 68, 69, 70, 71, 72, 74, 77, 78, 79, 84, 86], "over": [59, 62, 65, 66, 67, 68, 70, 72, 73, 76, 77, 78, 79, 81, 83, 84, 86], "thank": 59, "stellar": 59, "contirbut": 59, "foster": 59, "reproduc": [59, 61], "open": [59, 74], "research": [59, 84], "winner": 59, "announc": 59, "luca": 59, "scofano": 59, "claudio": 59, "guillermo": 59, "bernardez": 59, "simon": 59, "fiorellino": 59, "indro": 59, "spinelli": 59, "scardapan": 59, "lev": 59, "telyatninkov": 59, "olga": 59, "zaghen": 59, "sadrodin": 59, "barikbin": 59, "odin": 59, "hoff": 59, "gardaa": 59, "dmitrii": 59, "gavrilev": 59, "gleb": 59, "bazhenov": 59, "suraj": 59, "combinatori": 59, "rub\u00e9n": 59, "ballest": 59, "manuel": 59, "lecha": 59, "sergio": 59, "escalera": 59, "hoan": 59, "aiden": 59, "brent": 59, "honor": 59, "mention": 59, "jen": 59, "agerberg": 59, "georg": 59, "b\u00f6kman": 59, "pavlo": 59, "melnyk": 59, "alessandro": 59, "salatiello": 59, "alexand": 59, "nikitin": 59, "purpos": [59, 60, 63, 64, 65, 66, 67, 68, 70, 72, 73, 76, 77, 78, 79, 81, 82], "crowdsourc": 59, "ask": 59, "contribut": [59, 71, 82], "code": [59, 60, 86], "previous": 59, "exist": 59, "benchmark": [59, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 80, 81, 82, 83, 84, 85], "python": [59, 60, 61], "packag": [59, 61, 74, 77, 81, 82, 83, 85], "take": [59, 62, 79, 86], "pull": [59, 60], "request": [59, 60, 74, 86], "literatur": [59, 61, 74], "leverag": [59, 79], "infrastructur": 59, "invit": 59, "regularli": 59, "white": 59, "summar": 59, "find": [59, 62, 86], "publish": 59, "qualifi": 59, "opportun": 59, "author": [59, 61, 65, 70, 86], "top": [59, 62, 82], "8": [59, 62, 64, 65, 66, 67, 68, 69, 70, 71, 73, 74, 75, 76, 77, 78, 79, 81, 82, 84, 85, 86], "best": [59, 66], "addit": 59, "softwar": [59, 60], "journal": 59, "special": 59, "recognit": 59, "date": 59, "time": [59, 62, 65, 66, 67, 68, 69, 70, 71, 72, 77, 78, 79, 81, 82, 85, 86], "must": [59, 65, 66, 67, 69, 71, 72, 77, 78, 79, 81, 82, 85], "juli": 59, "13": [59, 63, 68, 69, 70, 73, 75, 76, 77, 78, 80, 81, 82, 83, 84, 85, 86], "16": [59, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 79, 80, 81, 82, 83, 84, 85, 86], "59": [59, 68, 69, 73, 77, 81], "pacif": 59, "standard": [59, 60, 74, 79], "modifi": [59, 60, 80], "until": 59, "everyon": [59, 60], "free": [59, 86], "suffici": 59, "accept": 59, "automat": [59, 79], "subscrib": 59, "encourag": 59, "earli": 59, "help": [59, 60], "debug": 59, "fail": 59, "test": [59, 62, 63, 64, 65, 66, 67, 68, 69, 70, 72, 73, 74, 75, 76, 77, 78, 79, 81, 82], "address": 59, "potenti": 59, "issu": [59, 74], "similar": [59, 81], "qualiti": 59, "earlier": [59, 81], "prioriti": 59, "consider": 59, "restrict": 59, "member": 59, "than": [59, 85, 86], "princip": 59, "develop": [59, 60], "allow": [59, 81], "fig": [59, 86], "compli": 59, "action": 59, "workflow": 59, "successfulli": 59, "lint": 59, "format": [59, 60, 74, 82], "black": [59, 86], "isort": 59, "flake8": 59, "_layer": 59, "ex": 59, "store": [59, 69], "directori": [59, 60], "primit": 59, "equival": [59, 60], "depict": 59, "_train": 59, "ipynb": 59, "hsn_train": 59, "tutori": [59, 61, 74], "process": [59, 60, 74], "well": [59, 60], "load": [59, 63, 64, 65, 66, 67, 70, 71, 72, 77, 78, 80, 81, 82, 83, 84, 85], "toponetx": [59, 62, 63, 64, 65, 66, 67, 69, 71, 72, 73, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86], "shrec16": [59, 63, 64, 65, 66, 67, 71, 72, 80, 82, 85], "suitabl": [59, 69], "template_lay": 59, "karat": [59, 69, 77, 78, 79, 81, 82], "club": [59, 69, 77, 78, 79, 81, 82], "choic": [59, 62, 63, 64, 65, 66, 67, 71, 72, 73, 75, 76, 80], "along": [59, 62, 79], "simpl": [59, 85], "depend": 59, "accuraci": [59, 69, 70, 73, 74, 75, 76, 77, 78, 79, 81, 82, 85, 86], "test_": [59, 60], "name_of_model": 59, "test_hsn_lay": 59, "testhsnlay": 59, "pleas": [59, 60, 62, 74, 79, 82, 85], "pytest": [59, 60], "unittest": 59, "further": [59, 62, 79], "manipul": 59, "modif": 59, "accompani": 59, "appropri": [59, 60], "locat": [59, 60, 74], "With": [59, 71], "said": 59, "highli": 59, "most": [59, 60, 79, 81], "resort": 59, "absolut": 59, "condorcet": 59, "decid": [59, 79], "criteria": 59, "chosen": [59, 81], "correctli": 59, "need": [59, 62, 63, 64, 65, 66, 67, 69, 71, 72, 77, 78, 79, 84, 86], "match": 59, "readabl": [59, 60], "clean": 59, "api": [59, 60], "written": 59, "clearli": 59, "explain": 59, "robust": 59, "reward": 59, "nor": 59, "goal": 59, "accur": 59, "our": [59, 60, 61, 62, 63, 64, 65, 66, 67, 69, 71, 72, 73, 75, 76, 77, 78, 79, 80, 81, 82, 84, 85, 86], "field": 59, "select": [59, 81, 85], "maintain": 59, "collabor": 59, "whose": [59, 60], "vote": 59, "onc": [59, 65, 66, 67, 71, 72], "googl": [59, 60], "express": [59, 71], "even": 59, "link": [59, 60], "record": [59, 60, 65, 66, 67, 71, 72], "email": 59, "identifi": 59, "voter": 59, "ident": [59, 66, 77, 78, 82], "remain": [59, 86], "secret": 59, "feel": [59, 86], "contact": 59, "slack": 59, "ucsb": 59, "edu": 59, "guid": 60, "aim": [60, 62, 79], "eas": 60, "both": [60, 62, 79, 86], "novic": 60, "experienc": 60, "commun": 60, "effort": 60, "fork": 60, "upstream": 60, "submit": [60, 74], "pr": 60, "synchron": 60, "your": [60, 65], "branch": 60, "git": 60, "checkout": 60, "sure": 60, "section": [60, 86], "re": [60, 77, 86], "done": [60, 63, 64, 65, 66, 67, 68, 71, 72, 80, 82, 83, 84, 85, 86], "commit": 60, "modified_fil": 60, "my": [60, 80], "push": 60, "toponextx": 60, "instruct": 60, "repeat": 60, "folder": 60, "filenam": 60, "test_add": 60, "def": [60, 70, 81, 82, 83, 85, 86], "test_capital_cas": 60, "assert": [60, 86], "9": [60, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 81, 82, 84, 85, 86], "statement": 60, "under": 60, "correct": [60, 62, 73, 75, 76, 86], "instal": 60, "tool": 60, "pip": 60, "dev": 60, "verifi": 60, "break": 60, "doc": 60, "descript": [60, 86], "usag": 60, "inform": [60, 65, 66, 68, 71, 72, 85], "markdown": 60, "languag": 60, "common": [60, 62], "restructuredtext": 60, "numpi": [60, 62, 63, 64, 65, 66, 67, 69, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86], "style": 60, "understand": 60, "role": 60, "syntax": 60, "autom": 60, "pars": 60, "inclus": 60, "print": [60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86], "__doc__": 60, "attribut": 60, "try": [60, 62, 63, 79, 86], "np": [60, 62, 63, 64, 65, 66, 67, 69, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86], "good": [60, 84], "These": 60, "ones": [60, 68, 70], "summari": 60, "line": 60, "79": [60, 68, 69, 73, 81, 85], "char": 60, "immedi": 60, "capit": 60, "letter": 60, "period": 60, "verb": 60, "imper": 60, "mood": 60, "possibl": [60, 74], "uncertain": 60, "oppos": 60, "evalu": [60, 62, 68, 69, 70, 74, 75], "separ": 60, "blank": 60, "argument": [60, 65, 66, 79], "On": 60, "state": [60, 72, 74], "rest": 60, "space": [60, 82, 86], "side": 60, "default_valu": 60, "indent": 60, "esp": 60, "would": [60, 77, 78], "want": [60, 74, 86], "veri": [60, 74], "rais": [60, 82, 85], "latex": 60, "cite": [60, 74], "my_method": 60, "my_param_1": 60, "my_param_2": 60, "big": 60, "o": [60, 66, 69, 79], "short": 60, "my_result": 60, "relev": 60, "snippet": 60, "show": [60, 69, 77, 78, 86], "script": 60, "wikipedia": 60, "page": [60, 84], "And": 60, "fill": 60, "scikit": 60, "fit_predict": 60, "sample_weight": 60, "cluster": [60, 80], "center": [60, 86], "conveni": 60, "fit": 60, "sparse_matrix": 60, "n_featur": 60, "ignor": [60, 73, 76], "Not": 60, "present": [60, 62], "convent": [60, 62], "observ": 60, "labels_": 60, "mind": 60, "instead": [60, 74, 82], "vari": 60, "notat": [60, 62, 63, 64, 69, 77, 78, 79, 80, 81, 83, 85, 86], "axi": [60, 86], "bracket": 60, "multinomi": 60, "1d": 60, "2d": 60, "subset": [60, 62, 68, 70], "datafram": 60, "explicitli": 60, "relat": [60, 68], "colon": 60, "explan": 60, "_weight_boost": 60, "adaboost": 60, "great": 60, "ve": 60, "discuss": 60, "Of": 60, "verbos": 60, "thei": [60, 62, 65, 66, 77, 78, 79, 81, 82, 85], "rst": 60, "80": [60, 68, 69, 70, 73, 80, 81], "charact": 60, "except": [60, 62, 79], "tabl": 60, "tdl": 61, "blue": 61, "laid": 61, "extend": [61, 82], "avail": [61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 74, 79, 80], "about": 61, "blueprint": 61, "misc": 61, "hajij2023topolog": 61, "titl": [61, 69], "year": 61, "eprint": 61, "archiveprefix": 61, "primaryclass": 61, "lg": 61, "papillon2023architectur": 61, "notebook": [62, 65, 66, 67, 68, 69, 70, 71, 72, 77, 78, 79, 82, 83, 84, 85, 86], "didact": [62, 79], "clear": [62, 79], "technic": [62, 79], "document": [62, 68, 70, 74, 79], "sinc": [62, 77, 78, 81, 82, 86], "introduct": 62, "achiev": [62, 66, 81, 86], "outstand": 62, "howev": [62, 74, 82, 85], "pairwis": [62, 65, 66, 67, 71, 72, 86], "relationship": 62, "among": 62, "abl": [62, 79], "fulli": 62, "interact": 62, "real": [62, 86], "world": [62, 86], "vertic": [62, 86], "captur": 62, "particular": 62, "encod": [62, 69, 77, 78, 79, 82, 86], "design": 62, "independ": [62, 66], "thu": [62, 79, 84], "strategi": [62, 72], "approach": 62, "hierarch": 62, "incorpor": 62, "algorithm": 62, "ii": 62, "optim": [62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86], "iii": 62, "extract": [62, 65, 66, 71, 72], "compact": 62, "meaning": 62, "remark": [62, 79], "custom": [62, 79], "symbol": [62, 79], "involv": [62, 79], "made": [62, 69, 77, 78, 79, 81, 82, 84, 85], "stage": 62, "nbsphinx": [62, 66, 71, 72, 85], "textrm": [62, 79], "parameter": 62, "mathbb": [62, 65, 66, 71, 86], "2f_0": 62, "f_0": 62, "textbf": [62, 65, 66, 72, 79], "bigg": [62, 79, 82, 85], "f": [62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86], "respons": [62, 74], "reciproc": 62, "round": [62, 83], "tcdot": 62, "xin": 62, "score": [62, 86], "_r": 62, "coars": 62, "mutag": [62, 73, 75, 76], "tudataset": [62, 73, 75, 76], "paperswithcod": 62, "com": [62, 68, 70, 74], "__": 62, "188": [62, 69, 73, 81], "chemic": 62, "compound": 62, "discret": 62, "mutagen": 62, "salmonella": 62, "typhimurium": 62, "sklearn": [62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 75, 76, 80, 82, 84, 85], "model_select": [62, 63, 64, 65, 66, 67, 69, 71, 72, 73, 75, 76, 80, 82, 84, 85], "train_test_split": [62, 63, 64, 65, 66, 67, 69, 71, 72, 73, 75, 76, 80, 82, 84, 85], "cell_complex": [62, 63, 64], "cellcomplex": 62, "torch_geometr": [62, 68, 70, 73, 75, 76, 79], "convert": [62, 69, 73, 75, 76, 77, 78, 79, 82, 84], "to_networkx": [62, 73, 75, 76, 79], "gpu": [62, 63, 64, 65, 66, 67, 68, 69, 70, 74, 80], "run": [62, 63, 64, 65, 66, 67, 68, 69, 70, 74, 81, 86], "cuda": [62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 74, 79, 80, 83, 84], "is_avail": [62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 74, 79, 80, 83, 84], "els": [62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 74, 79, 80, 82, 83, 84, 85], "root": [62, 73, 75, 76], "tmp": [62, 73, 74, 75, 76], "use_edge_attr": [62, 73, 75, 76], "use_node_attr": 62, "cc_list": [62, 63, 64], "x_0_list": 62, "x_1_list": [62, 73, 75, 76], "y_list": [62, 73, 75, 76], "append": [62, 63, 64, 65, 66, 67, 69, 71, 72, 73, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86], "edge_attr": 62, "i_cc": 62, "th": [62, 63, 64, 65, 66, 67, 71, 72, 79, 80, 83, 84], "36": [62, 68, 69, 70, 73, 75, 76, 77, 81], "0th": [62, 83], "17": [62, 63, 68, 69, 70, 73, 75, 76, 77, 80, 81, 82, 85], "38": [62, 68, 69, 70, 73, 75, 76, 77, 81], "lower_neighborhood_list": 62, "upper_neighborhood_list": 62, "adjacency_0_list": [62, 63], "adjacency_matrix": [62, 63, 64, 77, 78, 81, 82, 83, 86], "from_numpi": [62, 63, 64, 65, 66, 67, 69, 70, 71, 72, 73, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85], "todens": [62, 63, 64, 65, 66, 67, 69, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85], "to_spars": [62, 63, 64, 65, 66, 67, 69, 71, 72, 75, 77, 78, 79, 80, 81, 82, 83, 84, 85], "lower_neighborhood_t": 62, "down_laplacian_matrix": [62, 77, 79, 80, 82, 83, 85], "upper_neighborhood_t": 62, "up_laplacian_matrix": [62, 79, 82, 83, 85], "32": [62, 68, 69, 70, 71, 73, 75, 76, 77, 81, 86], "specifi": [62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 81, 82, 84, 85], "loss": [62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 81, 82, 83, 84, 85, 86], "without": [62, 63, 85], "6": [62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86], "crit": [62, 63, 73, 75, 76], "crossentropyloss": [62, 63, 68, 69, 70, 74, 75], "opt": [62, 63, 65, 66, 67, 71, 72, 80, 81, 82, 83, 85], "adam": [62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86], "lr": [62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86], "001": [62, 75, 84], "lift_lay": 62, "modulelist": [62, 82, 85], "lower_att": 62, "lin": 62, "64": [62, 65, 66, 68, 69, 73, 77, 81], "upper_att": 62, "lin_0": 62, "128": [62, 69, 73, 81], "lin_1": 62, "split": [62, 63, 64, 65, 66, 67, 69, 70, 72, 73, 74, 75, 76, 83, 86], "test_siz": [62, 63, 64, 65, 66, 67, 69, 71, 72, 73, 75, 76, 80, 82, 84, 85, 86], "x_1_train": [62, 63, 64, 73, 75, 76, 80, 82, 83], "x_1_test": [62, 63, 64, 73, 75, 76, 80, 82], "shuffl": [62, 63, 64, 65, 66, 67, 71, 72, 73, 75, 76, 80, 82, 84, 85, 86], "x_0_train": [62, 63, 64, 65, 66, 67, 70, 71, 72, 80, 82, 83], "x_0_test": [62, 63, 64, 65, 66, 67, 70, 71, 72, 80, 82], "lower_neighborhood_train": 62, "lower_neighborhood_test": 62, "upper_neighborhood_train": 62, "upper_neighborhood_test": 62, "adjacency_0_train": [62, 63], "adjacency_0_test": [62, 63], "y_train": [62, 63, 64, 65, 66, 67, 69, 71, 72, 73, 75, 76, 77, 78, 79, 80, 81, 82, 84, 85], "y_test": [62, 63, 64, 65, 66, 67, 69, 71, 72, 73, 75, 76, 77, 78, 79, 80, 81, 82, 84, 85], "test_interv": [62, 63, 64, 65, 66, 67, 69, 71, 72, 74, 77, 78, 79, 81, 82, 83, 84, 85], "num_epoch": [62, 63, 64, 65, 66, 67, 69, 71, 72, 73, 74, 75, 76, 77, 78, 79, 81, 82, 83, 84, 85], "epoch_i": [62, 63, 64, 65, 66, 67, 69, 71, 72, 77, 78, 79, 81, 82, 83, 84, 85], "rang": [62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 81, 82, 83, 84, 85, 86], "epoch_loss": [62, 63, 64, 65, 66, 67, 71, 72, 77, 78, 79, 81, 82, 83, 84, 85], "num_sampl": 62, "zip": [62, 63, 64, 65, 66, 67, 71, 72, 73, 75, 76, 82, 83, 84, 85], "dtype": [62, 65, 66, 67, 68, 69, 70, 71, 72, 74, 82, 85], "long": [62, 68, 70, 81], "zero_grad": [62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 81, 82, 83, 84, 85, 86], "argmax": [62, 68, 70, 74, 75, 85, 86], "backward": [62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 81, 82, 83, 84, 85, 86], "train_acc": [62, 68, 69, 74, 77, 78, 79, 81, 82, 85, 86], "epoch": [62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 81, 82, 83, 84, 85, 86], "4f": [62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 77, 78, 79, 81, 82, 84, 85], "flush": [62, 63, 64, 65, 66, 67, 69, 71, 72, 77, 78, 79, 81, 82, 83, 84, 85], "no_grad": [62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 81, 82, 83, 84, 85], "test_acc": [62, 68, 69, 74, 77, 78, 79, 81, 82, 85], "6251": 62, "6794": [62, 69], "5965": 62, "6007": 62, "6947": [62, 69], "6094": 62, "5881": 62, "5885": [62, 68], "7099": 62, "6316": 62, "5685": 62, "7252": 62, "5792": 62, "7176": 62, "6491": 62, "5614": 62, "7405": 62, "7368": 62, "small": [63, 64, 65, 66, 67, 71, 72, 80, 81, 82, 83, 84, 85], "14": [63, 68, 69, 70, 73, 74, 75, 76, 77, 80, 81, 82, 85, 86], "3d": [63, 64, 65, 66, 67, 71, 72, 80, 83, 84], "mesh": [63, 64, 65, 66, 67, 71, 72, 80, 82, 83, 84, 85], "15": [63, 68, 69, 70, 73, 74, 75, 76, 77, 80, 81, 82, 85, 86], "shrec": [63, 64, 65, 66, 67, 71, 72, 80, 83, 84, 85], "shrec_16": [63, 64, 65, 66, 67, 71, 72, 80, 82, 83, 84, 85], "kei": [63, 64, 65, 66, 67, 71, 72, 80, 82, 83, 84, 85], "node_feat": [63, 64, 65, 66, 67, 69, 71, 72, 77, 78, 79, 80, 81, 82, 83, 84, 85], "edge_feat": [63, 64, 65, 66, 67, 69, 71, 72, 77, 78, 79, 80, 81, 82, 83, 84, 85], "face_feat": [63, 64, 65, 66, 67, 71, 72, 77, 78, 79, 80, 81, 82, 83, 84, 85], "i_complex": [63, 64, 65, 66, 67, 71, 72, 80, 83, 84], "6th": [63, 64, 65, 66, 67, 71, 72, 80, 84], "252": [63, 64, 65, 66, 67, 69, 71, 72, 73, 80, 83, 84], "750": [63, 64, 65, 66, 67, 69, 71, 72, 80, 83, 84], "10": [63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86], "500": [63, 64, 65, 66, 67, 69, 71, 72, 73, 80, 83, 84], "messg": [63, 65, 66, 71, 72, 77, 78, 84], "incidence_2_t_list": 63, "to_cell_complex": [63, 64], "incidence_2_t": [63, 80], "incidence_matrix": [63, 64, 65, 66, 67, 69, 71, 72, 73, 75, 76, 77, 78, 79, 80, 81, 82, 83, 85], "18": [63, 68, 69, 70, 73, 75, 76, 77, 80, 81, 85], "19": [63, 68, 69, 70, 73, 75, 76, 77, 80, 81, 83, 86], "20": [63, 68, 69, 70, 73, 74, 75, 76, 77, 80, 81, 86], "loss_fn": [63, 65, 66, 67, 68, 70, 71, 72, 74, 80, 82, 83, 84, 85], "mseloss": [63, 64, 65, 66, 67, 71, 72, 80, 82, 83, 84, 85], "21": [63, 68, 69, 70, 73, 75, 76, 77, 80, 81, 84], "incidence_2_t_train": 63, "incidence_2_t_test": 63, "low": [63, 64, 65, 66, 67, 68, 70, 72, 73, 76, 77, 78, 79, 81, 82, 83, 84], "minim": [63, 64, 65, 66, 67, 68, 70, 72, 73, 76, 82], "rapid": [63, 64, 65, 66, 67, 68, 70, 72, 73, 76, 82], "22": [63, 68, 69, 70, 73, 74, 75, 76, 77, 81], "test_loss": [63, 64, 65, 66, 67, 68, 71, 72, 82, 83, 84, 85], "93": [63, 68, 69, 70, 73, 81, 83], "0369": [63, 69], "83": [63, 64, 68, 69, 70, 73, 81], "5211": 63, "45": [63, 68, 69, 70, 73, 74, 75, 76, 77, 78, 79, 81, 82, 83, 85], "6080": 63, "85": [63, 67, 68, 69, 70, 73, 81, 85], "5098": 63, "81": [63, 68, 69, 73, 81], "9403": 63, "34": [63, 68, 69, 70, 73, 75, 76, 77, 78, 79, 81, 82, 83, 85], "5233": 63, "23": [63, 68, 69, 70, 73, 75, 76, 77, 81], "24": [63, 68, 69, 70, 73, 75, 76, 77, 81], "7646": 63, "0686": [63, 69], "65": [63, 68, 69, 73, 77, 81], "3428": 63, "82": [63, 68, 69, 73, 81], "4051": 63, "0310": 63, "35": [63, 68, 69, 70, 73, 74, 75, 76, 77, 81], "6043": 63, "interc": 64, "incidence_2_list": [64, 82, 83, 85], "adjacency_1_list": 64, "incidence_1_t_list": 64, "adjacency_1": [64, 83], "incidence_1_t": [64, 80], "05": [64, 86], "criterion": [64, 69], "x_2_train": [64, 80, 82, 83], "x_2_test": [64, 80, 82], "adjacency_1_train": 64, "adjacency_1_test": 64, "incidence_2_train": [64, 82], "incidence_2_test": [64, 82], "incidence_1_t_train": 64, "incidence_1_t_test": 64, "107": [64, 69, 73, 81], "6411": 64, "84": [64, 68, 69, 70, 73, 81, 85], "9619": 64, "51": [64, 68, 69, 70, 73, 77, 81], "5892": 64, "2476": [64, 69], "7038": 64, "50": [64, 68, 69, 70, 73, 74, 75, 76, 77, 81], "7303": 64, "collect": [65, 66, 67, 68, 70, 71, 72, 85], "let": [65, 66, 71, 72, 86], "v_": [65, 66, 71], "e_": [65, 66], "rule": [65, 66], "put": [65, 66, 81], "f_": [65, 66], "permut": [65, 66], "invari": [65, 66], "parametr": [65, 66], "learnt": [65, 66], "solv": 65, "certain": [65, 85], "problem": [65, 72, 74], "what": [65, 66, 67, 71, 72, 73, 75, 76, 81], "feed": [65, 66, 67, 71, 72, 73, 75, 76, 86], "amtric": [65, 66, 67, 71, 72], "unsign": [65, 66, 67, 71, 72], "becom": [65, 66, 67, 71, 72, 79], "simplciial": [65, 66, 67, 71, 72], "wise": [65, 66, 67, 71, 72], "hg_list": [65, 66, 67, 71, 72, 73, 75, 76], "incidence_1_list": [65, 66, 67, 71, 72, 73, 75, 76, 82, 83, 85], "sign": [65, 66, 67, 69, 71, 72, 83, 86], "hg": [65, 66, 67, 71, 72, 73, 75, 76], "to_hypergraph": [65, 66, 67, 69, 71, 72, 73, 75, 76], "1250": [65, 66, 69, 71, 72], "incidence_1_train": [65, 66, 71, 72, 73, 75, 76, 82], "incidence_1_test": [65, 66, 71, 72, 73, 75, 76, 82], "to_edge_index": [65, 66, 71, 72], "274": [65, 66, 69, 71, 72, 73], "8176": 65, "529": [65, 66, 69, 71, 72], "0000": [65, 66, 69, 71, 72, 77, 78, 79, 81, 82, 85], "6125": [65, 66, 71, 72], "repo": [66, 70, 72], "rise": 66, "so": [66, 77, 78, 79, 81, 82, 85, 86], "dive": 66, "iter": 66, "Their": 66, "omega": 66, "overset": [66, 79], "delta": 66, "mathbin": 66, "ba": 66, "2016": [66, 68], "hf_": 66, "multihead": 66, "vaswani": 66, "row": 66, "load_ext": [66, 73, 76, 79], "autoreload": [66, 73, 76, 79], "q_n": 66, "9063": 66, "dir": 67, "templatelay": 67, "12": [67, 68, 69, 70, 72, 73, 74, 75, 76, 77, 78, 79, 81, 82, 84, 85, 86], "4791557": 67, "1252": [67, 69], "3105": 67, "2295": 67, "2015": 67, "3119": 67, "2987": 67, "2244": [67, 69], "4548": 67, "263": [67, 69, 73], "3355": 67, "280": [67, 69, 73], "4737": [67, 69], "110": [67, 69, 73, 74, 81], "2089": 67, "183": [67, 69, 73, 81], "3875": [67, 68], "26": [67, 68, 69, 70, 73, 75, 76, 77, 81], "0619": [67, 69], "cora": [68, 74], "2708": 68, "academ": [68, 70], "5429": 68, "citat": [68, 74], "categori": [68, 70], "case_bas": 68, "genetic_algorithm": 68, "neural_network": 68, "probabilistic_method": 68, "reinforcement_learn": 68, "rule_learn": 68, "theori": 68, "1433": [68, 69], "stand": [68, 70], "uniqu": [68, 69, 70, 83], "presenc": [68, 70], "planetoid": 68, "metric": [68, 69, 70], "accuracy_scor": [68, 70], "download": [68, 70, 74], "val": [68, 73, 75, 76, 86], "kimiyoung": 68, "raw": [68, 70, 74], "master": [68, 70, 74], "ind": 68, "tx": 68, "allx": 68, "ty": 68, "alli": 68, "manual_se": [68, 86], "41": [68, 69, 70, 73, 75, 76, 77, 81], "256": [68, 69, 73], "train_y_tru": [68, 70], "train_mask": [68, 70], "val_y_tru": 68, "val_mask": 68, "initial_x_1": 68, "zeros_lik": 68, "y_pred_logit": [68, 70], "train_loss": [68, 86], "eval": [68, 70, 73, 74, 75, 76, 86], "val_loss": [68, 86], "val_acc": [68, 86], "acc": [68, 70], "2f": [68, 70], "1079": [68, 69], "1436": [68, 69], "0234": 68, "1016": [68, 69], "9800": 68, "0681": [68, 69], "9504": 68, "0389": [68, 69], "9194": 68, "0137": 68, "9241": 68, "9917": 68, "8917": 68, "9729": 68, "8710": 68, "9556": 68, "8574": 68, "29": [68, 69, 70, 73, 75, 76, 77, 81], "9402": 68, "8646": 68, "9265": 68, "8540": 68, "33": [68, 69, 70, 73, 75, 76, 77, 81], "9137": 68, "8430": 68, "9012": 68, "8336": 68, "8886": 68, "25": [68, 69, 70, 73, 74, 75, 76, 77, 81, 86], "8405": 68, "8775": 68, "30": [68, 69, 70, 73, 74, 75, 76, 77, 78, 79, 81, 82, 83, 85, 86], "8264": 68, "8668": 68, "8065": 68, "8562": 68, "37": [68, 69, 70, 73, 75, 76, 77, 81], "8158": 68, "8456": 68, "7957": 68, "44": [68, 69, 70, 73, 75, 76, 77, 81], "8346": 68, "39": [68, 69, 70, 73, 75, 76, 77, 81, 82, 83], "8028": 68, "8249": 68, "7882": 68, "8156": 68, "42": [68, 69, 70, 73, 75, 76, 77, 81, 82], "7912": 68, "8070": 68, "7610": 68, "46": [68, 69, 70, 73, 74, 75, 76, 77, 81], "7985": 68, "7617": 68, "47": [68, 69, 70, 73, 75, 76, 77, 81], "7903": 68, "7596": 68, "7827": 68, "7391": 68, "7737": 68, "7316": 68, "7650": 68, "27": [68, 69, 70, 73, 75, 76, 77, 81, 83], "7364": 68, "7559": 68, "43": [68, 69, 70, 73, 75, 76, 77, 81, 83], "28": [68, 69, 70, 73, 75, 76, 77, 81], "7184": 68, "48": [68, 69, 70, 73, 75, 76, 77, 81], "7454": 68, "7086": [68, 69], "7362": 68, "6815": 68, "7276": 68, "31": [68, 69, 70, 73, 74, 75, 76, 77, 81, 82], "6673": 68, "7178": 68, "6846": [68, 69], "7080": 68, "6483": 68, "7007": 68, "6435": [68, 69], "54": [68, 69, 73, 74, 77, 81], "6982": 68, "6354": 68, "6998": 68, "6336": 68, "5937": 68, "60": [68, 69, 73, 77, 81, 86], "6972": [68, 69], "56": [68, 69, 73, 77, 81], "6962": 68, "5970": 68, "55": [68, 69, 73, 77, 81], "6861": [68, 69], "5597": 68, "52": [68, 69, 73, 77, 81], "5444": 68, "6510": 68, "5500": 68, "6320": 68, "5396": 68, "6160": 68, "5096": 68, "6032": 68, "4991": 68, "5924": 68, "5020": 68, "58": [68, 69, 73, 77, 81], "5830": 68, "49": [68, 69, 70, 73, 75, 76, 77, 81], "4710": 68, "5741": 68, "4607": 68, "67": [68, 69, 73, 77, 81], "5691": [68, 69], "4339": 68, "62": [68, 69, 70, 73, 77, 81], "5616": 68, "4425": 68, "66": [68, 69, 73, 77, 81], "5615": 68, "4206": 68, "5477": 68, "4142": 68, "5281": [68, 69], "53": [68, 69, 73, 77, 81], "4085": 68, "5042": 68, "4017": 68, "4884": 68, "3852": 68, "4838": 68, "3908": 68, "61": [68, 69, 73, 77, 81], "4847": 68, "57": [68, 69, 73, 77, 81], "3434": 68, "71": [68, 69, 73, 77, 81], "4878": [68, 69], "3259": 68, "69": [68, 69, 73, 77, 81], "4851": 68, "3378": 68, "4907": 68, "2930": 68, "4966": 68, "3125": 68, "5017": 68, "3093": 68, "4984": 68, "63": [68, 69, 73, 81], "2770": 68, "4915": 68, "2753": 68, "4854": 68, "2741": 68, "4831": [68, 69], "2784": 68, "4728": 68, "2445": 68, "4712": 68, "68": [68, 69, 70, 73, 77, 81], "4743": 68, "1976": [68, 69], "4705": 68, "70": [68, 69, 73, 77, 81], "1895": [68, 69], "4638": 68, "4672": 68, "72": [68, 69, 73, 77, 81], "1565": [68, 69], "74": [68, 69, 73, 77, 81], "4450": 68, "73": [68, 69, 73, 77, 81], "1744": [68, 69], "4279": 68, "1863": [68, 69], "4286": 68, "75": [68, 69, 73, 75, 77, 81], "1765": [68, 69], "4333": [68, 77], "76": [68, 69, 73, 77, 81], "1773": [68, 69], "4499": 68, "77": [68, 69, 73, 77, 81, 84], "1232": [68, 69], "4701": 68, "78": [68, 69, 73, 77, 78, 79, 81, 82, 85], "0667": [68, 69], "4692": 68, "0716": [68, 69], "4593": 68, "0567": [68, 69], "0626": [68, 69], "4309": 68, "0453": [68, 69], "4244": 68, "0300": 68, "4135": 68, "0505": [68, 69], "4152": 68, "0212": 68, "4340": 68, "86": [68, 69, 73, 81], "0401": [68, 69], "4580": 68, "87": [68, 69, 73, 81, 84], "9888": 68, "4668": 68, "88": [68, 69, 73, 81, 85], "0185": 68, "4771": 68, "89": [68, 69, 73, 81], "9739": 68, "90": [68, 69, 73, 81, 85], "9517": 68, "4661": 68, "91": [68, 69, 70, 73, 81], "9341": 68, "4519": 68, "92": [68, 69, 70, 73, 81], "9642": 68, "4457": 68, "9191": 68, "4126": 68, "94": [68, 69, 70, 73, 81], "8985": 68, "4001": 68, "95": [68, 69, 73, 81], "8884": 68, "3864": 68, "96": [68, 69, 70, 73, 81], "9020": 68, "97": [68, 69, 70, 73, 81, 83, 84], "8673": 68, "3893": 68, "98": [68, 69, 70, 73, 81], "9479": 68, "3814": [68, 69], "99": [68, 69, 73, 74, 81, 83, 86], "8873": 68, "3855": 68, "8565": 68, "3936": 68, "against": [68, 70], "test_y_tru": [68, 70], "test_mask": [68, 70], "3152": 68, "karateclub": [69, 77, 78, 79, 81, 82], "matplotlib": [69, 70, 86], "pyplot": [69, 70, 86], "plt": [69, 70, 86], "www": [69, 77, 78, 79, 81, 82], "jstor": [69, 77, 78, 79, 81, 82], "stabl": [69, 77, 78, 79, 81, 82], "3629752": [69, 77, 78, 79, 81, 82], "singular": [69, 77, 78, 79, 81, 82], "social": [69, 77, 78, 79, 81, 82], "group": [69, 77, 78, 79, 81, 82], "dataset_sim": 69, "karate_club": [69, 77, 78, 79, 81, 82, 85], "complex_typ": [69, 77, 78, 79, 81, 82, 85], "dataset_hyp": 69, "santii": [69, 77, 78], "classifi": [69, 70, 74], "channels_": 69, "get_simplex_attribut": [69, 77, 78, 79, 81, 82, 85], "y_1h": 69, "ey": [69, 74, 77, 81], "astyp": 69, "stratifi": 69, "ind_train": 69, "ind_test": 69, "arang": 69, "random_st": 69, "float32": [69, 74], "int32": 69, "2000": 69, "get_accuraci": 69, "lambda": 69, "yhat": 69, "ytrue": 69, "full": 69, "nan": [69, 83], "y_hat_cl": 69, "nloss": 69, "ntrain_acc": 69, "7254": 69, "5000": [69, 81], "7232": 69, "7211": 69, "7190": 69, "7170": 69, "7152": 69, "7134": 69, "7117": 69, "7101": 69, "7072": 69, "7058": 69, "7046": 69, "7034": 69, "7024": 69, "7014": 69, "7005": 69, "6997": 69, "6990": 69, "6983": 69, "6977": 69, "6967": 69, "6963": 69, "6960": 69, "6957": 69, "6954": 69, "6952": 69, "6951": 69, "6949": 69, "6948": 69, "6946": 69, "1429": [69, 77], "3571": 69, "6945": 69, "6944": 69, "6943": 69, "6942": 69, "6941": 69, "3214": 69, "0714": 69, "1071": 69, "6940": 69, "6939": 69, "6938": 69, "6937": 69, "6936": 69, "6935": 69, "101": [69, 73, 81], "102": [69, 73, 81], "103": [69, 73, 81, 83, 85], "104": [69, 73, 81, 84], "105": [69, 73, 81, 83], "106": [69, 73, 81], "108": [69, 73, 74, 81], "109": [69, 73, 74, 81], "6934": 69, "111": [69, 73, 74, 81], "112": [69, 73, 81], "113": [69, 73, 74, 81, 83], "114": [69, 73, 81], "115": [69, 73, 81], "116": [69, 73, 81], "117": [69, 73, 81], "118": [69, 73, 81], "119": [69, 73, 81], "6933": 69, "120": [69, 73, 81], "121": [69, 73, 81, 85], "122": [69, 73, 81], "123": [69, 73, 81], "124": [69, 73, 81], "125": [69, 73, 81], "126": [69, 73, 81], "127": [69, 73, 81], "129": [69, 73, 81], "130": [69, 73, 81], "6932": 69, "131": [69, 73, 81], "132": [69, 73, 81], "133": [69, 73, 74, 81], "134": [69, 73, 81], "135": [69, 73, 81], "136": [69, 73, 81], "137": [69, 73, 81], "138": [69, 73, 81], "139": [69, 73, 81], "140": [69, 73, 81], "141": [69, 73, 81], "6931": 69, "142": [69, 73, 81], "143": [69, 73, 77, 81, 82], "144": [69, 73, 81], "145": [69, 73, 81], "146": [69, 73, 81], "147": [69, 73, 81, 84], "148": [69, 73, 81], "149": [69, 73, 81], "150": [69, 73, 81, 86], "151": [69, 73, 81], "152": [69, 73, 81], "6930": 69, "153": [69, 73, 81], "154": [69, 73, 81], "155": [69, 73, 81], "156": [69, 73, 81], "157": [69, 73, 81], "158": [69, 73, 81], "159": [69, 73, 81], "160": [69, 73, 81], "161": [69, 73, 81], "162": [69, 73, 81], "163": [69, 73, 81], "6929": 69, "164": [69, 73, 81], "165": [69, 73, 81], "166": [69, 73, 81], "167": [69, 73, 81], "168": [69, 73, 81], "169": [69, 73, 81], "170": [69, 73, 81], "171": [69, 73, 81], "172": [69, 73, 81], "173": [69, 73, 81], "6928": 69, "174": [69, 73, 81], "175": [69, 73, 81], "176": [69, 73, 81], "177": [69, 73, 81, 85], "178": [69, 73, 81], "179": [69, 73, 81], "180": [69, 73, 81], "181": [69, 73, 81], "182": [69, 73, 81], "6927": 69, "184": [69, 73, 81], "185": [69, 73, 74, 81], "186": [69, 73, 81], "187": [69, 73, 81], "189": [69, 73, 81], "190": [69, 73, 81], "6926": 69, "191": [69, 73, 81], "192": [69, 73, 74, 81], "193": [69, 73, 81], "194": [69, 73, 81], "195": [69, 73, 81], "196": [69, 73, 81], "197": [69, 73, 81], "6925": 69, "198": [69, 73, 81, 84, 85], "199": [69, 73, 74, 81], "200": [69, 73, 74, 81], "201": [69, 73], "202": [69, 73], "203": [69, 73], "6924": 69, "204": [69, 73], "205": [69, 73], "206": [69, 73], "207": [69, 73], "208": [69, 73], "6923": 69, "209": [69, 73], "210": [69, 73], "211": [69, 73], "212": [69, 73], "213": [69, 73], "6922": 69, "214": [69, 73, 82], "215": [69, 73], "216": [69, 73], "217": [69, 73], "6921": 69, "218": [69, 73], "219": [69, 73], "220": [69, 73], "221": [69, 73], "6920": 69, "222": [69, 73], "223": [69, 73], "224": [69, 73], "225": [69, 73], "6919": 69, "226": [69, 73], "227": [69, 73, 84], "228": [69, 73], "229": [69, 73], "6918": [69, 79], "230": [69, 73], "231": [69, 73], "232": [69, 73, 84], "6917": 69, "233": [69, 73], "234": [69, 73], "235": [69, 73], "6916": 69, "236": [69, 73], "237": [69, 73], "6915": 69, "238": [69, 73], "239": [69, 73], "240": [69, 73], "6914": 69, "241": [69, 73], "242": [69, 73], "6913": 69, "243": [69, 73], "244": [69, 73], "245": [69, 73], "6912": 69, "246": [69, 73], "247": [69, 73], "6911": 69, "248": [69, 73], "249": [69, 73], "6910": 69, "250": [69, 73], "251": [69, 73], "6909": 69, "253": [69, 73], "6908": 69, "254": [69, 73], "6907": 69, "255": [69, 73, 74], "6906": 69, "257": [69, 73], "258": [69, 73], "6905": 69, "259": [69, 73], "6904": 69, "260": [69, 73], "261": [69, 73], "6903": 69, "262": [69, 73], "6902": 69, "264": [69, 73], "6901": 69, "265": [69, 73], "6900": 69, "266": [69, 73], "6899": 69, "267": [69, 73], "268": [69, 73], "6898": 69, "269": [69, 73], "6897": 69, "270": [69, 73, 77], "6896": 69, "271": [69, 73], "6895": 69, "272": [69, 73], "6894": 69, "273": [69, 73], "6893": 69, "275": [69, 72, 73], "6892": 69, "276": [69, 73], "6891": [69, 81], "277": [69, 73], "6890": 69, "278": [69, 73], "6889": 69, "279": [69, 73], "6888": 69, "6886": 69, "281": [69, 73], "6885": 69, "282": [69, 73, 84], "6884": 69, "283": [69, 73], "6883": 69, "284": [69, 73], "6882": 69, "285": [69, 73], "6881": 69, "286": [69, 73], "6879": [69, 77], "287": [69, 73], "6878": 69, "288": [69, 73], "6877": 69, "289": [69, 73], "6876": 69, "290": [69, 73], "6874": 69, "5357": 69, "291": [69, 73], "6873": 69, "292": [69, 73], "6871": 69, "293": [69, 73], "6870": 69, "294": [69, 73], "6868": 69, "295": [69, 73], "6867": 69, "296": [69, 73], "6865": 69, "297": [69, 73], "6864": 69, "298": [69, 73], "6862": 69, "299": [69, 73], "300": [69, 73], "6859": 69, "5714": 69, "6667": [69, 81, 82], "301": [69, 73], "6857": 69, "302": [69, 73, 74], "6855": 69, "303": [69, 73], "6854": 69, "304": [69, 73, 84], "6852": 69, "305": [69, 73], "6850": 69, "306": [69, 73], "6848": 69, "307": [69, 73], "308": [69, 73], "6844": 69, "6429": 69, "309": [69, 73], "6842": 69, "310": [69, 73], "6840": 69, "311": [69, 73], "6838": 69, "312": [69, 73], "6835": 69, "313": [69, 73], "6833": 69, "314": [69, 73], "6831": 69, "315": [69, 73], "6829": 69, "316": [69, 73], "6826": 69, "317": [69, 73], "6824": 69, "318": [69, 73], "6821": 69, "319": [69, 73], "6819": 69, "320": [69, 73], "6816": 69, "321": [69, 73], "6814": [69, 79], "322": [69, 73], "6811": 69, "323": [69, 73], "6808": 69, "324": [69, 73], "6806": 69, "325": [69, 73], "6803": 69, "326": [69, 73], "6800": 69, "327": [69, 73], "6797": 69, "328": [69, 73], "329": [69, 73], "6791": 69, "330": [69, 73], "6788": 69, "331": [69, 73], "6785": 69, "332": [69, 73], "6782": 69, "333": [69, 73], "6778": 69, "334": [69, 73], "6775": 69, "335": [69, 73], "6772": 69, "336": [69, 73], "6768": 69, "337": [69, 73], "6765": 69, "338": [69, 73], "6761": 69, "339": [69, 73], "6758": 69, "340": [69, 73], "6754": 69, "341": [69, 73], "6750": 69, "342": [69, 73], "6746": 69, "343": [69, 73], "6742": 69, "344": [69, 73], "6738": 69, "345": [69, 73], "6734": 69, "346": [69, 73], "6730": 69, "347": [69, 73], "6726": 69, "6786": 69, "348": [69, 73], "6722": 69, "349": [69, 73], "6717": [69, 79], "350": [69, 73], "6713": 69, "351": [69, 73], "6708": 69, "352": [69, 73], "6704": 69, "353": [69, 73], "6699": 69, "354": [69, 73], "6694": 69, "355": [69, 73], "6690": 69, "356": [69, 73], "6685": 69, "357": [69, 73], "6680": 69, "358": [69, 73], "6675": 69, "359": [69, 73], "6670": 69, "360": [69, 73], "6664": 69, "7143": 69, "361": [69, 73], "6659": 69, "362": [69, 73], "6654": 69, "7500": [69, 77, 81, 82], "363": [69, 73], "6648": 69, "364": [69, 73], "6642": 69, "365": [69, 73], "6637": 69, "366": [69, 73], "6631": 69, "367": [69, 73], "6625": 69, "8214": 69, "368": [69, 73], "6619": 69, "369": [69, 73], "6613": 69, "370": [69, 73], "6607": 69, "371": [69, 73], "6600": 69, "372": [69, 73], "6594": 69, "373": [69, 73], "6588": 69, "374": [69, 73], "6581": 69, "375": [69, 73], "6574": 69, "376": [69, 73], "6568": 69, "377": [69, 73], "6561": 69, "378": [69, 73], "6554": 69, "379": [69, 73], "6547": 69, "380": [69, 73], "6539": 69, "381": [69, 73], "6532": 69, "382": [69, 73], "6525": 69, "383": [69, 73], "6517": 69, "384": [69, 73], "6509": 69, "385": [69, 73], "6502": 69, "386": [69, 73], "6494": 69, "387": [69, 73], "6486": 69, "8571": 69, "388": [69, 73], "6478": 69, "389": [69, 73], "6469": 69, "390": [69, 73], "6461": 69, "391": [69, 73], "6453": 69, "392": [69, 73], "6444": 69, "393": [69, 73], "394": [69, 73], "6427": 69, "395": [69, 73, 74], "6418": 69, "396": [69, 73], "6409": 69, "397": [69, 73], "6399": 69, "398": [69, 73], "6390": 69, "6381": 69, "400": [69, 70, 73], "6371": 69, "8333": [69, 81], "401": [69, 73], "6361": [69, 82], "402": [69, 73], "6351": 69, "403": [69, 73], "6342": 69, "404": [69, 73], "6331": 69, "405": [69, 73], "6321": 69, "406": [69, 73], "6311": 69, "407": [69, 73], "6300": 69, "408": [69, 73, 82], "6290": [69, 77], "409": [69, 73], "6279": 69, "410": [69, 73], "6268": 69, "411": [69, 73], "6257": 69, "412": [69, 73], "6246": 69, "413": [69, 73], "6235": 69, "6223": 69, "415": [69, 73], "6212": 69, "416": [69, 73], "6200": 69, "417": [69, 73], "6188": 69, "418": [69, 73], "6176": 69, "419": [69, 73], "6164": 69, "420": [69, 73], "6152": 69, "421": [69, 73], "6139": 69, "422": [69, 73], "6127": 69, "423": [69, 73], "6114": 69, "424": [69, 73], "6101": 69, "425": [69, 73], "6088": 69, "426": [69, 73], "6075": 69, "427": [69, 73], "6061": 69, "428": [69, 73], "6048": 69, "429": [69, 73], "6034": 69, "430": [69, 73], "6020": 69, "431": [69, 73], "6006": 69, "432": [69, 73], "5992": 69, "433": [69, 73], "5978": 69, "434": [69, 73], "5963": 69, "435": [69, 73], "5949": 69, "436": [69, 73], "5934": 69, "8929": 69, "437": [69, 73], "5919": 69, "438": [69, 73], "5904": 69, "439": [69, 73], "5888": 69, "440": [69, 73], "5873": 69, "441": [69, 73], "5857": 69, "442": [69, 73], "5841": 69, "443": [69, 73, 74], "5825": 69, "9286": 69, "444": [69, 73], "5809": 69, "445": [69, 73], "5793": 69, "446": [69, 73], "5776": 69, "447": [69, 73], "5759": 69, "448": [69, 73], "5742": 69, "449": [69, 73], "5725": 69, "450": [69, 73], "5708": 69, "451": [69, 73], "452": [69, 73], "5673": 69, "453": [69, 73], "5655": 69, "454": [69, 73], "5637": 69, "455": [69, 73], "5619": 69, "456": [69, 73], "5600": 69, "457": [69, 73], "5582": 69, "458": [69, 73], "5563": 69, "459": [69, 73], "5544": [69, 77], "460": [69, 73], "5525": 69, "461": [69, 73], "5506": 69, "462": [69, 73], "5486": 69, "463": [69, 73], "5466": 69, "464": [69, 73], "5447": 69, "465": [69, 73], "5427": 69, "466": [69, 73], "5406": 69, "467": [69, 73], "5386": 69, "468": [69, 73], "5365": 69, "469": [69, 73], "5345": 69, "470": [69, 73], "5324": 69, "471": [69, 73], "5303": 69, "472": [69, 73], "473": [69, 73], "5260": 69, "474": [69, 73], "5239": 69, "475": [69, 73], "5217": 69, "476": [69, 73], "5195": 69, "477": [69, 73], "5173": 69, "478": [69, 73], "5151": 69, "479": [69, 73], "5129": 69, "480": [69, 73], "5107": 69, "481": [69, 73], "5084": 69, "482": [69, 73], "5062": 69, "483": [69, 73], "5039": 69, "484": [69, 73], "5016": [69, 81], "485": [69, 73], "4993": 69, "486": [69, 73], "4970": 69, "487": [69, 73], "4947": 69, "488": [69, 73], "4924": 69, "489": [69, 73], "4901": 69, "490": [69, 73], "491": [69, 73], "4855": 69, "492": [69, 73], "493": [69, 73], "4808": 69, "494": [69, 73], "4784": 69, "495": [69, 73], "4761": 69, "496": [69, 73], "497": [69, 73], "4714": 69, "498": [69, 73], "4690": 69, "499": [69, 73], "4667": 69, "4643": 69, "501": 69, "4619": 69, "502": 69, "4596": 69, "503": 69, "4572": 69, "504": 69, "4549": 69, "505": 69, "4525": 69, "506": 69, "4501": 69, "507": 69, "4478": 69, "508": 69, "4454": 69, "509": 69, "4431": 69, "510": 69, "4407": 69, "511": 69, "4384": 69, "512": 69, "4360": 69, "513": 69, "4337": 69, "514": [69, 84], "4313": 69, "515": 69, "4290": 69, "516": 69, "4267": 69, "517": 69, "4243": 69, "518": 69, "4220": 69, "519": 69, "4197": 69, "520": 69, "4174": 69, "521": 69, "4151": 69, "522": 69, "4128": 69, "523": 69, "4105": 69, "524": 69, "4082": 69, "525": 69, "4059": 69, "526": 69, "4036": 69, "527": 69, "4014": 69, "528": 69, "3991": 69, "3969": 69, "530": 69, "3946": 69, "531": 69, "3924": 69, "532": 69, "3902": 69, "533": 69, "3880": 69, "534": 69, "3858": 69, "535": 69, "3836": 69, "536": [69, 82, 85], "537": 69, "3792": 69, "538": 69, "3770": 69, "539": 69, "3749": 69, "540": 69, "3727": 69, "541": 69, "3706": 69, "542": 69, "3685": 69, "543": 69, "3664": 69, "544": 69, "3643": 69, "545": 69, "3622": 69, "546": [69, 83], "3601": 69, "547": 69, "3580": 69, "548": 69, "3560": 69, "549": 69, "3539": 69, "550": 69, "3519": 69, "551": 69, "3499": 69, "552": 69, "3479": 69, "553": 69, "3459": 69, "554": 69, "3439": 69, "555": 69, "3419": 69, "556": 69, "3400": 69, "557": 69, "3380": 69, "558": 69, "3361": 69, "559": 69, "3342": 69, "560": 69, "3323": 69, "561": 69, "3304": 69, "562": 69, "3285": 69, "563": 69, "3266": 69, "564": 69, "3248": 69, "565": 69, "3229": 69, "9643": 69, "566": 69, "3211": 69, "567": 69, "3193": 69, "568": 69, "3175": 69, "569": 69, "3157": 69, "570": 69, "3139": 69, "571": 69, "3121": 69, "572": 69, "3103": 69, "573": 69, "3086": 69, "574": 69, "3069": 69, "575": 69, "3052": [69, 70], "576": 69, "3034": 69, "577": 69, "3018": [69, 77], "578": 69, "3001": 69, "579": 69, "2984": 69, "580": 69, "2967": 69, "581": 69, "2951": 69, "582": 69, "2935": 69, "583": 69, "2919": 69, "584": 69, "2902": 69, "585": [69, 73], "2887": 69, "586": 69, "2871": 69, "587": 69, "2855": 69, "588": 69, "2839": 69, "589": 69, "2824": 69, "590": 69, "2809": 69, "591": 69, "2794": 69, "592": 69, "2778": 69, "593": 69, "2763": 69, "594": 69, "2749": 69, "595": 69, "2734": 69, "596": 69, "2719": 69, "597": 69, "2705": 69, "598": 69, "2690": 69, "599": 69, "2676": 69, "600": 69, "2662": 69, "601": 69, "2648": 69, "602": 69, "2634": 69, "603": 69, "2620": 69, "604": 69, "2607": 69, "605": 69, "2593": 69, "606": 69, "2579": 69, "607": 69, "2566": 69, "608": 69, "2553": 69, "609": 69, "2540": 69, "610": 69, "2527": 69, "611": 69, "2514": 69, "612": 69, "2501": 69, "613": [69, 86], "2488": 69, "614": 69, "615": 69, "2463": 69, "616": 69, "2451": 69, "617": 69, "2438": 69, "618": 69, "2426": [69, 70], "619": 69, "2414": 69, "620": 69, "2402": 69, "621": 69, "2390": 69, "622": 69, "2378": 69, "623": 69, "2367": 69, "624": 69, "2355": 69, "625": [69, 73], "2344": 69, "626": 69, "2332": 69, "627": 69, "2321": 69, "628": 69, "2310": 69, "629": 69, "2299": 69, "630": 69, "2287": 69, "631": 69, "2277": 69, "632": 69, "2266": 69, "633": 69, "2255": 69, "634": 69, "635": [69, 82], "2234": 69, "636": 69, "2223": 69, "637": 69, "2213": 69, "638": 69, "2202": 69, "639": 69, "2192": 69, "640": 69, "2182": 69, "641": 69, "2172": 69, "642": 69, "2162": 69, "643": 69, "2152": 69, "644": 69, "2142": 69, "645": 69, "2132": 69, "646": 69, "2123": 69, "647": 69, "2113": 69, "648": 69, "2104": 69, "649": 69, "2094": 69, "650": 69, "2085": 69, "651": 69, "2076": 69, "652": 69, "2066": 69, "653": 69, "2057": 69, "654": 69, "2048": 69, "655": 69, "2039": 69, "656": 69, "2030": 69, "657": 69, "658": 69, "2013": 69, "659": 69, "2004": 69, "660": 69, "1995": 69, "661": 69, "1987": 69, "662": 69, "1978": 69, "663": 69, "1970": 69, "664": 69, "1962": [69, 77], "665": 69, "1953": 69, "666": 69, "1945": 69, "667": 69, "1937": 69, "668": 69, "1929": 69, "669": 69, "1921": 69, "670": 69, "1913": [69, 81], "671": 69, "1905": 69, "672": 69, "1897": 69, "673": 69, "1889": 69, "674": 69, "1882": 69, "675": 69, "1874": 69, "676": 69, "1866": 69, "677": 69, "1859": 69, "678": 69, "1851": [69, 77], "679": 69, "1844": 69, "680": 69, "1837": 69, "681": 69, "1829": 69, "682": 69, "1822": 69, "683": 69, "1815": 69, "684": 69, "1808": 69, "685": 69, "1801": 69, "686": 69, "1794": 69, "687": 69, "1787": 69, "688": 69, "1780": [69, 77], "689": 69, "690": 69, "1766": 69, "691": 69, "1759": 69, "692": 69, "1753": 69, "693": 69, "1746": 69, "694": 69, "1740": 69, "695": 69, "1733": 69, "696": 69, "1726": 69, "697": 69, "1720": 69, "698": 69, "1714": 69, "699": 69, "1707": 69, "700": 69, "1701": 69, "701": 69, "1695": 69, "702": 69, "1688": 69, "703": 69, "1682": 69, "704": 69, "1676": 69, "705": 69, "1670": 69, "706": 69, "1664": 69, "707": 69, "1658": 69, "708": 69, "1652": 69, "709": 69, "1646": 69, "710": 69, "1640": 69, "711": 69, "1635": 69, "712": 69, "1629": 69, "713": 69, "1623": 69, "714": 69, "1617": 69, "715": 69, "1612": 69, "716": 69, "1606": 69, "717": 69, "1601": 69, "718": 69, "1595": 69, "719": 69, "1590": 69, "720": 69, "1584": 69, "721": 69, "1579": 69, "722": 69, "1573": 69, "723": 69, "1568": 69, "724": 69, "1563": 69, "725": 69, "1557": 69, "726": 69, "1552": 69, "727": 69, "1547": 69, "728": 69, "1542": 69, "729": 69, "1537": 69, "730": 69, "1532": 69, "731": 69, "1527": 69, "732": 69, "1522": 69, "733": 69, "1517": 69, "734": 69, "1512": 69, "735": 69, "1507": 69, "736": 69, "1502": 69, "737": 69, "1497": 69, "738": 69, "1492": 69, "739": 69, "1488": 69, "740": 69, "1483": 69, "741": 69, "1478": 69, "742": [69, 73], "1473": [69, 77], "743": 69, "1469": 69, "744": 69, "1464": 69, "745": 69, "1460": 69, "746": 69, "1455": 69, "747": 69, "1451": 69, "748": 69, "1446": 69, "749": 69, "1442": 69, "1437": 69, "751": 69, "752": 69, "1428": 69, "753": 69, "1424": 69, "754": 69, "1420": 69, "755": 69, "1415": 69, "756": 69, "1411": 69, "757": 69, "1407": 69, "758": 69, "1403": 69, "759": 69, "1399": 69, "760": 69, "1394": 69, "761": 69, "1390": 69, "762": 69, "1386": 69, "763": 69, "1382": 69, "764": 69, "1378": 69, "765": 69, "1374": 69, "766": 69, "1370": 69, "767": 69, "1366": 69, "768": 69, "1362": 69, "769": 69, "1358": 69, "770": 69, "1354": 69, "771": 69, "1351": 69, "772": 69, "1347": 69, "773": 69, "1343": [69, 77], "774": 69, "1339": 69, "775": 69, "1335": 69, "776": 69, "1332": [69, 77], "777": 69, "1328": 69, "778": 69, "1324": 69, "779": 69, "1320": 69, "780": 69, "1317": 69, "781": 69, "1313": 69, "782": 69, "1310": 69, "783": 69, "1306": 69, "784": 69, "1302": 69, "785": 69, "1299": 69, "786": 69, "1295": 69, "787": 69, "1292": 69, "788": 69, "1288": [69, 77], "789": 69, "1285": 69, "790": 69, "1281": 69, "791": 69, "1278": 69, "792": 69, "1275": 69, "793": 69, "1271": 69, "794": 69, "1268": 69, "795": 69, "1265": 69, "796": 69, "1261": 69, "797": 69, "1258": 69, "798": 69, "1255": [69, 77], "799": 69, "1251": 69, "800": 69, "1248": 69, "801": 69, "1245": 69, "802": 69, "1242": 69, "803": 69, "1239": 69, "804": 69, "1235": 69, "805": 69, "806": 69, "1229": 69, "807": 69, "1226": 69, "808": 69, "1223": 69, "809": 69, "1220": 69, "810": 69, "1217": 69, "811": 69, "1214": 69, "812": 69, "1211": 69, "813": 69, "1208": 69, "814": 69, "1205": 69, "815": 69, "1202": 69, "816": 69, "1199": 69, "817": 69, "1196": 69, "818": 69, "1193": 69, "819": 69, "1190": 69, "820": 69, "1187": 69, "821": 69, "1184": 69, "822": 69, "1181": 69, "823": 69, "1178": 69, "824": 69, "1176": 69, "825": 69, "1173": 69, "826": 69, "1170": 69, "827": 69, "1167": 69, "828": 69, "1164": 69, "829": 69, "1162": 69, "830": 69, "1159": 69, "831": 69, "1156": 69, "832": 69, "1153": 69, "833": 69, "1151": 69, "834": 69, "1148": 69, "835": 69, "1145": 69, "836": 69, "1143": 69, "837": 69, "1140": 69, "838": 69, "1137": 69, "839": 69, "1135": 69, "840": 69, "1132": 69, "841": 69, "1130": 69, "842": 69, "1127": 69, "843": 69, "1125": 69, "844": 69, "1122": 69, "845": 69, "1119": 69, "846": 69, "1117": 69, "847": 69, "1114": 69, "848": 69, "1112": 69, "849": 69, "1109": 69, "850": 69, "1107": [69, 70], "851": 69, "1105": 69, "852": 69, "1102": 69, "853": 69, "1100": 69, "854": 69, "1097": 69, "855": 69, "1095": 69, "856": 69, "1092": 69, "857": 69, "1090": 69, "858": 69, "1088": 69, "859": 69, "1085": [69, 70], "860": 69, "1083": 69, "861": 69, "1081": 69, "862": 69, "1078": 69, "863": 69, "1076": 69, "864": 69, "1074": 69, "865": 69, "866": 69, "1069": 69, "867": 69, "1067": 69, "868": 69, "1065": 69, "869": 69, "1062": 69, "870": 69, "1060": [69, 82], "871": 69, "1058": 69, "872": 69, "1056": 69, "873": 69, "1054": 69, "874": 69, "1051": 69, "875": 69, "1049": 69, "876": 69, "1047": 69, "877": 69, "1045": 69, "878": 69, "1043": 69, "879": 69, "1041": 69, "880": 69, "1038": 69, "881": 69, "1036": 69, "882": 69, "1034": 69, "883": 69, "1032": 69, "884": 69, "1030": 69, "885": 69, "1028": 69, "886": 69, "1026": 69, "887": 69, "1024": 69, "888": 69, "1022": 69, "889": 69, "1020": 69, "890": 69, "1018": 69, "891": 69, "892": 69, "1014": 69, "893": 69, "1012": [69, 77], "894": 69, "1010": 69, "895": 69, "1008": 69, "896": 69, "1006": [69, 77], "897": 69, "1004": 69, "898": 69, "1002": 69, "899": 69, "900": 69, "0998": [69, 77], "901": 69, "0996": [69, 70, 77], "902": [69, 73], "0994": 69, "903": 69, "0992": [69, 77], "904": 69, "0990": 69, "905": 69, "0988": 69, "906": 69, "0986": 69, "907": 69, "0985": 69, "908": 69, "0983": 69, "909": 69, "0981": 69, "910": 69, "0979": 69, "911": 69, "0977": 69, "912": 69, "0975": 69, "913": 69, "0973": 69, "914": 69, "0972": 69, "915": 69, "0970": 69, "916": 69, "0968": [69, 77], "917": 69, "0966": 69, "918": 69, "0964": 69, "919": 69, "0963": 69, "920": 69, "0961": 69, "921": 69, "0959": [69, 77], "922": 69, "0957": 69, "923": 69, "0956": 69, "924": 69, "0954": [69, 77], "925": 69, "0952": 69, "926": 69, "0950": 69, "927": 69, "0949": 69, "928": 69, "0947": 69, "929": 69, "0945": 69, "930": 69, "0944": 69, "931": 69, "0942": 69, "932": 69, "0940": 69, "933": 69, "0939": 69, "934": 69, "0937": [69, 77], "935": 69, "0935": 69, "936": 69, "0934": 69, "937": 69, "0932": 69, "938": 69, "0930": [69, 77], "939": 69, "0929": 69, "940": 69, "0927": [69, 77], "941": 69, "0925": [69, 70], "942": 69, "0924": 69, "943": 69, "0922": 69, "944": 69, "0920": [69, 77], "945": 69, "0919": 69, "946": 69, "0917": 69, "947": 69, "0916": 69, "948": 69, "0914": 69, "949": 69, "0913": 69, "950": 69, "0911": 69, "951": 69, "0909": 69, "952": 69, "0908": 69, "953": 69, "0906": 69, "954": 69, "0905": 69, "955": 69, "0903": 69, "956": 69, "0902": 69, "957": 69, "0900": 69, "958": 69, "0899": 69, "959": 69, "0897": [69, 77], "960": 69, "0896": 69, "961": 69, "0894": 69, "962": 69, "0893": 69, "963": 69, "0891": [69, 77], "964": 69, "0890": 69, "965": 69, "0888": 69, "966": 69, "0887": 69, "967": 69, "0885": 69, "968": 69, "0884": 69, "969": 69, "0882": 69, "970": 69, "0881": 69, "971": 69, "0880": 69, "972": 69, "0878": 69, "973": 69, "0877": 69, "974": 69, "0875": 69, "975": 69, "0874": 69, "976": 69, "0872": [69, 84], "977": 69, "0871": [69, 84], "978": 69, "0870": 69, "979": 69, "0868": 69, "980": 69, "0867": 69, "981": 69, "0865": 69, "982": 69, "0864": [69, 77], "983": 69, "0863": 69, "984": 69, "0861": 69, "985": 69, "0860": 69, "986": 69, "0859": 69, "987": 69, "0857": 69, "988": 69, "0856": 69, "989": 69, "0855": 69, "990": 69, "0853": 69, "991": 69, "0852": 69, "992": 69, "0851": [69, 81], "993": 69, "0849": 69, "994": 69, "0848": 69, "995": 69, "0847": 69, "996": 69, "0845": 69, "997": 69, "0844": 69, "998": 69, "0843": 69, "999": 69, "0841": 69, "0840": 69, "1001": 69, "0839": 69, "0838": 69, "1003": 69, "0836": 69, "0835": 69, "1005": 69, "0834": 69, "0832": 69, "1007": 69, "0831": 69, "0830": 69, "1009": [69, 77], "0829": 69, "0827": 69, "1011": 69, "0826": 69, "0825": [69, 70], "1013": 69, "0824": 69, "0823": 69, "1015": 69, "0821": 69, "0820": 69, "1017": 69, "0819": [69, 77], "0818": 69, "1019": [69, 77], "0816": 69, "0815": 69, "1021": 69, "0814": 69, "0813": 69, "1023": [69, 77], "0812": 69, "0810": 69, "1025": 69, "0809": 69, "0808": 69, "1027": 69, "0807": 69, "0806": 69, "1029": 69, "0805": 69, "0803": [69, 77], "1031": 69, "0802": 69, "0801": 69, "1033": 69, "0800": 69, "0799": 69, "1035": 69, "0798": 69, "0797": [69, 70], "1037": 69, "0795": 69, "0794": 69, "1039": 69, "0793": 69, "1040": 69, "0792": 69, "0791": 69, "1042": 69, "0790": 69, "0789": [69, 77], "1044": 69, "0788": 69, "0787": 69, "1046": [69, 77], "0785": 69, "0784": 69, "1048": 69, "0783": 69, "0782": 69, "1050": 69, "0781": 69, "0780": 69, "1052": 69, "0779": 69, "1053": 69, "0778": 69, "0777": 69, "1055": 69, "0776": 69, "0775": 69, "1057": 69, "0774": 69, "0772": 69, "1059": 69, "0771": 69, "0770": 69, "1061": 69, "0769": 69, "0768": 69, "1063": 69, "0767": 69, "1064": 69, "0766": 69, "0765": 69, "1066": 69, "0764": 69, "0763": 69, "1068": 69, "0762": [69, 77], "0761": 69, "1070": 69, "0760": 69, "0759": [69, 77], "1072": 69, "0758": 69, "1073": 69, "0757": 69, "0756": 69, "1075": 69, "0755": 69, "0754": [69, 77], "1077": 69, "0753": 69, "0752": 69, "0751": 69, "1080": [69, 77], "0750": 69, "0749": [69, 77], "1082": 69, "0748": 69, "0747": 69, "1084": 69, "0746": [69, 77], "0745": 69, "1086": 69, "0744": 69, "1087": 69, "0743": 69, "0742": 69, "1089": 69, "0741": 69, "0740": 69, "1091": 69, "0739": 69, "0738": 69, "1093": 69, "0737": 69, "1094": [69, 85], "0736": 69, "1096": 69, "0735": 69, "0734": 69, "1098": 69, "0733": 69, "1099": 69, "0732": 69, "0731": [69, 77], "1101": [69, 81], "0730": 69, "0729": 69, "1103": 69, "0728": 69, "1104": 69, "0727": 69, "0726": 69, "1106": [69, 77], "0725": [69, 77], "0724": [69, 77], "1108": 69, "0723": 69, "1110": 69, "0722": 69, "1111": 69, "0721": [69, 77], "0720": [69, 77], "1113": 69, "0719": 69, "0718": 69, "1115": 69, "0717": 69, "1116": 69, "1118": 69, "0715": 69, "1120": 69, "0713": 69, "1121": 69, "0712": 69, "0711": 69, "1123": 69, "0710": 69, "1124": 69, "0709": 69, "1126": 69, "0708": 69, "0707": 69, "1128": 69, "0706": 69, "1129": 69, "0705": 69, "0704": 69, "1131": 69, "0703": 69, "1133": 69, "0702": 69, "1134": 69, "0701": [69, 77], "0700": 69, "1136": [69, 81], "0699": [69, 77], "0698": 69, "1138": 69, "1139": 69, "0697": 69, "0696": 69, "1141": 69, "0695": 69, "1142": 69, "0694": 69, "0693": [69, 77], "1144": 69, "0692": 69, "1146": 69, "0691": 69, "1147": 69, "0690": 69, "0689": 69, "1149": 69, "1150": 69, "0688": 69, "0687": [69, 77], "1152": 69, "0685": [69, 77], "1154": 69, "1155": 69, "0684": 69, "0683": 69, "1157": 69, "0682": 69, "1158": 69, "1160": 69, "0680": 69, "1161": 69, "0679": 69, "0678": 69, "1163": 69, "0677": 69, "1165": 69, "0676": 69, "1166": 69, "0675": 69, "0674": [69, 81], "1168": 69, "1169": 69, "0673": 69, "0672": 69, "1171": 69, "0671": 69, "1172": 69, "0670": 69, "1174": 69, "0669": 69, "1175": 69, "0668": 69, "1177": 69, "0666": 69, "1179": [69, 70, 77], "0665": 69, "1180": 69, "0664": 69, "1182": 69, "0663": [69, 77], "1183": 69, "0662": 69, "1185": 69, "0661": 69, "1186": 69, "0660": 69, "0659": 69, "1188": 69, "1189": 69, "0658": 69, "0657": 69, "1191": 69, "0656": 69, "1192": 69, "0655": 69, "1194": [69, 70], "0654": 69, "1195": 69, "0653": 69, "1197": 69, "0652": 69, "1198": 69, "0651": 69, "1200": [69, 86], "0650": [69, 77], "1201": 69, "0649": 69, "1203": 69, "0648": 69, "1204": 69, "0647": 69, "0646": 69, "1206": 69, "1207": 69, "0645": 69, "0644": 69, "1209": 69, "1210": 69, "0643": 69, "0642": 69, "1212": 69, "1213": 69, "0641": 69, "0640": 69, "1215": 69, "1216": 69, "0639": 69, "0638": 69, "1218": 69, "1219": 69, "0637": 69, "0636": [69, 70], "1221": 69, "0635": 69, "1222": 69, "0634": 69, "1224": 69, "0633": 69, "1225": 69, "0632": 69, "1227": 69, "0631": 69, "1228": 69, "0630": 69, "1230": 69, "0629": 69, "1231": 69, "0628": 69, "1233": 69, "1234": 69, "0627": 69, "1236": 69, "1237": 69, "0625": 69, "1238": 69, "0624": 69, "1240": 69, "0623": [69, 70], "1241": 69, "0622": 69, "1243": 69, "0621": 69, "1244": 69, "0620": 69, "1246": 69, "1247": 69, "0618": 69, "1249": 69, "0617": 69, "0616": 69, "0615": 69, "1253": 69, "1254": 69, "0614": 69, "0613": 69, "1256": 69, "1257": [69, 81], "0612": 69, "1259": 69, "0611": 69, "1260": 69, "0610": 69, "1262": 69, "0609": 69, "1263": 69, "1264": 69, "0608": 69, "0607": [69, 70, 77], "1266": 69, "1267": 69, "0606": 69, "1269": 69, "0605": 69, "1270": 69, "0604": [69, 70], "1272": 69, "0603": [69, 70], "1273": 69, "1274": 69, "0602": 69, "0601": 69, "1276": 69, "1277": 69, "0600": 69, "1279": 69, "0599": 69, "1280": 69, "0598": 69, "1282": 69, "0597": [69, 70], "1283": [69, 81], "1284": 69, "0596": 69, "0595": 69, "1286": 69, "1287": 69, "0594": 69, "1289": 69, "0593": 69, "1290": 69, "1291": 69, "0592": 69, "0591": 69, "1293": [69, 77], "1294": [69, 77], "0590": 69, "1296": 69, "0589": 69, "1297": 69, "1298": 69, "0588": 69, "0587": 69, "1300": 69, "1301": 69, "0586": 69, "1303": 69, "0585": [69, 70], "1304": 69, "1305": 69, "0584": [69, 70], "0583": 69, "1307": 69, "1308": 69, "0582": [69, 70], "1309": 69, "0581": 69, "1311": 69, "1312": 69, "0580": [69, 77], "1314": 69, "0579": 69, "1315": 69, "0578": 69, "1316": [69, 77], "0577": 69, "1318": 69, "1319": 69, "0576": [69, 77], "1321": 69, "0575": 69, "1322": 69, "1323": 69, "0574": 69, "1325": 69, "0573": 69, "1326": 69, "1327": 69, "0572": 69, "0571": 69, "1329": 69, "1330": 69, "0570": 69, "1331": 69, "0569": 69, "1333": 69, "1334": 69, "0568": 69, "1336": 69, "1337": 69, "1338": 69, "0566": 69, "1340": 69, "0565": [69, 70], "1341": 69, "1342": 69, "0564": 69, "1344": 69, "0563": 69, "1345": 69, "1346": 69, "0562": [69, 81], "1348": [69, 81], "0561": 69, "1349": 69, "0560": 69, "1350": 69, "0559": 69, "1352": 69, "1353": 69, "0558": 69, "1355": 69, "0557": 69, "1356": [69, 77], "1357": 69, "0556": 69, "1359": 69, "0555": 69, "1360": 69, "1361": 69, "0554": [69, 70], "1363": 69, "0553": [69, 77], "1364": 69, "1365": 69, "0552": 69, "1367": 69, "0551": 69, "1368": 69, "1369": 69, "0550": 69, "1371": 69, "0549": 69, "1372": 69, "1373": 69, "0548": 69, "1375": 69, "1376": 69, "0547": 69, "1377": 69, "0546": [69, 77], "1379": 69, "1380": [69, 77], "0545": 69, "1381": 69, "0544": 69, "1383": 69, "1384": 69, "0543": [69, 70], "1385": 69, "0542": 69, "1387": 69, "1388": 69, "0541": 69, "1389": 69, "0540": 69, "1391": 69, "1392": 69, "0539": 69, "1393": 69, "0538": 69, "1395": 69, "1396": 69, "0537": [69, 77], "1397": 69, "1398": 69, "0536": [69, 70], "1400": 69, "1401": 69, "0535": 69, "1402": 69, "0534": 69, "1404": 69, "1405": 69, "0533": 69, "1406": 69, "0532": 69, "1408": 69, "1409": 69, "0531": 69, "1410": 69, "0530": 69, "1412": 69, "1413": 69, "1414": 69, "0529": [69, 81], "1416": 69, "0528": 69, "1417": 69, "1418": 69, "0527": 69, "1419": 69, "0526": 69, "1421": 69, "1422": 69, "1423": 69, "0525": 69, "1425": 69, "0524": 69, "1426": [69, 77], "1427": 69, "0523": 69, "0522": 69, "1430": 69, "1431": 69, "1432": 69, "0521": 69, "1434": 69, "0520": 69, "1435": 69, "0519": 69, "1438": 69, "0518": 69, "1439": 69, "1440": 69, "1441": 69, "0517": 69, "1443": 69, "0516": 69, "1444": 69, "1445": 69, "0515": 69, "1447": 69, "1448": 69, "0514": 69, "1449": 69, "1450": 69, "0513": 69, "1452": 69, "0512": [69, 81], "1453": 69, "1454": 69, "0511": [69, 70], "1456": 69, "1457": [69, 77], "0510": 69, "1458": 69, "1459": 69, "0509": 69, "1461": 69, "1462": 69, "0508": 69, "1463": 69, "0507": 69, "1465": 69, "1466": 69, "0506": 69, "1467": 69, "1468": 69, "1470": 69, "1471": 69, "0504": 69, "1472": 69, "1474": [69, 77], "0503": 69, "1475": [69, 77], "1476": 69, "0502": 69, "1477": 69, "0501": 69, "1479": 69, "1480": 69, "1481": 69, "0500": 69, "1482": 69, "0499": 69, "1484": [69, 77], "1485": 69, "1486": 69, "0498": 69, "1487": [69, 77], "0497": 69, "1489": 69, "1490": 69, "1491": 69, "0496": 69, "1493": 69, "0495": 69, "1494": 69, "1495": [69, 77], "1496": 69, "0494": 69, "1498": [69, 70], "0493": 69, "1499": 69, "1500": 69, "1501": 69, "0492": 69, "1503": 69, "0491": 69, "1504": 69, "1505": 69, "1506": 69, "0490": 69, "1508": 69, "0489": 69, "1509": 69, "1510": 69, "1511": [69, 77], "0488": 69, "1513": 69, "1514": 69, "0487": 69, "1515": 69, "1516": [69, 81], "0486": 69, "1518": [69, 77], "1519": [69, 77], "0485": 69, "1520": 69, "1521": 69, "0484": 69, "1523": 69, "1524": 69, "0483": 69, "1525": 69, "1526": 69, "0482": [69, 70], "1528": 69, "1529": 69, "0481": 69, "1530": 69, "1531": 69, "0480": 69, "1533": 69, "1534": 69, "0479": 69, "1535": 69, "1536": 69, "0478": [69, 70], "1538": 69, "1539": 69, "1540": 69, "0477": 69, "1541": 69, "0476": 69, "1543": 69, "1544": 69, "1545": [69, 77], "0475": 69, "1546": 69, "1548": 69, "0474": 69, "1549": 69, "1550": 69, "0473": 69, "1551": 69, "1553": 69, "0472": 69, "1554": 69, "1555": 69, "1556": 69, "0471": 69, "1558": 69, "1559": 69, "0470": 69, "1560": [69, 77], "1561": 69, "0469": [69, 81], "1562": 69, "1564": 69, "0468": 69, "1566": 69, "1567": 69, "0467": 69, "1569": 69, "1570": 69, "0466": 69, "1571": 69, "1572": 69, "0465": 69, "1574": 69, "1575": [69, 77], "0464": 69, "1576": 69, "1577": 69, "1578": 69, "0463": 69, "1580": 69, "1581": 69, "0462": 69, "1582": 69, "1583": 69, "0461": 69, "1585": 69, "1586": 69, "1587": 69, "0460": 69, "1588": 69, "1589": 69, "0459": 69, "1591": 69, "1592": 69, "0458": 69, "1593": 69, "1594": 69, "0457": 69, "1596": 69, "1597": 69, "1598": 69, "0456": 69, "1599": 69, "1600": 69, "0455": 69, "1602": 69, "1603": 69, "1604": 69, "0454": 69, "1605": 69, "1607": 69, "1608": 69, "1609": 69, "1610": 69, "0452": 69, "1611": 69, "1613": 69, "0451": 69, "1614": 69, "1615": 69, "1616": 69, "0450": 69, "1618": [69, 70], "1619": 69, "0449": 69, "1620": 69, "1621": 69, "0448": 69, "1622": 69, "1624": 69, "0447": 69, "1625": 69, "1626": 69, "1627": 69, "0446": 69, "1628": [69, 77], "1630": 69, "1631": 69, "0445": 69, "1632": 69, "1633": 69, "1634": 69, "0444": 69, "1636": 69, "0443": 69, "1637": 69, "1638": 69, "1639": 69, "0442": [69, 70], "1641": 69, "1642": 69, "1643": 69, "0441": 69, "1644": 69, "1645": 69, "0440": 69, "1647": 69, "1648": 69, "1649": 69, "0439": 69, "1650": 69, "1651": 69, "0438": 69, "1653": 69, "1654": [69, 77], "1655": 69, "0437": 69, "1656": 69, "1657": 69, "0436": 69, "1659": 69, "1660": 69, "1661": 69, "0435": 69, "1662": 69, "1663": 69, "0434": 69, "1665": 69, "1666": 69, "1667": 69, "0433": 69, "1668": 69, "1669": 69, "1671": 69, "0432": 69, "1672": 69, "1673": 69, "1674": 69, "0431": 69, "1675": 69, "1677": 69, "0430": 69, "1678": 69, "1679": 69, "1680": 69, "0429": 69, "1681": 69, "1683": 69, "0428": 69, "1684": 69, "1685": 69, "1686": 69, "0427": 69, "1687": 69, "1689": 69, "1690": 69, "0426": 69, "1691": 69, "1692": 69, "1693": 69, "0425": 69, "1694": 69, "1696": 69, "0424": 69, "1697": 69, "1698": 69, "1699": 69, "0423": 69, "1700": 69, "1702": 69, "1703": 69, "0422": 69, "1704": 69, "1705": 69, "1706": 69, "0421": 69, "1708": 69, "1709": 69, "0420": 69, "1711": 69, "1712": 69, "1713": 69, "0419": 69, "1715": 69, "1716": 69, "0418": [69, 70], "1717": 69, "1718": 69, "1719": 69, "0417": 69, "1721": 69, "1722": 69, "0416": 69, "1723": 69, "1724": [69, 77], "1725": [69, 77], "0415": 69, "1727": 69, "1728": 69, "1729": 69, "0414": 69, "1730": [69, 77], "1731": 69, "1732": 69, "0413": 69, "1734": [69, 77], "1735": 69, "1736": 69, "0412": 69, "1737": 69, "1738": [69, 77], "1739": [69, 70], "0411": [69, 70], "1741": 69, "1742": 69, "1743": 69, "0410": 69, "1745": 69, "0409": 69, "1747": 69, "1748": 69, "1749": 69, "1750": 69, "0408": 69, "1751": 69, "1752": 69, "0407": 69, "1754": 69, "1755": 69, "1756": 69, "1757": 69, "0406": [69, 70], "1758": 69, "1760": 69, "0405": 69, "1761": 69, "1762": 69, "1763": [69, 77], "0404": 69, "1764": 69, "1767": 69, "0403": 69, "1768": 69, "1769": 69, "1770": 69, "0402": 69, "1771": 69, "1772": 69, "1774": 69, "1775": 69, "1776": 69, "1777": 69, "1778": 69, "0400": [69, 70], "1779": 69, "1781": 69, "0399": 69, "1782": 69, "1783": 69, "1784": 69, "1785": 69, "0398": 69, "1786": 69, "1788": 69, "0397": 69, "1789": 69, "1790": 69, "1791": 69, "1792": 69, "0396": 69, "1793": 69, "1795": 69, "0395": 69, "1796": 69, "1797": 69, "1798": 69, "1799": 69, "0394": 69, "1800": 69, "1802": 69, "1803": 69, "0393": 69, "1804": [69, 77], "1805": 69, "1806": 69, "0392": 69, "1807": 69, "1809": 69, "1810": 69, "0391": [69, 84], "1811": 69, "1812": [69, 77], "1813": 69, "1814": 69, "0390": 69, "1816": 69, "1817": 69, "1818": 69, "1819": 69, "1820": 69, "1821": [69, 77], "0388": 69, "1823": 69, "1824": 69, "1825": 69, "0387": 69, "1826": 69, "1827": 69, "1828": 69, "0386": 69, "1830": 69, "1831": [69, 77], "1832": 69, "0385": 69, "1833": 69, "1834": 69, "1835": 69, "1836": [69, 77], "0384": 69, "1838": 69, "1839": 69, "1840": 69, "0383": 69, "1841": 69, "1842": 69, "1843": 69, "0382": 69, "1845": 69, "1846": 69, "1847": 69, "0381": 69, "1848": 69, "1849": 69, "1850": 69, "0380": 69, "1852": 69, "1853": 69, "1854": 69, "1855": 69, "0379": [69, 70], "1856": 69, "1857": 69, "1858": 69, "0378": 69, "1860": 69, "1861": 69, "1862": 69, "0377": 69, "1864": 69, "1865": 69, "0376": 69, "1867": 69, "1868": 69, "1869": 69, "1870": 69, "0375": [69, 70], "1871": 69, "1872": 69, "1873": 69, "0374": 69, "1875": 69, "1876": 69, "1877": 69, "1878": 69, "0373": 69, "1879": 69, "1880": 69, "1881": 69, "0372": 69, "1883": 69, "1884": 69, "1885": 69, "1886": 69, "0371": 69, "1887": 69, "1888": 69, "0370": 69, "1890": 69, "1891": 69, "1892": 69, "1893": 69, "1894": 69, "1896": 69, "0368": 69, "1898": 69, "1899": 69, "1900": 69, "1901": [69, 70], "0367": 69, "1902": 69, "1903": 69, "1904": 69, "0366": 69, "1906": 69, "1907": 69, "1908": 69, "1909": 69, "0365": 69, "1910": 69, "1911": 69, "1912": 69, "0364": 69, "1914": 69, "1915": 69, "1916": 69, "1917": 69, "0363": [69, 81], "1918": 69, "1919": 69, "1920": 69, "0362": 69, "1922": 69, "1923": 69, "1924": 69, "1925": 69, "0361": 69, "1926": [69, 85], "1927": 69, "1928": 69, "0360": 69, "1930": 69, "1931": 69, "1932": 69, "1933": 69, "0359": 69, "1934": 69, "1935": 69, "1936": 69, "1938": 69, "0358": 69, "1939": 69, "1940": 69, "1941": 69, "1942": 69, "0357": 69, "1943": 69, "1944": 69, "1946": 69, "0356": 69, "1947": 69, "1948": 69, "1949": 69, "1950": 69, "0355": 69, "1951": 69, "1952": 69, "1954": 69, "0354": 69, "1955": 69, "1956": 69, "1957": 69, "1958": 69, "0353": 69, "1959": 69, "1960": 69, "1961": 69, "0352": 69, "1963": 69, "1964": 69, "1965": 69, "1966": 69, "0351": 69, "1967": 69, "1968": 69, "1969": 69, "1971": 69, "0350": 69, "1972": 69, "1973": 69, "1974": 69, "1975": 69, "0349": 69, "1977": 69, "1979": 69, "0348": 69, "1980": 69, "1981": 69, "1982": 69, "1983": 69, "0347": 69, "1984": 69, "1985": 69, "1986": 69, "1988": 69, "0346": 69, "1989": 69, "1990": 69, "1991": 69, "1992": 69, "0345": 69, "1993": 69, "1994": 69, "1996": 69, "0344": 69, "1997": 69, "1998": 69, "1999": 69, "plot": [69, 86], "dpi": 69, "test_epoch": 69, "isfinit": 69, "linestyl": 69, "marker": 69, "legend": [69, 86], "xlabel": 69, "cites": 70, "3703": 70, "manifold": 70, "tsne": 70, "randomnodesplit": 70, "wget": [70, 74], "twistedcub": 70, "citeseer6cls3703": 70, "pt": [70, 86], "paper_x": 70, "longtensor": [70, 74], "paper_author": 70, "train_test_splitt": 70, "num_test": 70, "num_val": 70, "dropout_r": 70, "__init__": [70, 83, 86], "super": 70, "to_hidden_linear": 70, "to_categories_linear": 70, "enumer": [70, 74, 86], "schedul": 70, "initial_lr": 70, "04": [70, 77, 86], "lr_schedul": 70, "steplr": 70, "7841": 70, "6729": 70, "2133": 70, "0127": 70, "5536": 70, "4931": 70, "3316": 70, "5507": 70, "6564": [70, 77], "6441": 70, "2435": 70, "5326": 70, "3037": 70, "5990": 70, "2278": 70, "2163": 70, "worth": 70, "visual": [70, 86], "n_compon": 70, "fit_transform": 70, "ax1": 70, "ax2": 70, "subplot": [70, 86], "suptitl": 70, "set_titl": [70, 86], "As": 71, "j": [71, 72, 77, 81, 82, 83, 86], "highlight": 71, "formal": [71, 79], "alpha_": 71, "jk": 71, "nonlinear": [71, 85], "exp": 71, "u_": 71, "limits_": 71, "context": 71, "again": [71, 79, 80], "vi": [71, 72], "beta_": 71, "ij": 71, "anoth": 71, "measur": 71, "interpret": 72, "propag": 72, "divid": 72, "2170": 72, "simplicial_complex": [73, 75, 76, 86], "warn": [73, 76, 82, 83], "filterwarn": [73, 76], "to_sparse_csr": [73, 74, 76], "bceloss": [73, 76], "x_1_val": [73, 75, 76], "incidence_1_v": [73, 75, 76], "y_val": [73, 75, 76], "pred": [73, 75, 76, 86], "4901733398438": 73, "4375": 73, "79052734375": 73, "759521484375": 73, "1170654296875": 73, "8601379394531": 73, "66136169433594": 73, "54958724975586": 73, "5625": [73, 75, 76], "33615493774414": 73, "13643646240234": 73, "25325775146484": 73, "162841796875": 73, "53228759765625": 73, "69290924072266": 73, "98519897460938": 73, "16600036621094": 73, "80245208740234": 73, "98604583740234": 73, "02069854736328": 73, "27203369140625": 73, "05550384521484": 73, "7593765258789": 73, "24275970458984": 73, "24573516845703": 73, "25870132446289": 73, "992156982421875": 73, "94447326660156": 73, "74713897705078": 73, "584598541259766": 73, "6875": 73, "302337646484375": 73, "985782623291016": 73, "713340759277344": 73, "53618240356445": 73, "84739303588867": 73, "595306396484375": 73, "71818161010742": 73, "3809814453125": 73, "186588287353516": 73, "00263214111328": 73, "166587829589844": 73, "39109420776367": 73, "600830078125": 73, "973331451416016": 73, "825225830078125": 73, "596601486206055": 73, "79454231262207": 73, "8666934967041": 73, "986522674560547": 73, "880859375": 73, "91712188720703": 73, "44558334350586": 73, "152557373046875": 73, "227420806884766": 73, "160226821899414": 73, "39475440979004": 73, "113832473754883": 73, "215381622314453": 73, "52114486694336": 73, "805360794067383": 73, "915090560913086": 73, "797382354736328": 73, "480539321899414": 73, "068044662475586": 73, "685386657714844": 73, "43670654296875": 73, "364776611328125": 73, "432109832763672": 73, "538002014160156": 73, "5771427154541": 73, "4938907623291": 73, "30146026611328": 73, "066486358642578": 73, "864112854003906": 73, "738616943359375": 73, "689714431762695": 73, "684139251708984": 73, "6783390045166": 73, "640270233154297": 73, "568225860595703": 73, "463821411132812": 73, "352237701416016": 73, "259632110595703": 73, "200979232788086": 73, "173927307128906": 73, "162473678588867": 73, "145278930664062": 73, "108654022216797": 73, "052175521850586": 73, "986543655395508": 73, "926197052001953": 73, "880918502807617": 73, "85179901123047": 73, "83198356628418": 73, "812116622924805": 73, "78550148010254": 73, "751976013183594": 73, "7146053314209": 73, "67841148376465": 73, "64767074584961": 73, "623760223388672": 73, "604145050048828": 73, "58517074584961": 73, "56340789794922": 73, "537738800048828": 73, "5096435546875": 73, "481836318969727": 73, "456588745117188": 73, "435287475585938": 73, "416616439819336": 73, "397968292236328": 73, "377914428710938": 73, "355894088745117": 73, "332746505737305": 73, "310028076171875": 73, "288677215576172": 73, "26887321472168": 73, "249961853027344": 73, "23108673095703": 73, "212047576904297": 73, "192527770996094": 73, "173789978027344": 73, "15878677368164": 73, "14487648010254": 73, "13152313232422": 73, "117942810058594": 73, "103792190551758": 73, "088790893554688": 73, "0731201171875": 73, "057111740112305": 73, "041547775268555": 73, "026466369628906": 73, "011743545532227": 73, "996620178222656": 73, "98100471496582": 73, "965646743774414": 73, "950223922729492": 73, "93490982055664": 73, "919763565063477": 73, "90479850769043": 73, "890039443969727": 73, "875774383544922": 73, "861370086669922": 73, "847206115722656": 73, "833080291748047": 73, "819049835205078": 73, "805124282836914": 73, "79128074645996": 73, "777509689331055": 73, "76376724243164": 73, "74854278564453": 73, "73321533203125": 73, "717744827270508": 73, "702198028564453": 73, "686790466308594": 73, "671096801757812": 73, "655136108398438": 73, "63947296142578": 73, "623905181884766": 73, "608251571655273": 73, "592540740966797": 73, "576845169067383": 73, "56026840209961": 73, "543073654174805": 73, "525781631469727": 73, "50815773010254": 73, "49064064025879": 73, "472421646118164": 73, "450693130493164": 73, "427898406982422": 73, "402362823486328": 73, "37494659423828": 73, "34657859802246": 73, "317203521728516": 73, "286909103393555": 73, "2688045501709": 73, "2530574798584": 73, "237436294555664": 73, "222070693969727": 73, "206933975219727": 73, "19208335876465": 73, "177217483520508": 73, "16236114501953": 73, "147443771362305": 73, "132776260375977": 73, "118249893188477": 73, "103797912597656": 73, "089706420898438": 73, "075761795043945": 73, "06186294555664": 73, "048063278198242": 73, "034448623657227": 73, "020954132080078": 73, "006834030151367": 73, "993017196655273": 73, "979351043701172": 73, "965789794921875": 73, "952112197875977": 73, "938526153564453": 73, "924880981445312": 73, "911245346069336": 73, "897811889648438": 73, "88424301147461": 73, "870620727539062": 73, "856914520263672": 73, "843372344970703": 73, "82991600036621": 73, "81630516052246": 73, "802886962890625": 73, "78949737548828": 73, "776121139526367": 73, "76279640197754": 73, "74949836730957": 73, "737751007080078": 73, "729055404663086": 73, "715679168701172": 73, "70354461669922": 73, "692848205566406": 73, "682186126708984": 73, "671817779541016": 73, "66116714477539": 73, "65018081665039": 73, "638872146606445": 73, "627206802368164": 73, "615779876708984": 73, "6042423248291": 73, "59211540222168": 73, "58187484741211": 73, "570674896240234": 73, "558454513549805": 73, "547941207885742": 73, "537683486938477": 73, "526987075805664": 73, "515642166137695": 73, "504106521606445": 73, "49180030822754": 73, "479785919189453": 73, "46455955505371": 73, "45136070251465": 73, "43777847290039": 73, "424055099487305": 73, "409717559814453": 73, "400564193725586": 73, "39628028869629": 73, "390756607055664": 73, "38397979736328": 73, "37428855895996": 73, "362167358398438": 73, "34809112548828": 73, "3326473236084": 73, "31566047668457": 73, "30012321472168": 73, "2796630859375": 73, "261516571044922": 73, "250732421875": 73, "238964080810547": 73, "224985122680664": 73, "208303451538086": 73, "189075469970703": 73, "167694091796875": 73, "145658493041992": 73, "13551902770996": 73, "12187385559082": 73, "102636337280273": 73, "088523864746094": 73, "074373245239258": 73, "05896759033203": 73, "041915893554688": 73, "023332595825195": 73, "00411033630371": 73, "985628128051758": 73, "970544815063477": 73, "95485496520996": 73, "941598892211914": 73, "92700958251953": 73, "912050247192383": 73, "8923282623291": 73, "879552841186523": 73, "867490768432617": 73, "854406356811523": 73, "84088706970215": 73, "826339721679688": 73, "81657600402832": 73, "806161880493164": 73, "795644760131836": 73, "78535270690918": 73, "775672912597656": 73, "760944366455078": 73, "752826690673828": 73, "74382209777832": 73, "729705810546875": 73, "711393356323242": 73, "697967529296875": 73, "6846923828125": 73, "672142028808594": 73, "657779693603516": 73, "64270782470703": 73, "630765914916992": 73, "6149845123291": 73, "603057861328125": 73, "590776443481445": 73, "57485008239746": 73, "556278228759766": 73, "543123245239258": 73, "52927017211914": 73, "51650047302246": 73, "50031280517578": 73, "48053550720215": 73, "46886444091797": 73, "45595359802246": 73, "437623977661133": 73, "424482345581055": 73, "410137176513672": 73, "396631240844727": 73, "381338119506836": 73, "362808227539062": 73, "34630012512207": 73, "32933235168457": 73, "31389808654785": 73, "300844192504883": 73, "28453826904297": 73, "27028465270996": 73, "258142471313477": 73, "245216369628906": 73, "230785369873047": 73, "215072631835938": 73, "198575973510742": 73, "183231353759766": 73, "1727294921875": 73, "159303665161133": 73, "14360237121582": 73, "128843307495117": 73, "117029190063477": 73, "105663299560547": 73, "09357452392578": 73, "080169677734375": 73, "066062927246094": 73, "051239013671875": 73, "04073715209961": 73, "02910804748535": 73, "01323127746582": 73, "00203514099121": 73, "9904842376709": 73, "977466583251953": 73, "96651840209961": 73, "952547073364258": 73, "943500518798828": 73, "931215286254883": 73, "920156478881836": 73, "90941047668457": 73, "897315979003906": 73, "884029388427734": 73, "870525360107422": 73, "857534408569336": 73, "847549438476562": 73, "83431053161621": 73, "82513999938965": 73, "813926696777344": 73, "80405044555664": 73, "79069709777832": 73, "777841567993164": 73, "76871681213379": 73, "757659912109375": 73, "74506950378418": 73, "73027992248535": 73, "721105575561523": 73, "70802116394043": 73, "69561767578125": 73, "686477661132812": 73, "67528533935547": 73, "6632022857666": 73, "654359817504883": 73, "6440486907959": 73, "63262367248535": 73, "62179183959961": 73, "61004066467285": 73, "599712371826172": 73, "58791160583496": 73, "574491500854492": 73, "56350326538086": 73, "55362319946289": 73, "542652130126953": 73, "531946182250977": 73, "520584106445312": 73, "508346557617188": 73, "49889373779297": 73, "488155364990234": 73, "476438522338867": 73, "46401596069336": 73, "453584671020508": 73, "442052841186523": 73, "431604385375977": 73, "420881271362305": 73, "410736083984375": 73, "401206970214844": 73, "389665603637695": 73, "37841796875": 73, "36739730834961": 73, "357166290283203": 73, "347293853759766": 73, "335630416870117": 73, "327388763427734": 73, "315521240234375": 73, "30586051940918": 73, "29657745361328": 73, "28609848022461": 73, "276351928710938": 73, "265724182128906": 73, "25368309020996": 73, "241600036621094": 73, "229310989379883": 73, "2224178314209": 73, "2132511138916": 73, "199750900268555": 73, "18743133544922": 73, "1822509765625": 73, "17019271850586": 73, "155643463134766": 73, "146547317504883": 73, "136232376098633": 73, "123960494995117": 73, "11368751525879": 73, "101808547973633": 73, "09125518798828": 73, "083723068237305": 73, "07281494140625": 73, "058805465698242": 73, "049646377563477": 73, "04451560974121": 73, "0327091217041": 73, "01663589477539": 73, "00825309753418": 73, "000246047973633": 73, "990230560302734": 73, "977436065673828": 73, "96634864807129": 73, "95425796508789": 73, "939908981323242": 73, "92905616760254": 73, "918004989624023": 73, "902795791625977": 73, "884109497070312": 73, "871601104736328": 73, "85983657836914": 73, "84302520751953": 73, "82725715637207": 73, "81039047241211": 73, "79295539855957": 73, "775209426879883": 73, "761594772338867": 73, "746509552001953": 73, "734050750732422": 73, "72390365600586": 73, "70878028869629": 73, "69400405883789": 73, "678071975708008": 73, "666906356811523": 73, "6538143157959": 73, "6412410736084": 73, "629528045654297": 73, "61863899230957": 73, "606908798217773": 73, "595272064208984": 73, "58217430114746": 73, "568185806274414": 73, "558835983276367": 73, "548227310180664": 73, "534801483154297": 73, "52486228942871": 73, "514476776123047": 73, "506332397460938": 73, "492887496948242": 73, "481121063232422": 73, "469911575317383": 73, "46491813659668": 73, "452470779418945": 73, "435522079467773": 73, "428197860717773": 73, "420866012573242": 73, "40703773498535": 73, "395797729492188": 73, "38579559326172": 73, "372974395751953": 73, "36663246154785": 73, "354814529418945": 73, "3371524810791": 73, "334217071533203": 73, "32594871520996": 73, "31154441833496": 73, "300630569458008": 73, "291522979736328": 73, "277509689331055": 73, "264963150024414": 73, "253454208374023": 73, "240015029907227": 73, "22855567932129": 73, "216880798339844": 73, "206361770629883": 73, "196760177612305": 73, "184263229370117": 73, "171443939208984": 73, "160396575927734": 73, "cicitationcora": 74, "hgnn": 74, "utlil": 74, "neccessari": 74, "pickl": 74, "scipi": [74, 77, 81, 82, 83, 86], "sp": 74, "computation": 74, "expens": [74, 77, 81, 82], "malllabiisc": 74, "hypergcn": 74, "cocit": 74, "09": [74, 77, 84, 86], "07": [74, 77, 86], "resolv": 74, "await": 74, "githubusercont": 74, "08": [74, 77, 86], "ok": 74, "404937": 74, "395k": 74, "applic": 74, "octet": 74, "stream": 74, "save": 74, "gt": [74, 86], "45k": 74, "kb": 74, "mb": 74, "101905": 74, "100k": 74, "52k": 74, "006": 74, "5436": 74, "3k": 74, "31k": 74, "51582": 74, "50k": 74, "37k": 74, "003": 74, "rb": 74, "handl": 74, "ipykernel_1201318": 74, "2369537045": 74, "deprecationwarn": 74, "csr_matrix": [74, 77, 81, 82], "namespac": 74, "csr": 74, "deprec": [74, 82], "pytorch": 74, "floattensor": 74, "num": 74, "gcnii": 74, "h2": [74, 81], "hstack": 74, "2226475299": 74, "userwarn": [74, 82, 83, 85], "miss": 74, "trigger": 74, "intern": 74, "aten": 74, "sparsecsrtensorimpl": 74, "cpp": 74, "predefin": 74, "train_idx": 74, "test_idx": 74, "current": [74, 79, 82, 85], "readi": [74, 86], "8357142806053162": 74, "47507786750793457": 74, "7642857432365417": 74, "5183022022247314": 74, "9214285612106323": 74, "5611370801925659": 74, "9428571462631226": 74, "597741425037384": 74, "9642857313156128": 74, "5915108919143677": 74, "9857142567634583": 74, "5837227702140808": 74, "9571428298950195": 74, "579828679561615": 74, "5654205679893494": 74, "5735981464385986": 74, "5813862681388855": 74, "node_dim": 75, "seper": 75, "unsqueez": [75, 86], "388511657714844": 75, "726402282714844": 75, "228939056396484": 75, "930335998535156": 75, "89554214477539": 75, "096839904785156": 75, "53312301635742": 75, "17842483520508": 75, "982330322265625": 75, "8914680480957": 75, "861602783203125": 75, "86212921142578": 75, "87543869018555": 75, "89324951171875": 75, "91067886352539": 75, "92604446411133": 75, "93891525268555": 75, "949344635009766": 75, "9575080871582": 75, "963714599609375": 75, "96826934814453": 75, "97142791748047": 75, "97331619262695": 75, "974063873291016": 75, "97380828857422": 75, "97255325317383": 75, "97031784057617": 75, "96706008911133": 75, "96271896362305": 75, "957176208496094": 75, "95022964477539": 75, "941707611083984": 75, "93131637573242": 75, "918678283691406": 75, "90337371826172": 75, "884979248046875": 75, "863189697265625": 75, "837684631347656": 75, "808353424072266": 75, "776126861572266": 75, "743309020996094": 75, "71380615234375": 75, "691314697265625": 75, "67386245727539": 75, "649879455566406": 75, "607601165771484": 75, "54794692993164": 75, "47976303100586": 75, "408973693847656": 75, "33806228637695": 75, "11848068237305": 76, "48760986328125": 76, "41103744506836": 76, "46265411376953": 76, "654991149902344": 76, "48741149902344": 76, "493499755859375": 76, "90041732788086": 76, "03223419189453": 76, "20775604248047": 76, "63990020751953": 76, "34232711791992": 76, "30369567871094": 76, "4339485168457": 76, "60327911376953": 76, "69844055175781": 76, "66060256958008": 76, "48517990112305": 76, "20755386352539": 76, "90876007080078": 76, "64041519165039": 76, "4536018371582": 76, "356746673583984": 76, "32301712036133": 76, "31113052368164": 76, "28101348876953": 76, "19704818725586": 76, "05533218383789": 76, "8785400390625": 76, "683712005615234": 76, "50434875488281": 76, "371429443359375": 76, "28118133544922": 76, "21925735473633": 76, "157997131347656": 76, "07044219970703": 76, "944950103759766": 76, "793039321899414": 76, "64575958251953": 76, "5272216796875": 76, "440631866455078": 76, "375879287719727": 76, "311176300048828": 76, "22810173034668": 76, "12361717224121": 76, "010883331298828": 76, "924745559692383": 76, "86925506591797": 76, "828832626342773": 76, "78775405883789": 76, "alexandro": 77, "kero": 77, "linalg": 77, "npla": 77, "a0": [77, 78], "becaus": [77, 78, 80, 82, 85], "serv": [77, 78], "simpli": [77, 78, 86], "demonstr": [77, 78], "similarli": [77, 78, 81], "emerg": [77, 78, 79, 81, 82], "four": [77, 78, 79, 81, 82], "y_true": [77, 78, 79, 82, 85], "l_tilde_pinv": 77, "pinv": 77, "invers": 77, "0971": 77, "0923": 77, "0892": 77, "2140": 77, "2069": 77, "2927": 77, "2309": 77, "0943": 77, "0948": 77, "2678": 77, "0804": 77, "3090": 77, "0960": 77, "2077": 77, "2056": 77, "2813": 77, "nnz": 77, "layout": 77, "sparse_coo": 77, "56771909e": 77, "02": [77, 86], "49643084e": 77, "13434650e": 77, "60154799e": 77, "03": [77, 86], "73820292e": 77, "65885226e": 77, "04038181e": 77, "51925802e": 77, "73643677e": 77, "95577741e": 77, "09312067e": 77, "39698386e": 77, "11006736e": 77, "25540316e": 77, "87149896e": 77, "65674657e": 77, "43987098e": 77, "79396772e": 77, "00662204e": 77, "45058060e": 77, "36910174e": 77, "82942520e": 77, "24798042e": 77, "85055751e": 77, "78386103e": 77, "24821486e": 77, "81510593e": 77, "07917011e": 77, "30485535e": 77, "19925834e": 77, "56662779e": 77, "25658545e": 77, "29514395e": 77, "73054542e": 77, "57650283e": 77, "87089108e": 77, "31973699e": 77, "45874534e": 77, "78385898e": 77, "24821523e": 77, "38282800e": 77, "29527006e": 77, "24821542e": 77, "45585343e": 77, "20149602e": 77, "39614227e": 77, "52603984e": 77, "02427802e": 77, "38569428e": 77, "20058507e": 77, "89658767e": 77, "67997003e": 77, "90682733e": 77, "88636552e": 77, "61071175e": 77, "75768661e": 77, "22418800e": 77, "07488209e": 77, "26928225e": 77, "52925774e": 77, "50903371e": 77, "71863856e": 77, "40345353e": 77, "36909867e": 77, "82943824e": 77, "90223058e": 77, "08467136e": 77, "43380561e": 77, "27135092e": 77, "31898531e": 77, "01219751e": 77, "78963115e": 77, "97890193e": 77, "49229891e": 77, "67953214e": 77, "75078206e": 77, "75904313e": 77, "03583546e": 77, "12457962e": 77, "10897127e": 77, "18870673e": 77, "28672193e": 77, "61245163e": 77, "48166016e": 77, "75217551e": 77, "67996958e": 77, "90682673e": 77, "44834775e": 77, "90006804e": 77, "59747154e": 77, "69860917e": 77, "59747209e": 77, "69862127e": 77, "59747284e": 77, "69861429e": 77, "59747191e": 77, "59747247e": 77, "69860823e": 77, "59747135e": 77, "11979373e": 77, "90869734e": 77, "59747228e": 77, "69860637e": 77, "59747303e": 77, "69861010e": 77, "59747116e": 77, "17587730e": 77, "43268425e": 77, "43105909e": 77, "32787512e": 77, "03376685e": 77, "44168448e": 77, "62169540e": 77, "41996737e": 77, "73246880e": 77, "97727704e": 77, "03496753e": 77, "71378374e": 77, "92902595e": 77, "15740368e": 77, "94057676e": 77, "48602486e": 77, "40909785e": 77, "14646482e": 77, "38315065e": 77, "76777497e": 77, "38311899e": 77, "76780128e": 77, "37373477e": 77, "49392605e": 77, "30545244e": 77, "10224779e": 77, "69429579e": 77, "59057510e": 77, "11831834e": 77, "86165255e": 77, "07662510e": 77, "53556532e": 77, "82225195e": 77, "76254632e": 77, "62731223e": 77, "63466549e": 77, "16528196e": 77, "62805045e": 77, "36022410e": 77, "48832843e": 77, "19494419e": 77, "13972221e": 77, "zia003": 77, "anaconda3": [77, 81, 82, 83, 85], "env": [77, 81, 82, 83, 85], "topox2": 77, "lib": [77, 81, 82, 83, 85], "site": [77, 81, 82, 83, 85], "_index": [77, 81, 82], "sparseefficiencywarn": [77, 81, 82], "chang": [77, 81, 82, 86], "sparsiti": [77, 81, 82], "lil_matrix": [77, 81, 82], "_set_arrayxarrai": [77, 81, 82], "produc": [77, 78, 79, 81, 85], "compar": [77, 78, 79, 81, 85], "binary_cross_entropy_with_logit": [77, 78, 79, 81, 85], "y_hat_test": [77, 78, 79, 81, 82, 85], "y_pred_test": [77, 78, 79, 81, 82, 85], "test_accuraci": [77, 78, 79, 81, 82, 85], "7231": 77, "6000": [77, 82], "6989": 77, "5667": [77, 78, 81, 82], "2500": 77, "6737": [77, 82], "6434": 77, "6362": 77, "6199": 77, "6117": 77, "6057": 77, "6011": 77, "5964": 77, "5911": 77, "5855": 77, "5805": 77, "5764": 77, "4000": 77, "5730": 77, "5696": 77, "5660": 77, "5624": [77, 84], "5593": 77, "5567": 77, "5520": 77, "5496": 77, "5472": 77, "5451": 77, "5432": 77, "5414": 77, "5397": 77, "5379": 77, "5362": 77, "5346": 77, "5333": 77, "5320": 77, "5308": 77, "5295": 77, "5284": 77, "5273": 77, "5264": 77, "5255": 77, "5246": 77, "5238": 77, "5230": 77, "5223": 77, "5216": 77, "5210": 77, "5204": 77, "5198": 77, "5193": 77, "5188": 77, "5183": 77, "5178": 77, "5174": 77, "5170": 77, "5166": 77, "5163": 77, "5159": 77, "5156": 77, "7216": 78, "7169": 78, "7151": 78, "7109": 78, "7039": 78, "work": 79, "novel": 79, "hing": 79, "proper": 79, "orient": 79, "fashion": 79, "kernel": 79, "l_r": 79, "widetild": 79, "hy": 79, "neq": [79, 86], "affin": 79, "therefor": 79, "hop": [79, 85], "suppos": 79, "_j": 79, "underset": 79, "w_": 79, "q_": 79, "tb_1": 79, "b_2b_2": 79, "notic": 79, "pattern": 79, "just": [79, 81, 86], "maxium": 79, "valueerror": [79, 82, 85], "gradient": 79, "tx_0": 79, "estim": 79, "diverg": 79, "deriv": 79, "seen": 79, "incidence_0_1": 79, "accordingli": 79, "mm": 79, "henc": 79, "y_hat_edg": 79, "fn": 79, "y_hat_edge_test": 79, "_pred_test": 79, "ge": 79, "7322": 79, "3667": 79, "7208": 79, "7333": [79, 81], "7070": 79, "6753": 79, "6695": 79, "6682": 79, "6674": 79, "laplacian_down_1_list": [80, 82, 85], "laplacian_down_2_list": 80, "incidence1_t_list": 80, "incidence2_t_list": 80, "laplacian_down_2": [80, 82, 85], "laplacian_down_1_train": [80, 82], "laplacian_down_1_test": [80, 82], "laplacian_down_2_train": 80, "laplacian_down_2_test": 80, "incidence1_t_train": 80, "incidence1_t_test": 80, "incidence2_t_train": 80, "incidence2_t_test": 80, "hzpmc22": 80, "did": 80, "la": 81, "r_": 81, "mathrm": 81, "leq": [81, 86], "feat_dim": 81, "arbitrari": 81, "choos": [81, 86], "dictionari": 81, "tha": 81, "arbitrarili": 81, "formul": 81, "quit": 81, "close": 81, "usual": 81, "suggest": 81, "refrain": 81, "tetrahedron": 81, "sparse_to_torch": 81, "rank_": 81, "rank_0": 81, "coadjacency_matrix": [81, 82], "h0": 81, "h1": 81, "h3": 81, "b3": 81, "ninamiolan": [81, 82, 85], "tmx": [81, 82, 85], "python3": [81, 82, 83, 85], "x_3": 81, "tetrahedron_feat": 81, "track": 81, "rank_1": 81, "rank_2": 81, "rank_3": 81, "typic": 81, "due": [81, 82, 85], "6721": 81, "6333": [81, 82], "6284": 81, "6173": 81, "6110": 81, "7000": [81, 82], "5831": 81, "5695": 81, "5638": 81, "5493": 81, "5384": 81, "7667": 81, "5141": 81, "5201": 81, "5038": 81, "4906": 81, "4763": 81, "4545": 81, "4483": 81, "4153": 81, "8000": 81, "4062": 81, "3790": 81, "3916": 81, "3529": 81, "8667": 81, "2900": 81, "2359": 81, "9333": 81, "2002": 81, "9667": 81, "2970": 81, "9000": 81, "2032": 81, "2329": 81, "2019": 81, "0873": 81, "0293": 81, "0272": 81, "0264": 81, "0245": 81, "0207": 81, "0165": 81, "0132": 81, "0114": 81, "0113": 81, "0117": 81, "0101": 81, "0081": 81, "0071": 81, "0065": 81, "0061": 81, "0057": 81, "0054": 81, "0050": 81, "0046": 81, "0043": 81, "0040": 81, "0038": 81, "0036": 81, "0034": 81, "0033": 81, "0032": 81, "0031": 81, "0030": 81, "0029": 81, "0028": 81, "0027": 81, "0026": 81, "0025": 81, "0024": 81, "0023": 81, "0022": 81, "0021": 81, "0020": 81, "0019": 81, "0018": 81, "0017": 81, "0016": 81, "0015": 81, "0014": 81, "0013": 81, "0012": 81, "0011": 81, "0010": 81, "0009": 81, "0008": 81, "account": 82, "_t": [82, 85], "p_d": [82, 85], "p_u": [82, 85], "likewis": 82, "essenti": 82, "_0": 82, "yet": [82, 85], "laplacian_0_list": [82, 85], "laplacian_up_1_list": [82, 85], "laplacian_2_list": [82, 85], "hodge_laplacian_matrix": [82, 85], "size_averag": 82, "reduct": [82, 85], "in_linear_0": 82, "in_linear_1": 82, "in_linear_2": 82, "out_linear_0": 82, "out_linear_1": 82, "out_linear_2": 82, "_reduct": 82, "arg": [82, 83], "ret": 82, "laplacian_0_train": 82, "laplacian_0_test": 82, "laplacian_up_1_train": 82, "laplacian_up_1_test": 82, "laplacian_2_train": 82, "laplacian_2_test": 82, "lead": [82, 85], "incorrect": [82, 85], "ensur": [82, 85], "mse_loss": [82, 85], "944857": 82, "9959": 82, "5353": 82, "2055": 82, "6187": 82, "7969": 82, "4612": 82, "5951": 82, "8349": 82, "8397": 82, "9482": 82, "laplacian_up_2": [82, 85], "get_simplicial_featur": [82, 85], "which_feat": [82, 85], "elif": [82, 85], "binary_cross_entropi": 82, "7911": 82, "coo_matrix": 83, "diag": 83, "return_count": 83, "normalize_higher_order_adj": 83, "a_opt": 83, "cochain": 83, "num_of_k_simplic": 83, "num_of_j_simplic": 83, "rowsum": 83, "r_inv_sqrt": 83, "flatten": 83, "isinf": 83, "r_mat_inv_sqrt": 83, "a_opt_to": 83, "dot": 83, "neigborood": 83, "ssconv": 83, "get_neighborhood": 83, "incidence_1_norm_list": 83, "incidence_2_norm_list": 83, "adjacency_up_0_norm_list": 83, "adjacency_up_1_norm_list": 83, "adjacency_down_1_norm_list": 83, "adjacency_down_2_norm_list": 83, "up_laplacian_1_list": 83, "up_laplacian_2_list": 83, "down_laplacian_1_list": 83, "down_laplacian_2_list": 83, "up_laplacian_1": 83, "up_laplacian_2": 83, "down_laplacian_1": 83, "down_laplacian_2": 83, "todo": 83, "home": 83, "kha053": 83, "nvml": 83, "incid1": 83, "incid1_norm": 83, "incid2": 83, "incid2_norm": 83, "adj0_up_norm": 83, "adj1_up_norm": 83, "adj1_down_norm": 83, "adj2_down_norm": 83, "correct_count": 83, "x_0t": 83, "x_1t": 83, "x_2t": 83, "incid1t": 83, "incid1_normt": 83, "incid2t": 83, "incid2_normt": 83, "adj0_up_normt": 83, "adj1_up_normt": 83, "adj1_down_normt": 83, "adj2_down_normt": 83, "yt": 83, "ysb22": 84, "ruochen": 84, "freder": 84, "razvan": 84, "pascanu": 84, "editor": 84, "confer": 84, "volum": 84, "pmlr": 84, "dec": 84, "2022a": 84, "chose": 84, "reshap": 84, "normalized_laplacian_matrix": 84, "x_0s_train": 84, "x_0s_test": 84, "x_1s_train": 84, "x_1s_test": 84, "x_2s_train": 84, "x_2s_test": 84, "laplacian_0s_train": 84, "laplacian_0s_test": 84, "laplacian_1s_train": 84, "laplacian_1s_test": 84, "laplacian_2s_train": 84, "laplacian_2s_test": 84, "6056": 84, "2707": 84, "9831": 84, "8605": 84, "0164": 84, "0106": 84, "9957": 84, "5802": 84, "definit": 85, "itself": 85, "larger": 85, "x_train": 85, "x_test": 85, "laplacian_down_train": 85, "laplacian_down_test": 85, "laplacian_up_train": 85, "laplacian_up_test": 85, "simplex_order_select": 85, "7131": 85, "5218": 85, "6860": 85, "6109": 85, "7219": 85, "8411": 85, "7747": 85, "2382": 85, "0308": 85, "maxim": 85, "chennel_edg": 85, "channel_fac": 85, "classm": 85, "rm": 85, "channels_x": 85, "squeez": [85, 86], "8799": 85, "rgs21": 86, "spend": 86, "synthet": 86, "ahead": 86, "itertool": 86, "product": 86, "tnx": 86, "networkx": 86, "nx": 86, "random_split": 86, "tqdm": 86, "spatial": 86, "distanc": 86, "seed": 86, "lt": 86, "_c": 86, "0x1664f0f50": 86, "less": 86, "cloud": 86, "insid": 86, "remov": 86, "centroid": 86, "sort": 86, "coordin": 86, "argsort": 86, "tri": 86, "disk_cent": 86, "disk_radiu": 86, "indices_includ": 86, "cdist": 86, "idx_dict": 86, "instanc": 86, "euclidean": 86, "shortest": 86, "plot_complex": 86, "plane": 86, "idx": 86, "poli": 86, "polygon": 86, "color": 86, "green": 86, "gca": 86, "add_patch": 86, "vstack": 86, "i_1": 86, "i_2": 86, "ldot": 86, "i_m": 86, "i_j": 86, "i_": 86, "ground": 86, "truth": 86, "supervis": 86, "setup": 86, "subsect": 86, "randomli": 86, "pick": 86, "triplet": 86, "around": 86, "anti": 86, "diagon": 86, "mid": 86, "region": 86, "start_nod": 86, "mid_nod": 86, "end_nod": 86, "all_triplet": 86, "increas": 86, "underli": 86, "distance_matrix": 86, "squareform": 86, "pdist": 86, "toarrai": 86, "from_numpy_arrai": 86, "path_1": 86, "shortest_path": 86, "path_2": 86, "plot_path": 86, "red": 86, "arrow": 86, "quiver": 86, "scale_unit": 86, "angl": 86, "yield": 86, "vectorized_trajectori": 86, "neigbors_mask": 86, "last_nod": 86, "turn": 86, "a_1": 86, "a_2": 86, "a_j": 86, "i_n": 86, "later": 86, "lookup": 86, "speed": 86, "edge_lookup_t": 86, "__getitem__": 86, "discard": 86, "neighbors_mask": 86, "__len__": 86, "c0": 86, "loader": 86, "batch_siz": 86, "val_siz": 86, "train_siz": 86, "train_d": 86, "val_d": 86, "test_d": 86, "train_dl": 86, "val_dl": 86, "test_dl": 86, "c_1": 86, "partial_1": 86, "c_0": 86, "That": 86, "hat": 86, "_m": 86, "neg": 86, "likelihood": 86, "penal": 86, "weight_decai": 86, "5e": 86, "loss_funct": 86, "nllloss": 86, "nll": 86, "training_histori": 86, "training_loss": 86, "traj": 86, "06": 86, "quick": 86, "confirm": 86, "everyth": 86, "reason": 86, "ax": 86, "ncol": 86, "figsiz": 86, "better": 86, "guess": 86, "3f": 86, "constructor": 86, "affect": 86, "capabl": 86, "revers": 86, "Or": 86, "ocean": 86, "drifter": 86}, "objects": {"topomodelx.base": [[0, 0, 0, "-", "aggregation"], [1, 0, 0, "-", "conv"], [3, 0, 0, "-", "message_passing"]], "topomodelx.base.aggregation": [[0, 1, 1, "", "Aggregation"]], "topomodelx.base.aggregation.Aggregation": [[0, 2, 1, "", "forward"], [0, 2, 1, "", "update"]], "topomodelx.base.conv": [[1, 1, 1, "", "Conv"]], "topomodelx.base.conv.Conv": [[1, 2, 1, "", "forward"], [1, 2, 1, "", "update"]], "topomodelx.base.message_passing": [[3, 1, 1, "", "MessagePassing"]], "topomodelx.base.message_passing.MessagePassing": [[3, 2, 1, "", "aggregate"], [3, 2, 1, "", "attention"], [3, 2, 1, "", "forward"], [3, 2, 1, "", "message"], [3, 2, 1, "", "reset_parameters"]], "topomodelx.nn.cell": [[5, 0, 0, "-", "can"], [6, 0, 0, "-", "can_layer"], [7, 0, 0, "-", "ccxn"], [8, 0, 0, "-", "ccxn_layer"], [9, 0, 0, "-", "cwn"], [10, 0, 0, "-", "cwn_layer"]], "topomodelx.nn.cell.can": [[5, 1, 1, "", "CAN"]], "topomodelx.nn.cell.can.CAN": [[5, 2, 1, "", "forward"]], "topomodelx.nn.cell.can_layer": [[6, 1, 1, "", "CANLayer"], [6, 1, 1, "", "LiftLayer"], [6, 1, 1, "", "MultiHeadCellAttention"], [6, 1, 1, "", "MultiHeadCellAttention_v2"], [6, 1, 1, "", "MultiHeadLiftLayer"], [6, 1, 1, "", "PoolLayer"], [6, 3, 1, "", "add_self_loops"], [6, 3, 1, "", "softmax"]], "topomodelx.nn.cell.can_layer.CANLayer": [[6, 2, 1, "", "forward"], [6, 2, 1, "", "reset_parameters"]], "topomodelx.nn.cell.can_layer.LiftLayer": [[6, 2, 1, "", "forward"], [6, 2, 1, "", "message"], [6, 2, 1, "", "reset_parameters"]], "topomodelx.nn.cell.can_layer.MultiHeadCellAttention": [[6, 2, 1, "", "attention"], [6, 2, 1, "", "forward"], [6, 2, 1, "", "message"], [6, 2, 1, "", "reset_parameters"]], "topomodelx.nn.cell.can_layer.MultiHeadCellAttention_v2": [[6, 2, 1, "", "attention"], [6, 2, 1, "", "forward"], [6, 2, 1, "", "message"], [6, 2, 1, "", "reset_parameters"]], "topomodelx.nn.cell.can_layer.MultiHeadLiftLayer": [[6, 2, 1, "", "forward"], [6, 2, 1, "", "reset_parameters"]], "topomodelx.nn.cell.can_layer.PoolLayer": [[6, 2, 1, "", "forward"], [6, 2, 1, "", "reset_parameters"]], "topomodelx.nn.cell.ccxn": [[7, 1, 1, "", "CCXN"]], "topomodelx.nn.cell.ccxn.CCXN": [[7, 2, 1, "", "forward"]], "topomodelx.nn.cell.ccxn_layer": [[8, 1, 1, "", "CCXNLayer"]], "topomodelx.nn.cell.ccxn_layer.CCXNLayer": [[8, 2, 1, "", "forward"]], "topomodelx.nn.cell.cwn": [[9, 1, 1, "", "CWN"]], "topomodelx.nn.cell.cwn.CWN": [[9, 2, 1, "", "forward"]], "topomodelx.nn.cell.cwn_layer": [[10, 1, 1, "", "CWNLayer"]], "topomodelx.nn.cell.cwn_layer.CWNLayer": [[10, 2, 1, "", "forward"]], "topomodelx.nn.hypergraph": [[12, 0, 0, "-", "allset"], [13, 0, 0, "-", "allset_layer"], [14, 0, 0, "-", "allset_transformer"], [15, 0, 0, "-", "allset_transformer_layer"], [18, 0, 0, "-", "hmpnn"], [19, 0, 0, "-", "hmpnn_layer"], [20, 0, 0, "-", "hnhn"], [21, 0, 0, "-", "hnhn_layer"], [22, 0, 0, "-", "hnhn_layer_bis"], [23, 0, 0, "-", "hypergat"], [24, 0, 0, "-", "hypergat_layer"], [25, 0, 0, "-", "hypersage"], [26, 0, 0, "-", "hypersage_layer"], [28, 0, 0, "-", "unigcn"], [29, 0, 0, "-", "unigcn_layer"], [30, 0, 0, "-", "unigcnii"], [31, 0, 0, "-", "unigcnii_layer"], [32, 0, 0, "-", "unigin"], [33, 0, 0, "-", "unigin_layer"], [34, 0, 0, "-", "unisage"], [35, 0, 0, "-", "unisage_layer"]], "topomodelx.nn.hypergraph.allset": [[12, 1, 1, "", "AllSet"]], "topomodelx.nn.hypergraph.allset.AllSet": [[12, 2, 1, "", "forward"]], "topomodelx.nn.hypergraph.allset_layer": [[13, 1, 1, "", "AllSetBlock"], [13, 1, 1, "", "AllSetLayer"], [13, 1, 1, "", "MLP"]], "topomodelx.nn.hypergraph.allset_layer.AllSetBlock": [[13, 2, 1, "", "forward"], [13, 2, 1, "", "reset_parameters"]], "topomodelx.nn.hypergraph.allset_layer.AllSetLayer": [[13, 2, 1, "", "forward"], [13, 2, 1, "", "reset_parameters"]], "topomodelx.nn.hypergraph.allset_transformer": [[14, 1, 1, "", "AllSetTransformer"]], "topomodelx.nn.hypergraph.allset_transformer.AllSetTransformer": [[14, 2, 1, "", "forward"]], "topomodelx.nn.hypergraph.allset_transformer_layer": [[15, 1, 1, "", "AllSetTransformerBlock"], [15, 1, 1, "", "AllSetTransformerLayer"], [15, 1, 1, "", "MLP"], [15, 1, 1, "", "MultiHeadAttention"]], "topomodelx.nn.hypergraph.allset_transformer_layer.AllSetTransformerBlock": [[15, 2, 1, "", "forward"], [15, 2, 1, "", "reset_parameters"]], "topomodelx.nn.hypergraph.allset_transformer_layer.AllSetTransformerLayer": [[15, 2, 1, "", "forward"], [15, 2, 1, "", "reset_parameters"]], "topomodelx.nn.hypergraph.allset_transformer_layer.MultiHeadAttention": [[15, 2, 1, "", "attention"], [15, 2, 1, "", "forward"], [15, 2, 1, "", "reset_parameters"]], "topomodelx.nn.hypergraph.hmpnn": [[18, 1, 1, "", "HMPNN"]], "topomodelx.nn.hypergraph.hmpnn.HMPNN": [[18, 2, 1, "", "forward"]], "topomodelx.nn.hypergraph.hmpnn_layer": [[19, 1, 1, "", "HMPNNLayer"]], "topomodelx.nn.hypergraph.hmpnn_layer.HMPNNLayer": [[19, 2, 1, "", "apply_regular_dropout"], [19, 2, 1, "", "forward"]], "topomodelx.nn.hypergraph.hnhn": [[20, 1, 1, "", "HNHN"], [20, 1, 1, "", "HNHNNetwork"]], "topomodelx.nn.hypergraph.hnhn.HNHN": [[20, 2, 1, "", "forward"]], "topomodelx.nn.hypergraph.hnhn.HNHNNetwork": [[20, 2, 1, "", "forward"]], "topomodelx.nn.hypergraph.hnhn_layer": [[21, 1, 1, "", "HNHNLayer"]], "topomodelx.nn.hypergraph.hnhn_layer.HNHNLayer": [[21, 2, 1, "", "compute_normalization_matrices"], [21, 2, 1, "", "forward"], [21, 2, 1, "", "init_biases"], [21, 2, 1, "", "normalize_incidence_matrices"], [21, 2, 1, "", "reset_parameters"]], "topomodelx.nn.hypergraph.hnhn_layer_bis": [[22, 1, 1, "", "HNHNLayer"]], "topomodelx.nn.hypergraph.hnhn_layer_bis.HNHNLayer": [[22, 2, 1, "", "forward"]], "topomodelx.nn.hypergraph.hypergat": [[23, 1, 1, "", "HyperGAT"]], "topomodelx.nn.hypergraph.hypergat.HyperGAT": [[23, 2, 1, "", "forward"]], "topomodelx.nn.hypergraph.hypergat_layer": [[24, 1, 1, "", "HyperGATLayer"]], "topomodelx.nn.hypergraph.hypergat_layer.HyperGATLayer": [[24, 2, 1, "", "attention"], [24, 2, 1, "", "forward"], [24, 2, 1, "", "reset_parameters"], [24, 2, 1, "", "update"]], "topomodelx.nn.hypergraph.hypersage": [[25, 1, 1, "", "HyperSAGE"]], "topomodelx.nn.hypergraph.hypersage.HyperSAGE": [[25, 2, 1, "", "forward"]], "topomodelx.nn.hypergraph.hypersage_layer": [[26, 1, 1, "", "GeneralizedMean"], [26, 1, 1, "", "HyperSAGELayer"]], "topomodelx.nn.hypergraph.hypersage_layer.GeneralizedMean": [[26, 2, 1, "", "forward"]], "topomodelx.nn.hypergraph.hypersage_layer.HyperSAGELayer": [[26, 2, 1, "", "aggregate"], [26, 2, 1, "", "forward"], [26, 2, 1, "", "update"]], "topomodelx.nn.hypergraph.unigcn": [[28, 1, 1, "", "UniGCN"]], "topomodelx.nn.hypergraph.unigcn.UniGCN": [[28, 2, 1, "", "forward"]], "topomodelx.nn.hypergraph.unigcn_layer": [[29, 1, 1, "", "UniGCNLayer"]], "topomodelx.nn.hypergraph.unigcn_layer.UniGCNLayer": [[29, 2, 1, "", "forward"], [29, 2, 1, "", "reset_parameters"]], "topomodelx.nn.hypergraph.unigcnii": [[30, 1, 1, "", "UniGCNII"]], "topomodelx.nn.hypergraph.unigcnii.UniGCNII": [[30, 2, 1, "", "forward"]], "topomodelx.nn.hypergraph.unigcnii_layer": [[31, 1, 1, "", "UniGCNIILayer"]], "topomodelx.nn.hypergraph.unigcnii_layer.UniGCNIILayer": [[31, 2, 1, "", "forward"], [31, 2, 1, "", "reset_parameters"]], "topomodelx.nn.hypergraph.unigin": [[32, 1, 1, "", "UniGIN"]], "topomodelx.nn.hypergraph.unigin.UniGIN": [[32, 2, 1, "", "forward"]], "topomodelx.nn.hypergraph.unigin_layer": [[33, 1, 1, "", "UniGINLayer"]], "topomodelx.nn.hypergraph.unigin_layer.UniGINLayer": [[33, 2, 1, "", "forward"]], "topomodelx.nn.hypergraph.unisage": [[34, 1, 1, "", "UniSAGE"]], "topomodelx.nn.hypergraph.unisage.UniSAGE": [[34, 2, 1, "", "forward"]], "topomodelx.nn.hypergraph.unisage_layer": [[35, 1, 1, "", "UniSAGELayer"]], "topomodelx.nn.hypergraph.unisage_layer.UniSAGELayer": [[35, 2, 1, "", "forward"], [35, 2, 1, "", "reset_parameters"]], "topomodelx.nn.simplicial": [[37, 0, 0, "-", "dist2cycle"], [38, 0, 0, "-", "dist2cycle_layer"], [39, 0, 0, "-", "hsn"], [40, 0, 0, "-", "hsn_layer"], [42, 0, 0, "-", "san"], [43, 0, 0, "-", "san_layer"], [44, 0, 0, "-", "sca_cmps"], [45, 0, 0, "-", "sca_cmps_layer"], [46, 0, 0, "-", "sccn"], [47, 0, 0, "-", "sccn_layer"], [48, 0, 0, "-", "sccnn"], [49, 0, 0, "-", "sccnn_layer"], [50, 0, 0, "-", "scconv"], [51, 0, 0, "-", "scconv_layer"], [52, 0, 0, "-", "scn2"], [53, 0, 0, "-", "scn2_layer"], [54, 0, 0, "-", "scnn"], [55, 0, 0, "-", "scnn_layer"], [56, 0, 0, "-", "scone"], [57, 0, 0, "-", "scone_layer"]], "topomodelx.nn.simplicial.dist2cycle": [[37, 1, 1, "", "Dist2Cycle"]], "topomodelx.nn.simplicial.dist2cycle.Dist2Cycle": [[37, 2, 1, "", "forward"]], "topomodelx.nn.simplicial.dist2cycle_layer": [[38, 1, 1, "", "Dist2CycleLayer"]], "topomodelx.nn.simplicial.dist2cycle_layer.Dist2CycleLayer": [[38, 2, 1, "", "forward"], [38, 2, 1, "", "reset_parameters"]], "topomodelx.nn.simplicial.hsn": [[39, 1, 1, "", "HSN"]], "topomodelx.nn.simplicial.hsn.HSN": [[39, 2, 1, "", "forward"]], "topomodelx.nn.simplicial.hsn_layer": [[40, 1, 1, "", "HSNLayer"]], "topomodelx.nn.simplicial.hsn_layer.HSNLayer": [[40, 2, 1, "", "forward"], [40, 2, 1, "", "reset_parameters"]], "topomodelx.nn.simplicial.san": [[42, 1, 1, "", "SAN"]], "topomodelx.nn.simplicial.san.SAN": [[42, 2, 1, "", "compute_projection_matrix"], [42, 2, 1, "", "forward"]], "topomodelx.nn.simplicial.san_layer": [[43, 1, 1, "", "SANConv"], [43, 1, 1, "", "SANLayer"]], "topomodelx.nn.simplicial.san_layer.SANConv": [[43, 2, 1, "", "forward"]], "topomodelx.nn.simplicial.san_layer.SANLayer": [[43, 2, 1, "", "forward"], [43, 2, 1, "", "reset_parameters"]], "topomodelx.nn.simplicial.sca_cmps": [[44, 1, 1, "", "SCACMPS"]], "topomodelx.nn.simplicial.sca_cmps.SCACMPS": [[44, 2, 1, "", "forward"]], "topomodelx.nn.simplicial.sca_cmps_layer": [[45, 1, 1, "", "SCACMPSLayer"]], "topomodelx.nn.simplicial.sca_cmps_layer.SCACMPSLayer": [[45, 2, 1, "", "forward"], [45, 2, 1, "", "intra_aggr"], [45, 2, 1, "", "reset_parameters"], [45, 2, 1, "", "weight_func"]], "topomodelx.nn.simplicial.sccn": [[46, 1, 1, "", "SCCN"]], "topomodelx.nn.simplicial.sccn.SCCN": [[46, 2, 1, "", "forward"]], "topomodelx.nn.simplicial.sccn_layer": [[47, 1, 1, "", "SCCNLayer"]], "topomodelx.nn.simplicial.sccn_layer.SCCNLayer": [[47, 2, 1, "", "forward"], [47, 2, 1, "", "reset_parameters"]], "topomodelx.nn.simplicial.sccnn": [[48, 1, 1, "", "SCCNN"], [48, 1, 1, "", "SCCNNComplex"]], "topomodelx.nn.simplicial.sccnn.SCCNN": [[48, 2, 1, "", "forward"]], "topomodelx.nn.simplicial.sccnn.SCCNNComplex": [[48, 2, 1, "", "forward"]], "topomodelx.nn.simplicial.sccnn_layer": [[49, 1, 1, "", "SCCNNLayer"]], "topomodelx.nn.simplicial.sccnn_layer.SCCNNLayer": [[49, 2, 1, "", "aggr_norm_func"], [49, 2, 1, "", "chebyshev_conv"], [49, 2, 1, "", "forward"], [49, 2, 1, "", "reset_parameters"], [49, 2, 1, "", "update"]], "topomodelx.nn.simplicial.scconv": [[50, 1, 1, "", "SCConv"]], "topomodelx.nn.simplicial.scconv.SCConv": [[50, 2, 1, "", "forward"]], "topomodelx.nn.simplicial.scconv_layer": [[51, 1, 1, "", "SCConvLayer"]], "topomodelx.nn.simplicial.scconv_layer.SCConvLayer": [[51, 2, 1, "", "forward"], [51, 2, 1, "", "reset_parameters"]], "topomodelx.nn.simplicial.scn2": [[52, 1, 1, "", "SCN2"]], "topomodelx.nn.simplicial.scn2.SCN2": [[52, 2, 1, "", "forward"]], "topomodelx.nn.simplicial.scn2_layer": [[53, 1, 1, "", "SCN2Layer"]], "topomodelx.nn.simplicial.scn2_layer.SCN2Layer": [[53, 2, 1, "", "forward"], [53, 2, 1, "", "reset_parameters"]], "topomodelx.nn.simplicial.scnn": [[54, 1, 1, "", "SCNN"]], "topomodelx.nn.simplicial.scnn.SCNN": [[54, 2, 1, "", "forward"]], "topomodelx.nn.simplicial.scnn_layer": [[55, 1, 1, "", "SCNNLayer"]], "topomodelx.nn.simplicial.scnn_layer.SCNNLayer": [[55, 2, 1, "", "aggr_norm_func"], [55, 2, 1, "", "chebyshev_conv"], [55, 2, 1, "", "forward"], [55, 2, 1, "", "reset_parameters"], [55, 2, 1, "", "update"]], "topomodelx.nn.simplicial.scone": [[56, 1, 1, "", "SCoNe"], [56, 1, 1, "", "TrajectoriesDataset"], [56, 3, 1, "", "generate_complex"], [56, 3, 1, "", "generate_trajectories"]], "topomodelx.nn.simplicial.scone.SCoNe": [[56, 2, 1, "", "forward"]], "topomodelx.nn.simplicial.scone.TrajectoriesDataset": [[56, 2, 1, "", "vectorize_path"]], "topomodelx.nn.simplicial.scone_layer": [[57, 1, 1, "", "SCoNeLayer"]], "topomodelx.nn.simplicial.scone_layer.SCoNeLayer": [[57, 2, 1, "", "forward"], [57, 2, 1, "", "reset_parameters"]], "topomodelx.utils": [[58, 0, 0, "-", "scatter"]], "topomodelx.utils.scatter": [[58, 3, 1, "", "broadcast"], [58, 3, 1, "", "scatter"], [58, 3, 1, "", "scatter_add"], [58, 3, 1, "", "scatter_mean"], [58, 3, 1, "", "scatter_sum"]]}, "objtypes": {"0": "py:module", "1": "py:class", "2": "py:method", "3": "py:function"}, "objnames": {"0": ["py", "module", "Python module"], "1": ["py", "class", "Python class"], "2": ["py", "method", "Python method"], "3": ["py", "function", "Python function"]}, "titleterms": {"aggreg": 0, "conv": 1, "base": 2, "messag": [3, 68, 80], "pass": [3, 68, 80], "api": 4, "refer": [4, 61, 84], "packag": 4, "modul": 4, "can": [5, 62], "can_lay": 6, "ccxn": [7, 63], "ccxn_layer": 8, "cwn": [9, 64], "cwn_layer": 10, "cell": [11, 62, 63], "allset": 12, "allset_lay": 13, "allset_transform": 14, "allset_transformer_lay": 15, "dhgcn": [16, 67], "dhgcn_layer": 17, "hmpnn": [18, 68], "hmpnn_layer": 19, "hnhn": [20, 69, 70], "hnhn_layer": 21, "hnhn_layer_bi": 22, "hypergat": 23, "hypergat_lay": 24, "hypersag": [25, 72], "hypersage_lay": 26, "hypergraph": [27, 67, 68, 69, 70, 71, 74, 87], "unigcn": [28, 73], "unigcn_lay": 29, "unigcnii": [30, 74], "unigcnii_lay": 31, "neural": [36, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87], "network": [36, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86], "simplici": [41, 77, 78, 79, 80, 81, 82, 83, 85, 86, 87], "util": 58, "icml": 59, "2023": 59, "topolog": [59, 87], "deep": 59, "learn": 59, "challeng": 59, "descript": 59, "public": 59, "outcom": 59, "particip": 59, "deadlin": 59, "how": 59, "submit": 59, "guidelin": 59, "submiss": 59, "requir": 59, "evalu": [59, 86], "question": 59, "contribut": 60, "make": 60, "chang": 60, "write": 60, "test": [60, 80, 83, 85, 86], "run": 60, "document": 60, "intro": 60, "docstr": 60, "The": [60, 62, 63, 64, 79], "anatomi": 60, "exampl": 60, "topomodelx": 61, "tmx": 61, "get": 61, "start": 61, "train": [62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86], "attent": [62, 63, 79], "abstract": [62, 79], "task": [62, 63, 64, 79], "set": [62, 63, 64, 65, 66], "up": [62, 63, 64], "pre": [62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85], "process": [62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85], "creat": [62, 63, 64, 65, 67, 69, 70, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 85, 86], "convolut": [63, 81, 82, 83, 84, 85], "complex": [63, 80, 81, 82, 83, 85, 86, 87], "cw": 64, "an": [65, 66], "all": [65, 66], "tnn": [65, 66, 67, 72, 73, 75, 76], "addit": [65, 66, 72], "theoret": [65, 66, 72], "clarif": [65, 66, 72], "transform": 66, "defin": [66, 67, 69, 71, 72, 77, 78, 81, 82, 83, 84, 85], "import": [67, 69, 71, 73, 75, 76, 77, 78, 80, 81, 82, 83, 84, 85], "data": [67, 71, 73, 74, 75, 76, 80, 83, 86], "neighborhood": [67, 69, 71, 77, 78, 81, 82, 84, 85], "structur": [67, 69, 71, 77, 78, 81, 83, 84], "lift": [67, 71], "domain": [67, 71], "hyperedg": [69, 70], "neuron": [69, 70], "dataset": [69, 77, 78, 81, 82, 83, 84, 85, 86], "signal": [69, 77, 78, 81, 82, 85], "us": 74, "layer": 74, "load": 74, "unigin": 75, "uni": 76, "sage": 76, "homologi": 77, "local": 77, "dist2cycl": 77, "binari": [77, 78, 81, 82, 85], "label": [77, 78, 81, 82, 85], "featur": 77, "high": 78, "skip": 78, "hsn": 78, "san": 79, "autoencod": 80, "sca": 80, "coadjac": 80, "scheme": 80, "cmp": 80, "prepar": [80, 83, 85], "input": 80, "each": 80, "split": [80, 85], "model": [80, 82, 85, 86], "sccn": 81, "sccnn": 82, "we": [82, 85], "perform": [82, 85], "1": [82, 85], "classif": [82, 85], "shrec": 82, "strcture": [82, 85], "2": [82, 83, 84, 85], "node": [82, 85], "scconv": 83, "helper": 83, "function": 83, "neighbourhood": 83, "simplex": 84, "scn": 84, "rank": 84, "scnn": 85, "karat": 85, "weight": 85, "hodg": 85, "laplacian": 85, "net": [86, 87], "scone": 86, "tabl": 86, "content": 86, "gener": 86, "trajectori": 86, "pytorch": 86, "dataload": 86, "suggest": 86, "further": 86, "experiment": 86, "tutori": 87, "cellular": 87}, "envversion": {"sphinx.domains.c": 3, "sphinx.domains.changeset": 1, "sphinx.domains.citation": 1, "sphinx.domains.cpp": 9, "sphinx.domains.index": 1, "sphinx.domains.javascript": 3, "sphinx.domains.math": 2, "sphinx.domains.python": 4, "sphinx.domains.rst": 2, "sphinx.domains.std": 2, "nbsphinx": 4, "sphinx.ext.viewcode": 1, "sphinx": 60}, "alltitles": {"Aggregation": [[0, "module-topomodelx.base.aggregation"]], "Conv": [[1, "module-topomodelx.base.conv"]], "Base": [[2, "base"]], "Message Passing": [[3, "module-topomodelx.base.message_passing"]], "API Reference": [[4, "api-reference"]], "Packages & Modules": [[4, null]], "CAN": [[5, "module-topomodelx.nn.cell.can"]], "Can_Layer": [[6, "module-topomodelx.nn.cell.can_layer"]], "CCXN": [[7, "module-topomodelx.nn.cell.ccxn"]], "CCXN_Layer": [[8, "module-topomodelx.nn.cell.ccxn_layer"]], "CWN": [[9, "module-topomodelx.nn.cell.cwn"]], "Cwn_Layer": [[10, "module-topomodelx.nn.cell.cwn_layer"]], "Cell": [[11, "cell"]], "AllSet": [[12, "module-topomodelx.nn.hypergraph.allset"]], "AllSet_Layer": [[13, "module-topomodelx.nn.hypergraph.allset_layer"]], "AllSet_Transformer": [[14, "module-topomodelx.nn.hypergraph.allset_transformer"]], "AllSet_Transformer_Layer": [[15, "module-topomodelx.nn.hypergraph.allset_transformer_layer"]], "DHGCN": [[16, "dhgcn"]], "DHGCN_Layer": [[17, "dhgcn-layer"]], "HMPNN": [[18, "module-topomodelx.nn.hypergraph.hmpnn"]], "HMPNN_Layer": [[19, "module-topomodelx.nn.hypergraph.hmpnn_layer"]], "HNHN": [[20, "module-topomodelx.nn.hypergraph.hnhn"]], "HNHN_Layer": [[21, "module-topomodelx.nn.hypergraph.hnhn_layer"]], "HNHN_Layer_Bis": [[22, "module-topomodelx.nn.hypergraph.hnhn_layer_bis"]], "Hypergat": [[23, "module-topomodelx.nn.hypergraph.hypergat"]], "Hypergat_Layer": [[24, "module-topomodelx.nn.hypergraph.hypergat_layer"]], "Hypersage": [[25, "module-topomodelx.nn.hypergraph.hypersage"]], "Hypersage_Layer": [[26, "module-topomodelx.nn.hypergraph.hypersage_layer"]], "Hypergraph": [[27, "hypergraph"]], "Unigcn": [[28, "module-topomodelx.nn.hypergraph.unigcn"]], "Unigcn_Layer": [[29, "module-topomodelx.nn.hypergraph.unigcn_layer"]], "Unigcnii": [[30, "module-topomodelx.nn.hypergraph.unigcnii"]], "Unigcnii_Layer": [[31, "module-topomodelx.nn.hypergraph.unigcnii_layer"]], "Neural Networks": [[36, "neural-networks"]], "Simplicial": [[41, "simplicial"]], "Utils": [[58, "utils"]], "ICML 2023 Topological Deep Learning Challenge": [[59, "icml-2023-topological-deep-learning-challenge"]], "Description of the Challenge": [[59, "description-of-the-challenge"]], "\u2b50\ufe0f Publication Outcomes for Participants \u2b50\ufe0f": [[59, "publication-outcomes-for-participants"]], "Deadline": [[59, "deadline"]], "How to Submit": [[59, "how-to-submit"]], "Guidelines": [[59, "guidelines"]], "Submission Requirements": [[59, "submission-requirements"]], "Evaluation": [[59, "evaluation"]], "Questions": [[59, "questions"]], "Contributing": [[60, "contributing"]], "Making Changes": [[60, "making-changes"]], "Write Tests": [[60, "write-tests"]], "Run Tests": [[60, "run-tests"]], "Write Documentation": [[60, "write-documentation"]], "Intro to Docstrings": [[60, "intro-to-docstrings"]], "The Anatomy of a Docstring": [[60, "the-anatomy-of-a-docstring"]], "Docstring Examples": [[60, "docstring-examples"]], "\ud83c\udf10 TopoModelX (TMX) \ud83c\udf69": [[61, "topomodelx-tmx"]], "\ud83d\udd0d References": [[61, "references"]], "\ud83e\uddbe Getting Started": [[61, "getting-started"]], "Train a Cell Attention Network (CAN)": [[62, "Train-a-Cell-Attention-Network-(CAN)"]], "Abstract:": [[62, "Abstract:"]], "The Neural Network:": [[62, "The-Neural-Network:"], [63, "The-Neural-Network:"], [64, "The-Neural-Network:"]], "The Task:": [[62, "The-Task:"], [63, "The-Task:"], [64, "The-Task:"], [79, "The-Task:"]], "Set-up": [[62, "Set-up"], [63, "Set-up"], [64, "Set-up"]], "Pre-processing": [[62, "Pre-processing"], [63, "Pre-processing"], [64, "Pre-processing"], [65, "Pre-processing"], [66, "Pre-processing"], [67, "Pre-processing"], [68, "Pre-processing"], [69, "Pre-processing"], [70, "Pre-processing"], [71, "Pre-processing"], [72, "Pre-processing"], [73, "Pre-processing"], [75, "Pre-processing"], [76, "Pre-processing"], [77, "Pre-processing"], [78, "Pre-processing"], [79, "Pre-processing"], [80, "Pre-processing"], [81, "Pre-processing"], [82, "Pre-processing"], [83, "Pre-processing"], [84, "Pre-processing"], [85, "Pre-processing"]], "Create the Neural Network": [[62, "Create-the-Neural-Network"], [63, "Create-the-Neural-Network"], [64, "Create-the-Neural-Network"], [65, "Create-the-Neural-Network"], [67, "Create-the-Neural-Network"], [69, "Create-the-Neural-Network"], [70, "Create-the-Neural-Network"], [73, "Create-the-Neural-Network"], [75, "Create-the-Neural-Network"], [76, "Create-the-Neural-Network"], [77, "Create-the-Neural-Network"], [78, "Create-the-Neural-Network"], [79, "Create-the-Neural-Network"], [81, "Create-the-Neural-Network"]], "Train the Neural Network": [[62, "Train-the-Neural-Network"], [63, "Train-the-Neural-Network"], [64, "Train-the-Neural-Network"], [65, "Train-the-Neural-Network"], [66, "Train-the-Neural-Network"], [67, "Train-the-Neural-Network"], [68, "Train-the-Neural-Network"], [69, "Train-the-Neural-Network"], [70, "Train-the-Neural-Network"], [71, "Train-the-Neural-Network"], [72, "Train-the-Neural-Network"], [73, "Train-the-Neural-Network"], [75, "Train-the-Neural-Network"], [76, "Train-the-Neural-Network"], [77, "Train-the-Neural-Network"], [78, "Train-the-Neural-Network"], [79, "Train-the-Neural-Network"], [81, "Train-the-Neural-Network"], [84, "Train-the-Neural-Network"], [85, "Train-the-Neural-Network"]], "Train a Convolutional Cell Complex Network (CCXN)": [[63, "Train-a-Convolutional-Cell-Complex-Network-(CCXN)"]], "Train the Neural Network with Attention": [[63, "Train-the-Neural-Network-with-Attention"]], "Train a CW Network (CWN)": [[64, "Train-a-CW-Network-(CWN)"]], "Train an All-Set TNN": [[65, "Train-an-All-Set-TNN"]], "Additional theoretical clarifications": [[65, "Additional-theoretical-clarifications"], [66, "Additional-theoretical-clarifications"], [72, "Additional-theoretical-clarifications"]], "Train an All-Set-Transformer TNN": [[66, "Train-an-All-Set-Transformer-TNN"]], "Define the Neural Network": [[66, "Define-the-Neural-Network"], [72, "Define-the-Neural-Network"]], "Train a DHGCN TNN": [[67, "Train-a-DHGCN-TNN"]], "Import data": [[67, "Import-data"], [71, "Import-data"], [73, "Import-data"], [75, "Import-data"], [76, "Import-data"], [80, "Import-data"]], "Define neighborhood structures and lift into hypergraph domain.": [[67, "Define-neighborhood-structures-and-lift-into-hypergraph-domain."], [71, "Define-neighborhood-structures-and-lift-into-hypergraph-domain."]], "Train a Hypergraph Message Passing Neural Network (HMPNN)": [[68, "Train-a-Hypergraph-Message-Passing-Neural-Network-(HMPNN)"]], "Train a Hypergraph Networks with Hyperedge Neurons (HNHN)": [[69, "Train-a-Hypergraph-Networks-with-Hyperedge-Neurons-(HNHN)"]], "Import dataset": [[69, "Import-dataset"], [77, "Import-dataset"], [78, "Import-dataset"], [81, "Import-dataset"], [83, "Import-dataset"], [84, "Import-dataset"]], "Define neighborhood structures.": [[69, "Define-neighborhood-structures."], [77, "Define-neighborhood-structures."], [78, "Define-neighborhood-structures."], [81, "Define-neighborhood-structures."], [84, "Define-neighborhood-structures."]], "Import signal": [[69, "Import-signal"], [77, "Import-signal"], [78, "Import-signal"], [81, "Import-signal"], [82, "Import-signal"]], "Train a Hypergraph Network with Hyperedge Neurons (HNHN)": [[70, "Train-a-Hypergraph-Network-with-Hyperedge-Neurons-(HNHN)"]], "Train a Hypergraph Neural Network": [[71, "Train-a-Hypergraph-Neural-Network"]], "Train a Hypersage TNN": [[72, "Train-a-Hypersage-TNN"]], "Train a UNIGCN TNN": [[73, "Train-a-UNIGCN-TNN"]], "Train a hypergraph neural network using UniGCNII layers": [[74, "Train-a-hypergraph-neural-network-using-UniGCNII-layers"]], "Loading the data": [[74, "Loading-the-data"]], "Creating a neural network": [[74, "Creating-a-neural-network"]], "Training the neural network": [[74, "Training-the-neural-network"]], "Train a UNIGIN TNN": [[75, "Train-a-UNIGIN-TNN"]], "Train a Uni-sage TNN": [[76, "Train-a-Uni-sage-TNN"]], "Train a Simplicial Neural Network for Homology Localization (Dist2Cycle)": [[77, "Train-a-Simplicial-Neural-Network-for-Homology-Localization-(Dist2Cycle)"]], "Define binary labels": [[77, "Define-binary-labels"], [78, "Define-binary-labels"], [81, "Define-binary-labels"], [82, "Define-binary-labels"]], "Create Features": [[77, "Create-Features"]], "Train a Simplicial High-Skip Network (HSN)": [[78, "Train-a-Simplicial-High-Skip-Network-(HSN)"]], "Train a Simplicial Attention Network (SAN)": [[79, "Train-a-Simplicial-Attention-Network-(SAN)"]], "Abstract": [[79, "Abstract"]], "The Neural Network": [[79, "The-Neural-Network"]], "Train a Simplicial Complex Autoencoder (SCA) with Coadjacency Message Passing Scheme (CMPS)": [[80, "Train-a-Simplicial-Complex-Autoencoder-(SCA)-with-Coadjacency-Message-Passing-Scheme-(CMPS)"]], "Preparing the inputs to test each message passing scheme:": [[80, "Preparing-the-inputs-to-test-each-message-passing-scheme:"]], "Coadjacency Message Passing Scheme (CMPS):": [[80, "Coadjacency-Message-Passing-Scheme-(CMPS):"]], "Create the Neural Networks": [[80, "Create-the-Neural-Networks"]], "Train and Test Split": [[80, "Train-and-Test-Split"]], "Training and Testing Model": [[80, "Training-and-Testing-Model"]], "Train a Simplicial Complex Convolutional Network (SCCN)": [[81, "Train-a-Simplicial-Complex-Convolutional-Network-(SCCN)"]], "Train a SCCNN": [[82, "Train-a-SCCNN"]], "We train the model to perform:": [[82, "We-train-the-model-to-perform:"], [85, "We-train-the-model-to-perform:"]], "Simplicial Complex Convolutional Neural Networks [SCCNN]": [[82, "Simplicial-Complex-Convolutional-Neural-Networks-[SCCNN]"]], "1. Complex Classification": [[82, "1.-Complex-Classification"], [85, "1.-Complex-Classification"]], "Import shrec dataset": [[82, "Import-shrec-dataset"]], "Define Neighborhood Strctures": [[82, "Define-Neighborhood-Strctures"], [82, "id1"], [85, "Define-Neighborhood-Strctures"], [85, "id1"]], "Create and Train the Neural Network": [[82, "Create-and-Train-the-Neural-Network"], [82, "id2"], [83, "Create-and-Train-the-Neural-Network"]], "2. Node Classification": [[82, "2.-Node-Classification"], [85, "2.-Node-Classification"]], "Train a Simplicial 2-complex convolutional neural network (SCConv)": [[83, "Train-a-Simplicial-2-complex-convolutional-neural-network-(SCConv)"]], "Helper functions": [[83, "Helper-functions"]], "Define Neighbourhood Structures": [[83, "Define-Neighbourhood-Structures"]], "prepare training and test data": [[83, "prepare-training-and-test-data"]], "Train a Simplex Convolutional Network (SCN) of Rank 2": [[84, "Train-a-Simplex-Convolutional-Network-(SCN)-of-Rank-2"]], "References": [[84, "References"]], "Train a Simplicial Convolutional Neural Network (SCNN)": [[85, "Train-a-Simplicial-Convolutional-Neural-Network-(SCNN)"]], "Simplicial Convolutional Neural Networks [SCNN]": [[85, "Simplicial-Convolutional-Neural-Networks-[SCNN]"]], "Import Karate dataset": [[85, "Import-Karate-dataset"]], "Weighted Hodge Laplacians": [[85, "Weighted-Hodge-Laplacians"]], "Import signals": [[85, "Import-signals"]], "Define binary labels and Prepare the training-testing split": [[85, "Define-binary-labels-and-Prepare-the-training-testing-split"]], "Create the SCNN for node classification": [[85, "Create-the-SCNN-for-node-classification"]], "Train the SCNN": [[85, "Train-the-SCNN"]], "Train a Simplicial Complex Net (SCoNe)": [[86, "Train-a-Simplicial-Complex-Net-(SCoNe)"]], "Table of contents": [[86, "Table-of-contents"]], "Dataset generation": [[86, "Dataset-generation"]], "Generating trajectories": [[86, "Generating-trajectories"]], "Creating PyTorch dataloaders": [[86, "Creating-PyTorch-dataloaders"]], "Creating the Neural Network": [[86, "Creating-the-Neural-Network"]], "Training the Neural Network": [[86, "Training-the-Neural-Network"]], "Evaluating the model on test data": [[86, "Evaluating-the-model-on-test-data"]], "Suggestions for further experimentation": [[86, "Suggestions-for-further-experimentation"]], "Tutorials": [[87, "tutorials"]], "Topological Neural Nets on Cellular Complexes": [[87, "topological-neural-nets-on-cellular-complexes"]], "Topological Neural Nets on Hypergraphs": [[87, "topological-neural-nets-on-hypergraphs"]], "Topological Neural Nets on Simplicial Complexes": [[87, "topological-neural-nets-on-simplicial-complexes"]]}, "indexentries": {"aggregation (class in topomodelx.base.aggregation)": [[0, "topomodelx.base.aggregation.Aggregation"]], "forward() (topomodelx.base.aggregation.aggregation method)": [[0, "topomodelx.base.aggregation.Aggregation.forward"]], "module": [[0, "module-topomodelx.base.aggregation"], [1, "module-topomodelx.base.conv"], [3, "module-topomodelx.base.message_passing"], [5, "module-topomodelx.nn.cell.can"], [6, "module-topomodelx.nn.cell.can_layer"], [7, "module-topomodelx.nn.cell.ccxn"], [8, "module-topomodelx.nn.cell.ccxn_layer"], [9, "module-topomodelx.nn.cell.cwn"], [10, "module-topomodelx.nn.cell.cwn_layer"], [12, "module-topomodelx.nn.hypergraph.allset"], [13, "module-topomodelx.nn.hypergraph.allset_layer"], [14, "module-topomodelx.nn.hypergraph.allset_transformer"], [15, "module-topomodelx.nn.hypergraph.allset_transformer_layer"], [18, "module-topomodelx.nn.hypergraph.hmpnn"], [19, "module-topomodelx.nn.hypergraph.hmpnn_layer"], [20, "module-topomodelx.nn.hypergraph.hnhn"], [21, "module-topomodelx.nn.hypergraph.hnhn_layer"], [22, "module-topomodelx.nn.hypergraph.hnhn_layer_bis"], [23, "module-topomodelx.nn.hypergraph.hypergat"], [24, "module-topomodelx.nn.hypergraph.hypergat_layer"], [25, "module-topomodelx.nn.hypergraph.hypersage"], [26, "module-topomodelx.nn.hypergraph.hypersage_layer"], [28, "module-topomodelx.nn.hypergraph.unigcn"], [29, "module-topomodelx.nn.hypergraph.unigcn_layer"], [30, "module-topomodelx.nn.hypergraph.unigcnii"], [31, "module-topomodelx.nn.hypergraph.unigcnii_layer"], [32, "module-topomodelx.nn.hypergraph.unigin"], [33, "module-topomodelx.nn.hypergraph.unigin_layer"], [34, "module-topomodelx.nn.hypergraph.unisage"], [35, "module-topomodelx.nn.hypergraph.unisage_layer"], [37, "module-topomodelx.nn.simplicial.dist2cycle"], [38, "module-topomodelx.nn.simplicial.dist2cycle_layer"], [39, "module-topomodelx.nn.simplicial.hsn"], [40, "module-topomodelx.nn.simplicial.hsn_layer"], [42, "module-topomodelx.nn.simplicial.san"], [43, "module-topomodelx.nn.simplicial.san_layer"], [44, "module-topomodelx.nn.simplicial.sca_cmps"], [45, "module-topomodelx.nn.simplicial.sca_cmps_layer"], [46, "module-topomodelx.nn.simplicial.sccn"], [47, "module-topomodelx.nn.simplicial.sccn_layer"], [48, "module-topomodelx.nn.simplicial.sccnn"], [49, "module-topomodelx.nn.simplicial.sccnn_layer"], [50, "module-topomodelx.nn.simplicial.scconv"], [51, "module-topomodelx.nn.simplicial.scconv_layer"], [52, "module-topomodelx.nn.simplicial.scn2"], [53, "module-topomodelx.nn.simplicial.scn2_layer"], [54, "module-topomodelx.nn.simplicial.scnn"], [55, "module-topomodelx.nn.simplicial.scnn_layer"], [56, "module-topomodelx.nn.simplicial.scone"], [57, "module-topomodelx.nn.simplicial.scone_layer"], [58, "module-topomodelx.utils.scatter"]], "topomodelx.base.aggregation": [[0, "module-topomodelx.base.aggregation"]], "update() (topomodelx.base.aggregation.aggregation method)": [[0, "topomodelx.base.aggregation.Aggregation.update"]], "conv (class in topomodelx.base.conv)": [[1, "topomodelx.base.conv.Conv"]], "forward() (topomodelx.base.conv.conv method)": [[1, "topomodelx.base.conv.Conv.forward"]], "topomodelx.base.conv": [[1, "module-topomodelx.base.conv"]], "update() (topomodelx.base.conv.conv method)": [[1, "topomodelx.base.conv.Conv.update"]], "messagepassing (class in topomodelx.base.message_passing)": [[3, "topomodelx.base.message_passing.MessagePassing"]], "aggregate() (topomodelx.base.message_passing.messagepassing method)": [[3, "topomodelx.base.message_passing.MessagePassing.aggregate"]], "attention() (topomodelx.base.message_passing.messagepassing method)": [[3, "topomodelx.base.message_passing.MessagePassing.attention"]], "forward() (topomodelx.base.message_passing.messagepassing method)": [[3, "topomodelx.base.message_passing.MessagePassing.forward"]], "message() (topomodelx.base.message_passing.messagepassing method)": [[3, "topomodelx.base.message_passing.MessagePassing.message"]], "reset_parameters() (topomodelx.base.message_passing.messagepassing method)": [[3, "topomodelx.base.message_passing.MessagePassing.reset_parameters"]], "topomodelx.base.message_passing": [[3, "module-topomodelx.base.message_passing"]], "can (class in topomodelx.nn.cell.can)": [[5, "topomodelx.nn.cell.can.CAN"]], "forward() (topomodelx.nn.cell.can.can method)": [[5, "topomodelx.nn.cell.can.CAN.forward"]], "topomodelx.nn.cell.can": [[5, "module-topomodelx.nn.cell.can"]], "canlayer (class in topomodelx.nn.cell.can_layer)": [[6, "topomodelx.nn.cell.can_layer.CANLayer"]], "liftlayer (class in topomodelx.nn.cell.can_layer)": [[6, "topomodelx.nn.cell.can_layer.LiftLayer"]], "multiheadcellattention (class in topomodelx.nn.cell.can_layer)": [[6, "topomodelx.nn.cell.can_layer.MultiHeadCellAttention"]], "multiheadcellattention_v2 (class in topomodelx.nn.cell.can_layer)": [[6, "topomodelx.nn.cell.can_layer.MultiHeadCellAttention_v2"]], "multiheadliftlayer (class in topomodelx.nn.cell.can_layer)": [[6, "topomodelx.nn.cell.can_layer.MultiHeadLiftLayer"]], "poollayer (class in topomodelx.nn.cell.can_layer)": [[6, "topomodelx.nn.cell.can_layer.PoolLayer"]], "add_self_loops() (in module topomodelx.nn.cell.can_layer)": [[6, "topomodelx.nn.cell.can_layer.add_self_loops"]], "attention() (topomodelx.nn.cell.can_layer.multiheadcellattention method)": [[6, "topomodelx.nn.cell.can_layer.MultiHeadCellAttention.attention"]], "attention() (topomodelx.nn.cell.can_layer.multiheadcellattention_v2 method)": [[6, "topomodelx.nn.cell.can_layer.MultiHeadCellAttention_v2.attention"]], "forward() (topomodelx.nn.cell.can_layer.canlayer method)": [[6, "topomodelx.nn.cell.can_layer.CANLayer.forward"]], "forward() (topomodelx.nn.cell.can_layer.liftlayer method)": [[6, "topomodelx.nn.cell.can_layer.LiftLayer.forward"]], "forward() (topomodelx.nn.cell.can_layer.multiheadcellattention method)": [[6, "topomodelx.nn.cell.can_layer.MultiHeadCellAttention.forward"]], "forward() (topomodelx.nn.cell.can_layer.multiheadcellattention_v2 method)": [[6, "topomodelx.nn.cell.can_layer.MultiHeadCellAttention_v2.forward"]], "forward() (topomodelx.nn.cell.can_layer.multiheadliftlayer method)": [[6, "topomodelx.nn.cell.can_layer.MultiHeadLiftLayer.forward"]], "forward() (topomodelx.nn.cell.can_layer.poollayer method)": [[6, "topomodelx.nn.cell.can_layer.PoolLayer.forward"]], "message() (topomodelx.nn.cell.can_layer.liftlayer method)": [[6, "topomodelx.nn.cell.can_layer.LiftLayer.message"]], "message() (topomodelx.nn.cell.can_layer.multiheadcellattention method)": [[6, "topomodelx.nn.cell.can_layer.MultiHeadCellAttention.message"]], "message() (topomodelx.nn.cell.can_layer.multiheadcellattention_v2 method)": [[6, "topomodelx.nn.cell.can_layer.MultiHeadCellAttention_v2.message"]], "reset_parameters() (topomodelx.nn.cell.can_layer.canlayer method)": [[6, "topomodelx.nn.cell.can_layer.CANLayer.reset_parameters"]], "reset_parameters() (topomodelx.nn.cell.can_layer.liftlayer method)": [[6, "topomodelx.nn.cell.can_layer.LiftLayer.reset_parameters"]], "reset_parameters() (topomodelx.nn.cell.can_layer.multiheadcellattention method)": [[6, "topomodelx.nn.cell.can_layer.MultiHeadCellAttention.reset_parameters"]], "reset_parameters() (topomodelx.nn.cell.can_layer.multiheadcellattention_v2 method)": [[6, "topomodelx.nn.cell.can_layer.MultiHeadCellAttention_v2.reset_parameters"]], "reset_parameters() (topomodelx.nn.cell.can_layer.multiheadliftlayer method)": [[6, "topomodelx.nn.cell.can_layer.MultiHeadLiftLayer.reset_parameters"]], "reset_parameters() (topomodelx.nn.cell.can_layer.poollayer method)": [[6, "topomodelx.nn.cell.can_layer.PoolLayer.reset_parameters"]], "softmax() (in module topomodelx.nn.cell.can_layer)": [[6, "topomodelx.nn.cell.can_layer.softmax"]], "topomodelx.nn.cell.can_layer": [[6, "module-topomodelx.nn.cell.can_layer"]], "ccxn (class in topomodelx.nn.cell.ccxn)": [[7, "topomodelx.nn.cell.ccxn.CCXN"]], "forward() (topomodelx.nn.cell.ccxn.ccxn method)": [[7, "topomodelx.nn.cell.ccxn.CCXN.forward"]], "topomodelx.nn.cell.ccxn": [[7, "module-topomodelx.nn.cell.ccxn"]], "ccxnlayer (class in topomodelx.nn.cell.ccxn_layer)": [[8, "topomodelx.nn.cell.ccxn_layer.CCXNLayer"]], "forward() (topomodelx.nn.cell.ccxn_layer.ccxnlayer method)": [[8, "topomodelx.nn.cell.ccxn_layer.CCXNLayer.forward"]], "topomodelx.nn.cell.ccxn_layer": [[8, "module-topomodelx.nn.cell.ccxn_layer"]], "cwn (class in topomodelx.nn.cell.cwn)": [[9, "topomodelx.nn.cell.cwn.CWN"]], "forward() (topomodelx.nn.cell.cwn.cwn method)": [[9, "topomodelx.nn.cell.cwn.CWN.forward"]], "topomodelx.nn.cell.cwn": [[9, "module-topomodelx.nn.cell.cwn"]], "cwnlayer (class in topomodelx.nn.cell.cwn_layer)": [[10, "topomodelx.nn.cell.cwn_layer.CWNLayer"]], "forward() (topomodelx.nn.cell.cwn_layer.cwnlayer method)": [[10, "topomodelx.nn.cell.cwn_layer.CWNLayer.forward"]], "topomodelx.nn.cell.cwn_layer": [[10, "module-topomodelx.nn.cell.cwn_layer"]], "allset (class in topomodelx.nn.hypergraph.allset)": [[12, "topomodelx.nn.hypergraph.allset.AllSet"]], "forward() (topomodelx.nn.hypergraph.allset.allset method)": [[12, "topomodelx.nn.hypergraph.allset.AllSet.forward"]], "topomodelx.nn.hypergraph.allset": [[12, "module-topomodelx.nn.hypergraph.allset"]], "allsetblock (class in topomodelx.nn.hypergraph.allset_layer)": [[13, "topomodelx.nn.hypergraph.allset_layer.AllSetBlock"]], "allsetlayer (class in topomodelx.nn.hypergraph.allset_layer)": [[13, "topomodelx.nn.hypergraph.allset_layer.AllSetLayer"]], "mlp (class in topomodelx.nn.hypergraph.allset_layer)": [[13, "topomodelx.nn.hypergraph.allset_layer.MLP"]], "forward() (topomodelx.nn.hypergraph.allset_layer.allsetblock method)": [[13, "topomodelx.nn.hypergraph.allset_layer.AllSetBlock.forward"]], "forward() (topomodelx.nn.hypergraph.allset_layer.allsetlayer method)": [[13, "topomodelx.nn.hypergraph.allset_layer.AllSetLayer.forward"]], "reset_parameters() (topomodelx.nn.hypergraph.allset_layer.allsetblock method)": [[13, "topomodelx.nn.hypergraph.allset_layer.AllSetBlock.reset_parameters"]], "reset_parameters() (topomodelx.nn.hypergraph.allset_layer.allsetlayer method)": [[13, "topomodelx.nn.hypergraph.allset_layer.AllSetLayer.reset_parameters"]], "topomodelx.nn.hypergraph.allset_layer": [[13, "module-topomodelx.nn.hypergraph.allset_layer"]], "allsettransformer (class in topomodelx.nn.hypergraph.allset_transformer)": [[14, "topomodelx.nn.hypergraph.allset_transformer.AllSetTransformer"]], "forward() (topomodelx.nn.hypergraph.allset_transformer.allsettransformer method)": [[14, "topomodelx.nn.hypergraph.allset_transformer.AllSetTransformer.forward"]], "topomodelx.nn.hypergraph.allset_transformer": [[14, "module-topomodelx.nn.hypergraph.allset_transformer"]], "allsettransformerblock (class in topomodelx.nn.hypergraph.allset_transformer_layer)": [[15, "topomodelx.nn.hypergraph.allset_transformer_layer.AllSetTransformerBlock"]], "allsettransformerlayer (class in topomodelx.nn.hypergraph.allset_transformer_layer)": [[15, "topomodelx.nn.hypergraph.allset_transformer_layer.AllSetTransformerLayer"]], "mlp (class in topomodelx.nn.hypergraph.allset_transformer_layer)": [[15, "topomodelx.nn.hypergraph.allset_transformer_layer.MLP"]], "multiheadattention (class in topomodelx.nn.hypergraph.allset_transformer_layer)": [[15, "topomodelx.nn.hypergraph.allset_transformer_layer.MultiHeadAttention"]], "attention() (topomodelx.nn.hypergraph.allset_transformer_layer.multiheadattention method)": [[15, "topomodelx.nn.hypergraph.allset_transformer_layer.MultiHeadAttention.attention"]], "forward() (topomodelx.nn.hypergraph.allset_transformer_layer.allsettransformerblock method)": [[15, "topomodelx.nn.hypergraph.allset_transformer_layer.AllSetTransformerBlock.forward"]], "forward() (topomodelx.nn.hypergraph.allset_transformer_layer.allsettransformerlayer method)": [[15, "topomodelx.nn.hypergraph.allset_transformer_layer.AllSetTransformerLayer.forward"]], "forward() (topomodelx.nn.hypergraph.allset_transformer_layer.multiheadattention method)": [[15, "topomodelx.nn.hypergraph.allset_transformer_layer.MultiHeadAttention.forward"]], "reset_parameters() (topomodelx.nn.hypergraph.allset_transformer_layer.allsettransformerblock method)": [[15, "topomodelx.nn.hypergraph.allset_transformer_layer.AllSetTransformerBlock.reset_parameters"]], "reset_parameters() (topomodelx.nn.hypergraph.allset_transformer_layer.allsettransformerlayer method)": [[15, "topomodelx.nn.hypergraph.allset_transformer_layer.AllSetTransformerLayer.reset_parameters"]], "reset_parameters() (topomodelx.nn.hypergraph.allset_transformer_layer.multiheadattention method)": [[15, "topomodelx.nn.hypergraph.allset_transformer_layer.MultiHeadAttention.reset_parameters"]], "topomodelx.nn.hypergraph.allset_transformer_layer": [[15, "module-topomodelx.nn.hypergraph.allset_transformer_layer"]], "hmpnn (class in topomodelx.nn.hypergraph.hmpnn)": [[18, "topomodelx.nn.hypergraph.hmpnn.HMPNN"]], "forward() (topomodelx.nn.hypergraph.hmpnn.hmpnn method)": [[18, "topomodelx.nn.hypergraph.hmpnn.HMPNN.forward"]], "topomodelx.nn.hypergraph.hmpnn": [[18, "module-topomodelx.nn.hypergraph.hmpnn"]], "hmpnnlayer (class in topomodelx.nn.hypergraph.hmpnn_layer)": [[19, "topomodelx.nn.hypergraph.hmpnn_layer.HMPNNLayer"]], "apply_regular_dropout() (topomodelx.nn.hypergraph.hmpnn_layer.hmpnnlayer method)": [[19, "topomodelx.nn.hypergraph.hmpnn_layer.HMPNNLayer.apply_regular_dropout"]], "forward() (topomodelx.nn.hypergraph.hmpnn_layer.hmpnnlayer method)": [[19, "topomodelx.nn.hypergraph.hmpnn_layer.HMPNNLayer.forward"]], "topomodelx.nn.hypergraph.hmpnn_layer": [[19, "module-topomodelx.nn.hypergraph.hmpnn_layer"]], "hnhn (class in topomodelx.nn.hypergraph.hnhn)": [[20, "topomodelx.nn.hypergraph.hnhn.HNHN"]], "hnhnnetwork (class in topomodelx.nn.hypergraph.hnhn)": [[20, "topomodelx.nn.hypergraph.hnhn.HNHNNetwork"]], "forward() (topomodelx.nn.hypergraph.hnhn.hnhn method)": [[20, "topomodelx.nn.hypergraph.hnhn.HNHN.forward"]], "forward() (topomodelx.nn.hypergraph.hnhn.hnhnnetwork method)": [[20, "topomodelx.nn.hypergraph.hnhn.HNHNNetwork.forward"]], "topomodelx.nn.hypergraph.hnhn": [[20, "module-topomodelx.nn.hypergraph.hnhn"]], "hnhnlayer (class in topomodelx.nn.hypergraph.hnhn_layer)": [[21, "topomodelx.nn.hypergraph.hnhn_layer.HNHNLayer"]], "compute_normalization_matrices() (topomodelx.nn.hypergraph.hnhn_layer.hnhnlayer method)": [[21, "topomodelx.nn.hypergraph.hnhn_layer.HNHNLayer.compute_normalization_matrices"]], "forward() (topomodelx.nn.hypergraph.hnhn_layer.hnhnlayer method)": [[21, "topomodelx.nn.hypergraph.hnhn_layer.HNHNLayer.forward"]], "init_biases() (topomodelx.nn.hypergraph.hnhn_layer.hnhnlayer method)": [[21, "topomodelx.nn.hypergraph.hnhn_layer.HNHNLayer.init_biases"]], "normalize_incidence_matrices() (topomodelx.nn.hypergraph.hnhn_layer.hnhnlayer method)": [[21, "topomodelx.nn.hypergraph.hnhn_layer.HNHNLayer.normalize_incidence_matrices"]], "reset_parameters() (topomodelx.nn.hypergraph.hnhn_layer.hnhnlayer method)": [[21, "topomodelx.nn.hypergraph.hnhn_layer.HNHNLayer.reset_parameters"]], "topomodelx.nn.hypergraph.hnhn_layer": [[21, "module-topomodelx.nn.hypergraph.hnhn_layer"]], "hnhnlayer (class in topomodelx.nn.hypergraph.hnhn_layer_bis)": [[22, "topomodelx.nn.hypergraph.hnhn_layer_bis.HNHNLayer"]], "forward() (topomodelx.nn.hypergraph.hnhn_layer_bis.hnhnlayer method)": [[22, "topomodelx.nn.hypergraph.hnhn_layer_bis.HNHNLayer.forward"]], "topomodelx.nn.hypergraph.hnhn_layer_bis": [[22, "module-topomodelx.nn.hypergraph.hnhn_layer_bis"]], "hypergat (class in topomodelx.nn.hypergraph.hypergat)": [[23, "topomodelx.nn.hypergraph.hypergat.HyperGAT"]], "forward() (topomodelx.nn.hypergraph.hypergat.hypergat method)": [[23, "topomodelx.nn.hypergraph.hypergat.HyperGAT.forward"]], "topomodelx.nn.hypergraph.hypergat": [[23, "module-topomodelx.nn.hypergraph.hypergat"]], "hypergatlayer (class in topomodelx.nn.hypergraph.hypergat_layer)": [[24, "topomodelx.nn.hypergraph.hypergat_layer.HyperGATLayer"]], "attention() (topomodelx.nn.hypergraph.hypergat_layer.hypergatlayer method)": [[24, "topomodelx.nn.hypergraph.hypergat_layer.HyperGATLayer.attention"]], "forward() (topomodelx.nn.hypergraph.hypergat_layer.hypergatlayer method)": [[24, "topomodelx.nn.hypergraph.hypergat_layer.HyperGATLayer.forward"]], "reset_parameters() (topomodelx.nn.hypergraph.hypergat_layer.hypergatlayer method)": [[24, "topomodelx.nn.hypergraph.hypergat_layer.HyperGATLayer.reset_parameters"]], "topomodelx.nn.hypergraph.hypergat_layer": [[24, "module-topomodelx.nn.hypergraph.hypergat_layer"]], "update() (topomodelx.nn.hypergraph.hypergat_layer.hypergatlayer method)": [[24, "topomodelx.nn.hypergraph.hypergat_layer.HyperGATLayer.update"]], "hypersage (class in topomodelx.nn.hypergraph.hypersage)": [[25, "topomodelx.nn.hypergraph.hypersage.HyperSAGE"]], "forward() (topomodelx.nn.hypergraph.hypersage.hypersage method)": [[25, "topomodelx.nn.hypergraph.hypersage.HyperSAGE.forward"]], "topomodelx.nn.hypergraph.hypersage": [[25, "module-topomodelx.nn.hypergraph.hypersage"]], "generalizedmean (class in topomodelx.nn.hypergraph.hypersage_layer)": [[26, "topomodelx.nn.hypergraph.hypersage_layer.GeneralizedMean"]], "hypersagelayer (class in topomodelx.nn.hypergraph.hypersage_layer)": [[26, "topomodelx.nn.hypergraph.hypersage_layer.HyperSAGELayer"]], "aggregate() (topomodelx.nn.hypergraph.hypersage_layer.hypersagelayer method)": [[26, "topomodelx.nn.hypergraph.hypersage_layer.HyperSAGELayer.aggregate"]], "forward() (topomodelx.nn.hypergraph.hypersage_layer.generalizedmean method)": [[26, "topomodelx.nn.hypergraph.hypersage_layer.GeneralizedMean.forward"]], "forward() (topomodelx.nn.hypergraph.hypersage_layer.hypersagelayer method)": [[26, "topomodelx.nn.hypergraph.hypersage_layer.HyperSAGELayer.forward"]], "topomodelx.nn.hypergraph.hypersage_layer": [[26, "module-topomodelx.nn.hypergraph.hypersage_layer"]], "update() (topomodelx.nn.hypergraph.hypersage_layer.hypersagelayer method)": [[26, "topomodelx.nn.hypergraph.hypersage_layer.HyperSAGELayer.update"]], "unigcn (class in topomodelx.nn.hypergraph.unigcn)": [[28, "topomodelx.nn.hypergraph.unigcn.UniGCN"]], "forward() (topomodelx.nn.hypergraph.unigcn.unigcn method)": [[28, "topomodelx.nn.hypergraph.unigcn.UniGCN.forward"]], "topomodelx.nn.hypergraph.unigcn": [[28, "module-topomodelx.nn.hypergraph.unigcn"]], "unigcnlayer (class in topomodelx.nn.hypergraph.unigcn_layer)": [[29, "topomodelx.nn.hypergraph.unigcn_layer.UniGCNLayer"]], "forward() (topomodelx.nn.hypergraph.unigcn_layer.unigcnlayer method)": [[29, "topomodelx.nn.hypergraph.unigcn_layer.UniGCNLayer.forward"]], "reset_parameters() (topomodelx.nn.hypergraph.unigcn_layer.unigcnlayer method)": [[29, "topomodelx.nn.hypergraph.unigcn_layer.UniGCNLayer.reset_parameters"]], "topomodelx.nn.hypergraph.unigcn_layer": [[29, "module-topomodelx.nn.hypergraph.unigcn_layer"]], "unigcnii (class in topomodelx.nn.hypergraph.unigcnii)": [[30, "topomodelx.nn.hypergraph.unigcnii.UniGCNII"]], "forward() (topomodelx.nn.hypergraph.unigcnii.unigcnii method)": [[30, "topomodelx.nn.hypergraph.unigcnii.UniGCNII.forward"]], "topomodelx.nn.hypergraph.unigcnii": [[30, "module-topomodelx.nn.hypergraph.unigcnii"]], "unigcniilayer (class in topomodelx.nn.hypergraph.unigcnii_layer)": [[31, "topomodelx.nn.hypergraph.unigcnii_layer.UniGCNIILayer"]], "forward() (topomodelx.nn.hypergraph.unigcnii_layer.unigcniilayer method)": [[31, "topomodelx.nn.hypergraph.unigcnii_layer.UniGCNIILayer.forward"]], "reset_parameters() (topomodelx.nn.hypergraph.unigcnii_layer.unigcniilayer method)": [[31, "topomodelx.nn.hypergraph.unigcnii_layer.UniGCNIILayer.reset_parameters"]], "topomodelx.nn.hypergraph.unigcnii_layer": [[31, "module-topomodelx.nn.hypergraph.unigcnii_layer"]], "unigin (class in topomodelx.nn.hypergraph.unigin)": [[32, "topomodelx.nn.hypergraph.unigin.UniGIN"]], "forward() (topomodelx.nn.hypergraph.unigin.unigin method)": [[32, "topomodelx.nn.hypergraph.unigin.UniGIN.forward"]], "topomodelx.nn.hypergraph.unigin": [[32, "module-topomodelx.nn.hypergraph.unigin"]], "uniginlayer (class in topomodelx.nn.hypergraph.unigin_layer)": [[33, "topomodelx.nn.hypergraph.unigin_layer.UniGINLayer"]], "forward() (topomodelx.nn.hypergraph.unigin_layer.uniginlayer method)": [[33, "topomodelx.nn.hypergraph.unigin_layer.UniGINLayer.forward"]], "topomodelx.nn.hypergraph.unigin_layer": [[33, "module-topomodelx.nn.hypergraph.unigin_layer"]], "unisage (class in topomodelx.nn.hypergraph.unisage)": [[34, "topomodelx.nn.hypergraph.unisage.UniSAGE"]], "forward() (topomodelx.nn.hypergraph.unisage.unisage method)": [[34, "topomodelx.nn.hypergraph.unisage.UniSAGE.forward"]], "topomodelx.nn.hypergraph.unisage": [[34, "module-topomodelx.nn.hypergraph.unisage"]], "unisagelayer (class in topomodelx.nn.hypergraph.unisage_layer)": [[35, "topomodelx.nn.hypergraph.unisage_layer.UniSAGELayer"]], "forward() (topomodelx.nn.hypergraph.unisage_layer.unisagelayer method)": [[35, "topomodelx.nn.hypergraph.unisage_layer.UniSAGELayer.forward"]], "reset_parameters() (topomodelx.nn.hypergraph.unisage_layer.unisagelayer method)": [[35, "topomodelx.nn.hypergraph.unisage_layer.UniSAGELayer.reset_parameters"]], "topomodelx.nn.hypergraph.unisage_layer": [[35, "module-topomodelx.nn.hypergraph.unisage_layer"]], "dist2cycle (class in topomodelx.nn.simplicial.dist2cycle)": [[37, "topomodelx.nn.simplicial.dist2cycle.Dist2Cycle"]], "forward() (topomodelx.nn.simplicial.dist2cycle.dist2cycle method)": [[37, "topomodelx.nn.simplicial.dist2cycle.Dist2Cycle.forward"]], "topomodelx.nn.simplicial.dist2cycle": [[37, "module-topomodelx.nn.simplicial.dist2cycle"]], "dist2cyclelayer (class in topomodelx.nn.simplicial.dist2cycle_layer)": [[38, "topomodelx.nn.simplicial.dist2cycle_layer.Dist2CycleLayer"]], "forward() (topomodelx.nn.simplicial.dist2cycle_layer.dist2cyclelayer method)": [[38, "topomodelx.nn.simplicial.dist2cycle_layer.Dist2CycleLayer.forward"]], "reset_parameters() (topomodelx.nn.simplicial.dist2cycle_layer.dist2cyclelayer method)": [[38, "topomodelx.nn.simplicial.dist2cycle_layer.Dist2CycleLayer.reset_parameters"]], "topomodelx.nn.simplicial.dist2cycle_layer": [[38, "module-topomodelx.nn.simplicial.dist2cycle_layer"]], "hsn (class in topomodelx.nn.simplicial.hsn)": [[39, "topomodelx.nn.simplicial.hsn.HSN"]], "forward() (topomodelx.nn.simplicial.hsn.hsn method)": [[39, "topomodelx.nn.simplicial.hsn.HSN.forward"]], "topomodelx.nn.simplicial.hsn": [[39, "module-topomodelx.nn.simplicial.hsn"]], "hsnlayer (class in topomodelx.nn.simplicial.hsn_layer)": [[40, "topomodelx.nn.simplicial.hsn_layer.HSNLayer"]], "forward() (topomodelx.nn.simplicial.hsn_layer.hsnlayer method)": [[40, "topomodelx.nn.simplicial.hsn_layer.HSNLayer.forward"]], "reset_parameters() (topomodelx.nn.simplicial.hsn_layer.hsnlayer method)": [[40, "topomodelx.nn.simplicial.hsn_layer.HSNLayer.reset_parameters"]], "topomodelx.nn.simplicial.hsn_layer": [[40, "module-topomodelx.nn.simplicial.hsn_layer"]], "san (class in topomodelx.nn.simplicial.san)": [[42, "topomodelx.nn.simplicial.san.SAN"]], "compute_projection_matrix() (topomodelx.nn.simplicial.san.san method)": [[42, "topomodelx.nn.simplicial.san.SAN.compute_projection_matrix"]], "forward() (topomodelx.nn.simplicial.san.san method)": [[42, "topomodelx.nn.simplicial.san.SAN.forward"]], "topomodelx.nn.simplicial.san": [[42, "module-topomodelx.nn.simplicial.san"]], "sanconv (class in topomodelx.nn.simplicial.san_layer)": [[43, "topomodelx.nn.simplicial.san_layer.SANConv"]], "sanlayer (class in topomodelx.nn.simplicial.san_layer)": [[43, "topomodelx.nn.simplicial.san_layer.SANLayer"]], "forward() (topomodelx.nn.simplicial.san_layer.sanconv method)": [[43, "topomodelx.nn.simplicial.san_layer.SANConv.forward"]], "forward() (topomodelx.nn.simplicial.san_layer.sanlayer method)": [[43, "topomodelx.nn.simplicial.san_layer.SANLayer.forward"]], "reset_parameters() (topomodelx.nn.simplicial.san_layer.sanlayer method)": [[43, "topomodelx.nn.simplicial.san_layer.SANLayer.reset_parameters"]], "topomodelx.nn.simplicial.san_layer": [[43, "module-topomodelx.nn.simplicial.san_layer"]], "scacmps (class in topomodelx.nn.simplicial.sca_cmps)": [[44, "topomodelx.nn.simplicial.sca_cmps.SCACMPS"]], "forward() (topomodelx.nn.simplicial.sca_cmps.scacmps method)": [[44, "topomodelx.nn.simplicial.sca_cmps.SCACMPS.forward"]], "topomodelx.nn.simplicial.sca_cmps": [[44, "module-topomodelx.nn.simplicial.sca_cmps"]], "scacmpslayer (class in topomodelx.nn.simplicial.sca_cmps_layer)": [[45, "topomodelx.nn.simplicial.sca_cmps_layer.SCACMPSLayer"]], "forward() (topomodelx.nn.simplicial.sca_cmps_layer.scacmpslayer method)": [[45, "topomodelx.nn.simplicial.sca_cmps_layer.SCACMPSLayer.forward"]], "intra_aggr() (topomodelx.nn.simplicial.sca_cmps_layer.scacmpslayer method)": [[45, "topomodelx.nn.simplicial.sca_cmps_layer.SCACMPSLayer.intra_aggr"]], "reset_parameters() (topomodelx.nn.simplicial.sca_cmps_layer.scacmpslayer method)": [[45, "topomodelx.nn.simplicial.sca_cmps_layer.SCACMPSLayer.reset_parameters"]], "topomodelx.nn.simplicial.sca_cmps_layer": [[45, "module-topomodelx.nn.simplicial.sca_cmps_layer"]], "weight_func() (topomodelx.nn.simplicial.sca_cmps_layer.scacmpslayer method)": [[45, "topomodelx.nn.simplicial.sca_cmps_layer.SCACMPSLayer.weight_func"]], "sccn (class in topomodelx.nn.simplicial.sccn)": [[46, "topomodelx.nn.simplicial.sccn.SCCN"]], "forward() (topomodelx.nn.simplicial.sccn.sccn method)": [[46, "topomodelx.nn.simplicial.sccn.SCCN.forward"]], "topomodelx.nn.simplicial.sccn": [[46, "module-topomodelx.nn.simplicial.sccn"]], "sccnlayer (class in topomodelx.nn.simplicial.sccn_layer)": [[47, "topomodelx.nn.simplicial.sccn_layer.SCCNLayer"]], "forward() (topomodelx.nn.simplicial.sccn_layer.sccnlayer method)": [[47, "topomodelx.nn.simplicial.sccn_layer.SCCNLayer.forward"]], "reset_parameters() (topomodelx.nn.simplicial.sccn_layer.sccnlayer method)": [[47, "topomodelx.nn.simplicial.sccn_layer.SCCNLayer.reset_parameters"]], "topomodelx.nn.simplicial.sccn_layer": [[47, "module-topomodelx.nn.simplicial.sccn_layer"]], "sccnn (class in topomodelx.nn.simplicial.sccnn)": [[48, "topomodelx.nn.simplicial.sccnn.SCCNN"]], "sccnncomplex (class in topomodelx.nn.simplicial.sccnn)": [[48, "topomodelx.nn.simplicial.sccnn.SCCNNComplex"]], "forward() (topomodelx.nn.simplicial.sccnn.sccnn method)": [[48, "topomodelx.nn.simplicial.sccnn.SCCNN.forward"]], "forward() (topomodelx.nn.simplicial.sccnn.sccnncomplex method)": [[48, "topomodelx.nn.simplicial.sccnn.SCCNNComplex.forward"]], "topomodelx.nn.simplicial.sccnn": [[48, "module-topomodelx.nn.simplicial.sccnn"]], "sccnnlayer (class in topomodelx.nn.simplicial.sccnn_layer)": [[49, "topomodelx.nn.simplicial.sccnn_layer.SCCNNLayer"]], "aggr_norm_func() (topomodelx.nn.simplicial.sccnn_layer.sccnnlayer method)": [[49, "topomodelx.nn.simplicial.sccnn_layer.SCCNNLayer.aggr_norm_func"]], "chebyshev_conv() (topomodelx.nn.simplicial.sccnn_layer.sccnnlayer method)": [[49, "topomodelx.nn.simplicial.sccnn_layer.SCCNNLayer.chebyshev_conv"]], "forward() (topomodelx.nn.simplicial.sccnn_layer.sccnnlayer method)": [[49, "topomodelx.nn.simplicial.sccnn_layer.SCCNNLayer.forward"]], "reset_parameters() (topomodelx.nn.simplicial.sccnn_layer.sccnnlayer method)": [[49, "topomodelx.nn.simplicial.sccnn_layer.SCCNNLayer.reset_parameters"]], "topomodelx.nn.simplicial.sccnn_layer": [[49, "module-topomodelx.nn.simplicial.sccnn_layer"]], "update() (topomodelx.nn.simplicial.sccnn_layer.sccnnlayer method)": [[49, "topomodelx.nn.simplicial.sccnn_layer.SCCNNLayer.update"]], "scconv (class in topomodelx.nn.simplicial.scconv)": [[50, "topomodelx.nn.simplicial.scconv.SCConv"]], "forward() (topomodelx.nn.simplicial.scconv.scconv method)": [[50, "topomodelx.nn.simplicial.scconv.SCConv.forward"]], "topomodelx.nn.simplicial.scconv": [[50, "module-topomodelx.nn.simplicial.scconv"]], "scconvlayer (class in topomodelx.nn.simplicial.scconv_layer)": [[51, "topomodelx.nn.simplicial.scconv_layer.SCConvLayer"]], "forward() (topomodelx.nn.simplicial.scconv_layer.scconvlayer method)": [[51, "topomodelx.nn.simplicial.scconv_layer.SCConvLayer.forward"]], "reset_parameters() (topomodelx.nn.simplicial.scconv_layer.scconvlayer method)": [[51, "topomodelx.nn.simplicial.scconv_layer.SCConvLayer.reset_parameters"]], "topomodelx.nn.simplicial.scconv_layer": [[51, "module-topomodelx.nn.simplicial.scconv_layer"]], "scn2 (class in topomodelx.nn.simplicial.scn2)": [[52, "topomodelx.nn.simplicial.scn2.SCN2"]], "forward() (topomodelx.nn.simplicial.scn2.scn2 method)": [[52, "topomodelx.nn.simplicial.scn2.SCN2.forward"]], "topomodelx.nn.simplicial.scn2": [[52, "module-topomodelx.nn.simplicial.scn2"]], "scn2layer (class in topomodelx.nn.simplicial.scn2_layer)": [[53, "topomodelx.nn.simplicial.scn2_layer.SCN2Layer"]], "forward() (topomodelx.nn.simplicial.scn2_layer.scn2layer method)": [[53, "topomodelx.nn.simplicial.scn2_layer.SCN2Layer.forward"]], "reset_parameters() (topomodelx.nn.simplicial.scn2_layer.scn2layer method)": [[53, "topomodelx.nn.simplicial.scn2_layer.SCN2Layer.reset_parameters"]], "topomodelx.nn.simplicial.scn2_layer": [[53, "module-topomodelx.nn.simplicial.scn2_layer"]], "scnn (class in topomodelx.nn.simplicial.scnn)": [[54, "topomodelx.nn.simplicial.scnn.SCNN"]], "forward() (topomodelx.nn.simplicial.scnn.scnn method)": [[54, "topomodelx.nn.simplicial.scnn.SCNN.forward"]], "topomodelx.nn.simplicial.scnn": [[54, "module-topomodelx.nn.simplicial.scnn"]], "scnnlayer (class in topomodelx.nn.simplicial.scnn_layer)": [[55, "topomodelx.nn.simplicial.scnn_layer.SCNNLayer"]], "aggr_norm_func() (topomodelx.nn.simplicial.scnn_layer.scnnlayer method)": [[55, "topomodelx.nn.simplicial.scnn_layer.SCNNLayer.aggr_norm_func"]], "chebyshev_conv() (topomodelx.nn.simplicial.scnn_layer.scnnlayer method)": [[55, "topomodelx.nn.simplicial.scnn_layer.SCNNLayer.chebyshev_conv"]], "forward() (topomodelx.nn.simplicial.scnn_layer.scnnlayer method)": [[55, "topomodelx.nn.simplicial.scnn_layer.SCNNLayer.forward"]], "reset_parameters() (topomodelx.nn.simplicial.scnn_layer.scnnlayer method)": [[55, "topomodelx.nn.simplicial.scnn_layer.SCNNLayer.reset_parameters"]], "topomodelx.nn.simplicial.scnn_layer": [[55, "module-topomodelx.nn.simplicial.scnn_layer"]], "update() (topomodelx.nn.simplicial.scnn_layer.scnnlayer method)": [[55, "topomodelx.nn.simplicial.scnn_layer.SCNNLayer.update"]], "scone (class in topomodelx.nn.simplicial.scone)": [[56, "topomodelx.nn.simplicial.scone.SCoNe"]], "trajectoriesdataset (class in topomodelx.nn.simplicial.scone)": [[56, "topomodelx.nn.simplicial.scone.TrajectoriesDataset"]], "forward() (topomodelx.nn.simplicial.scone.scone method)": [[56, "topomodelx.nn.simplicial.scone.SCoNe.forward"]], "generate_complex() (in module topomodelx.nn.simplicial.scone)": [[56, "topomodelx.nn.simplicial.scone.generate_complex"]], "generate_trajectories() (in module topomodelx.nn.simplicial.scone)": [[56, "topomodelx.nn.simplicial.scone.generate_trajectories"]], "topomodelx.nn.simplicial.scone": [[56, "module-topomodelx.nn.simplicial.scone"]], "vectorize_path() (topomodelx.nn.simplicial.scone.trajectoriesdataset method)": [[56, "topomodelx.nn.simplicial.scone.TrajectoriesDataset.vectorize_path"]], "sconelayer (class in topomodelx.nn.simplicial.scone_layer)": [[57, "topomodelx.nn.simplicial.scone_layer.SCoNeLayer"]], "forward() (topomodelx.nn.simplicial.scone_layer.sconelayer method)": [[57, "topomodelx.nn.simplicial.scone_layer.SCoNeLayer.forward"]], "reset_parameters() (topomodelx.nn.simplicial.scone_layer.sconelayer method)": [[57, "topomodelx.nn.simplicial.scone_layer.SCoNeLayer.reset_parameters"]], "topomodelx.nn.simplicial.scone_layer": [[57, "module-topomodelx.nn.simplicial.scone_layer"]], "broadcast() (in module topomodelx.utils.scatter)": [[58, "topomodelx.utils.scatter.broadcast"]], "scatter() (in module topomodelx.utils.scatter)": [[58, "topomodelx.utils.scatter.scatter"]], "scatter_add() (in module topomodelx.utils.scatter)": [[58, "topomodelx.utils.scatter.scatter_add"]], "scatter_mean() (in module topomodelx.utils.scatter)": [[58, "topomodelx.utils.scatter.scatter_mean"]], "scatter_sum() (in module topomodelx.utils.scatter)": [[58, "topomodelx.utils.scatter.scatter_sum"]], "topomodelx.utils.scatter": [[58, "module-topomodelx.utils.scatter"]]}})